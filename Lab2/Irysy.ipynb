{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "seed = np.random.seed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\wwojt\\OneDrive\\Programowanie\\ProgramowanieSieciowe\\Lab2\\iris.data\", header=None,\n",
    "                 names = ['Sepal length', 'Sepal width',\n",
    "                             'Petal length', 'Petal width', 'species',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   Sepal length  Sepal width  Petal length  Petal width      species\n0           5.1          3.5           1.4          0.2  Iris-setosa\n1           4.9          3.0           1.4          0.2  Iris-setosa\n2           4.7          3.2           1.3          0.2  Iris-setosa\n3           4.6          3.1           1.5          0.2  Iris-setosa\n4           5.0          3.6           1.4          0.2  Iris-setosa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sepal length</th>\n      <th>Sepal width</th>\n      <th>Petal length</th>\n      <th>Petal width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(df.head())\n",
    "X = df[['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.factorize(df['species'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "enc.fit(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1c28bb7f3f11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_enc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_enc = y_train.reshape(-1,1)\n",
    "enc.fit(y_train_enc)\n",
    "y_enc_train = enc.transform(y_train_enc).toarray()\n",
    "\n",
    "y_test_enc = y_test.reshape(-1,1)\n",
    "enc.fit(y_test_enc)\n",
    "y_enc_test = enc.transform(y_test_enc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6 2.5 3.9 1.1]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.6 3.2 1.4 0.2]] [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, yy_hot_full, test_size = 0.33, random_state = 0)\n",
    "print(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, \n",
    "                    alpha=1e-4, solver = 'sgd', verbose=10, \n",
    "                    random_state=1, learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.out_activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.64060248\n",
      "Iteration 2, loss = 5.21503975\n",
      "Iteration 3, loss = 2.64371970\n",
      "Iteration 4, loss = 1.93836652\n",
      "Iteration 5, loss = 1.50892858\n",
      "Iteration 6, loss = 1.16262119\n",
      "Iteration 7, loss = 1.04551836\n",
      "Iteration 8, loss = 0.95732274\n",
      "Iteration 9, loss = 0.89380179\n",
      "Iteration 10, loss = 0.84625862\n",
      "Iteration 11, loss = 0.81003101\n",
      "Iteration 12, loss = 0.78047684\n",
      "Iteration 13, loss = 0.75265841\n",
      "Iteration 14, loss = 0.71947347\n",
      "Iteration 15, loss = 0.68351513\n",
      "Iteration 16, loss = 0.64446696\n",
      "Iteration 17, loss = 0.60357669\n",
      "Iteration 18, loss = 0.56188702\n",
      "Iteration 19, loss = 0.53178837\n",
      "Iteration 20, loss = 0.64500942\n",
      "Iteration 21, loss = 1.80933448\n",
      "Iteration 22, loss = 0.74890202\n",
      "Iteration 23, loss = 0.68340590\n",
      "Iteration 24, loss = 0.73076369\n",
      "Iteration 25, loss = 0.90073366\n",
      "Iteration 26, loss = 1.03991484\n",
      "Iteration 27, loss = 1.29443877\n",
      "Iteration 28, loss = 0.79520230\n",
      "Iteration 29, loss = 0.61818007\n",
      "Iteration 30, loss = 0.40084797\n",
      "Iteration 31, loss = 0.36884651\n",
      "Iteration 32, loss = 0.35157137\n",
      "Iteration 33, loss = 0.34076521\n",
      "Iteration 34, loss = 0.44197450\n",
      "Iteration 35, loss = 1.69864863\n",
      "Iteration 36, loss = 3.43582090\n",
      "Iteration 37, loss = 0.72555419\n",
      "Iteration 38, loss = 0.70750306\n",
      "Iteration 39, loss = 0.59808414\n",
      "Iteration 40, loss = 0.53733477\n",
      "Iteration 41, loss = 0.51908099\n",
      "Iteration 42, loss = 0.50324627\n",
      "Iteration 43, loss = 0.48141266\n",
      "Iteration 44, loss = 0.45430663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=0.1, max_iter=1000,\n",
       "              random_state=1, solver='sgd', verbose=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.960000\n",
      "Test set score: 0.960000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate_init':(0.1, 0.01, 0.001), 'hidden_layer_sizes':(3, 5 , 10, 15), 'solver': ('adam', 'lfbs', 'sgd')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(mlp, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33048782\n",
      "Iteration 2, loss = 1.27643600\n",
      "Iteration 3, loss = 1.23382556\n",
      "Iteration 4, loss = 1.19594317\n",
      "Iteration 5, loss = 1.16353582\n",
      "Iteration 6, loss = 1.13729966\n",
      "Iteration 7, loss = 1.11777687\n",
      "Iteration 8, loss = 1.10520620\n",
      "Iteration 9, loss = 1.09934512\n",
      "Iteration 10, loss = 1.10051779\n",
      "Iteration 11, loss = 1.10110023\n",
      "Iteration 12, loss = 1.09456001\n",
      "Iteration 13, loss = 1.07128096\n",
      "Iteration 14, loss = 1.04668739\n",
      "Iteration 15, loss = 0.99496381\n",
      "Iteration 16, loss = 0.94988189\n",
      "Iteration 17, loss = 0.90708737\n",
      "Iteration 18, loss = 0.86736443\n",
      "Iteration 19, loss = 0.82876807\n",
      "Iteration 20, loss = 0.79532049\n",
      "Iteration 21, loss = 0.76407219\n",
      "Iteration 22, loss = 0.73232343\n",
      "Iteration 23, loss = 0.70135658\n",
      "Iteration 24, loss = 0.67315722\n",
      "Iteration 25, loss = 0.64558621\n",
      "Iteration 26, loss = 0.62003442\n",
      "Iteration 27, loss = 0.59584421\n",
      "Iteration 28, loss = 0.57340755\n",
      "Iteration 29, loss = 0.55242561\n",
      "Iteration 30, loss = 0.53261864\n",
      "Iteration 31, loss = 0.51401539\n",
      "Iteration 32, loss = 0.49599257\n",
      "Iteration 33, loss = 0.47887978\n",
      "Iteration 34, loss = 0.46178541\n",
      "Iteration 35, loss = 0.44582609\n",
      "Iteration 36, loss = 0.43168334\n",
      "Iteration 37, loss = 0.41966234\n",
      "Iteration 38, loss = 0.40706342\n",
      "Iteration 39, loss = 0.39419368\n",
      "Iteration 40, loss = 0.38024007\n",
      "Iteration 41, loss = 0.36726683\n",
      "Iteration 42, loss = 0.35569204\n",
      "Iteration 43, loss = 0.34489071\n",
      "Iteration 44, loss = 0.33497198\n",
      "Iteration 45, loss = 0.32447112\n",
      "Iteration 46, loss = 0.31455435\n",
      "Iteration 47, loss = 0.30452553\n",
      "Iteration 48, loss = 0.29583439\n",
      "Iteration 49, loss = 0.28787881\n",
      "Iteration 50, loss = 0.28070849\n",
      "Iteration 51, loss = 0.27351158\n",
      "Iteration 52, loss = 0.26646256\n",
      "Iteration 53, loss = 0.25987222\n",
      "Iteration 54, loss = 0.25374353\n",
      "Iteration 55, loss = 0.24843075\n",
      "Iteration 56, loss = 0.24332217\n",
      "Iteration 57, loss = 0.23856842\n",
      "Iteration 58, loss = 0.23378543\n",
      "Iteration 59, loss = 0.22927823\n",
      "Iteration 60, loss = 0.22506379\n",
      "Iteration 61, loss = 0.22112421\n",
      "Iteration 62, loss = 0.21749863\n",
      "Iteration 63, loss = 0.21385876\n",
      "Iteration 64, loss = 0.21034144\n",
      "Iteration 65, loss = 0.20687102\n",
      "Iteration 66, loss = 0.20355498\n",
      "Iteration 67, loss = 0.20048588\n",
      "Iteration 68, loss = 0.19749604\n",
      "Iteration 69, loss = 0.19463550\n",
      "Iteration 70, loss = 0.19179966\n",
      "Iteration 71, loss = 0.18903390\n",
      "Iteration 72, loss = 0.18642092\n",
      "Iteration 73, loss = 0.18391392\n",
      "Iteration 74, loss = 0.18154620\n",
      "Iteration 75, loss = 0.17924225\n",
      "Iteration 76, loss = 0.17697835\n",
      "Iteration 77, loss = 0.17481613\n",
      "Iteration 78, loss = 0.17273560\n",
      "Iteration 79, loss = 0.17076302\n",
      "Iteration 80, loss = 0.16887461\n",
      "Iteration 81, loss = 0.16702143\n",
      "Iteration 82, loss = 0.16523026\n",
      "Iteration 83, loss = 0.16350101\n",
      "Iteration 84, loss = 0.16183872\n",
      "Iteration 85, loss = 0.16025229\n",
      "Iteration 86, loss = 0.15870485\n",
      "Iteration 87, loss = 0.15719260\n",
      "Iteration 88, loss = 0.15572895\n",
      "Iteration 89, loss = 0.15431004\n",
      "Iteration 90, loss = 0.15294199\n",
      "Iteration 91, loss = 0.15161659\n",
      "Iteration 92, loss = 0.15031598\n",
      "Iteration 93, loss = 0.14904812\n",
      "Iteration 94, loss = 0.14781804\n",
      "Iteration 95, loss = 0.14662206\n",
      "Iteration 96, loss = 0.14546074\n",
      "Iteration 97, loss = 0.14432644\n",
      "Iteration 98, loss = 0.14321366\n",
      "Iteration 99, loss = 0.14212979\n",
      "Iteration 100, loss = 0.14107617\n",
      "Iteration 101, loss = 0.14004752\n",
      "Iteration 102, loss = 0.13904245\n",
      "Iteration 103, loss = 0.13805903\n",
      "Iteration 104, loss = 0.13709608\n",
      "Iteration 105, loss = 0.13615692\n",
      "Iteration 106, loss = 0.13524097\n",
      "Iteration 107, loss = 0.13434360\n",
      "Iteration 108, loss = 0.13346400\n",
      "Iteration 109, loss = 0.13260332\n",
      "Iteration 110, loss = 0.13176110\n",
      "Iteration 111, loss = 0.13093697\n",
      "Iteration 112, loss = 0.13013017\n",
      "Iteration 113, loss = 0.12933865\n",
      "Iteration 114, loss = 0.12856192\n",
      "Iteration 115, loss = 0.12780108\n",
      "Iteration 116, loss = 0.12705577\n",
      "Iteration 117, loss = 0.12632435\n",
      "Iteration 118, loss = 0.12560611\n",
      "Iteration 119, loss = 0.12490109\n",
      "Iteration 120, loss = 0.12420904\n",
      "Iteration 121, loss = 0.12352966\n",
      "Iteration 122, loss = 0.12286260\n",
      "Iteration 123, loss = 0.12220708\n",
      "Iteration 124, loss = 0.12156255\n",
      "Iteration 125, loss = 0.12092912\n",
      "Iteration 126, loss = 0.12030681\n",
      "Iteration 127, loss = 0.11969497\n",
      "Iteration 128, loss = 0.11909301\n",
      "Iteration 129, loss = 0.11850077\n",
      "Iteration 130, loss = 0.11791826\n",
      "Iteration 131, loss = 0.11734521\n",
      "Iteration 132, loss = 0.11678124\n",
      "Iteration 133, loss = 0.11622607\n",
      "Iteration 134, loss = 0.11567950\n",
      "Iteration 135, loss = 0.11514135\n",
      "Iteration 136, loss = 0.11461147\n",
      "Iteration 137, loss = 0.11408964\n",
      "Iteration 138, loss = 0.11357562\n",
      "Iteration 139, loss = 0.11306916\n",
      "Iteration 140, loss = 0.11257014\n",
      "Iteration 141, loss = 0.11207846\n",
      "Iteration 142, loss = 0.11159393\n",
      "Iteration 143, loss = 0.11111632\n",
      "Iteration 144, loss = 0.11064545\n",
      "Iteration 145, loss = 0.11018123\n",
      "Iteration 146, loss = 0.10972354\n",
      "Iteration 147, loss = 0.10927222\n",
      "Iteration 148, loss = 0.10882708\n",
      "Iteration 149, loss = 0.10838799\n",
      "Iteration 150, loss = 0.10795485\n",
      "Iteration 151, loss = 0.10752756\n",
      "Iteration 152, loss = 0.10710598\n",
      "Iteration 153, loss = 0.10668995\n",
      "Iteration 154, loss = 0.10627937\n",
      "Iteration 155, loss = 0.10587415\n",
      "Iteration 156, loss = 0.10547419\n",
      "Iteration 157, loss = 0.10507937\n",
      "Iteration 158, loss = 0.10468957\n",
      "Iteration 159, loss = 0.10430469\n",
      "Iteration 160, loss = 0.10392466\n",
      "Iteration 161, loss = 0.10354937\n",
      "Iteration 162, loss = 0.10317874\n",
      "Iteration 163, loss = 0.10281266\n",
      "Iteration 164, loss = 0.10245105\n",
      "Iteration 165, loss = 0.10209383\n",
      "Iteration 166, loss = 0.10174093\n",
      "Iteration 167, loss = 0.10139224\n",
      "Iteration 168, loss = 0.10104771\n",
      "Iteration 169, loss = 0.10070724\n",
      "Iteration 170, loss = 0.10037077\n",
      "Iteration 171, loss = 0.10003822\n",
      "Iteration 172, loss = 0.09970953\n",
      "Iteration 173, loss = 0.09938462\n",
      "Iteration 174, loss = 0.09906343\n",
      "Iteration 175, loss = 0.09874589\n",
      "Iteration 176, loss = 0.09843194\n",
      "Iteration 177, loss = 0.09812151\n",
      "Iteration 178, loss = 0.09781455\n",
      "Iteration 179, loss = 0.09751098\n",
      "Iteration 180, loss = 0.09721077\n",
      "Iteration 181, loss = 0.09691385\n",
      "Iteration 182, loss = 0.09662016\n",
      "Iteration 183, loss = 0.09632965\n",
      "Iteration 184, loss = 0.09604226\n",
      "Iteration 185, loss = 0.09575795\n",
      "Iteration 186, loss = 0.09547666\n",
      "Iteration 187, loss = 0.09519835\n",
      "Iteration 188, loss = 0.09492296\n",
      "Iteration 189, loss = 0.09465046\n",
      "Iteration 190, loss = 0.09438078\n",
      "Iteration 191, loss = 0.09411389\n",
      "Iteration 192, loss = 0.09384975\n",
      "Iteration 193, loss = 0.09358830\n",
      "Iteration 194, loss = 0.09332951\n",
      "Iteration 195, loss = 0.09307334\n",
      "Iteration 196, loss = 0.09281974\n",
      "Iteration 197, loss = 0.09256868\n",
      "Iteration 198, loss = 0.09232011\n",
      "Iteration 199, loss = 0.09207400\n",
      "Iteration 200, loss = 0.09183031\n",
      "Iteration 201, loss = 0.09158901\n",
      "Iteration 202, loss = 0.09135005\n",
      "Iteration 203, loss = 0.09111341\n",
      "Iteration 204, loss = 0.09087905\n",
      "Iteration 205, loss = 0.09064693\n",
      "Iteration 206, loss = 0.09041703\n",
      "Iteration 207, loss = 0.09018931\n",
      "Iteration 208, loss = 0.08996374\n",
      "Iteration 209, loss = 0.08974029\n",
      "Iteration 210, loss = 0.08951893\n",
      "Iteration 211, loss = 0.08929965\n",
      "Iteration 212, loss = 0.08908243\n",
      "Iteration 213, loss = 0.08886727\n",
      "Iteration 214, loss = 0.08865423\n",
      "Iteration 215, loss = 0.08844345\n",
      "Iteration 216, loss = 0.08823536\n",
      "Iteration 217, loss = 0.08803086\n",
      "Iteration 218, loss = 0.08783242\n",
      "Iteration 219, loss = 0.08764543\n",
      "Iteration 220, loss = 0.08748517\n",
      "Iteration 221, loss = 0.08738212\n",
      "Iteration 222, loss = 0.08743439\n",
      "Iteration 223, loss = 0.08776646\n",
      "Iteration 224, loss = 0.08888598\n",
      "Iteration 225, loss = 0.09015823\n",
      "Iteration 226, loss = 0.09202076\n",
      "Iteration 227, loss = 0.08884491\n",
      "Iteration 228, loss = 0.08608302\n",
      "Iteration 229, loss = 0.08650916\n",
      "Iteration 230, loss = 0.08794256\n",
      "Iteration 231, loss = 0.08738814\n",
      "Iteration 232, loss = 0.08521809\n",
      "Iteration 233, loss = 0.08593050\n",
      "Iteration 234, loss = 0.08715004\n",
      "Iteration 235, loss = 0.08524339\n",
      "Iteration 236, loss = 0.08461535\n",
      "Iteration 237, loss = 0.08567390\n",
      "Iteration 238, loss = 0.08486084\n",
      "Iteration 239, loss = 0.08394578\n",
      "Iteration 240, loss = 0.08443302\n",
      "Iteration 241, loss = 0.08427189\n",
      "Iteration 242, loss = 0.08349362\n",
      "Iteration 243, loss = 0.08350792\n",
      "Iteration 244, loss = 0.08364883\n",
      "Iteration 245, loss = 0.08314703\n",
      "Iteration 246, loss = 0.08283354\n",
      "Iteration 247, loss = 0.08299878\n",
      "Iteration 248, loss = 0.08276819\n",
      "Iteration 249, loss = 0.08233456\n",
      "Iteration 250, loss = 0.08238475\n",
      "Iteration 251, loss = 0.08232038\n",
      "Iteration 252, loss = 0.08190426\n",
      "Iteration 253, loss = 0.08181664\n",
      "Iteration 254, loss = 0.08182338\n",
      "Iteration 255, loss = 0.08150044\n",
      "Iteration 256, loss = 0.08130293\n",
      "Iteration 257, loss = 0.08129768\n",
      "Iteration 258, loss = 0.08109133\n",
      "Iteration 259, loss = 0.08084942\n",
      "Iteration 260, loss = 0.08078353\n",
      "Iteration 261, loss = 0.08066423\n",
      "Iteration 262, loss = 0.08044000\n",
      "Iteration 263, loss = 0.08030829\n",
      "Iteration 264, loss = 0.08022414\n",
      "Iteration 265, loss = 0.08004915\n",
      "Iteration 266, loss = 0.07987596\n",
      "Iteration 267, loss = 0.07978187\n",
      "Iteration 268, loss = 0.07965569\n",
      "Iteration 269, loss = 0.07947842\n",
      "Iteration 270, loss = 0.07935272\n",
      "Iteration 271, loss = 0.07924954\n",
      "Iteration 272, loss = 0.07909731\n",
      "Iteration 273, loss = 0.07894941\n",
      "Iteration 274, loss = 0.07884072\n",
      "Iteration 275, loss = 0.07871692\n",
      "Iteration 276, loss = 0.07856981\n",
      "Iteration 277, loss = 0.07844387\n",
      "Iteration 278, loss = 0.07833287\n",
      "Iteration 279, loss = 0.07820272\n",
      "Iteration 280, loss = 0.07806723\n",
      "Iteration 281, loss = 0.07795109\n",
      "Iteration 282, loss = 0.07783605\n",
      "Iteration 283, loss = 0.07770664\n",
      "Iteration 284, loss = 0.07758190\n",
      "Iteration 285, loss = 0.07746922\n",
      "Iteration 286, loss = 0.07735191\n",
      "Iteration 287, loss = 0.07722767\n",
      "Iteration 288, loss = 0.07710992\n",
      "Iteration 289, loss = 0.07699842\n",
      "Iteration 290, loss = 0.07688236\n",
      "Iteration 291, loss = 0.07676330\n",
      "Iteration 292, loss = 0.07664992\n",
      "Iteration 293, loss = 0.07653976\n",
      "Iteration 294, loss = 0.07642611\n",
      "Iteration 295, loss = 0.07631167\n",
      "Iteration 296, loss = 0.07620145\n",
      "Iteration 297, loss = 0.07609300\n",
      "Iteration 298, loss = 0.07598245\n",
      "Iteration 299, loss = 0.07587183\n",
      "Iteration 300, loss = 0.07576424\n",
      "Iteration 301, loss = 0.07565796\n",
      "Iteration 302, loss = 0.07555049\n",
      "Iteration 303, loss = 0.07544317\n",
      "Iteration 304, loss = 0.07533801\n",
      "Iteration 305, loss = 0.07523403\n",
      "Iteration 306, loss = 0.07512957\n",
      "Iteration 307, loss = 0.07502523\n",
      "Iteration 308, loss = 0.07492244\n",
      "Iteration 309, loss = 0.07482080\n",
      "Iteration 310, loss = 0.07471914\n",
      "Iteration 311, loss = 0.07461762\n",
      "Iteration 312, loss = 0.07451717\n",
      "Iteration 313, loss = 0.07441781\n",
      "Iteration 314, loss = 0.07431879\n",
      "Iteration 315, loss = 0.07421994\n",
      "Iteration 316, loss = 0.07412182\n",
      "Iteration 317, loss = 0.07402467\n",
      "Iteration 318, loss = 0.07392811\n",
      "Iteration 319, loss = 0.07383182\n",
      "Iteration 320, loss = 0.07373604\n",
      "Iteration 321, loss = 0.07364105\n",
      "Iteration 322, loss = 0.07354677\n",
      "Iteration 323, loss = 0.07345291\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33104251\n",
      "Iteration 2, loss = 1.27643600\n",
      "Iteration 3, loss = 1.23382942\n",
      "Iteration 4, loss = 1.19595151\n",
      "Iteration 5, loss = 1.16354786\n",
      "Iteration 6, loss = 1.13731361\n",
      "Iteration 7, loss = 1.11779018\n",
      "Iteration 8, loss = 1.10521592\n",
      "Iteration 9, loss = 1.09934851\n",
      "Iteration 10, loss = 1.09997600\n",
      "Iteration 11, loss = 1.09511696\n",
      "Iteration 12, loss = 1.08281664\n",
      "Iteration 13, loss = 1.05168183\n",
      "Iteration 14, loss = 1.03310910\n",
      "Iteration 15, loss = 0.96973755\n",
      "Iteration 16, loss = 0.97147919\n",
      "Iteration 17, loss = 0.89934099\n",
      "Iteration 18, loss = 0.85162541\n",
      "Iteration 19, loss = 0.79291760\n",
      "Iteration 20, loss = 0.70633083\n",
      "Iteration 21, loss = 0.61086828\n",
      "Iteration 22, loss = 0.53597817\n",
      "Iteration 23, loss = 0.46238810\n",
      "Iteration 24, loss = 0.40102139\n",
      "Iteration 25, loss = 0.35212790\n",
      "Iteration 26, loss = 0.30686263\n",
      "Iteration 27, loss = 0.26253032\n",
      "Iteration 28, loss = 0.21988236\n",
      "Iteration 29, loss = 0.18144039\n",
      "Iteration 30, loss = 0.15088935\n",
      "Iteration 31, loss = 0.15271249\n",
      "Iteration 32, loss = 0.19743025\n",
      "Iteration 33, loss = 0.11265812\n",
      "Iteration 34, loss = 0.15916904\n",
      "Iteration 35, loss = 0.11151370\n",
      "Iteration 36, loss = 0.14092802\n",
      "Iteration 37, loss = 0.09437215\n",
      "Iteration 38, loss = 0.13460947\n",
      "Iteration 39, loss = 0.08810619\n",
      "Iteration 40, loss = 0.11855924\n",
      "Iteration 41, loss = 0.09193579\n",
      "Iteration 42, loss = 0.09749998\n",
      "Iteration 43, loss = 0.09691199\n",
      "Iteration 44, loss = 0.08270345\n",
      "Iteration 45, loss = 0.09765034\n",
      "Iteration 46, loss = 0.07823454\n",
      "Iteration 47, loss = 0.08873232\n",
      "Iteration 48, loss = 0.08152575\n",
      "Iteration 49, loss = 0.07813939\n",
      "Iteration 50, loss = 0.08459059\n",
      "Iteration 51, loss = 0.07312427\n",
      "Iteration 52, loss = 0.08094726\n",
      "Iteration 53, loss = 0.07449689\n",
      "Iteration 54, loss = 0.07446316\n",
      "Iteration 55, loss = 0.07689802\n",
      "Iteration 56, loss = 0.07042240\n",
      "Iteration 57, loss = 0.07554802\n",
      "Iteration 58, loss = 0.07054702\n",
      "Iteration 59, loss = 0.07179241\n",
      "Iteration 60, loss = 0.07201014\n",
      "Iteration 61, loss = 0.06875544\n",
      "Iteration 62, loss = 0.07172472\n",
      "Iteration 63, loss = 0.06812730\n",
      "Iteration 64, loss = 0.06975288\n",
      "Iteration 65, loss = 0.06873608\n",
      "Iteration 66, loss = 0.06764141\n",
      "Iteration 67, loss = 0.06883055\n",
      "Iteration 68, loss = 0.06665237\n",
      "Iteration 69, loss = 0.06799916\n",
      "Iteration 70, loss = 0.06654904\n",
      "Iteration 71, loss = 0.06673916\n",
      "Iteration 72, loss = 0.06659177\n",
      "Iteration 73, loss = 0.06574205\n",
      "Iteration 74, loss = 0.06635595\n",
      "Iteration 75, loss = 0.06519056\n",
      "Iteration 76, loss = 0.06576109\n",
      "Iteration 77, loss = 0.06498451\n",
      "Iteration 78, loss = 0.06502784\n",
      "Iteration 79, loss = 0.06486759\n",
      "Iteration 80, loss = 0.06439318\n",
      "Iteration 81, loss = 0.06461536\n",
      "Iteration 82, loss = 0.06398110\n",
      "Iteration 83, loss = 0.06421865\n",
      "Iteration 84, loss = 0.06371839\n",
      "Iteration 85, loss = 0.06376697\n",
      "Iteration 86, loss = 0.06350400\n",
      "Iteration 87, loss = 0.06334028\n",
      "Iteration 88, loss = 0.06328169\n",
      "Iteration 89, loss = 0.06297537\n",
      "Iteration 90, loss = 0.06301523\n",
      "Iteration 91, loss = 0.06268592\n",
      "Iteration 92, loss = 0.06271386\n",
      "Iteration 93, loss = 0.06244253\n",
      "Iteration 94, loss = 0.06240715\n",
      "Iteration 95, loss = 0.06221694\n",
      "Iteration 96, loss = 0.06211260\n",
      "Iteration 97, loss = 0.06199617\n",
      "Iteration 98, loss = 0.06183933\n",
      "Iteration 99, loss = 0.06176914\n",
      "Iteration 100, loss = 0.06159159\n",
      "Iteration 101, loss = 0.06153726\n",
      "Iteration 102, loss = 0.06136206\n",
      "Iteration 103, loss = 0.06130560\n",
      "Iteration 104, loss = 0.06114625\n",
      "Iteration 105, loss = 0.06107687\n",
      "Iteration 106, loss = 0.06093940\n",
      "Iteration 107, loss = 0.06085518\n",
      "Iteration 108, loss = 0.06073738\n",
      "Iteration 109, loss = 0.06064157\n",
      "Iteration 110, loss = 0.06053956\n",
      "Iteration 111, loss = 0.06043600\n",
      "Iteration 112, loss = 0.06034484\n",
      "Iteration 113, loss = 0.06023832\n",
      "Iteration 114, loss = 0.06015359\n",
      "Iteration 115, loss = 0.06004719\n",
      "Iteration 116, loss = 0.05996612\n",
      "Iteration 117, loss = 0.05986209\n",
      "Iteration 118, loss = 0.05978258\n",
      "Iteration 119, loss = 0.05968207\n",
      "Iteration 120, loss = 0.05960333\n",
      "Iteration 121, loss = 0.05950670\n",
      "Iteration 122, loss = 0.05942825\n",
      "Iteration 123, loss = 0.05933556\n",
      "Iteration 124, loss = 0.05925748\n",
      "Iteration 125, loss = 0.05916834\n",
      "Iteration 126, loss = 0.05909074\n",
      "Iteration 127, loss = 0.05900486\n",
      "Iteration 128, loss = 0.05892798\n",
      "Iteration 129, loss = 0.05884494\n",
      "Iteration 130, loss = 0.05876897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33079619\n",
      "Iteration 2, loss = 1.27643600\n",
      "Iteration 3, loss = 1.23382480\n",
      "Iteration 4, loss = 1.19594151\n",
      "Iteration 5, loss = 1.16353342\n",
      "Iteration 6, loss = 1.13729689\n",
      "Iteration 7, loss = 1.11777422\n",
      "Iteration 8, loss = 1.10520427\n",
      "Iteration 9, loss = 1.09934445\n",
      "Iteration 10, loss = 1.10047426\n",
      "Iteration 11, loss = 1.10360320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 1.11019223\n",
      "Iteration 13, loss = 1.11703088\n",
      "Iteration 14, loss = 1.12244079\n",
      "Iteration 15, loss = 1.12589508\n",
      "Iteration 16, loss = 1.12573731\n",
      "Iteration 17, loss = 1.12364713\n",
      "Iteration 18, loss = 1.11981188\n",
      "Iteration 19, loss = 1.11503444\n",
      "Iteration 20, loss = 1.11010250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32935999\n",
      "Iteration 2, loss = 1.27643600\n",
      "Iteration 3, loss = 1.23383210\n",
      "Iteration 4, loss = 1.19595731\n",
      "Iteration 5, loss = 1.16355620\n",
      "Iteration 6, loss = 1.13732326\n",
      "Iteration 7, loss = 1.11779936\n",
      "Iteration 8, loss = 1.10522259\n",
      "Iteration 9, loss = 1.09935079\n",
      "Iteration 10, loss = 1.10006254\n",
      "Iteration 11, loss = 1.10087167\n",
      "Iteration 12, loss = 1.09330638\n",
      "Iteration 13, loss = 1.07037867\n",
      "Iteration 14, loss = 1.03650898\n",
      "Iteration 15, loss = 0.99745034\n",
      "Iteration 16, loss = 0.94477956\n",
      "Iteration 17, loss = 0.90540511\n",
      "Iteration 18, loss = 0.86664520\n",
      "Iteration 19, loss = 0.82745948\n",
      "Iteration 20, loss = 0.79197633\n",
      "Iteration 21, loss = 0.76260704\n",
      "Iteration 22, loss = 0.73144957\n",
      "Iteration 23, loss = 0.70016377\n",
      "Iteration 24, loss = 0.67132016\n",
      "Iteration 25, loss = 0.64420746\n",
      "Iteration 26, loss = 0.61849366\n",
      "Iteration 27, loss = 0.59449950\n",
      "Iteration 28, loss = 0.57180705\n",
      "Iteration 29, loss = 0.55071522\n",
      "Iteration 30, loss = 0.53053714\n",
      "Iteration 31, loss = 0.51155462\n",
      "Iteration 32, loss = 0.49295961\n",
      "Iteration 33, loss = 0.47480673\n",
      "Iteration 34, loss = 0.45716984\n",
      "Iteration 35, loss = 0.44097240\n",
      "Iteration 36, loss = 0.42700288\n",
      "Iteration 37, loss = 0.41348692\n",
      "Iteration 38, loss = 0.40045565\n",
      "Iteration 39, loss = 0.38604491\n",
      "Iteration 40, loss = 0.37135658\n",
      "Iteration 41, loss = 0.35798800\n",
      "Iteration 42, loss = 0.34573148\n",
      "Iteration 43, loss = 0.33493995\n",
      "Iteration 44, loss = 0.32387734\n",
      "Iteration 45, loss = 0.31307677\n",
      "Iteration 46, loss = 0.30205699\n",
      "Iteration 47, loss = 0.29203811\n",
      "Iteration 48, loss = 0.28306207\n",
      "Iteration 49, loss = 0.27512756\n",
      "Iteration 50, loss = 0.26738985\n",
      "Iteration 51, loss = 0.25976852\n",
      "Iteration 52, loss = 0.25239130\n",
      "Iteration 53, loss = 0.24551276\n",
      "Iteration 54, loss = 0.23948248\n",
      "Iteration 55, loss = 0.23389828\n",
      "Iteration 56, loss = 0.22869034\n",
      "Iteration 57, loss = 0.22348106\n",
      "Iteration 58, loss = 0.21847869\n",
      "Iteration 59, loss = 0.21375581\n",
      "Iteration 60, loss = 0.20938837\n",
      "Iteration 61, loss = 0.20535441\n",
      "Iteration 62, loss = 0.20137754\n",
      "Iteration 63, loss = 0.19748843\n",
      "Iteration 64, loss = 0.19362235\n",
      "Iteration 65, loss = 0.18994160\n",
      "Iteration 66, loss = 0.18650612\n",
      "Iteration 67, loss = 0.18320658\n",
      "Iteration 68, loss = 0.18003160\n",
      "Iteration 69, loss = 0.17686912\n",
      "Iteration 70, loss = 0.17380472\n",
      "Iteration 71, loss = 0.17088589\n",
      "Iteration 72, loss = 0.16812020\n",
      "Iteration 73, loss = 0.16550012\n",
      "Iteration 74, loss = 0.16293118\n",
      "Iteration 75, loss = 0.16042983\n",
      "Iteration 76, loss = 0.15802804\n",
      "Iteration 77, loss = 0.15573853\n",
      "Iteration 78, loss = 0.15357665\n",
      "Iteration 79, loss = 0.15148056\n",
      "Iteration 80, loss = 0.14943742\n",
      "Iteration 81, loss = 0.14746015\n",
      "Iteration 82, loss = 0.14555838\n",
      "Iteration 83, loss = 0.14375027\n",
      "Iteration 84, loss = 0.14200402\n",
      "Iteration 85, loss = 0.14029776\n",
      "Iteration 86, loss = 0.13863929\n",
      "Iteration 87, loss = 0.13703203\n",
      "Iteration 88, loss = 0.13548851\n",
      "Iteration 89, loss = 0.13399723\n",
      "Iteration 90, loss = 0.13253795\n",
      "Iteration 91, loss = 0.13111434\n",
      "Iteration 92, loss = 0.12973050\n",
      "Iteration 93, loss = 0.12839037\n",
      "Iteration 94, loss = 0.12709271\n",
      "Iteration 95, loss = 0.12582362\n",
      "Iteration 96, loss = 0.12458137\n",
      "Iteration 97, loss = 0.12337200\n",
      "Iteration 98, loss = 0.12219598\n",
      "Iteration 99, loss = 0.12105230\n",
      "Iteration 100, loss = 0.11993537\n",
      "Iteration 101, loss = 0.11884067\n",
      "Iteration 102, loss = 0.11777226\n",
      "Iteration 103, loss = 0.11673172\n",
      "Iteration 104, loss = 0.11571628\n",
      "Iteration 105, loss = 0.11472358\n",
      "Iteration 106, loss = 0.11375103\n",
      "Iteration 107, loss = 0.11279912\n",
      "Iteration 108, loss = 0.11187004\n",
      "Iteration 109, loss = 0.11096205\n",
      "Iteration 110, loss = 0.11007242\n",
      "Iteration 111, loss = 0.10920054\n",
      "Iteration 112, loss = 0.10834629\n",
      "Iteration 113, loss = 0.10751004\n",
      "Iteration 114, loss = 0.10669152\n",
      "Iteration 115, loss = 0.10588871\n",
      "Iteration 116, loss = 0.10510054\n",
      "Iteration 117, loss = 0.10432752\n",
      "Iteration 118, loss = 0.10356960\n",
      "Iteration 119, loss = 0.10282607\n",
      "Iteration 120, loss = 0.10209613\n",
      "Iteration 121, loss = 0.10137897\n",
      "Iteration 122, loss = 0.10067446\n",
      "Iteration 123, loss = 0.09998282\n",
      "Iteration 124, loss = 0.09930350\n",
      "Iteration 125, loss = 0.09863564\n",
      "Iteration 126, loss = 0.09797894\n",
      "Iteration 127, loss = 0.09733330\n",
      "Iteration 128, loss = 0.09669855\n",
      "Iteration 129, loss = 0.09607439\n",
      "Iteration 130, loss = 0.09546035\n",
      "Iteration 131, loss = 0.09485599\n",
      "Iteration 132, loss = 0.09426127\n",
      "Iteration 133, loss = 0.09367611\n",
      "Iteration 134, loss = 0.09310014\n",
      "Iteration 135, loss = 0.09253299\n",
      "Iteration 136, loss = 0.09197446\n",
      "Iteration 137, loss = 0.09142443\n",
      "Iteration 138, loss = 0.09088273\n",
      "Iteration 139, loss = 0.09034914\n",
      "Iteration 140, loss = 0.08982339\n",
      "Iteration 141, loss = 0.08930527\n",
      "Iteration 142, loss = 0.08879468\n",
      "Iteration 143, loss = 0.08829149\n",
      "Iteration 144, loss = 0.08779548\n",
      "Iteration 145, loss = 0.08730643\n",
      "Iteration 146, loss = 0.08682422\n",
      "Iteration 147, loss = 0.08634872\n",
      "Iteration 148, loss = 0.08587980\n",
      "Iteration 149, loss = 0.08541728\n",
      "Iteration 150, loss = 0.08496102\n",
      "Iteration 151, loss = 0.08451087\n",
      "Iteration 152, loss = 0.08406675\n",
      "Iteration 153, loss = 0.08362852\n",
      "Iteration 154, loss = 0.08319604\n",
      "Iteration 155, loss = 0.08276918\n",
      "Iteration 156, loss = 0.08234783\n",
      "Iteration 157, loss = 0.08193191\n",
      "Iteration 158, loss = 0.08152129\n",
      "Iteration 159, loss = 0.08111586\n",
      "Iteration 160, loss = 0.08071551\n",
      "Iteration 161, loss = 0.08032015\n",
      "Iteration 162, loss = 0.07992970\n",
      "Iteration 163, loss = 0.07954404\n",
      "Iteration 164, loss = 0.07916308\n",
      "Iteration 165, loss = 0.07878673\n",
      "Iteration 166, loss = 0.07841491\n",
      "Iteration 167, loss = 0.07804754\n",
      "Iteration 168, loss = 0.07768453\n",
      "Iteration 169, loss = 0.07732580\n",
      "Iteration 170, loss = 0.07697126\n",
      "Iteration 171, loss = 0.07662085\n",
      "Iteration 172, loss = 0.07627449\n",
      "Iteration 173, loss = 0.07593211\n",
      "Iteration 174, loss = 0.07559362\n",
      "Iteration 175, loss = 0.07525897\n",
      "Iteration 176, loss = 0.07492809\n",
      "Iteration 177, loss = 0.07460091\n",
      "Iteration 178, loss = 0.07427737\n",
      "Iteration 179, loss = 0.07395739\n",
      "Iteration 180, loss = 0.07364093\n",
      "Iteration 181, loss = 0.07332792\n",
      "Iteration 182, loss = 0.07301830\n",
      "Iteration 183, loss = 0.07271201\n",
      "Iteration 184, loss = 0.07240900\n",
      "Iteration 185, loss = 0.07210922\n",
      "Iteration 186, loss = 0.07181260\n",
      "Iteration 187, loss = 0.07151910\n",
      "Iteration 188, loss = 0.07122866\n",
      "Iteration 189, loss = 0.07094124\n",
      "Iteration 190, loss = 0.07065678\n",
      "Iteration 191, loss = 0.07037525\n",
      "Iteration 192, loss = 0.07009658\n",
      "Iteration 193, loss = 0.06982073\n",
      "Iteration 194, loss = 0.06954767\n",
      "Iteration 195, loss = 0.06927734\n",
      "Iteration 196, loss = 0.06900970\n",
      "Iteration 197, loss = 0.06874471\n",
      "Iteration 198, loss = 0.06848233\n",
      "Iteration 199, loss = 0.06822251\n",
      "Iteration 200, loss = 0.06796523\n",
      "Iteration 201, loss = 0.06771043\n",
      "Iteration 202, loss = 0.06745808\n",
      "Iteration 203, loss = 0.06720815\n",
      "Iteration 204, loss = 0.06696059\n",
      "Iteration 205, loss = 0.06671538\n",
      "Iteration 206, loss = 0.06647247\n",
      "Iteration 207, loss = 0.06623183\n",
      "Iteration 208, loss = 0.06599343\n",
      "Iteration 209, loss = 0.06575723\n",
      "Iteration 210, loss = 0.06552320\n",
      "Iteration 211, loss = 0.06529132\n",
      "Iteration 212, loss = 0.06506154\n",
      "Iteration 213, loss = 0.06483385\n",
      "Iteration 214, loss = 0.06460820\n",
      "Iteration 215, loss = 0.06438457\n",
      "Iteration 216, loss = 0.06416293\n",
      "Iteration 217, loss = 0.06394326\n",
      "Iteration 218, loss = 0.06372552\n",
      "Iteration 219, loss = 0.06350968\n",
      "Iteration 220, loss = 0.06329573\n",
      "Iteration 221, loss = 0.06308363\n",
      "Iteration 222, loss = 0.06287336\n",
      "Iteration 223, loss = 0.06266489\n",
      "Iteration 224, loss = 0.06245820\n",
      "Iteration 225, loss = 0.06225326\n",
      "Iteration 226, loss = 0.06205006\n",
      "Iteration 227, loss = 0.06184856\n",
      "Iteration 228, loss = 0.06164875\n",
      "Iteration 229, loss = 0.06145060\n",
      "Iteration 230, loss = 0.06125408\n",
      "Iteration 231, loss = 0.06105919\n",
      "Iteration 232, loss = 0.06086589\n",
      "Iteration 233, loss = 0.06067417\n",
      "Iteration 234, loss = 0.06048400\n",
      "Iteration 235, loss = 0.06029536\n",
      "Iteration 236, loss = 0.06010824\n",
      "Iteration 237, loss = 0.05992262\n",
      "Iteration 238, loss = 0.05973847\n",
      "Iteration 239, loss = 0.05955578\n",
      "Iteration 240, loss = 0.05937452\n",
      "Iteration 241, loss = 0.05919469\n",
      "Iteration 242, loss = 0.05901626\n",
      "Iteration 243, loss = 0.05883921\n",
      "Iteration 244, loss = 0.05866353\n",
      "Iteration 245, loss = 0.05848920\n",
      "Iteration 246, loss = 0.05831620\n",
      "Iteration 247, loss = 0.05814453\n",
      "Iteration 248, loss = 0.05797415\n",
      "Iteration 249, loss = 0.05780506\n",
      "Iteration 250, loss = 0.05763724\n",
      "Iteration 251, loss = 0.05747067\n",
      "Iteration 252, loss = 0.05730534\n",
      "Iteration 253, loss = 0.05714123\n",
      "Iteration 254, loss = 0.05697834\n",
      "Iteration 255, loss = 0.05681664\n",
      "Iteration 256, loss = 0.05665612\n",
      "Iteration 257, loss = 0.05649677\n",
      "Iteration 258, loss = 0.05633858\n",
      "Iteration 259, loss = 0.05618152\n",
      "Iteration 260, loss = 0.05602559\n",
      "Iteration 261, loss = 0.05587078\n",
      "Iteration 262, loss = 0.05571707\n",
      "Iteration 263, loss = 0.05556444\n",
      "Iteration 264, loss = 0.05541289\n",
      "Iteration 265, loss = 0.05526241\n",
      "Iteration 266, loss = 0.05511298\n",
      "Iteration 267, loss = 0.05496459\n",
      "Iteration 268, loss = 0.05481722\n",
      "Iteration 269, loss = 0.05467088\n",
      "Iteration 270, loss = 0.05452554\n",
      "Iteration 271, loss = 0.05438120\n",
      "Iteration 272, loss = 0.05423783\n",
      "Iteration 273, loss = 0.05409545\n",
      "Iteration 274, loss = 0.05395402\n",
      "Iteration 275, loss = 0.05381355\n",
      "Iteration 276, loss = 0.05367402\n",
      "Iteration 277, loss = 0.05353542\n",
      "Iteration 278, loss = 0.05339774\n",
      "Iteration 279, loss = 0.05326097\n",
      "Iteration 280, loss = 0.05312511\n",
      "Iteration 281, loss = 0.05299014\n",
      "Iteration 282, loss = 0.05285605\n",
      "Iteration 283, loss = 0.05272284\n",
      "Iteration 284, loss = 0.05259049\n",
      "Iteration 285, loss = 0.05245900\n",
      "Iteration 286, loss = 0.05232836\n",
      "Iteration 287, loss = 0.05219855\n",
      "Iteration 288, loss = 0.05206958\n",
      "Iteration 289, loss = 0.05194145\n",
      "Iteration 290, loss = 0.05181414\n",
      "Iteration 291, loss = 0.05168768\n",
      "Iteration 292, loss = 0.05156211\n",
      "Iteration 293, loss = 0.05143753\n",
      "Iteration 294, loss = 0.05131418\n",
      "Iteration 295, loss = 0.05119256\n",
      "Iteration 296, loss = 0.05107398\n",
      "Iteration 297, loss = 0.05096107\n",
      "Iteration 298, loss = 0.05086108\n",
      "Iteration 299, loss = 0.05078710\n",
      "Iteration 300, loss = 0.05078177\n",
      "Iteration 301, loss = 0.05089650\n",
      "Iteration 302, loss = 0.05137115\n",
      "Iteration 303, loss = 0.05210998\n",
      "Iteration 304, loss = 0.05388672\n",
      "Iteration 305, loss = 0.05348653\n",
      "Iteration 306, loss = 0.05259350\n",
      "Iteration 307, loss = 0.05005301\n",
      "Iteration 308, loss = 0.05029077\n",
      "Iteration 309, loss = 0.05204960\n",
      "Iteration 310, loss = 0.05096368\n",
      "Iteration 311, loss = 0.04950134\n",
      "Iteration 312, loss = 0.04951671\n",
      "Iteration 313, loss = 0.05028881\n",
      "Iteration 314, loss = 0.05011003\n",
      "Iteration 315, loss = 0.04894993\n",
      "Iteration 316, loss = 0.04924564\n",
      "Iteration 317, loss = 0.04983242\n",
      "Iteration 318, loss = 0.04889310\n",
      "Iteration 319, loss = 0.04853237\n",
      "Iteration 320, loss = 0.04903612\n",
      "Iteration 321, loss = 0.04873000\n",
      "Iteration 322, loss = 0.04817454\n",
      "Iteration 323, loss = 0.04823727\n",
      "Iteration 324, loss = 0.04835025\n",
      "Iteration 325, loss = 0.04801639\n",
      "Iteration 326, loss = 0.04775814\n",
      "Iteration 327, loss = 0.04789197\n",
      "Iteration 328, loss = 0.04781508\n",
      "Iteration 329, loss = 0.04746502\n",
      "Iteration 330, loss = 0.04744359\n",
      "Iteration 331, loss = 0.04750191\n",
      "Iteration 332, loss = 0.04723835\n",
      "Iteration 333, loss = 0.04706433\n",
      "Iteration 334, loss = 0.04709797\n",
      "Iteration 335, loss = 0.04698185\n",
      "Iteration 336, loss = 0.04677626\n",
      "Iteration 337, loss = 0.04671580\n",
      "Iteration 338, loss = 0.04668104\n",
      "Iteration 339, loss = 0.04652939\n",
      "Iteration 340, loss = 0.04639346\n",
      "Iteration 341, loss = 0.04635598\n",
      "Iteration 342, loss = 0.04627524\n",
      "Iteration 343, loss = 0.04612413\n",
      "Iteration 344, loss = 0.04603768\n",
      "Iteration 345, loss = 0.04598648\n",
      "Iteration 346, loss = 0.04587108\n",
      "Iteration 347, loss = 0.04575253\n",
      "Iteration 348, loss = 0.04568563\n",
      "Iteration 349, loss = 0.04560754\n",
      "Iteration 350, loss = 0.04549436\n",
      "Iteration 351, loss = 0.04539953\n",
      "Iteration 352, loss = 0.04533078\n",
      "Iteration 353, loss = 0.04524223\n",
      "Iteration 354, loss = 0.04513726\n",
      "Iteration 355, loss = 0.04505443\n",
      "Iteration 356, loss = 0.04498039\n",
      "Iteration 357, loss = 0.04488700\n",
      "Iteration 358, loss = 0.04479247\n",
      "Iteration 359, loss = 0.04471428\n",
      "Iteration 360, loss = 0.04463483\n",
      "Iteration 361, loss = 0.04454379\n",
      "Iteration 362, loss = 0.04445635\n",
      "Iteration 363, loss = 0.04437887\n",
      "Iteration 364, loss = 0.04429783\n",
      "Iteration 365, loss = 0.04420989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33009363\n",
      "Iteration 2, loss = 1.27643600\n",
      "Iteration 3, loss = 1.23382799\n",
      "Iteration 4, loss = 1.19594841\n",
      "Iteration 5, loss = 1.16354339\n",
      "Iteration 6, loss = 1.13730844\n",
      "Iteration 7, loss = 1.11778527\n",
      "Iteration 8, loss = 1.10521237\n",
      "Iteration 9, loss = 1.09934733\n",
      "Iteration 10, loss = 1.10018197\n",
      "Iteration 11, loss = 1.10084762\n",
      "Iteration 12, loss = 1.09319055\n",
      "Iteration 13, loss = 1.06987259\n",
      "Iteration 14, loss = 1.03499413\n",
      "Iteration 15, loss = 0.99538543\n",
      "Iteration 16, loss = 0.94147143\n",
      "Iteration 17, loss = 0.90233069\n",
      "Iteration 18, loss = 0.86312352\n",
      "Iteration 19, loss = 0.82494227\n",
      "Iteration 20, loss = 0.79083463\n",
      "Iteration 21, loss = 0.76119456\n",
      "Iteration 22, loss = 0.73136992\n",
      "Iteration 23, loss = 0.70076502\n",
      "Iteration 24, loss = 0.67201914\n",
      "Iteration 25, loss = 0.64503720\n",
      "Iteration 26, loss = 0.62005940\n",
      "Iteration 27, loss = 0.59654180\n",
      "Iteration 28, loss = 0.57467436\n",
      "Iteration 29, loss = 0.55418414\n",
      "Iteration 30, loss = 0.53500570\n",
      "Iteration 31, loss = 0.51682192\n",
      "Iteration 32, loss = 0.49946469\n",
      "Iteration 33, loss = 0.48227148\n",
      "Iteration 34, loss = 0.46564734\n",
      "Iteration 35, loss = 0.44962847\n",
      "Iteration 36, loss = 0.43580080\n",
      "Iteration 37, loss = 0.42313431\n",
      "Iteration 38, loss = 0.41058103\n",
      "Iteration 39, loss = 0.39826832\n",
      "Iteration 40, loss = 0.38396522\n",
      "Iteration 41, loss = 0.37104891\n",
      "Iteration 42, loss = 0.35870553\n",
      "Iteration 43, loss = 0.34814410\n",
      "Iteration 44, loss = 0.33788317\n",
      "Iteration 45, loss = 0.32763266\n",
      "Iteration 46, loss = 0.31733791\n",
      "Iteration 47, loss = 0.30727251\n",
      "Iteration 48, loss = 0.29826729\n",
      "Iteration 49, loss = 0.29041487\n",
      "Iteration 50, loss = 0.28300882\n",
      "Iteration 51, loss = 0.27599578\n",
      "Iteration 52, loss = 0.26879609\n",
      "Iteration 53, loss = 0.26221118\n",
      "Iteration 54, loss = 0.25605880\n",
      "Iteration 55, loss = 0.25067777\n",
      "Iteration 56, loss = 0.24572731\n",
      "Iteration 57, loss = 0.24092380\n",
      "Iteration 58, loss = 0.23632881\n",
      "Iteration 59, loss = 0.23179168\n",
      "Iteration 60, loss = 0.22766598\n",
      "Iteration 61, loss = 0.22377984\n",
      "Iteration 62, loss = 0.22017287\n",
      "Iteration 63, loss = 0.21671273\n",
      "Iteration 64, loss = 0.21322033\n",
      "Iteration 65, loss = 0.20985292\n",
      "Iteration 66, loss = 0.20658795\n",
      "Iteration 67, loss = 0.20349551\n",
      "Iteration 68, loss = 0.20060196\n",
      "Iteration 69, loss = 0.19776844\n",
      "Iteration 70, loss = 0.19502456\n",
      "Iteration 71, loss = 0.19232329\n",
      "Iteration 72, loss = 0.18970311\n",
      "Iteration 73, loss = 0.18723658\n",
      "Iteration 74, loss = 0.18486965\n",
      "Iteration 75, loss = 0.18261321\n",
      "Iteration 76, loss = 0.18042327\n",
      "Iteration 77, loss = 0.17827225\n",
      "Iteration 78, loss = 0.17621240\n",
      "Iteration 79, loss = 0.17423819\n",
      "Iteration 80, loss = 0.17235057\n",
      "Iteration 81, loss = 0.17054645\n",
      "Iteration 82, loss = 0.16878146\n",
      "Iteration 83, loss = 0.16706489\n",
      "Iteration 84, loss = 0.16540599\n",
      "Iteration 85, loss = 0.16380125\n",
      "Iteration 86, loss = 0.16226318\n",
      "Iteration 87, loss = 0.16077284\n",
      "Iteration 88, loss = 0.15931227\n",
      "Iteration 89, loss = 0.15788993\n",
      "Iteration 90, loss = 0.15650439\n",
      "Iteration 91, loss = 0.15515903\n",
      "Iteration 92, loss = 0.15385796\n",
      "Iteration 93, loss = 0.15258737\n",
      "Iteration 94, loss = 0.15134106\n",
      "Iteration 95, loss = 0.15012411\n",
      "Iteration 96, loss = 0.14893582\n",
      "Iteration 97, loss = 0.14777771\n",
      "Iteration 98, loss = 0.14665055\n",
      "Iteration 99, loss = 0.14554657\n",
      "Iteration 100, loss = 0.14446267\n",
      "Iteration 101, loss = 0.14340275\n",
      "Iteration 102, loss = 0.14236710\n",
      "Iteration 103, loss = 0.14135462\n",
      "Iteration 104, loss = 0.14036488\n",
      "Iteration 105, loss = 0.13939469\n",
      "Iteration 106, loss = 0.13844199\n",
      "Iteration 107, loss = 0.13750903\n",
      "Iteration 108, loss = 0.13659668\n",
      "Iteration 109, loss = 0.13570279\n",
      "Iteration 110, loss = 0.13482597\n",
      "Iteration 111, loss = 0.13396567\n",
      "Iteration 112, loss = 0.13312117\n",
      "Iteration 113, loss = 0.13229270\n",
      "Iteration 114, loss = 0.13148072\n",
      "Iteration 115, loss = 0.13068411\n",
      "Iteration 116, loss = 0.12990126\n",
      "Iteration 117, loss = 0.12913191\n",
      "Iteration 118, loss = 0.12837653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119, loss = 0.12763486\n",
      "Iteration 120, loss = 0.12690613\n",
      "Iteration 121, loss = 0.12618982\n",
      "Iteration 122, loss = 0.12548548\n",
      "Iteration 123, loss = 0.12479278\n",
      "Iteration 124, loss = 0.12411167\n",
      "Iteration 125, loss = 0.12344205\n",
      "Iteration 126, loss = 0.12278343\n",
      "Iteration 127, loss = 0.12213519\n",
      "Iteration 128, loss = 0.12149708\n",
      "Iteration 129, loss = 0.12086918\n",
      "Iteration 130, loss = 0.12025135\n",
      "Iteration 131, loss = 0.11964313\n",
      "Iteration 132, loss = 0.11904413\n",
      "Iteration 133, loss = 0.11845419\n",
      "Iteration 134, loss = 0.11787324\n",
      "Iteration 135, loss = 0.11730111\n",
      "Iteration 136, loss = 0.11673753\n",
      "Iteration 137, loss = 0.11618223\n",
      "Iteration 138, loss = 0.11563503\n",
      "Iteration 139, loss = 0.11509581\n",
      "Iteration 140, loss = 0.11456442\n",
      "Iteration 141, loss = 0.11404067\n",
      "Iteration 142, loss = 0.11352437\n",
      "Iteration 143, loss = 0.11301534\n",
      "Iteration 144, loss = 0.11251345\n",
      "Iteration 145, loss = 0.11201857\n",
      "Iteration 146, loss = 0.11153056\n",
      "Iteration 147, loss = 0.11104925\n",
      "Iteration 148, loss = 0.11057449\n",
      "Iteration 149, loss = 0.11010615\n",
      "Iteration 150, loss = 0.10964414\n",
      "Iteration 151, loss = 0.10918831\n",
      "Iteration 152, loss = 0.10873853\n",
      "Iteration 153, loss = 0.10829467\n",
      "Iteration 154, loss = 0.10785663\n",
      "Iteration 155, loss = 0.10742430\n",
      "Iteration 156, loss = 0.10699758\n",
      "Iteration 157, loss = 0.10657633\n",
      "Iteration 158, loss = 0.10616046\n",
      "Iteration 159, loss = 0.10574986\n",
      "Iteration 160, loss = 0.10534445\n",
      "Iteration 161, loss = 0.10494413\n",
      "Iteration 162, loss = 0.10454880\n",
      "Iteration 163, loss = 0.10415836\n",
      "Iteration 164, loss = 0.10377272\n",
      "Iteration 165, loss = 0.10339180\n",
      "Iteration 166, loss = 0.10301552\n",
      "Iteration 167, loss = 0.10264379\n",
      "Iteration 168, loss = 0.10227653\n",
      "Iteration 169, loss = 0.10191367\n",
      "Iteration 170, loss = 0.10155511\n",
      "Iteration 171, loss = 0.10120080\n",
      "Iteration 172, loss = 0.10085064\n",
      "Iteration 173, loss = 0.10050458\n",
      "Iteration 174, loss = 0.10016254\n",
      "Iteration 175, loss = 0.09982446\n",
      "Iteration 176, loss = 0.09949026\n",
      "Iteration 177, loss = 0.09915988\n",
      "Iteration 178, loss = 0.09883326\n",
      "Iteration 179, loss = 0.09851033\n",
      "Iteration 180, loss = 0.09819103\n",
      "Iteration 181, loss = 0.09787531\n",
      "Iteration 182, loss = 0.09756309\n",
      "Iteration 183, loss = 0.09725434\n",
      "Iteration 184, loss = 0.09694898\n",
      "Iteration 185, loss = 0.09664697\n",
      "Iteration 186, loss = 0.09634826\n",
      "Iteration 187, loss = 0.09605278\n",
      "Iteration 188, loss = 0.09576049\n",
      "Iteration 189, loss = 0.09547133\n",
      "Iteration 190, loss = 0.09518527\n",
      "Iteration 191, loss = 0.09490224\n",
      "Iteration 192, loss = 0.09462221\n",
      "Iteration 193, loss = 0.09434513\n",
      "Iteration 194, loss = 0.09407096\n",
      "Iteration 195, loss = 0.09379966\n",
      "Iteration 196, loss = 0.09353121\n",
      "Iteration 197, loss = 0.09326559\n",
      "Iteration 198, loss = 0.09300285\n",
      "Iteration 199, loss = 0.09274310\n",
      "Iteration 200, loss = 0.09248668\n",
      "Iteration 201, loss = 0.09223431\n",
      "Iteration 202, loss = 0.09198788\n",
      "Iteration 203, loss = 0.09175138\n",
      "Iteration 204, loss = 0.09153536\n",
      "Iteration 205, loss = 0.09136104\n",
      "Iteration 206, loss = 0.09128907\n",
      "Iteration 207, loss = 0.09141248\n",
      "Iteration 208, loss = 0.09202515\n",
      "Iteration 209, loss = 0.09304320\n",
      "Iteration 210, loss = 0.09470565\n",
      "Iteration 211, loss = 0.09355711\n",
      "Iteration 212, loss = 0.09110021\n",
      "Iteration 213, loss = 0.08938305\n",
      "Iteration 214, loss = 0.09058687\n",
      "Iteration 215, loss = 0.09173330\n",
      "Iteration 216, loss = 0.08970901\n",
      "Iteration 217, loss = 0.08852437\n",
      "Iteration 218, loss = 0.08950836\n",
      "Iteration 219, loss = 0.08946021\n",
      "Iteration 220, loss = 0.08815922\n",
      "Iteration 221, loss = 0.08782023\n",
      "Iteration 222, loss = 0.08841218\n",
      "Iteration 223, loss = 0.08799749\n",
      "Iteration 224, loss = 0.08705976\n",
      "Iteration 225, loss = 0.08730111\n",
      "Iteration 226, loss = 0.08746638\n",
      "Iteration 227, loss = 0.08661934\n",
      "Iteration 228, loss = 0.08637667\n",
      "Iteration 229, loss = 0.08665108\n",
      "Iteration 230, loss = 0.08618455\n",
      "Iteration 231, loss = 0.08569300\n",
      "Iteration 232, loss = 0.08577981\n",
      "Iteration 233, loss = 0.08564426\n",
      "Iteration 234, loss = 0.08518114\n",
      "Iteration 235, loss = 0.08502897\n",
      "Iteration 236, loss = 0.08502570\n",
      "Iteration 237, loss = 0.08472334\n",
      "Iteration 238, loss = 0.08442076\n",
      "Iteration 239, loss = 0.08438159\n",
      "Iteration 240, loss = 0.08423793\n",
      "Iteration 241, loss = 0.08391637\n",
      "Iteration 242, loss = 0.08376671\n",
      "Iteration 243, loss = 0.08369267\n",
      "Iteration 244, loss = 0.08344928\n",
      "Iteration 245, loss = 0.08322120\n",
      "Iteration 246, loss = 0.08312356\n",
      "Iteration 247, loss = 0.08296992\n",
      "Iteration 248, loss = 0.08273904\n",
      "Iteration 249, loss = 0.08258040\n",
      "Iteration 250, loss = 0.08246623\n",
      "Iteration 251, loss = 0.08228467\n",
      "Iteration 252, loss = 0.08208858\n",
      "Iteration 253, loss = 0.08195665\n",
      "Iteration 254, loss = 0.08182108\n",
      "Iteration 255, loss = 0.08163616\n",
      "Iteration 256, loss = 0.08147157\n",
      "Iteration 257, loss = 0.08134368\n",
      "Iteration 258, loss = 0.08119310\n",
      "Iteration 259, loss = 0.08102171\n",
      "Iteration 260, loss = 0.08087499\n",
      "Iteration 261, loss = 0.08074318\n",
      "Iteration 262, loss = 0.08059154\n",
      "Iteration 263, loss = 0.08043318\n",
      "Iteration 264, loss = 0.08029473\n",
      "Iteration 265, loss = 0.08016098\n",
      "Iteration 266, loss = 0.08001250\n",
      "Iteration 267, loss = 0.07986457\n",
      "Iteration 268, loss = 0.07973040\n",
      "Iteration 269, loss = 0.07959661\n",
      "Iteration 270, loss = 0.07945394\n",
      "Iteration 271, loss = 0.07931341\n",
      "Iteration 272, loss = 0.07918206\n",
      "Iteration 273, loss = 0.07905054\n",
      "Iteration 274, loss = 0.07891340\n",
      "Iteration 275, loss = 0.07877843\n",
      "Iteration 276, loss = 0.07864972\n",
      "Iteration 277, loss = 0.07852114\n",
      "Iteration 278, loss = 0.07838932\n",
      "Iteration 279, loss = 0.07825890\n",
      "Iteration 280, loss = 0.07813289\n",
      "Iteration 281, loss = 0.07800769\n",
      "Iteration 282, loss = 0.07788057\n",
      "Iteration 283, loss = 0.07775422\n",
      "Iteration 284, loss = 0.07763102\n",
      "Iteration 285, loss = 0.07750910\n",
      "Iteration 286, loss = 0.07738634\n",
      "Iteration 287, loss = 0.07726382\n",
      "Iteration 288, loss = 0.07714348\n",
      "Iteration 289, loss = 0.07702471\n",
      "Iteration 290, loss = 0.07690587\n",
      "Iteration 291, loss = 0.07678709\n",
      "Iteration 292, loss = 0.07666969\n",
      "Iteration 293, loss = 0.07655385\n",
      "Iteration 294, loss = 0.07643858\n",
      "Iteration 295, loss = 0.07632342\n",
      "Iteration 296, loss = 0.07620906\n",
      "Iteration 297, loss = 0.07609603\n",
      "Iteration 298, loss = 0.07598393\n",
      "Iteration 299, loss = 0.07587222\n",
      "Iteration 300, loss = 0.07576100\n",
      "Iteration 301, loss = 0.07565073\n",
      "Iteration 302, loss = 0.07554151\n",
      "Iteration 303, loss = 0.07543296\n",
      "Iteration 304, loss = 0.07532488\n",
      "Iteration 305, loss = 0.07521746\n",
      "Iteration 306, loss = 0.07511093\n",
      "Iteration 307, loss = 0.07500524\n",
      "Iteration 308, loss = 0.07490016\n",
      "Iteration 309, loss = 0.07479564\n",
      "Iteration 310, loss = 0.07469179\n",
      "Iteration 311, loss = 0.07458873\n",
      "Iteration 312, loss = 0.07448641\n",
      "Iteration 313, loss = 0.07438470\n",
      "Iteration 314, loss = 0.07428357\n",
      "Iteration 315, loss = 0.07418309\n",
      "Iteration 316, loss = 0.07408333\n",
      "Iteration 317, loss = 0.07398424\n",
      "Iteration 318, loss = 0.07388576\n",
      "Iteration 319, loss = 0.07378787\n",
      "Iteration 320, loss = 0.07369058\n",
      "Iteration 321, loss = 0.07359395\n",
      "Iteration 322, loss = 0.07349796\n",
      "Iteration 323, loss = 0.07340257\n",
      "Iteration 324, loss = 0.07330776\n",
      "Iteration 325, loss = 0.07321352\n",
      "Iteration 326, loss = 0.07311988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33048782\n",
      "Iteration 2, loss = 1.30759909\n",
      "Iteration 3, loss = 1.28512973\n",
      "Iteration 4, loss = 1.25853468\n",
      "Iteration 5, loss = 1.22966987\n",
      "Iteration 6, loss = 1.20057256\n",
      "Iteration 7, loss = 1.17292682\n",
      "Iteration 8, loss = 1.14838424\n",
      "Iteration 9, loss = 1.12824582\n",
      "Iteration 10, loss = 1.11331368\n",
      "Iteration 11, loss = 1.10379308\n",
      "Iteration 12, loss = 1.09928477\n",
      "Iteration 13, loss = 1.09888346\n",
      "Iteration 14, loss = 1.10136310\n",
      "Iteration 15, loss = 1.10540080\n",
      "Iteration 16, loss = 1.10978463\n",
      "Iteration 17, loss = 1.11356432\n",
      "Iteration 18, loss = 1.11612747\n",
      "Iteration 19, loss = 1.11720553\n",
      "Iteration 20, loss = 1.11682613\n",
      "Iteration 21, loss = 1.11523362\n",
      "Iteration 22, loss = 1.11279811\n",
      "Iteration 23, loss = 1.10992947\n",
      "Iteration 24, loss = 1.10700799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33104251\n",
      "Iteration 2, loss = 1.30739995\n",
      "Iteration 3, loss = 1.28491087\n",
      "Iteration 4, loss = 1.25828897\n",
      "Iteration 5, loss = 1.22957644\n",
      "Iteration 6, loss = 1.20047601\n",
      "Iteration 7, loss = 1.17283456\n",
      "Iteration 8, loss = 1.14830286\n",
      "Iteration 9, loss = 1.12818040\n",
      "Iteration 10, loss = 1.11326722\n",
      "Iteration 11, loss = 1.10376618\n",
      "Iteration 12, loss = 1.09927566\n",
      "Iteration 13, loss = 1.09888852\n",
      "Iteration 14, loss = 1.10137762\n",
      "Iteration 15, loss = 1.10541987\n",
      "Iteration 16, loss = 1.10980387\n",
      "Iteration 17, loss = 1.11358042\n",
      "Iteration 18, loss = 1.11613843\n",
      "Iteration 19, loss = 1.11721059\n",
      "Iteration 20, loss = 1.11682560\n",
      "Iteration 21, loss = 1.11522856\n",
      "Iteration 22, loss = 1.11278996\n",
      "Iteration 23, loss = 1.10991980\n",
      "Iteration 24, loss = 1.10699818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33079619\n",
      "Iteration 2, loss = 1.30762128\n",
      "Iteration 3, loss = 1.28513111\n",
      "Iteration 4, loss = 1.25853354\n",
      "Iteration 5, loss = 1.22968341\n",
      "Iteration 6, loss = 1.20058595\n",
      "Iteration 7, loss = 1.17293921\n",
      "Iteration 8, loss = 1.14839490\n",
      "Iteration 9, loss = 1.12825420\n",
      "Iteration 10, loss = 1.11331951\n",
      "Iteration 11, loss = 1.10379640\n",
      "Iteration 12, loss = 1.09928587\n",
      "Iteration 13, loss = 1.09888287\n",
      "Iteration 14, loss = 1.10136143\n",
      "Iteration 15, loss = 1.10539866\n",
      "Iteration 16, loss = 1.10978256\n",
      "Iteration 17, loss = 1.11356270\n",
      "Iteration 18, loss = 1.11612652\n",
      "Iteration 19, loss = 1.11720530\n",
      "Iteration 20, loss = 1.11682655\n",
      "Iteration 21, loss = 1.11523456\n",
      "Iteration 22, loss = 1.11279936\n",
      "Iteration 23, loss = 1.10993085\n",
      "Iteration 24, loss = 1.10700932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32935999\n",
      "Iteration 2, loss = 1.30769754\n",
      "Iteration 3, loss = 1.28506491\n",
      "Iteration 4, loss = 1.25845162\n",
      "Iteration 5, loss = 1.22957233\n",
      "Iteration 6, loss = 1.20047588\n",
      "Iteration 7, loss = 1.17283725\n",
      "Iteration 8, loss = 1.14830716\n",
      "Iteration 9, loss = 1.12818514\n",
      "Iteration 10, loss = 1.11327139\n",
      "Iteration 11, loss = 1.10376903\n",
      "Iteration 12, loss = 1.09927677\n",
      "Iteration 13, loss = 1.09888781\n",
      "Iteration 14, loss = 1.10137532\n",
      "Iteration 15, loss = 1.10541640\n",
      "Iteration 16, loss = 1.10979977\n",
      "Iteration 17, loss = 1.11357621\n",
      "Iteration 18, loss = 1.11613455\n",
      "Iteration 19, loss = 1.11720735\n",
      "Iteration 20, loss = 1.11682316\n",
      "Iteration 21, loss = 1.11522696\n",
      "Iteration 22, loss = 1.11278912\n",
      "Iteration 23, loss = 1.10991956\n",
      "Iteration 24, loss = 1.10699837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33009363\n",
      "Iteration 2, loss = 1.30697558\n",
      "Iteration 3, loss = 1.28470133\n",
      "Iteration 4, loss = 1.25828214\n",
      "Iteration 5, loss = 1.22956562\n",
      "Iteration 6, loss = 1.20046278\n",
      "Iteration 7, loss = 1.17282052\n",
      "Iteration 8, loss = 1.14828951\n",
      "Iteration 9, loss = 1.12816902\n",
      "Iteration 10, loss = 1.11325874\n",
      "Iteration 11, loss = 1.10376105\n",
      "Iteration 12, loss = 1.09927384\n",
      "Iteration 13, loss = 1.09888956\n",
      "Iteration 14, loss = 1.10138076\n",
      "Iteration 15, loss = 1.10542421\n",
      "Iteration 16, loss = 1.10980855\n",
      "Iteration 17, loss = 1.11358472\n",
      "Iteration 18, loss = 1.11614185\n",
      "Iteration 19, loss = 1.11721288\n",
      "Iteration 20, loss = 1.11682671\n",
      "Iteration 21, loss = 1.11522863\n",
      "Iteration 22, loss = 1.11278923\n",
      "Iteration 23, loss = 1.10991855\n",
      "Iteration 24, loss = 1.10699671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33048782\n",
      "Iteration 2, loss = 1.32170545\n",
      "Iteration 3, loss = 1.31467707\n",
      "Iteration 4, loss = 1.30918184\n",
      "Iteration 5, loss = 1.30432917\n",
      "Iteration 6, loss = 1.29954919\n",
      "Iteration 7, loss = 1.29482947\n",
      "Iteration 8, loss = 1.29016588\n",
      "Iteration 9, loss = 1.28555593\n",
      "Iteration 10, loss = 1.28099814\n",
      "Iteration 11, loss = 1.27649787\n",
      "Iteration 12, loss = 1.27207245\n",
      "Iteration 13, loss = 1.26769335\n",
      "Iteration 14, loss = 1.26336175\n",
      "Iteration 15, loss = 1.25907878\n",
      "Iteration 16, loss = 1.25484555\n",
      "Iteration 17, loss = 1.25066316\n",
      "Iteration 18, loss = 1.24653265\n",
      "Iteration 19, loss = 1.24245507\n",
      "Iteration 20, loss = 1.23843141\n",
      "Iteration 21, loss = 1.23446264\n",
      "Iteration 22, loss = 1.23054969\n",
      "Iteration 23, loss = 1.22669346\n",
      "Iteration 24, loss = 1.22289481\n",
      "Iteration 25, loss = 1.21915457\n",
      "Iteration 26, loss = 1.21547353\n",
      "Iteration 27, loss = 1.21185243\n",
      "Iteration 28, loss = 1.20829198\n",
      "Iteration 29, loss = 1.20479282\n",
      "Iteration 30, loss = 1.20135559\n",
      "Iteration 31, loss = 1.19798085\n",
      "Iteration 32, loss = 1.19466912\n",
      "Iteration 33, loss = 1.19142088\n",
      "Iteration 34, loss = 1.18823655\n",
      "Iteration 35, loss = 1.18511652\n",
      "Iteration 36, loss = 1.18206110\n",
      "Iteration 37, loss = 1.17907058\n",
      "Iteration 38, loss = 1.17614519\n",
      "Iteration 39, loss = 1.17328508\n",
      "Iteration 40, loss = 1.17049038\n",
      "Iteration 41, loss = 1.16776117\n",
      "Iteration 42, loss = 1.16509745\n",
      "Iteration 43, loss = 1.16249918\n",
      "Iteration 44, loss = 1.15996627\n",
      "Iteration 45, loss = 1.15749858\n",
      "Iteration 46, loss = 1.15509590\n",
      "Iteration 47, loss = 1.15275799\n",
      "Iteration 48, loss = 1.15048454\n",
      "Iteration 49, loss = 1.14827519\n",
      "Iteration 50, loss = 1.14612954\n",
      "Iteration 51, loss = 1.14404712\n",
      "Iteration 52, loss = 1.14202744\n",
      "Iteration 53, loss = 1.14006992\n",
      "Iteration 54, loss = 1.13817397\n",
      "Iteration 55, loss = 1.13633894\n",
      "Iteration 56, loss = 1.13456411\n",
      "Iteration 57, loss = 1.13284876\n",
      "Iteration 58, loss = 1.13119210\n",
      "Iteration 59, loss = 1.12959329\n",
      "Iteration 60, loss = 1.12805147\n",
      "Iteration 61, loss = 1.12656573\n",
      "Iteration 62, loss = 1.12513514\n",
      "Iteration 63, loss = 1.12375871\n",
      "Iteration 64, loss = 1.12243544\n",
      "Iteration 65, loss = 1.12116429\n",
      "Iteration 66, loss = 1.11994419\n",
      "Iteration 67, loss = 1.11877404\n",
      "Iteration 68, loss = 1.11765273\n",
      "Iteration 69, loss = 1.11657912\n",
      "Iteration 70, loss = 1.11555204\n",
      "Iteration 71, loss = 1.11457034\n",
      "Iteration 72, loss = 1.11363280\n",
      "Iteration 73, loss = 1.11273823\n",
      "Iteration 74, loss = 1.11188543\n",
      "Iteration 75, loss = 1.11107316\n",
      "Iteration 76, loss = 1.11030020\n",
      "Iteration 77, loss = 1.10956533\n",
      "Iteration 78, loss = 1.10886731\n",
      "Iteration 79, loss = 1.10820492\n",
      "Iteration 80, loss = 1.10757694\n",
      "Iteration 81, loss = 1.10698215\n",
      "Iteration 82, loss = 1.10641934\n",
      "Iteration 83, loss = 1.10588731\n",
      "Iteration 84, loss = 1.10538488\n",
      "Iteration 85, loss = 1.10491088\n",
      "Iteration 86, loss = 1.10446415\n",
      "Iteration 87, loss = 1.10404355\n",
      "Iteration 88, loss = 1.10364796\n",
      "Iteration 89, loss = 1.10327629\n",
      "Iteration 90, loss = 1.10292746\n",
      "Iteration 91, loss = 1.10260041\n",
      "Iteration 92, loss = 1.10229411\n",
      "Iteration 93, loss = 1.10200757\n",
      "Iteration 94, loss = 1.10173980\n",
      "Iteration 95, loss = 1.10148986\n",
      "Iteration 96, loss = 1.10125682\n",
      "Iteration 97, loss = 1.10103979\n",
      "Iteration 98, loss = 1.10083790\n",
      "Iteration 99, loss = 1.10065031\n",
      "Iteration 100, loss = 1.10047623\n",
      "Iteration 101, loss = 1.10031487\n",
      "Iteration 102, loss = 1.10016549\n",
      "Iteration 103, loss = 1.10002736\n",
      "Iteration 104, loss = 1.09989980\n",
      "Iteration 105, loss = 1.09978216\n",
      "Iteration 106, loss = 1.09967380\n",
      "Iteration 107, loss = 1.09957411\n",
      "Iteration 108, loss = 1.09948254\n",
      "Iteration 109, loss = 1.09939852\n",
      "Iteration 110, loss = 1.09932155\n",
      "Iteration 111, loss = 1.09925113\n",
      "Iteration 112, loss = 1.09918680\n",
      "Iteration 113, loss = 1.09912811\n",
      "Iteration 114, loss = 1.09907466\n",
      "Iteration 115, loss = 1.09902604\n",
      "Iteration 116, loss = 1.09898189\n",
      "Iteration 117, loss = 1.09894186\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33104251\n",
      "Iteration 2, loss = 1.32155523\n",
      "Iteration 3, loss = 1.31465158\n",
      "Iteration 4, loss = 1.30920795\n",
      "Iteration 5, loss = 1.30434877\n",
      "Iteration 6, loss = 1.29957339\n",
      "Iteration 7, loss = 1.29485703\n",
      "Iteration 8, loss = 1.29019574\n",
      "Iteration 9, loss = 1.28558720\n",
      "Iteration 10, loss = 1.28103012\n",
      "Iteration 11, loss = 1.27652381\n",
      "Iteration 12, loss = 1.27207849\n",
      "Iteration 13, loss = 1.26770012\n",
      "Iteration 14, loss = 1.26336920\n",
      "Iteration 15, loss = 1.25908689\n",
      "Iteration 16, loss = 1.25485428\n",
      "Iteration 17, loss = 1.25067248\n",
      "Iteration 18, loss = 1.24654254\n",
      "Iteration 19, loss = 1.24246549\n",
      "Iteration 20, loss = 1.23844233\n",
      "Iteration 21, loss = 1.23447403\n",
      "Iteration 22, loss = 1.23056152\n",
      "Iteration 23, loss = 1.22670571\n",
      "Iteration 24, loss = 1.22290745\n",
      "Iteration 25, loss = 1.21916757\n",
      "Iteration 26, loss = 1.21548686\n",
      "Iteration 27, loss = 1.21186607\n",
      "Iteration 28, loss = 1.20830589\n",
      "Iteration 29, loss = 1.20480699\n",
      "Iteration 30, loss = 1.20136999\n",
      "Iteration 31, loss = 1.19799545\n",
      "Iteration 32, loss = 1.19468390\n",
      "Iteration 33, loss = 1.19143581\n",
      "Iteration 34, loss = 1.18825162\n",
      "Iteration 35, loss = 1.18513169\n",
      "Iteration 36, loss = 1.18207636\n",
      "Iteration 37, loss = 1.17908591\n",
      "Iteration 38, loss = 1.17616055\n",
      "Iteration 39, loss = 1.17330047\n",
      "Iteration 40, loss = 1.17050578\n",
      "Iteration 41, loss = 1.16777655\n",
      "Iteration 42, loss = 1.16511279\n",
      "Iteration 43, loss = 1.16251446\n",
      "Iteration 44, loss = 1.15998148\n",
      "Iteration 45, loss = 1.15751370\n",
      "Iteration 46, loss = 1.15511092\n",
      "Iteration 47, loss = 1.15277289\n",
      "Iteration 48, loss = 1.15049930\n",
      "Iteration 49, loss = 1.14828980\n",
      "Iteration 50, loss = 1.14614398\n",
      "Iteration 51, loss = 1.14406139\n",
      "Iteration 52, loss = 1.14204151\n",
      "Iteration 53, loss = 1.14008380\n",
      "Iteration 54, loss = 1.13818763\n",
      "Iteration 55, loss = 1.13635237\n",
      "Iteration 56, loss = 1.13457732\n",
      "Iteration 57, loss = 1.13286172\n",
      "Iteration 58, loss = 1.13120481\n",
      "Iteration 59, loss = 1.12960574\n",
      "Iteration 60, loss = 1.12806366\n",
      "Iteration 61, loss = 1.12657765\n",
      "Iteration 62, loss = 1.12514678\n",
      "Iteration 63, loss = 1.12377007\n",
      "Iteration 64, loss = 1.12244651\n",
      "Iteration 65, loss = 1.12117507\n",
      "Iteration 66, loss = 1.11995467\n",
      "Iteration 67, loss = 1.11878423\n",
      "Iteration 68, loss = 1.11766262\n",
      "Iteration 69, loss = 1.11658871\n",
      "Iteration 70, loss = 1.11556134\n",
      "Iteration 71, loss = 1.11457934\n",
      "Iteration 72, loss = 1.11364150\n",
      "Iteration 73, loss = 1.11274664\n",
      "Iteration 74, loss = 1.11189354\n",
      "Iteration 75, loss = 1.11108098\n",
      "Iteration 76, loss = 1.11030773\n",
      "Iteration 77, loss = 1.10957257\n",
      "Iteration 78, loss = 1.10887427\n",
      "Iteration 79, loss = 1.10821160\n",
      "Iteration 80, loss = 1.10758335\n",
      "Iteration 81, loss = 1.10698829\n",
      "Iteration 82, loss = 1.10642521\n",
      "Iteration 83, loss = 1.10589292\n",
      "Iteration 84, loss = 1.10539024\n",
      "Iteration 85, loss = 1.10491599\n",
      "Iteration 86, loss = 1.10446902\n",
      "Iteration 87, loss = 1.10404818\n",
      "Iteration 88, loss = 1.10365237\n",
      "Iteration 89, loss = 1.10328047\n",
      "Iteration 90, loss = 1.10293142\n",
      "Iteration 91, loss = 1.10260417\n",
      "Iteration 92, loss = 1.10229767\n",
      "Iteration 93, loss = 1.10201093\n",
      "Iteration 94, loss = 1.10174298\n",
      "Iteration 95, loss = 1.10149285\n",
      "Iteration 96, loss = 1.10125964\n",
      "Iteration 97, loss = 1.10104244\n",
      "Iteration 98, loss = 1.10084039\n",
      "Iteration 99, loss = 1.10065265\n",
      "Iteration 100, loss = 1.10047842\n",
      "Iteration 101, loss = 1.10031692\n",
      "Iteration 102, loss = 1.10016740\n",
      "Iteration 103, loss = 1.10002915\n",
      "Iteration 104, loss = 1.09990147\n",
      "Iteration 105, loss = 1.09978371\n",
      "Iteration 106, loss = 1.09967524\n",
      "Iteration 107, loss = 1.09957545\n",
      "Iteration 108, loss = 1.09948378\n",
      "Iteration 109, loss = 1.09939967\n",
      "Iteration 110, loss = 1.09932261\n",
      "Iteration 111, loss = 1.09925211\n",
      "Iteration 112, loss = 1.09918770\n",
      "Iteration 113, loss = 1.09912894\n",
      "Iteration 114, loss = 1.09907542\n",
      "Iteration 115, loss = 1.09902674\n",
      "Iteration 116, loss = 1.09898253\n",
      "Iteration 117, loss = 1.09894244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33079619\n",
      "Iteration 2, loss = 1.32187210\n",
      "Iteration 3, loss = 1.31473083\n",
      "Iteration 4, loss = 1.30918543\n",
      "Iteration 5, loss = 1.30432490\n",
      "Iteration 6, loss = 1.29954307\n",
      "Iteration 7, loss = 1.29482194\n",
      "Iteration 8, loss = 1.29015716\n",
      "Iteration 9, loss = 1.28554613\n",
      "Iteration 10, loss = 1.28098728\n",
      "Iteration 11, loss = 1.27649671\n",
      "Iteration 12, loss = 1.27207112\n",
      "Iteration 13, loss = 1.26769185\n",
      "Iteration 14, loss = 1.26336009\n",
      "Iteration 15, loss = 1.25907697\n",
      "Iteration 16, loss = 1.25484360\n",
      "Iteration 17, loss = 1.25066107\n",
      "Iteration 18, loss = 1.24653044\n",
      "Iteration 19, loss = 1.24245274\n",
      "Iteration 20, loss = 1.23842897\n",
      "Iteration 21, loss = 1.23446009\n",
      "Iteration 22, loss = 1.23054704\n",
      "Iteration 23, loss = 1.22669072\n",
      "Iteration 24, loss = 1.22289198\n",
      "Iteration 25, loss = 1.21915167\n",
      "Iteration 26, loss = 1.21547055\n",
      "Iteration 27, loss = 1.21184938\n",
      "Iteration 28, loss = 1.20828886\n",
      "Iteration 29, loss = 1.20478966\n",
      "Iteration 30, loss = 1.20135237\n",
      "Iteration 31, loss = 1.19797759\n",
      "Iteration 32, loss = 1.19466582\n",
      "Iteration 33, loss = 1.19141754\n",
      "Iteration 34, loss = 1.18823319\n",
      "Iteration 35, loss = 1.18511313\n",
      "Iteration 36, loss = 1.18205769\n",
      "Iteration 37, loss = 1.17906716\n",
      "Iteration 38, loss = 1.17614175\n",
      "Iteration 39, loss = 1.17328164\n",
      "Iteration 40, loss = 1.17048695\n",
      "Iteration 41, loss = 1.16775774\n",
      "Iteration 42, loss = 1.16509402\n",
      "Iteration 43, loss = 1.16249577\n",
      "Iteration 44, loss = 1.15996288\n",
      "Iteration 45, loss = 1.15749520\n",
      "Iteration 46, loss = 1.15509255\n",
      "Iteration 47, loss = 1.15275467\n",
      "Iteration 48, loss = 1.15048125\n",
      "Iteration 49, loss = 1.14827193\n",
      "Iteration 50, loss = 1.14612632\n",
      "Iteration 51, loss = 1.14404394\n",
      "Iteration 52, loss = 1.14202430\n",
      "Iteration 53, loss = 1.14006683\n",
      "Iteration 54, loss = 1.13817093\n",
      "Iteration 55, loss = 1.13633594\n",
      "Iteration 56, loss = 1.13456117\n",
      "Iteration 57, loss = 1.13284588\n",
      "Iteration 58, loss = 1.13118927\n",
      "Iteration 59, loss = 1.12959051\n",
      "Iteration 60, loss = 1.12804875\n",
      "Iteration 61, loss = 1.12656308\n",
      "Iteration 62, loss = 1.12513255\n",
      "Iteration 63, loss = 1.12375619\n",
      "Iteration 64, loss = 1.12243298\n",
      "Iteration 65, loss = 1.12116189\n",
      "Iteration 66, loss = 1.11994185\n",
      "Iteration 67, loss = 1.11877177\n",
      "Iteration 68, loss = 1.11765053\n",
      "Iteration 69, loss = 1.11657698\n",
      "Iteration 70, loss = 1.11554998\n",
      "Iteration 71, loss = 1.11456833\n",
      "Iteration 72, loss = 1.11363086\n",
      "Iteration 73, loss = 1.11273637\n",
      "Iteration 74, loss = 1.11188362\n",
      "Iteration 75, loss = 1.11107142\n",
      "Iteration 76, loss = 1.11029852\n",
      "Iteration 77, loss = 1.10956372\n",
      "Iteration 78, loss = 1.10886576\n",
      "Iteration 79, loss = 1.10820344\n",
      "Iteration 80, loss = 1.10757551\n",
      "Iteration 81, loss = 1.10698078\n",
      "Iteration 82, loss = 1.10641803\n",
      "Iteration 83, loss = 1.10588606\n",
      "Iteration 84, loss = 1.10538369\n",
      "Iteration 85, loss = 1.10490974\n",
      "Iteration 86, loss = 1.10446307\n",
      "Iteration 87, loss = 1.10404252\n",
      "Iteration 88, loss = 1.10364698\n",
      "Iteration 89, loss = 1.10327536\n",
      "Iteration 90, loss = 1.10292658\n",
      "Iteration 91, loss = 1.10259957\n",
      "Iteration 92, loss = 1.10229332\n",
      "Iteration 93, loss = 1.10200682\n",
      "Iteration 94, loss = 1.10173910\n",
      "Iteration 95, loss = 1.10148920\n",
      "Iteration 96, loss = 1.10125619\n",
      "Iteration 97, loss = 1.10103920\n",
      "Iteration 98, loss = 1.10083734\n",
      "Iteration 99, loss = 1.10064979\n",
      "Iteration 100, loss = 1.10047574\n",
      "Iteration 101, loss = 1.10031441\n",
      "Iteration 102, loss = 1.10016506\n",
      "Iteration 103, loss = 1.10002696\n",
      "Iteration 104, loss = 1.09989943\n",
      "Iteration 105, loss = 1.09978181\n",
      "Iteration 106, loss = 1.09967347\n",
      "Iteration 107, loss = 1.09957381\n",
      "Iteration 108, loss = 1.09948226\n",
      "Iteration 109, loss = 1.09939826\n",
      "Iteration 110, loss = 1.09932131\n",
      "Iteration 111, loss = 1.09925091\n",
      "Iteration 112, loss = 1.09918660\n",
      "Iteration 113, loss = 1.09912793\n",
      "Iteration 114, loss = 1.09907449\n",
      "Iteration 115, loss = 1.09902588\n",
      "Iteration 116, loss = 1.09898174\n",
      "Iteration 117, loss = 1.09894173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32935999\n",
      "Iteration 2, loss = 1.32117941\n",
      "Iteration 3, loss = 1.31467769\n",
      "Iteration 4, loss = 1.30920855\n",
      "Iteration 5, loss = 1.30433788\n",
      "Iteration 6, loss = 1.29955814\n",
      "Iteration 7, loss = 1.29483858\n",
      "Iteration 8, loss = 1.29017516\n",
      "Iteration 9, loss = 1.28556543\n",
      "Iteration 10, loss = 1.28100792\n",
      "Iteration 11, loss = 1.27650560\n",
      "Iteration 12, loss = 1.27208116\n",
      "Iteration 13, loss = 1.26770301\n",
      "Iteration 14, loss = 1.26337232\n",
      "Iteration 15, loss = 1.25909024\n",
      "Iteration 16, loss = 1.25485787\n",
      "Iteration 17, loss = 1.25067630\n",
      "Iteration 18, loss = 1.24654658\n",
      "Iteration 19, loss = 1.24246975\n",
      "Iteration 20, loss = 1.23844681\n",
      "Iteration 21, loss = 1.23447871\n",
      "Iteration 22, loss = 1.23056640\n",
      "Iteration 23, loss = 1.22671077\n",
      "Iteration 24, loss = 1.22291270\n",
      "Iteration 25, loss = 1.21917299\n",
      "Iteration 26, loss = 1.21549244\n",
      "Iteration 27, loss = 1.21187180\n",
      "Iteration 28, loss = 1.20831176\n",
      "Iteration 29, loss = 1.20481300\n",
      "Iteration 30, loss = 1.20137611\n",
      "Iteration 31, loss = 1.19800169\n",
      "Iteration 32, loss = 1.19469024\n",
      "Iteration 33, loss = 1.19144224\n",
      "Iteration 34, loss = 1.18825813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 1.18513827\n",
      "Iteration 36, loss = 1.18208300\n",
      "Iteration 37, loss = 1.17909260\n",
      "Iteration 38, loss = 1.17616728\n",
      "Iteration 39, loss = 1.17330723\n",
      "Iteration 40, loss = 1.17051256\n",
      "Iteration 41, loss = 1.16778334\n",
      "Iteration 42, loss = 1.16511958\n",
      "Iteration 43, loss = 1.16252125\n",
      "Iteration 44, loss = 1.15998826\n",
      "Iteration 45, loss = 1.15752045\n",
      "Iteration 46, loss = 1.15511764\n",
      "Iteration 47, loss = 1.15277957\n",
      "Iteration 48, loss = 1.15050593\n",
      "Iteration 49, loss = 1.14829638\n",
      "Iteration 50, loss = 1.14615050\n",
      "Iteration 51, loss = 1.14406784\n",
      "Iteration 52, loss = 1.14204789\n",
      "Iteration 53, loss = 1.14009009\n",
      "Iteration 54, loss = 1.13819384\n",
      "Iteration 55, loss = 1.13635849\n",
      "Iteration 56, loss = 1.13458334\n",
      "Iteration 57, loss = 1.13286764\n",
      "Iteration 58, loss = 1.13121062\n",
      "Iteration 59, loss = 1.12961144\n",
      "Iteration 60, loss = 1.12806924\n",
      "Iteration 61, loss = 1.12658312\n",
      "Iteration 62, loss = 1.12515213\n",
      "Iteration 63, loss = 1.12377530\n",
      "Iteration 64, loss = 1.12245162\n",
      "Iteration 65, loss = 1.12118005\n",
      "Iteration 66, loss = 1.11995952\n",
      "Iteration 67, loss = 1.11878895\n",
      "Iteration 68, loss = 1.11766721\n",
      "Iteration 69, loss = 1.11659316\n",
      "Iteration 70, loss = 1.11556566\n",
      "Iteration 71, loss = 1.11458352\n",
      "Iteration 72, loss = 1.11364555\n",
      "Iteration 73, loss = 1.11275056\n",
      "Iteration 74, loss = 1.11189732\n",
      "Iteration 75, loss = 1.11108463\n",
      "Iteration 76, loss = 1.11031125\n",
      "Iteration 77, loss = 1.10957596\n",
      "Iteration 78, loss = 1.10887753\n",
      "Iteration 79, loss = 1.10821473\n",
      "Iteration 80, loss = 1.10758635\n",
      "Iteration 81, loss = 1.10699117\n",
      "Iteration 82, loss = 1.10642797\n",
      "Iteration 83, loss = 1.10589556\n",
      "Iteration 84, loss = 1.10539277\n",
      "Iteration 85, loss = 1.10491840\n",
      "Iteration 86, loss = 1.10447132\n",
      "Iteration 87, loss = 1.10405037\n",
      "Iteration 88, loss = 1.10365445\n",
      "Iteration 89, loss = 1.10328245\n",
      "Iteration 90, loss = 1.10293330\n",
      "Iteration 91, loss = 1.10260595\n",
      "Iteration 92, loss = 1.10229936\n",
      "Iteration 93, loss = 1.10201253\n",
      "Iteration 94, loss = 1.10174448\n",
      "Iteration 95, loss = 1.10149428\n",
      "Iteration 96, loss = 1.10126098\n",
      "Iteration 97, loss = 1.10104370\n",
      "Iteration 98, loss = 1.10084157\n",
      "Iteration 99, loss = 1.10065377\n",
      "Iteration 100, loss = 1.10047947\n",
      "Iteration 101, loss = 1.10031790\n",
      "Iteration 102, loss = 1.10016832\n",
      "Iteration 103, loss = 1.10003001\n",
      "Iteration 104, loss = 1.09990227\n",
      "Iteration 105, loss = 1.09978446\n",
      "Iteration 106, loss = 1.09967593\n",
      "Iteration 107, loss = 1.09957610\n",
      "Iteration 108, loss = 1.09948438\n",
      "Iteration 109, loss = 1.09940023\n",
      "Iteration 110, loss = 1.09932313\n",
      "Iteration 111, loss = 1.09925259\n",
      "Iteration 112, loss = 1.09918815\n",
      "Iteration 113, loss = 1.09912935\n",
      "Iteration 114, loss = 1.09907580\n",
      "Iteration 115, loss = 1.09902709\n",
      "Iteration 116, loss = 1.09898285\n",
      "Iteration 117, loss = 1.09894274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33009363\n",
      "Iteration 2, loss = 1.32112651\n",
      "Iteration 3, loss = 1.31409996\n",
      "Iteration 4, loss = 1.30868520\n",
      "Iteration 5, loss = 1.30395545\n",
      "Iteration 6, loss = 1.29927774\n",
      "Iteration 7, loss = 1.29463902\n",
      "Iteration 8, loss = 1.29004066\n",
      "Iteration 9, loss = 1.28548393\n",
      "Iteration 10, loss = 1.28097006\n",
      "Iteration 11, loss = 1.27650027\n",
      "Iteration 12, loss = 1.27207574\n",
      "Iteration 13, loss = 1.26769763\n",
      "Iteration 14, loss = 1.26336708\n",
      "Iteration 15, loss = 1.25908521\n",
      "Iteration 16, loss = 1.25485312\n",
      "Iteration 17, loss = 1.25067187\n",
      "Iteration 18, loss = 1.24654252\n",
      "Iteration 19, loss = 1.24246608\n",
      "Iteration 20, loss = 1.23844355\n",
      "Iteration 21, loss = 1.23447589\n",
      "Iteration 22, loss = 1.23056402\n",
      "Iteration 23, loss = 1.22670884\n",
      "Iteration 24, loss = 1.22291121\n",
      "Iteration 25, loss = 1.21917196\n",
      "Iteration 26, loss = 1.21549185\n",
      "Iteration 27, loss = 1.21187165\n",
      "Iteration 28, loss = 1.20831204\n",
      "Iteration 29, loss = 1.20481370\n",
      "Iteration 30, loss = 1.20137722\n",
      "Iteration 31, loss = 1.19800319\n",
      "Iteration 32, loss = 1.19469213\n",
      "Iteration 33, loss = 1.19144450\n",
      "Iteration 34, loss = 1.18826074\n",
      "Iteration 35, loss = 1.18514122\n",
      "Iteration 36, loss = 1.18208628\n",
      "Iteration 37, loss = 1.17909618\n",
      "Iteration 38, loss = 1.17617115\n",
      "Iteration 39, loss = 1.17331137\n",
      "Iteration 40, loss = 1.17051695\n",
      "Iteration 41, loss = 1.16778797\n",
      "Iteration 42, loss = 1.16512443\n",
      "Iteration 43, loss = 1.16252631\n",
      "Iteration 44, loss = 1.15999350\n",
      "Iteration 45, loss = 1.15752586\n",
      "Iteration 46, loss = 1.15512320\n",
      "Iteration 47, loss = 1.15278526\n",
      "Iteration 48, loss = 1.15051175\n",
      "Iteration 49, loss = 1.14830230\n",
      "Iteration 50, loss = 1.14615651\n",
      "Iteration 51, loss = 1.14407392\n",
      "Iteration 52, loss = 1.14205403\n",
      "Iteration 53, loss = 1.14009628\n",
      "Iteration 54, loss = 1.13820007\n",
      "Iteration 55, loss = 1.13636473\n",
      "Iteration 56, loss = 1.13458959\n",
      "Iteration 57, loss = 1.13287389\n",
      "Iteration 58, loss = 1.13121685\n",
      "Iteration 59, loss = 1.12961764\n",
      "Iteration 60, loss = 1.12807541\n",
      "Iteration 61, loss = 1.12658923\n",
      "Iteration 62, loss = 1.12515818\n",
      "Iteration 63, loss = 1.12378128\n",
      "Iteration 64, loss = 1.12245753\n",
      "Iteration 65, loss = 1.12118587\n",
      "Iteration 66, loss = 1.11996526\n",
      "Iteration 67, loss = 1.11879458\n",
      "Iteration 68, loss = 1.11767274\n",
      "Iteration 69, loss = 1.11659859\n",
      "Iteration 70, loss = 1.11557097\n",
      "Iteration 71, loss = 1.11458871\n",
      "Iteration 72, loss = 1.11365062\n",
      "Iteration 73, loss = 1.11275550\n",
      "Iteration 74, loss = 1.11190213\n",
      "Iteration 75, loss = 1.11108930\n",
      "Iteration 76, loss = 1.11031579\n",
      "Iteration 77, loss = 1.10958036\n",
      "Iteration 78, loss = 1.10888179\n",
      "Iteration 79, loss = 1.10821886\n",
      "Iteration 80, loss = 1.10759034\n",
      "Iteration 81, loss = 1.10699501\n",
      "Iteration 82, loss = 1.10643168\n",
      "Iteration 83, loss = 1.10589913\n",
      "Iteration 84, loss = 1.10539619\n",
      "Iteration 85, loss = 1.10492169\n",
      "Iteration 86, loss = 1.10447447\n",
      "Iteration 87, loss = 1.10405339\n",
      "Iteration 88, loss = 1.10365734\n",
      "Iteration 89, loss = 1.10328521\n",
      "Iteration 90, loss = 1.10293594\n",
      "Iteration 91, loss = 1.10260846\n",
      "Iteration 92, loss = 1.10230174\n",
      "Iteration 93, loss = 1.10201480\n",
      "Iteration 94, loss = 1.10174664\n",
      "Iteration 95, loss = 1.10149632\n",
      "Iteration 96, loss = 1.10126291\n",
      "Iteration 97, loss = 1.10104553\n",
      "Iteration 98, loss = 1.10084330\n",
      "Iteration 99, loss = 1.10065539\n",
      "Iteration 100, loss = 1.10048100\n",
      "Iteration 101, loss = 1.10031934\n",
      "Iteration 102, loss = 1.10016967\n",
      "Iteration 103, loss = 1.10003127\n",
      "Iteration 104, loss = 1.09990346\n",
      "Iteration 105, loss = 1.09978557\n",
      "Iteration 106, loss = 1.09967697\n",
      "Iteration 107, loss = 1.09957707\n",
      "Iteration 108, loss = 1.09948528\n",
      "Iteration 109, loss = 1.09940107\n",
      "Iteration 110, loss = 1.09932391\n",
      "Iteration 111, loss = 1.09925331\n",
      "Iteration 112, loss = 1.09918882\n",
      "Iteration 113, loss = 1.09912997\n",
      "Iteration 114, loss = 1.09907637\n",
      "Iteration 115, loss = 1.09902761\n",
      "Iteration 116, loss = 1.09898333\n",
      "Iteration 117, loss = 1.09894318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33048782\n",
      "Iteration 2, loss = 1.32768475\n",
      "Iteration 3, loss = 1.32400232\n",
      "Iteration 4, loss = 1.31961372\n",
      "Iteration 5, loss = 1.31483861\n",
      "Iteration 6, loss = 1.31007639\n",
      "Iteration 7, loss = 1.30519260\n",
      "Iteration 8, loss = 1.30029261\n",
      "Iteration 9, loss = 1.29518199\n",
      "Iteration 10, loss = 1.28985537\n",
      "Iteration 11, loss = 1.28433027\n",
      "Iteration 12, loss = 1.27864667\n",
      "Iteration 13, loss = 1.27284198\n",
      "Iteration 14, loss = 1.26695027\n",
      "Iteration 15, loss = 1.26100257\n",
      "Iteration 16, loss = 1.25502703\n",
      "Iteration 17, loss = 1.24904912\n",
      "Iteration 18, loss = 1.24309184\n",
      "Iteration 19, loss = 1.23717589\n",
      "Iteration 20, loss = 1.23131982\n",
      "Iteration 21, loss = 1.22554017\n",
      "Iteration 22, loss = 1.21985165\n",
      "Iteration 23, loss = 1.21426723\n",
      "Iteration 24, loss = 1.20879828\n",
      "Iteration 25, loss = 1.20345467\n",
      "Iteration 26, loss = 1.19825456\n",
      "Iteration 27, loss = 1.19320680\n",
      "Iteration 28, loss = 1.18830588\n",
      "Iteration 29, loss = 1.18355678\n",
      "Iteration 30, loss = 1.17896342\n",
      "Iteration 31, loss = 1.17452877\n",
      "Iteration 32, loss = 1.17025490\n",
      "Iteration 33, loss = 1.16614304\n",
      "Iteration 34, loss = 1.16219368\n",
      "Iteration 35, loss = 1.15840661\n",
      "Iteration 36, loss = 1.15478097\n",
      "Iteration 37, loss = 1.15131534\n",
      "Iteration 38, loss = 1.14800776\n",
      "Iteration 39, loss = 1.14485583\n",
      "Iteration 40, loss = 1.14185671\n",
      "Iteration 41, loss = 1.13900718\n",
      "Iteration 42, loss = 1.13630371\n",
      "Iteration 43, loss = 1.13374250\n",
      "Iteration 44, loss = 1.13131947\n",
      "Iteration 45, loss = 1.12903039\n",
      "Iteration 46, loss = 1.12687083\n",
      "Iteration 47, loss = 1.12483623\n",
      "Iteration 48, loss = 1.12292197\n",
      "Iteration 49, loss = 1.12112331\n",
      "Iteration 50, loss = 1.11943552\n",
      "Iteration 51, loss = 1.11785384\n",
      "Iteration 52, loss = 1.11637350\n",
      "Iteration 53, loss = 1.11498980\n",
      "Iteration 54, loss = 1.11369806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 55, loss = 1.11249370\n",
      "Iteration 56, loss = 1.11137220\n",
      "Iteration 57, loss = 1.11032915\n",
      "Iteration 58, loss = 1.10936025\n",
      "Iteration 59, loss = 1.10846132\n",
      "Iteration 60, loss = 1.10762832\n",
      "Iteration 61, loss = 1.10685732\n",
      "Iteration 62, loss = 1.10614456\n",
      "Iteration 63, loss = 1.10548641\n",
      "Iteration 64, loss = 1.10487940\n",
      "Iteration 65, loss = 1.10432020\n",
      "Iteration 66, loss = 1.10380564\n",
      "Iteration 67, loss = 1.10333269\n",
      "Iteration 68, loss = 1.10289849\n",
      "Iteration 69, loss = 1.10250030\n",
      "Iteration 70, loss = 1.10213555\n",
      "Iteration 71, loss = 1.10180180\n",
      "Iteration 72, loss = 1.10149676\n",
      "Iteration 73, loss = 1.10121826\n",
      "Iteration 74, loss = 1.10096428\n",
      "Iteration 75, loss = 1.10073291\n",
      "Iteration 76, loss = 1.10052236\n",
      "Iteration 77, loss = 1.10033098\n",
      "Iteration 78, loss = 1.10015720\n",
      "Iteration 79, loss = 1.09999957\n",
      "Iteration 80, loss = 1.09985676\n",
      "Iteration 81, loss = 1.09972750\n",
      "Iteration 82, loss = 1.09961064\n",
      "Iteration 83, loss = 1.09950510\n",
      "Iteration 84, loss = 1.09940989\n",
      "Iteration 85, loss = 1.09932409\n",
      "Iteration 86, loss = 1.09924686\n",
      "Iteration 87, loss = 1.09917740\n",
      "Iteration 88, loss = 1.09911502\n",
      "Iteration 89, loss = 1.09905905\n",
      "Iteration 90, loss = 1.09900889\n",
      "Iteration 91, loss = 1.09896398\n",
      "Iteration 92, loss = 1.09892382\n",
      "Iteration 93, loss = 1.09888795\n",
      "Iteration 94, loss = 1.09885594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33104251\n",
      "Iteration 2, loss = 1.32750745\n",
      "Iteration 3, loss = 1.32329860\n",
      "Iteration 4, loss = 1.31874354\n",
      "Iteration 5, loss = 1.31422148\n",
      "Iteration 6, loss = 1.30961595\n",
      "Iteration 7, loss = 1.30497395\n",
      "Iteration 8, loss = 1.30015367\n",
      "Iteration 9, loss = 1.29506765\n",
      "Iteration 10, loss = 1.28973815\n",
      "Iteration 11, loss = 1.28420986\n",
      "Iteration 12, loss = 1.27852369\n",
      "Iteration 13, loss = 1.27271701\n",
      "Iteration 14, loss = 1.26682388\n",
      "Iteration 15, loss = 1.26087527\n",
      "Iteration 16, loss = 1.25489930\n",
      "Iteration 17, loss = 1.24892142\n",
      "Iteration 18, loss = 1.24296458\n",
      "Iteration 19, loss = 1.23704946\n",
      "Iteration 20, loss = 1.23119456\n",
      "Iteration 21, loss = 1.22541641\n",
      "Iteration 22, loss = 1.21972967\n",
      "Iteration 23, loss = 1.21414728\n",
      "Iteration 24, loss = 1.20868059\n",
      "Iteration 25, loss = 1.20334068\n",
      "Iteration 26, loss = 1.19815422\n",
      "Iteration 27, loss = 1.19310872\n",
      "Iteration 28, loss = 1.18821024\n",
      "Iteration 29, loss = 1.18346373\n",
      "Iteration 30, loss = 1.17887311\n",
      "Iteration 31, loss = 1.17444130\n",
      "Iteration 32, loss = 1.17017036\n",
      "Iteration 33, loss = 1.16606149\n",
      "Iteration 34, loss = 1.16211517\n",
      "Iteration 35, loss = 1.15833116\n",
      "Iteration 36, loss = 1.15470860\n",
      "Iteration 37, loss = 1.15124604\n",
      "Iteration 38, loss = 1.14794152\n",
      "Iteration 39, loss = 1.14479262\n",
      "Iteration 40, loss = 1.14179648\n",
      "Iteration 41, loss = 1.13894989\n",
      "Iteration 42, loss = 1.13624930\n",
      "Iteration 43, loss = 1.13369089\n",
      "Iteration 44, loss = 1.13127061\n",
      "Iteration 45, loss = 1.12898419\n",
      "Iteration 46, loss = 1.12682721\n",
      "Iteration 47, loss = 1.12479512\n",
      "Iteration 48, loss = 1.12288326\n",
      "Iteration 49, loss = 1.12108693\n",
      "Iteration 50, loss = 1.11940136\n",
      "Iteration 51, loss = 1.11782181\n",
      "Iteration 52, loss = 1.11634352\n",
      "Iteration 53, loss = 1.11496176\n",
      "Iteration 54, loss = 1.11367189\n",
      "Iteration 55, loss = 1.11246929\n",
      "Iteration 56, loss = 1.11134946\n",
      "Iteration 57, loss = 1.11030800\n",
      "Iteration 58, loss = 1.10934060\n",
      "Iteration 59, loss = 1.10844309\n",
      "Iteration 60, loss = 1.10761142\n",
      "Iteration 61, loss = 1.10684168\n",
      "Iteration 62, loss = 1.10613010\n",
      "Iteration 63, loss = 1.10547306\n",
      "Iteration 64, loss = 1.10486709\n",
      "Iteration 65, loss = 1.10430886\n",
      "Iteration 66, loss = 1.10379520\n",
      "Iteration 67, loss = 1.10332310\n",
      "Iteration 68, loss = 1.10288968\n",
      "Iteration 69, loss = 1.10249222\n",
      "Iteration 70, loss = 1.10212815\n",
      "Iteration 71, loss = 1.10179503\n",
      "Iteration 72, loss = 1.10149057\n",
      "Iteration 73, loss = 1.10121262\n",
      "Iteration 74, loss = 1.10095913\n",
      "Iteration 75, loss = 1.10072822\n",
      "Iteration 76, loss = 1.10051810\n",
      "Iteration 77, loss = 1.10032710\n",
      "Iteration 78, loss = 1.10015368\n",
      "Iteration 79, loss = 1.09999638\n",
      "Iteration 80, loss = 1.09985387\n",
      "Iteration 81, loss = 1.09972488\n",
      "Iteration 82, loss = 1.09960827\n",
      "Iteration 83, loss = 1.09950297\n",
      "Iteration 84, loss = 1.09940796\n",
      "Iteration 85, loss = 1.09932236\n",
      "Iteration 86, loss = 1.09924529\n",
      "Iteration 87, loss = 1.09917600\n",
      "Iteration 88, loss = 1.09911376\n",
      "Iteration 89, loss = 1.09905792\n",
      "Iteration 90, loss = 1.09900788\n",
      "Iteration 91, loss = 1.09896307\n",
      "Iteration 92, loss = 1.09892301\n",
      "Iteration 93, loss = 1.09888723\n",
      "Iteration 94, loss = 1.09885530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33079619\n",
      "Iteration 2, loss = 1.32799192\n",
      "Iteration 3, loss = 1.32424009\n",
      "Iteration 4, loss = 1.31972517\n",
      "Iteration 5, loss = 1.31491887\n",
      "Iteration 6, loss = 1.31002714\n",
      "Iteration 7, loss = 1.30517424\n",
      "Iteration 8, loss = 1.30027594\n",
      "Iteration 9, loss = 1.29516972\n",
      "Iteration 10, loss = 1.28984311\n",
      "Iteration 11, loss = 1.28431712\n",
      "Iteration 12, loss = 1.27863266\n",
      "Iteration 13, loss = 1.27282710\n",
      "Iteration 14, loss = 1.26693454\n",
      "Iteration 15, loss = 1.26098597\n",
      "Iteration 16, loss = 1.25500956\n",
      "Iteration 17, loss = 1.24903076\n",
      "Iteration 18, loss = 1.24307259\n",
      "Iteration 19, loss = 1.23715574\n",
      "Iteration 20, loss = 1.23129876\n",
      "Iteration 21, loss = 1.22551820\n",
      "Iteration 22, loss = 1.21982876\n",
      "Iteration 23, loss = 1.21424343\n",
      "Iteration 24, loss = 1.20877356\n",
      "Iteration 25, loss = 1.20344688\n",
      "Iteration 26, loss = 1.19825799\n",
      "Iteration 27, loss = 1.19320986\n",
      "Iteration 28, loss = 1.18830861\n",
      "Iteration 29, loss = 1.18355921\n",
      "Iteration 30, loss = 1.17896558\n",
      "Iteration 31, loss = 1.17453069\n",
      "Iteration 32, loss = 1.17025660\n",
      "Iteration 33, loss = 1.16614454\n",
      "Iteration 34, loss = 1.16219501\n",
      "Iteration 35, loss = 1.15840777\n",
      "Iteration 36, loss = 1.15478199\n",
      "Iteration 37, loss = 1.15131623\n",
      "Iteration 38, loss = 1.14800854\n",
      "Iteration 39, loss = 1.14485651\n",
      "Iteration 40, loss = 1.14185729\n",
      "Iteration 41, loss = 1.13900768\n",
      "Iteration 42, loss = 1.13630415\n",
      "Iteration 43, loss = 1.13374287\n",
      "Iteration 44, loss = 1.13131979\n",
      "Iteration 45, loss = 1.12903065\n",
      "Iteration 46, loss = 1.12687104\n",
      "Iteration 47, loss = 1.12483641\n",
      "Iteration 48, loss = 1.12292211\n",
      "Iteration 49, loss = 1.12112343\n",
      "Iteration 50, loss = 1.11943562\n",
      "Iteration 51, loss = 1.11785391\n",
      "Iteration 52, loss = 1.11637355\n",
      "Iteration 53, loss = 1.11498983\n",
      "Iteration 54, loss = 1.11369809\n",
      "Iteration 55, loss = 1.11249371\n",
      "Iteration 56, loss = 1.11137220\n",
      "Iteration 57, loss = 1.11032914\n",
      "Iteration 58, loss = 1.10936024\n",
      "Iteration 59, loss = 1.10846130\n",
      "Iteration 60, loss = 1.10762829\n",
      "Iteration 61, loss = 1.10685729\n",
      "Iteration 62, loss = 1.10614453\n",
      "Iteration 63, loss = 1.10548638\n",
      "Iteration 64, loss = 1.10487937\n",
      "Iteration 65, loss = 1.10432017\n",
      "Iteration 66, loss = 1.10380561\n",
      "Iteration 67, loss = 1.10333266\n",
      "Iteration 68, loss = 1.10289845\n",
      "Iteration 69, loss = 1.10250027\n",
      "Iteration 70, loss = 1.10213552\n",
      "Iteration 71, loss = 1.10180177\n",
      "Iteration 72, loss = 1.10149673\n",
      "Iteration 73, loss = 1.10121824\n",
      "Iteration 74, loss = 1.10096425\n",
      "Iteration 75, loss = 1.10073288\n",
      "Iteration 76, loss = 1.10052234\n",
      "Iteration 77, loss = 1.10033095\n",
      "Iteration 78, loss = 1.10015718\n",
      "Iteration 79, loss = 1.09999955\n",
      "Iteration 80, loss = 1.09985674\n",
      "Iteration 81, loss = 1.09972748\n",
      "Iteration 82, loss = 1.09961062\n",
      "Iteration 83, loss = 1.09950509\n",
      "Iteration 84, loss = 1.09940988\n",
      "Iteration 85, loss = 1.09932408\n",
      "Iteration 86, loss = 1.09924684\n",
      "Iteration 87, loss = 1.09917740\n",
      "Iteration 88, loss = 1.09911501\n",
      "Iteration 89, loss = 1.09905904\n",
      "Iteration 90, loss = 1.09900888\n",
      "Iteration 91, loss = 1.09896397\n",
      "Iteration 92, loss = 1.09892381\n",
      "Iteration 93, loss = 1.09888794\n",
      "Iteration 94, loss = 1.09885594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32935999\n",
      "Iteration 2, loss = 1.32676673\n",
      "Iteration 3, loss = 1.32336853\n",
      "Iteration 4, loss = 1.31936685\n",
      "Iteration 5, loss = 1.31492794\n",
      "Iteration 6, loss = 1.31034216\n",
      "Iteration 7, loss = 1.30550176\n",
      "Iteration 8, loss = 1.30043638\n",
      "Iteration 9, loss = 1.29523389\n",
      "Iteration 10, loss = 1.28988359\n",
      "Iteration 11, loss = 1.28435681\n",
      "Iteration 12, loss = 1.27867506\n",
      "Iteration 13, loss = 1.27287239\n",
      "Iteration 14, loss = 1.26698289\n",
      "Iteration 15, loss = 1.26103755\n",
      "Iteration 16, loss = 1.25506453\n",
      "Iteration 17, loss = 1.24908927\n",
      "Iteration 18, loss = 1.24313477\n",
      "Iteration 19, loss = 1.23722171\n",
      "Iteration 20, loss = 1.23136861\n",
      "Iteration 21, loss = 1.22559200\n",
      "Iteration 22, loss = 1.21990658\n",
      "Iteration 23, loss = 1.21432530\n",
      "Iteration 24, loss = 1.20885951\n",
      "Iteration 25, loss = 1.20351907\n",
      "Iteration 26, loss = 1.19831245\n",
      "Iteration 27, loss = 1.19324683\n",
      "Iteration 28, loss = 1.18832820\n",
      "Iteration 29, loss = 1.18356141\n",
      "Iteration 30, loss = 1.17895031\n",
      "Iteration 31, loss = 1.17451304\n",
      "Iteration 32, loss = 1.17024051\n",
      "Iteration 33, loss = 1.16612988\n",
      "Iteration 34, loss = 1.16218164\n",
      "Iteration 35, loss = 1.15839560\n",
      "Iteration 36, loss = 1.15477091\n",
      "Iteration 37, loss = 1.15130614\n",
      "Iteration 38, loss = 1.14799937\n",
      "Iteration 39, loss = 1.14484817\n",
      "Iteration 40, loss = 1.14184972\n",
      "Iteration 41, loss = 1.13900080\n",
      "Iteration 42, loss = 1.13629790\n",
      "Iteration 43, loss = 1.13373720\n",
      "Iteration 44, loss = 1.13131465\n",
      "Iteration 45, loss = 1.12902600\n",
      "Iteration 46, loss = 1.12686683\n",
      "Iteration 47, loss = 1.12483260\n",
      "Iteration 48, loss = 1.12291867\n",
      "Iteration 49, loss = 1.12112032\n",
      "Iteration 50, loss = 1.11943281\n",
      "Iteration 51, loss = 1.11785137\n",
      "Iteration 52, loss = 1.11637127\n",
      "Iteration 53, loss = 1.11498778\n",
      "Iteration 54, loss = 1.11369624\n",
      "Iteration 55, loss = 1.11249205\n",
      "Iteration 56, loss = 1.11137071\n",
      "Iteration 57, loss = 1.11032780\n",
      "Iteration 58, loss = 1.10935904\n",
      "Iteration 59, loss = 1.10846023\n",
      "Iteration 60, loss = 1.10762733\n",
      "Iteration 61, loss = 1.10685643\n",
      "Iteration 62, loss = 1.10614376\n",
      "Iteration 63, loss = 1.10548570\n",
      "Iteration 64, loss = 1.10487876\n",
      "Iteration 65, loss = 1.10431963\n",
      "Iteration 66, loss = 1.10380513\n",
      "Iteration 67, loss = 1.10333223\n",
      "Iteration 68, loss = 1.10289808\n",
      "Iteration 69, loss = 1.10249993\n",
      "Iteration 70, loss = 1.10213522\n",
      "Iteration 71, loss = 1.10180151\n",
      "Iteration 72, loss = 1.10149650\n",
      "Iteration 73, loss = 1.10121803\n",
      "Iteration 74, loss = 1.10096408\n",
      "Iteration 75, loss = 1.10073273\n",
      "Iteration 76, loss = 1.10052220\n",
      "Iteration 77, loss = 1.10033083\n",
      "Iteration 78, loss = 1.10015707\n",
      "Iteration 79, loss = 1.09999946\n",
      "Iteration 80, loss = 1.09985666\n",
      "Iteration 81, loss = 1.09972741\n",
      "Iteration 82, loss = 1.09961056\n",
      "Iteration 83, loss = 1.09950504\n",
      "Iteration 84, loss = 1.09940983\n",
      "Iteration 85, loss = 1.09932404\n",
      "Iteration 86, loss = 1.09924681\n",
      "Iteration 87, loss = 1.09917737\n",
      "Iteration 88, loss = 1.09911499\n",
      "Iteration 89, loss = 1.09905902\n",
      "Iteration 90, loss = 1.09900886\n",
      "Iteration 91, loss = 1.09896396\n",
      "Iteration 92, loss = 1.09892380\n",
      "Iteration 93, loss = 1.09888793\n",
      "Iteration 94, loss = 1.09885593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33009363\n",
      "Iteration 2, loss = 1.32720263\n",
      "Iteration 3, loss = 1.32334112\n",
      "Iteration 4, loss = 1.31884711\n",
      "Iteration 5, loss = 1.31406810\n",
      "Iteration 6, loss = 1.30928200\n",
      "Iteration 7, loss = 1.30449490\n",
      "Iteration 8, loss = 1.29964161\n",
      "Iteration 9, loss = 1.29458932\n",
      "Iteration 10, loss = 1.28931082\n",
      "Iteration 11, loss = 1.28382769\n",
      "Iteration 12, loss = 1.27818179\n",
      "Iteration 13, loss = 1.27241129\n",
      "Iteration 14, loss = 1.26655093\n",
      "Iteration 15, loss = 1.26063224\n",
      "Iteration 16, loss = 1.25468385\n",
      "Iteration 17, loss = 1.24873160\n",
      "Iteration 18, loss = 1.24279883\n",
      "Iteration 19, loss = 1.23690650\n",
      "Iteration 20, loss = 1.23107339\n",
      "Iteration 21, loss = 1.22531624\n",
      "Iteration 22, loss = 1.21964992\n",
      "Iteration 23, loss = 1.21408755\n",
      "Iteration 24, loss = 1.20864059\n",
      "Iteration 25, loss = 1.20331904\n",
      "Iteration 26, loss = 1.19813144\n",
      "Iteration 27, loss = 1.19308507\n",
      "Iteration 28, loss = 1.18818596\n",
      "Iteration 29, loss = 1.18343905\n",
      "Iteration 30, loss = 1.17884821\n",
      "Iteration 31, loss = 1.17441635\n",
      "Iteration 32, loss = 1.17014550\n",
      "Iteration 33, loss = 1.16603686\n",
      "Iteration 34, loss = 1.16209088\n",
      "Iteration 35, loss = 1.15830731\n",
      "Iteration 36, loss = 1.15468526\n",
      "Iteration 37, loss = 1.15122329\n",
      "Iteration 38, loss = 1.14791942\n",
      "Iteration 39, loss = 1.14477121\n",
      "Iteration 40, loss = 1.14177580\n",
      "Iteration 41, loss = 1.13892996\n",
      "Iteration 42, loss = 1.13623015\n",
      "Iteration 43, loss = 1.13367254\n",
      "Iteration 44, loss = 1.13125306\n",
      "Iteration 45, loss = 1.12896745\n",
      "Iteration 46, loss = 1.12681126\n",
      "Iteration 47, loss = 1.12477996\n",
      "Iteration 48, loss = 1.12286888\n",
      "Iteration 49, loss = 1.12107331\n",
      "Iteration 50, loss = 1.11938850\n",
      "Iteration 51, loss = 1.11780967\n",
      "Iteration 52, loss = 1.11633208\n",
      "Iteration 53, loss = 1.11495101\n",
      "Iteration 54, loss = 1.11366179\n",
      "Iteration 55, loss = 1.11245983\n",
      "Iteration 56, loss = 1.11134061\n",
      "Iteration 57, loss = 1.11029973\n",
      "Iteration 58, loss = 1.10933288\n",
      "Iteration 59, loss = 1.10843590\n",
      "Iteration 60, loss = 1.10760473\n",
      "Iteration 61, loss = 1.10683546\n",
      "Iteration 62, loss = 1.10612433\n",
      "Iteration 63, loss = 1.10546771\n",
      "Iteration 64, loss = 1.10486214\n",
      "Iteration 65, loss = 1.10430428\n",
      "Iteration 66, loss = 1.10379098\n",
      "Iteration 67, loss = 1.10331921\n",
      "Iteration 68, loss = 1.10288610\n",
      "Iteration 69, loss = 1.10248893\n",
      "Iteration 70, loss = 1.10212512\n",
      "Iteration 71, loss = 1.10179226\n",
      "Iteration 72, loss = 1.10148803\n",
      "Iteration 73, loss = 1.10121029\n",
      "Iteration 74, loss = 1.10095700\n",
      "Iteration 75, loss = 1.10072628\n",
      "Iteration 76, loss = 1.10051632\n",
      "Iteration 77, loss = 1.10032548\n",
      "Iteration 78, loss = 1.10015221\n",
      "Iteration 79, loss = 1.09999504\n",
      "Iteration 80, loss = 1.09985265\n",
      "Iteration 81, loss = 1.09972378\n",
      "Iteration 82, loss = 1.09960728\n",
      "Iteration 83, loss = 1.09950206\n",
      "Iteration 84, loss = 1.09940715\n",
      "Iteration 85, loss = 1.09932162\n",
      "Iteration 86, loss = 1.09924463\n",
      "Iteration 87, loss = 1.09917540\n",
      "Iteration 88, loss = 1.09911323\n",
      "Iteration 89, loss = 1.09905744\n",
      "Iteration 90, loss = 1.09900744\n",
      "Iteration 91, loss = 1.09896268\n",
      "Iteration 92, loss = 1.09892266\n",
      "Iteration 93, loss = 1.09888691\n",
      "Iteration 94, loss = 1.09885502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33048782\n",
      "Iteration 2, loss = 1.32956054\n",
      "Iteration 3, loss = 1.32867055\n",
      "Iteration 4, loss = 1.32780807\n",
      "Iteration 5, loss = 1.32691784\n",
      "Iteration 6, loss = 1.32598522\n",
      "Iteration 7, loss = 1.32508927\n",
      "Iteration 8, loss = 1.32422705\n",
      "Iteration 9, loss = 1.32338644\n",
      "Iteration 10, loss = 1.32255517\n",
      "Iteration 11, loss = 1.32175281\n",
      "Iteration 12, loss = 1.32096434\n",
      "Iteration 13, loss = 1.32018654\n",
      "Iteration 14, loss = 1.31942403\n",
      "Iteration 15, loss = 1.31868850\n",
      "Iteration 16, loss = 1.31799728\n",
      "Iteration 17, loss = 1.31732966\n",
      "Iteration 18, loss = 1.31667566\n",
      "Iteration 19, loss = 1.31604191\n",
      "Iteration 20, loss = 1.31542499\n",
      "Iteration 21, loss = 1.31482117\n",
      "Iteration 22, loss = 1.31422341\n",
      "Iteration 23, loss = 1.31363049\n",
      "Iteration 24, loss = 1.31304932\n",
      "Iteration 25, loss = 1.31247946\n",
      "Iteration 26, loss = 1.31193313\n",
      "Iteration 27, loss = 1.31140067\n",
      "Iteration 28, loss = 1.31087168\n",
      "Iteration 29, loss = 1.31035099\n",
      "Iteration 30, loss = 1.30983607\n",
      "Iteration 31, loss = 1.30932374\n",
      "Iteration 32, loss = 1.30881448\n",
      "Iteration 33, loss = 1.30831373\n",
      "Iteration 34, loss = 1.30781532\n",
      "Iteration 35, loss = 1.30732547\n",
      "Iteration 36, loss = 1.30684204\n",
      "Iteration 37, loss = 1.30635989\n",
      "Iteration 38, loss = 1.30587893\n",
      "Iteration 39, loss = 1.30539910\n",
      "Iteration 40, loss = 1.30492311\n",
      "Iteration 41, loss = 1.30444904\n",
      "Iteration 42, loss = 1.30397586\n",
      "Iteration 43, loss = 1.30350354\n",
      "Iteration 44, loss = 1.30303205\n",
      "Iteration 45, loss = 1.30256138\n",
      "Iteration 46, loss = 1.30209149\n",
      "Iteration 47, loss = 1.30162236\n",
      "Iteration 48, loss = 1.30115398\n",
      "Iteration 49, loss = 1.30068632\n",
      "Iteration 50, loss = 1.30021938\n",
      "Iteration 51, loss = 1.29975314\n",
      "Iteration 52, loss = 1.29928758\n",
      "Iteration 53, loss = 1.29882269\n",
      "Iteration 54, loss = 1.29835846\n",
      "Iteration 55, loss = 1.29789488\n",
      "Iteration 56, loss = 1.29743195\n",
      "Iteration 57, loss = 1.29696966\n",
      "Iteration 58, loss = 1.29650799\n",
      "Iteration 59, loss = 1.29604694\n",
      "Iteration 60, loss = 1.29558650\n",
      "Iteration 61, loss = 1.29512668\n",
      "Iteration 62, loss = 1.29466746\n",
      "Iteration 63, loss = 1.29420884\n",
      "Iteration 64, loss = 1.29375082\n",
      "Iteration 65, loss = 1.29329339\n",
      "Iteration 66, loss = 1.29283655\n",
      "Iteration 67, loss = 1.29238029\n",
      "Iteration 68, loss = 1.29192462\n",
      "Iteration 69, loss = 1.29146952\n",
      "Iteration 70, loss = 1.29101501\n",
      "Iteration 71, loss = 1.29056107\n",
      "Iteration 72, loss = 1.29010771\n",
      "Iteration 73, loss = 1.28965492\n",
      "Iteration 74, loss = 1.28920271\n",
      "Iteration 75, loss = 1.28875106\n",
      "Iteration 76, loss = 1.28829998\n",
      "Iteration 77, loss = 1.28784948\n",
      "Iteration 78, loss = 1.28739954\n",
      "Iteration 79, loss = 1.28695016\n",
      "Iteration 80, loss = 1.28650136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 81, loss = 1.28605312\n",
      "Iteration 82, loss = 1.28560544\n",
      "Iteration 83, loss = 1.28515833\n",
      "Iteration 84, loss = 1.28471178\n",
      "Iteration 85, loss = 1.28426580\n",
      "Iteration 86, loss = 1.28382038\n",
      "Iteration 87, loss = 1.28337553\n",
      "Iteration 88, loss = 1.28293124\n",
      "Iteration 89, loss = 1.28248751\n",
      "Iteration 90, loss = 1.28204435\n",
      "Iteration 91, loss = 1.28160175\n",
      "Iteration 92, loss = 1.28115971\n",
      "Iteration 93, loss = 1.28071824\n",
      "Iteration 94, loss = 1.28027733\n",
      "Iteration 95, loss = 1.27983699\n",
      "Iteration 96, loss = 1.27939721\n",
      "Iteration 97, loss = 1.27895800\n",
      "Iteration 98, loss = 1.27851935\n",
      "Iteration 99, loss = 1.27808126\n",
      "Iteration 100, loss = 1.27764374\n",
      "Iteration 101, loss = 1.27720678\n",
      "Iteration 102, loss = 1.27677040\n",
      "Iteration 103, loss = 1.27633457\n",
      "Iteration 104, loss = 1.27589931\n",
      "Iteration 105, loss = 1.27546462\n",
      "Iteration 106, loss = 1.27503050\n",
      "Iteration 107, loss = 1.27459694\n",
      "Iteration 108, loss = 1.27416395\n",
      "Iteration 109, loss = 1.27373153\n",
      "Iteration 110, loss = 1.27329968\n",
      "Iteration 111, loss = 1.27286839\n",
      "Iteration 112, loss = 1.27243768\n",
      "Iteration 113, loss = 1.27200753\n",
      "Iteration 114, loss = 1.27157795\n",
      "Iteration 115, loss = 1.27114894\n",
      "Iteration 116, loss = 1.27072051\n",
      "Iteration 117, loss = 1.27029264\n",
      "Iteration 118, loss = 1.26986534\n",
      "Iteration 119, loss = 1.26943862\n",
      "Iteration 120, loss = 1.26901246\n",
      "Iteration 121, loss = 1.26858688\n",
      "Iteration 122, loss = 1.26816187\n",
      "Iteration 123, loss = 1.26773744\n",
      "Iteration 124, loss = 1.26731358\n",
      "Iteration 125, loss = 1.26689029\n",
      "Iteration 126, loss = 1.26646757\n",
      "Iteration 127, loss = 1.26604543\n",
      "Iteration 128, loss = 1.26562386\n",
      "Iteration 129, loss = 1.26520287\n",
      "Iteration 130, loss = 1.26478246\n",
      "Iteration 131, loss = 1.26436262\n",
      "Iteration 132, loss = 1.26394335\n",
      "Iteration 133, loss = 1.26352467\n",
      "Iteration 134, loss = 1.26310656\n",
      "Iteration 135, loss = 1.26268902\n",
      "Iteration 136, loss = 1.26227207\n",
      "Iteration 137, loss = 1.26185569\n",
      "Iteration 138, loss = 1.26143989\n",
      "Iteration 139, loss = 1.26102467\n",
      "Iteration 140, loss = 1.26061003\n",
      "Iteration 141, loss = 1.26019597\n",
      "Iteration 142, loss = 1.25978249\n",
      "Iteration 143, loss = 1.25936959\n",
      "Iteration 144, loss = 1.25895727\n",
      "Iteration 145, loss = 1.25854553\n",
      "Iteration 146, loss = 1.25813438\n",
      "Iteration 147, loss = 1.25772380\n",
      "Iteration 148, loss = 1.25731381\n",
      "Iteration 149, loss = 1.25690440\n",
      "Iteration 150, loss = 1.25649557\n",
      "Iteration 151, loss = 1.25608733\n",
      "Iteration 152, loss = 1.25567966\n",
      "Iteration 153, loss = 1.25527259\n",
      "Iteration 154, loss = 1.25486609\n",
      "Iteration 155, loss = 1.25446019\n",
      "Iteration 156, loss = 1.25405486\n",
      "Iteration 157, loss = 1.25365012\n",
      "Iteration 158, loss = 1.25324597\n",
      "Iteration 159, loss = 1.25284241\n",
      "Iteration 160, loss = 1.25243943\n",
      "Iteration 161, loss = 1.25203703\n",
      "Iteration 162, loss = 1.25163523\n",
      "Iteration 163, loss = 1.25123401\n",
      "Iteration 164, loss = 1.25083338\n",
      "Iteration 165, loss = 1.25043333\n",
      "Iteration 166, loss = 1.25003388\n",
      "Iteration 167, loss = 1.24963501\n",
      "Iteration 168, loss = 1.24923673\n",
      "Iteration 169, loss = 1.24883905\n",
      "Iteration 170, loss = 1.24844195\n",
      "Iteration 171, loss = 1.24804544\n",
      "Iteration 172, loss = 1.24764952\n",
      "Iteration 173, loss = 1.24725419\n",
      "Iteration 174, loss = 1.24685945\n",
      "Iteration 175, loss = 1.24646531\n",
      "Iteration 176, loss = 1.24607175\n",
      "Iteration 177, loss = 1.24567879\n",
      "Iteration 178, loss = 1.24528642\n",
      "Iteration 179, loss = 1.24489464\n",
      "Iteration 180, loss = 1.24450345\n",
      "Iteration 181, loss = 1.24411286\n",
      "Iteration 182, loss = 1.24372286\n",
      "Iteration 183, loss = 1.24333345\n",
      "Iteration 184, loss = 1.24294464\n",
      "Iteration 185, loss = 1.24255642\n",
      "Iteration 186, loss = 1.24216880\n",
      "Iteration 187, loss = 1.24178177\n",
      "Iteration 188, loss = 1.24139533\n",
      "Iteration 189, loss = 1.24100949\n",
      "Iteration 190, loss = 1.24062425\n",
      "Iteration 191, loss = 1.24023960\n",
      "Iteration 192, loss = 1.23985554\n",
      "Iteration 193, loss = 1.23947209\n",
      "Iteration 194, loss = 1.23908922\n",
      "Iteration 195, loss = 1.23870696\n",
      "Iteration 196, loss = 1.23832529\n",
      "Iteration 197, loss = 1.23794422\n",
      "Iteration 198, loss = 1.23756375\n",
      "Iteration 199, loss = 1.23718387\n",
      "Iteration 200, loss = 1.23680460\n",
      "Iteration 201, loss = 1.23642592\n",
      "Iteration 202, loss = 1.23604784\n",
      "Iteration 203, loss = 1.23567035\n",
      "Iteration 204, loss = 1.23529347\n",
      "Iteration 205, loss = 1.23491718\n",
      "Iteration 206, loss = 1.23454150\n",
      "Iteration 207, loss = 1.23416641\n",
      "Iteration 208, loss = 1.23379192\n",
      "Iteration 209, loss = 1.23341804\n",
      "Iteration 210, loss = 1.23304475\n",
      "Iteration 211, loss = 1.23267206\n",
      "Iteration 212, loss = 1.23229998\n",
      "Iteration 213, loss = 1.23192849\n",
      "Iteration 214, loss = 1.23155761\n",
      "Iteration 215, loss = 1.23118732\n",
      "Iteration 216, loss = 1.23081764\n",
      "Iteration 217, loss = 1.23044856\n",
      "Iteration 218, loss = 1.23008008\n",
      "Iteration 219, loss = 1.22971220\n",
      "Iteration 220, loss = 1.22934493\n",
      "Iteration 221, loss = 1.22897826\n",
      "Iteration 222, loss = 1.22861219\n",
      "Iteration 223, loss = 1.22824672\n",
      "Iteration 224, loss = 1.22788185\n",
      "Iteration 225, loss = 1.22751759\n",
      "Iteration 226, loss = 1.22715393\n",
      "Iteration 227, loss = 1.22679087\n",
      "Iteration 228, loss = 1.22642842\n",
      "Iteration 229, loss = 1.22606657\n",
      "Iteration 230, loss = 1.22570532\n",
      "Iteration 231, loss = 1.22534468\n",
      "Iteration 232, loss = 1.22498464\n",
      "Iteration 233, loss = 1.22462521\n",
      "Iteration 234, loss = 1.22426638\n",
      "Iteration 235, loss = 1.22390815\n",
      "Iteration 236, loss = 1.22355053\n",
      "Iteration 237, loss = 1.22319351\n",
      "Iteration 238, loss = 1.22283710\n",
      "Iteration 239, loss = 1.22248130\n",
      "Iteration 240, loss = 1.22212609\n",
      "Iteration 241, loss = 1.22177150\n",
      "Iteration 242, loss = 1.22141751\n",
      "Iteration 243, loss = 1.22106412\n",
      "Iteration 244, loss = 1.22071134\n",
      "Iteration 245, loss = 1.22035916\n",
      "Iteration 246, loss = 1.22000759\n",
      "Iteration 247, loss = 1.21965663\n",
      "Iteration 248, loss = 1.21930627\n",
      "Iteration 249, loss = 1.21895652\n",
      "Iteration 250, loss = 1.21860737\n",
      "Iteration 251, loss = 1.21825883\n",
      "Iteration 252, loss = 1.21791090\n",
      "Iteration 253, loss = 1.21756357\n",
      "Iteration 254, loss = 1.21721685\n",
      "Iteration 255, loss = 1.21687074\n",
      "Iteration 256, loss = 1.21652523\n",
      "Iteration 257, loss = 1.21618033\n",
      "Iteration 258, loss = 1.21583604\n",
      "Iteration 259, loss = 1.21549235\n",
      "Iteration 260, loss = 1.21514927\n",
      "Iteration 261, loss = 1.21480679\n",
      "Iteration 262, loss = 1.21446493\n",
      "Iteration 263, loss = 1.21412367\n",
      "Iteration 264, loss = 1.21378301\n",
      "Iteration 265, loss = 1.21344297\n",
      "Iteration 266, loss = 1.21310353\n",
      "Iteration 267, loss = 1.21276470\n",
      "Iteration 268, loss = 1.21242647\n",
      "Iteration 269, loss = 1.21208886\n",
      "Iteration 270, loss = 1.21175185\n",
      "Iteration 271, loss = 1.21141544\n",
      "Iteration 272, loss = 1.21107965\n",
      "Iteration 273, loss = 1.21074446\n",
      "Iteration 274, loss = 1.21040988\n",
      "Iteration 275, loss = 1.21007590\n",
      "Iteration 276, loss = 1.20974254\n",
      "Iteration 277, loss = 1.20940978\n",
      "Iteration 278, loss = 1.20907763\n",
      "Iteration 279, loss = 1.20874609\n",
      "Iteration 280, loss = 1.20841515\n",
      "Iteration 281, loss = 1.20808482\n",
      "Iteration 282, loss = 1.20775510\n",
      "Iteration 283, loss = 1.20742598\n",
      "Iteration 284, loss = 1.20709748\n",
      "Iteration 285, loss = 1.20676958\n",
      "Iteration 286, loss = 1.20644228\n",
      "Iteration 287, loss = 1.20611560\n",
      "Iteration 288, loss = 1.20578952\n",
      "Iteration 289, loss = 1.20546405\n",
      "Iteration 290, loss = 1.20513919\n",
      "Iteration 291, loss = 1.20481493\n",
      "Iteration 292, loss = 1.20449128\n",
      "Iteration 293, loss = 1.20416824\n",
      "Iteration 294, loss = 1.20384580\n",
      "Iteration 295, loss = 1.20352397\n",
      "Iteration 296, loss = 1.20320275\n",
      "Iteration 297, loss = 1.20288214\n",
      "Iteration 298, loss = 1.20256213\n",
      "Iteration 299, loss = 1.20224273\n",
      "Iteration 300, loss = 1.20192393\n",
      "Iteration 301, loss = 1.20160575\n",
      "Iteration 302, loss = 1.20128816\n",
      "Iteration 303, loss = 1.20097119\n",
      "Iteration 304, loss = 1.20065482\n",
      "Iteration 305, loss = 1.20033906\n",
      "Iteration 306, loss = 1.20002390\n",
      "Iteration 307, loss = 1.19970935\n",
      "Iteration 308, loss = 1.19939541\n",
      "Iteration 309, loss = 1.19908207\n",
      "Iteration 310, loss = 1.19876934\n",
      "Iteration 311, loss = 1.19845721\n",
      "Iteration 312, loss = 1.19814569\n",
      "Iteration 313, loss = 1.19783477\n",
      "Iteration 314, loss = 1.19752446\n",
      "Iteration 315, loss = 1.19721475\n",
      "Iteration 316, loss = 1.19690565\n",
      "Iteration 317, loss = 1.19659716\n",
      "Iteration 318, loss = 1.19628927\n",
      "Iteration 319, loss = 1.19598198\n",
      "Iteration 320, loss = 1.19567530\n",
      "Iteration 321, loss = 1.19536922\n",
      "Iteration 322, loss = 1.19506375\n",
      "Iteration 323, loss = 1.19475888\n",
      "Iteration 324, loss = 1.19445461\n",
      "Iteration 325, loss = 1.19415095\n",
      "Iteration 326, loss = 1.19384789\n",
      "Iteration 327, loss = 1.19354544\n",
      "Iteration 328, loss = 1.19324359\n",
      "Iteration 329, loss = 1.19294234\n",
      "Iteration 330, loss = 1.19264170\n",
      "Iteration 331, loss = 1.19234165\n",
      "Iteration 332, loss = 1.19204221\n",
      "Iteration 333, loss = 1.19174338\n",
      "Iteration 334, loss = 1.19144514\n",
      "Iteration 335, loss = 1.19114751\n",
      "Iteration 336, loss = 1.19085048\n",
      "Iteration 337, loss = 1.19055405\n",
      "Iteration 338, loss = 1.19025822\n",
      "Iteration 339, loss = 1.18996299\n",
      "Iteration 340, loss = 1.18966837\n",
      "Iteration 341, loss = 1.18937434\n",
      "Iteration 342, loss = 1.18908092\n",
      "Iteration 343, loss = 1.18878809\n",
      "Iteration 344, loss = 1.18849587\n",
      "Iteration 345, loss = 1.18820424\n",
      "Iteration 346, loss = 1.18791322\n",
      "Iteration 347, loss = 1.18762279\n",
      "Iteration 348, loss = 1.18733296\n",
      "Iteration 349, loss = 1.18704374\n",
      "Iteration 350, loss = 1.18675511\n",
      "Iteration 351, loss = 1.18646708\n",
      "Iteration 352, loss = 1.18617964\n",
      "Iteration 353, loss = 1.18589281\n",
      "Iteration 354, loss = 1.18560657\n",
      "Iteration 355, loss = 1.18532093\n",
      "Iteration 356, loss = 1.18503589\n",
      "Iteration 357, loss = 1.18475144\n",
      "Iteration 358, loss = 1.18446759\n",
      "Iteration 359, loss = 1.18418463\n",
      "Iteration 360, loss = 1.18390307\n",
      "Iteration 361, loss = 1.18362210\n",
      "Iteration 362, loss = 1.18334173\n",
      "Iteration 363, loss = 1.18306197\n",
      "Iteration 364, loss = 1.18278280\n",
      "Iteration 365, loss = 1.18250424\n",
      "Iteration 366, loss = 1.18222627\n",
      "Iteration 367, loss = 1.18194890\n",
      "Iteration 368, loss = 1.18167214\n",
      "Iteration 369, loss = 1.18139597\n",
      "Iteration 370, loss = 1.18112041\n",
      "Iteration 371, loss = 1.18084544\n",
      "Iteration 372, loss = 1.18057107\n",
      "Iteration 373, loss = 1.18029730\n",
      "Iteration 374, loss = 1.18002413\n",
      "Iteration 375, loss = 1.17975156\n",
      "Iteration 376, loss = 1.17947959\n",
      "Iteration 377, loss = 1.17920822\n",
      "Iteration 378, loss = 1.17893744\n",
      "Iteration 379, loss = 1.17866726\n",
      "Iteration 380, loss = 1.17839769\n",
      "Iteration 381, loss = 1.17812870\n",
      "Iteration 382, loss = 1.17786032\n",
      "Iteration 383, loss = 1.17759253\n",
      "Iteration 384, loss = 1.17732535\n",
      "Iteration 385, loss = 1.17705875\n",
      "Iteration 386, loss = 1.17679276\n",
      "Iteration 387, loss = 1.17652736\n",
      "Iteration 388, loss = 1.17626256\n",
      "Iteration 389, loss = 1.17599835\n",
      "Iteration 390, loss = 1.17573474\n",
      "Iteration 391, loss = 1.17547172\n",
      "Iteration 392, loss = 1.17520930\n",
      "Iteration 393, loss = 1.17494747\n",
      "Iteration 394, loss = 1.17468624\n",
      "Iteration 395, loss = 1.17442561\n",
      "Iteration 396, loss = 1.17416556\n",
      "Iteration 397, loss = 1.17390611\n",
      "Iteration 398, loss = 1.17364726\n",
      "Iteration 399, loss = 1.17338900\n",
      "Iteration 400, loss = 1.17313133\n",
      "Iteration 401, loss = 1.17287425\n",
      "Iteration 402, loss = 1.17261776\n",
      "Iteration 403, loss = 1.17236187\n",
      "Iteration 404, loss = 1.17210657\n",
      "Iteration 405, loss = 1.17185186\n",
      "Iteration 406, loss = 1.17159774\n",
      "Iteration 407, loss = 1.17134422\n",
      "Iteration 408, loss = 1.17109128\n",
      "Iteration 409, loss = 1.17083893\n",
      "Iteration 410, loss = 1.17058717\n",
      "Iteration 411, loss = 1.17033601\n",
      "Iteration 412, loss = 1.17008543\n",
      "Iteration 413, loss = 1.16983544\n",
      "Iteration 414, loss = 1.16958604\n",
      "Iteration 415, loss = 1.16933722\n",
      "Iteration 416, loss = 1.16908900\n",
      "Iteration 417, loss = 1.16884136\n",
      "Iteration 418, loss = 1.16859431\n",
      "Iteration 419, loss = 1.16834785\n",
      "Iteration 420, loss = 1.16810197\n",
      "Iteration 421, loss = 1.16785668\n",
      "Iteration 422, loss = 1.16761198\n",
      "Iteration 423, loss = 1.16736786\n",
      "Iteration 424, loss = 1.16712432\n",
      "Iteration 425, loss = 1.16688137\n",
      "Iteration 426, loss = 1.16663901\n",
      "Iteration 427, loss = 1.16639722\n",
      "Iteration 428, loss = 1.16615602\n",
      "Iteration 429, loss = 1.16591541\n",
      "Iteration 430, loss = 1.16567538\n",
      "Iteration 431, loss = 1.16543593\n",
      "Iteration 432, loss = 1.16519706\n",
      "Iteration 433, loss = 1.16495877\n",
      "Iteration 434, loss = 1.16472107\n",
      "Iteration 435, loss = 1.16448394\n",
      "Iteration 436, loss = 1.16424740\n",
      "Iteration 437, loss = 1.16401144\n",
      "Iteration 438, loss = 1.16377605\n",
      "Iteration 439, loss = 1.16354125\n",
      "Iteration 440, loss = 1.16330702\n",
      "Iteration 441, loss = 1.16307337\n",
      "Iteration 442, loss = 1.16284030\n",
      "Iteration 443, loss = 1.16260781\n",
      "Iteration 444, loss = 1.16237590\n",
      "Iteration 445, loss = 1.16214456\n",
      "Iteration 446, loss = 1.16191380\n",
      "Iteration 447, loss = 1.16168361\n",
      "Iteration 448, loss = 1.16145400\n",
      "Iteration 449, loss = 1.16122496\n",
      "Iteration 450, loss = 1.16099650\n",
      "Iteration 451, loss = 1.16076862\n",
      "Iteration 452, loss = 1.16054130\n",
      "Iteration 453, loss = 1.16031456\n",
      "Iteration 454, loss = 1.16008840\n",
      "Iteration 455, loss = 1.15986280\n",
      "Iteration 456, loss = 1.15963778\n",
      "Iteration 457, loss = 1.15941333\n",
      "Iteration 458, loss = 1.15918945\n",
      "Iteration 459, loss = 1.15896614\n",
      "Iteration 460, loss = 1.15874340\n",
      "Iteration 461, loss = 1.15852123\n",
      "Iteration 462, loss = 1.15829962\n",
      "Iteration 463, loss = 1.15807859\n",
      "Iteration 464, loss = 1.15785813\n",
      "Iteration 465, loss = 1.15763823\n",
      "Iteration 466, loss = 1.15741890\n",
      "Iteration 467, loss = 1.15720014\n",
      "Iteration 468, loss = 1.15698194\n",
      "Iteration 469, loss = 1.15676431\n",
      "Iteration 470, loss = 1.15654725\n",
      "Iteration 471, loss = 1.15633075\n",
      "Iteration 472, loss = 1.15611481\n",
      "Iteration 473, loss = 1.15589944\n",
      "Iteration 474, loss = 1.15568463\n",
      "Iteration 475, loss = 1.15547038\n",
      "Iteration 476, loss = 1.15525670\n",
      "Iteration 477, loss = 1.15504357\n",
      "Iteration 478, loss = 1.15483101\n",
      "Iteration 479, loss = 1.15461901\n",
      "Iteration 480, loss = 1.15440757\n",
      "Iteration 481, loss = 1.15419669\n",
      "Iteration 482, loss = 1.15398637\n",
      "Iteration 483, loss = 1.15377660\n",
      "Iteration 484, loss = 1.15356740\n",
      "Iteration 485, loss = 1.15335875\n",
      "Iteration 486, loss = 1.15315066\n",
      "Iteration 487, loss = 1.15294313\n",
      "Iteration 488, loss = 1.15273615\n",
      "Iteration 489, loss = 1.15252973\n",
      "Iteration 490, loss = 1.15232386\n",
      "Iteration 491, loss = 1.15211855\n",
      "Iteration 492, loss = 1.15191379\n",
      "Iteration 493, loss = 1.15170958\n",
      "Iteration 494, loss = 1.15150593\n",
      "Iteration 495, loss = 1.15130282\n",
      "Iteration 496, loss = 1.15110027\n",
      "Iteration 497, loss = 1.15089827\n",
      "Iteration 498, loss = 1.15069682\n",
      "Iteration 499, loss = 1.15049593\n",
      "Iteration 500, loss = 1.15029558\n",
      "Iteration 501, loss = 1.15009577\n",
      "Iteration 502, loss = 1.14989652\n",
      "Iteration 503, loss = 1.14969782\n",
      "Iteration 504, loss = 1.14949966\n",
      "Iteration 505, loss = 1.14930204\n",
      "Iteration 506, loss = 1.14910498\n",
      "Iteration 507, loss = 1.14890846\n",
      "Iteration 508, loss = 1.14871248\n",
      "Iteration 509, loss = 1.14851705\n",
      "Iteration 510, loss = 1.14832216\n",
      "Iteration 511, loss = 1.14812781\n",
      "Iteration 512, loss = 1.14793401\n",
      "Iteration 513, loss = 1.14774074\n",
      "Iteration 514, loss = 1.14754802\n",
      "Iteration 515, loss = 1.14735584\n",
      "Iteration 516, loss = 1.14716420\n",
      "Iteration 517, loss = 1.14697310\n",
      "Iteration 518, loss = 1.14678253\n",
      "Iteration 519, loss = 1.14659251\n",
      "Iteration 520, loss = 1.14640302\n",
      "Iteration 521, loss = 1.14621407\n",
      "Iteration 522, loss = 1.14602565\n",
      "Iteration 523, loss = 1.14583777\n",
      "Iteration 524, loss = 1.14565042\n",
      "Iteration 525, loss = 1.14546361\n",
      "Iteration 526, loss = 1.14527733\n",
      "Iteration 527, loss = 1.14509159\n",
      "Iteration 528, loss = 1.14490638\n",
      "Iteration 529, loss = 1.14472169\n",
      "Iteration 530, loss = 1.14453754\n",
      "Iteration 531, loss = 1.14435392\n",
      "Iteration 532, loss = 1.14417083\n",
      "Iteration 533, loss = 1.14398827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 534, loss = 1.14380624\n",
      "Iteration 535, loss = 1.14362473\n",
      "Iteration 536, loss = 1.14344375\n",
      "Iteration 537, loss = 1.14326330\n",
      "Iteration 538, loss = 1.14308338\n",
      "Iteration 539, loss = 1.14290398\n",
      "Iteration 540, loss = 1.14272510\n",
      "Iteration 541, loss = 1.14254675\n",
      "Iteration 542, loss = 1.14236891\n",
      "Iteration 543, loss = 1.14219161\n",
      "Iteration 544, loss = 1.14201482\n",
      "Iteration 545, loss = 1.14183856\n",
      "Iteration 546, loss = 1.14166281\n",
      "Iteration 547, loss = 1.14148759\n",
      "Iteration 548, loss = 1.14131288\n",
      "Iteration 549, loss = 1.14113869\n",
      "Iteration 550, loss = 1.14096502\n",
      "Iteration 551, loss = 1.14079187\n",
      "Iteration 552, loss = 1.14061923\n",
      "Iteration 553, loss = 1.14044711\n",
      "Iteration 554, loss = 1.14027550\n",
      "Iteration 555, loss = 1.14010441\n",
      "Iteration 556, loss = 1.13993383\n",
      "Iteration 557, loss = 1.13976376\n",
      "Iteration 558, loss = 1.13959421\n",
      "Iteration 559, loss = 1.13942516\n",
      "Iteration 560, loss = 1.13925663\n",
      "Iteration 561, loss = 1.13908861\n",
      "Iteration 562, loss = 1.13892109\n",
      "Iteration 563, loss = 1.13875409\n",
      "Iteration 564, loss = 1.13858759\n",
      "Iteration 565, loss = 1.13842160\n",
      "Iteration 566, loss = 1.13825612\n",
      "Iteration 567, loss = 1.13809114\n",
      "Iteration 568, loss = 1.13792666\n",
      "Iteration 569, loss = 1.13776269\n",
      "Iteration 570, loss = 1.13759922\n",
      "Iteration 571, loss = 1.13743626\n",
      "Iteration 572, loss = 1.13727380\n",
      "Iteration 573, loss = 1.13711184\n",
      "Iteration 574, loss = 1.13695037\n",
      "Iteration 575, loss = 1.13678941\n",
      "Iteration 576, loss = 1.13662895\n",
      "Iteration 577, loss = 1.13646899\n",
      "Iteration 578, loss = 1.13630952\n",
      "Iteration 579, loss = 1.13615055\n",
      "Iteration 580, loss = 1.13599207\n",
      "Iteration 581, loss = 1.13583409\n",
      "Iteration 582, loss = 1.13567661\n",
      "Iteration 583, loss = 1.13551962\n",
      "Iteration 584, loss = 1.13536312\n",
      "Iteration 585, loss = 1.13520711\n",
      "Iteration 586, loss = 1.13505160\n",
      "Iteration 587, loss = 1.13489657\n",
      "Iteration 588, loss = 1.13474204\n",
      "Iteration 589, loss = 1.13458799\n",
      "Iteration 590, loss = 1.13443444\n",
      "Iteration 591, loss = 1.13428137\n",
      "Iteration 592, loss = 1.13412878\n",
      "Iteration 593, loss = 1.13397669\n",
      "Iteration 594, loss = 1.13382507\n",
      "Iteration 595, loss = 1.13367395\n",
      "Iteration 596, loss = 1.13352330\n",
      "Iteration 597, loss = 1.13337314\n",
      "Iteration 598, loss = 1.13322346\n",
      "Iteration 599, loss = 1.13307426\n",
      "Iteration 600, loss = 1.13292555\n",
      "Iteration 601, loss = 1.13277731\n",
      "Iteration 602, loss = 1.13262955\n",
      "Iteration 603, loss = 1.13248227\n",
      "Iteration 604, loss = 1.13233547\n",
      "Iteration 605, loss = 1.13218914\n",
      "Iteration 606, loss = 1.13204329\n",
      "Iteration 607, loss = 1.13189791\n",
      "Iteration 608, loss = 1.13175301\n",
      "Iteration 609, loss = 1.13160858\n",
      "Iteration 610, loss = 1.13146462\n",
      "Iteration 611, loss = 1.13132114\n",
      "Iteration 612, loss = 1.13117812\n",
      "Iteration 613, loss = 1.13103558\n",
      "Iteration 614, loss = 1.13089351\n",
      "Iteration 615, loss = 1.13075190\n",
      "Iteration 616, loss = 1.13061076\n",
      "Iteration 617, loss = 1.13047009\n",
      "Iteration 618, loss = 1.13032989\n",
      "Iteration 619, loss = 1.13019015\n",
      "Iteration 620, loss = 1.13005087\n",
      "Iteration 621, loss = 1.12991206\n",
      "Iteration 622, loss = 1.12977371\n",
      "Iteration 623, loss = 1.12963582\n",
      "Iteration 624, loss = 1.12949839\n",
      "Iteration 625, loss = 1.12936143\n",
      "Iteration 626, loss = 1.12922492\n",
      "Iteration 627, loss = 1.12908887\n",
      "Iteration 628, loss = 1.12895328\n",
      "Iteration 629, loss = 1.12881815\n",
      "Iteration 630, loss = 1.12868347\n",
      "Iteration 631, loss = 1.12854925\n",
      "Iteration 632, loss = 1.12841548\n",
      "Iteration 633, loss = 1.12828216\n",
      "Iteration 634, loss = 1.12814930\n",
      "Iteration 635, loss = 1.12801689\n",
      "Iteration 636, loss = 1.12788493\n",
      "Iteration 637, loss = 1.12775342\n",
      "Iteration 638, loss = 1.12762237\n",
      "Iteration 639, loss = 1.12749176\n",
      "Iteration 640, loss = 1.12736159\n",
      "Iteration 641, loss = 1.12723188\n",
      "Iteration 642, loss = 1.12710261\n",
      "Iteration 643, loss = 1.12697378\n",
      "Iteration 644, loss = 1.12684540\n",
      "Iteration 645, loss = 1.12671746\n",
      "Iteration 646, loss = 1.12658997\n",
      "Iteration 647, loss = 1.12646292\n",
      "Iteration 648, loss = 1.12633630\n",
      "Iteration 649, loss = 1.12621013\n",
      "Iteration 650, loss = 1.12608440\n",
      "Iteration 651, loss = 1.12595910\n",
      "Iteration 652, loss = 1.12583425\n",
      "Iteration 653, loss = 1.12570982\n",
      "Iteration 654, loss = 1.12558584\n",
      "Iteration 655, loss = 1.12546229\n",
      "Iteration 656, loss = 1.12533917\n",
      "Iteration 657, loss = 1.12521649\n",
      "Iteration 658, loss = 1.12509424\n",
      "Iteration 659, loss = 1.12497242\n",
      "Iteration 660, loss = 1.12485103\n",
      "Iteration 661, loss = 1.12473007\n",
      "Iteration 662, loss = 1.12460954\n",
      "Iteration 663, loss = 1.12448944\n",
      "Iteration 664, loss = 1.12436976\n",
      "Iteration 665, loss = 1.12425051\n",
      "Iteration 666, loss = 1.12413168\n",
      "Iteration 667, loss = 1.12401328\n",
      "Iteration 668, loss = 1.12389531\n",
      "Iteration 669, loss = 1.12377775\n",
      "Iteration 670, loss = 1.12366062\n",
      "Iteration 671, loss = 1.12354391\n",
      "Iteration 672, loss = 1.12342762\n",
      "Iteration 673, loss = 1.12331174\n",
      "Iteration 674, loss = 1.12319629\n",
      "Iteration 675, loss = 1.12308125\n",
      "Iteration 676, loss = 1.12296663\n",
      "Iteration 677, loss = 1.12285242\n",
      "Iteration 678, loss = 1.12273863\n",
      "Iteration 679, loss = 1.12262526\n",
      "Iteration 680, loss = 1.12251229\n",
      "Iteration 681, loss = 1.12239974\n",
      "Iteration 682, loss = 1.12228760\n",
      "Iteration 683, loss = 1.12217587\n",
      "Iteration 684, loss = 1.12206455\n",
      "Iteration 685, loss = 1.12195363\n",
      "Iteration 686, loss = 1.12184313\n",
      "Iteration 687, loss = 1.12173303\n",
      "Iteration 688, loss = 1.12162333\n",
      "Iteration 689, loss = 1.12151405\n",
      "Iteration 690, loss = 1.12140516\n",
      "Iteration 691, loss = 1.12129668\n",
      "Iteration 692, loss = 1.12118860\n",
      "Iteration 693, loss = 1.12108092\n",
      "Iteration 694, loss = 1.12097364\n",
      "Iteration 695, loss = 1.12086677\n",
      "Iteration 696, loss = 1.12076029\n",
      "Iteration 697, loss = 1.12065420\n",
      "Iteration 698, loss = 1.12054852\n",
      "Iteration 699, loss = 1.12044323\n",
      "Iteration 700, loss = 1.12033833\n",
      "Iteration 701, loss = 1.12023383\n",
      "Iteration 702, loss = 1.12012972\n",
      "Iteration 703, loss = 1.12002601\n",
      "Iteration 704, loss = 1.11992268\n",
      "Iteration 705, loss = 1.11981975\n",
      "Iteration 706, loss = 1.11971721\n",
      "Iteration 707, loss = 1.11961505\n",
      "Iteration 708, loss = 1.11951328\n",
      "Iteration 709, loss = 1.11941190\n",
      "Iteration 710, loss = 1.11931090\n",
      "Iteration 711, loss = 1.11921029\n",
      "Iteration 712, loss = 1.11911007\n",
      "Iteration 713, loss = 1.11901022\n",
      "Iteration 714, loss = 1.11891076\n",
      "Iteration 715, loss = 1.11881168\n",
      "Iteration 716, loss = 1.11871298\n",
      "Iteration 717, loss = 1.11861466\n",
      "Iteration 718, loss = 1.11851672\n",
      "Iteration 719, loss = 1.11841916\n",
      "Iteration 720, loss = 1.11832197\n",
      "Iteration 721, loss = 1.11822516\n",
      "Iteration 722, loss = 1.11812873\n",
      "Iteration 723, loss = 1.11803266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33104251\n",
      "Iteration 2, loss = 1.32996579\n",
      "Iteration 3, loss = 1.32891267\n",
      "Iteration 4, loss = 1.32787247\n",
      "Iteration 5, loss = 1.32688035\n",
      "Iteration 6, loss = 1.32591298\n",
      "Iteration 7, loss = 1.32499866\n",
      "Iteration 8, loss = 1.32412170\n",
      "Iteration 9, loss = 1.32326853\n",
      "Iteration 10, loss = 1.32243059\n",
      "Iteration 11, loss = 1.32163475\n",
      "Iteration 12, loss = 1.32085560\n",
      "Iteration 13, loss = 1.32008716\n",
      "Iteration 14, loss = 1.31934278\n",
      "Iteration 15, loss = 1.31862265\n",
      "Iteration 16, loss = 1.31794372\n",
      "Iteration 17, loss = 1.31729256\n",
      "Iteration 18, loss = 1.31666031\n",
      "Iteration 19, loss = 1.31604018\n",
      "Iteration 20, loss = 1.31543864\n",
      "Iteration 21, loss = 1.31484768\n",
      "Iteration 22, loss = 1.31426203\n",
      "Iteration 23, loss = 1.31368122\n",
      "Iteration 24, loss = 1.31310482\n",
      "Iteration 25, loss = 1.31253504\n",
      "Iteration 26, loss = 1.31197340\n",
      "Iteration 27, loss = 1.31144184\n",
      "Iteration 28, loss = 1.31092183\n",
      "Iteration 29, loss = 1.31040490\n",
      "Iteration 30, loss = 1.30989082\n",
      "Iteration 31, loss = 1.30938413\n",
      "Iteration 32, loss = 1.30888180\n",
      "Iteration 33, loss = 1.30838160\n",
      "Iteration 34, loss = 1.30788339\n",
      "Iteration 35, loss = 1.30739038\n",
      "Iteration 36, loss = 1.30690132\n",
      "Iteration 37, loss = 1.30641380\n",
      "Iteration 38, loss = 1.30593207\n",
      "Iteration 39, loss = 1.30545214\n",
      "Iteration 40, loss = 1.30497339\n",
      "Iteration 41, loss = 1.30449577\n",
      "Iteration 42, loss = 1.30401920\n",
      "Iteration 43, loss = 1.30354366\n",
      "Iteration 44, loss = 1.30306909\n",
      "Iteration 45, loss = 1.30259575\n",
      "Iteration 46, loss = 1.30212617\n",
      "Iteration 47, loss = 1.30165739\n",
      "Iteration 48, loss = 1.30118939\n",
      "Iteration 49, loss = 1.30072214\n",
      "Iteration 50, loss = 1.30025562\n",
      "Iteration 51, loss = 1.29978983\n",
      "Iteration 52, loss = 1.29932473\n",
      "Iteration 53, loss = 1.29886032\n",
      "Iteration 54, loss = 1.29839659\n",
      "Iteration 55, loss = 1.29793352\n",
      "Iteration 56, loss = 1.29747111\n",
      "Iteration 57, loss = 1.29700934\n",
      "Iteration 58, loss = 1.29654820\n",
      "Iteration 59, loss = 1.29608770\n",
      "Iteration 60, loss = 1.29562781\n",
      "Iteration 61, loss = 1.29516853\n",
      "Iteration 62, loss = 1.29470987\n",
      "Iteration 63, loss = 1.29425180\n",
      "Iteration 64, loss = 1.29379434\n",
      "Iteration 65, loss = 1.29333746\n",
      "Iteration 66, loss = 1.29288118\n",
      "Iteration 67, loss = 1.29242548\n",
      "Iteration 68, loss = 1.29197036\n",
      "Iteration 69, loss = 1.29151583\n",
      "Iteration 70, loss = 1.29106187\n",
      "Iteration 71, loss = 1.29060848\n",
      "Iteration 72, loss = 1.29015567\n",
      "Iteration 73, loss = 1.28970343\n",
      "Iteration 74, loss = 1.28925176\n",
      "Iteration 75, loss = 1.28880066\n",
      "Iteration 76, loss = 1.28835012\n",
      "Iteration 77, loss = 1.28790014\n",
      "Iteration 78, loss = 1.28745073\n",
      "Iteration 79, loss = 1.28700189\n",
      "Iteration 80, loss = 1.28655360\n",
      "Iteration 81, loss = 1.28610588\n",
      "Iteration 82, loss = 1.28565872\n",
      "Iteration 83, loss = 1.28521212\n",
      "Iteration 84, loss = 1.28476608\n",
      "Iteration 85, loss = 1.28432059\n",
      "Iteration 86, loss = 1.28387567\n",
      "Iteration 87, loss = 1.28343131\n",
      "Iteration 88, loss = 1.28298750\n",
      "Iteration 89, loss = 1.28254426\n",
      "Iteration 90, loss = 1.28210157\n",
      "Iteration 91, loss = 1.28165944\n",
      "Iteration 92, loss = 1.28121787\n",
      "Iteration 93, loss = 1.28077686\n",
      "Iteration 94, loss = 1.28033641\n",
      "Iteration 95, loss = 1.27989651\n",
      "Iteration 96, loss = 1.27945718\n",
      "Iteration 97, loss = 1.27901840\n",
      "Iteration 98, loss = 1.27858018\n",
      "Iteration 99, loss = 1.27814252\n",
      "Iteration 100, loss = 1.27770543\n",
      "Iteration 101, loss = 1.27726889\n",
      "Iteration 102, loss = 1.27683291\n",
      "Iteration 103, loss = 1.27639749\n",
      "Iteration 104, loss = 1.27596263\n",
      "Iteration 105, loss = 1.27552834\n",
      "Iteration 106, loss = 1.27509460\n",
      "Iteration 107, loss = 1.27466143\n",
      "Iteration 108, loss = 1.27422882\n",
      "Iteration 109, loss = 1.27379677\n",
      "Iteration 110, loss = 1.27336528\n",
      "Iteration 111, loss = 1.27293436\n",
      "Iteration 112, loss = 1.27250400\n",
      "Iteration 113, loss = 1.27207420\n",
      "Iteration 114, loss = 1.27164496\n",
      "Iteration 115, loss = 1.27121629\n",
      "Iteration 116, loss = 1.27078819\n",
      "Iteration 117, loss = 1.27036065\n",
      "Iteration 118, loss = 1.26993367\n",
      "Iteration 119, loss = 1.26950726\n",
      "Iteration 120, loss = 1.26908142\n",
      "Iteration 121, loss = 1.26865614\n",
      "Iteration 122, loss = 1.26823144\n",
      "Iteration 123, loss = 1.26780729\n",
      "Iteration 124, loss = 1.26738372\n",
      "Iteration 125, loss = 1.26696071\n",
      "Iteration 126, loss = 1.26653827\n",
      "Iteration 127, loss = 1.26611640\n",
      "Iteration 128, loss = 1.26569510\n",
      "Iteration 129, loss = 1.26527437\n",
      "Iteration 130, loss = 1.26485421\n",
      "Iteration 131, loss = 1.26443462\n",
      "Iteration 132, loss = 1.26401560\n",
      "Iteration 133, loss = 1.26359715\n",
      "Iteration 134, loss = 1.26317927\n",
      "Iteration 135, loss = 1.26276196\n",
      "Iteration 136, loss = 1.26234523\n",
      "Iteration 137, loss = 1.26192907\n",
      "Iteration 138, loss = 1.26151348\n",
      "Iteration 139, loss = 1.26109846\n",
      "Iteration 140, loss = 1.26068402\n",
      "Iteration 141, loss = 1.26027016\n",
      "Iteration 142, loss = 1.25985686\n",
      "Iteration 143, loss = 1.25944415\n",
      "Iteration 144, loss = 1.25903200\n",
      "Iteration 145, loss = 1.25862044\n",
      "Iteration 146, loss = 1.25820945\n",
      "Iteration 147, loss = 1.25779903\n",
      "Iteration 148, loss = 1.25738919\n",
      "Iteration 149, loss = 1.25697993\n",
      "Iteration 150, loss = 1.25657125\n",
      "Iteration 151, loss = 1.25616315\n",
      "Iteration 152, loss = 1.25575562\n",
      "Iteration 153, loss = 1.25534867\n",
      "Iteration 154, loss = 1.25494230\n",
      "Iteration 155, loss = 1.25453651\n",
      "Iteration 156, loss = 1.25413130\n",
      "Iteration 157, loss = 1.25372667\n",
      "Iteration 158, loss = 1.25332263\n",
      "Iteration 159, loss = 1.25291916\n",
      "Iteration 160, loss = 1.25251627\n",
      "Iteration 161, loss = 1.25211396\n",
      "Iteration 162, loss = 1.25171224\n",
      "Iteration 163, loss = 1.25131110\n",
      "Iteration 164, loss = 1.25091054\n",
      "Iteration 165, loss = 1.25051056\n",
      "Iteration 166, loss = 1.25011117\n",
      "Iteration 167, loss = 1.24971236\n",
      "Iteration 168, loss = 1.24931414\n",
      "Iteration 169, loss = 1.24891650\n",
      "Iteration 170, loss = 1.24851944\n",
      "Iteration 171, loss = 1.24812297\n",
      "Iteration 172, loss = 1.24772708\n",
      "Iteration 173, loss = 1.24733178\n",
      "Iteration 174, loss = 1.24693707\n",
      "Iteration 175, loss = 1.24654294\n",
      "Iteration 176, loss = 1.24614940\n",
      "Iteration 177, loss = 1.24575644\n",
      "Iteration 178, loss = 1.24536407\n",
      "Iteration 179, loss = 1.24497229\n",
      "Iteration 180, loss = 1.24458110\n",
      "Iteration 181, loss = 1.24419050\n",
      "Iteration 182, loss = 1.24380048\n",
      "Iteration 183, loss = 1.24341105\n",
      "Iteration 184, loss = 1.24302222\n",
      "Iteration 185, loss = 1.24263397\n",
      "Iteration 186, loss = 1.24224631\n",
      "Iteration 187, loss = 1.24185924\n",
      "Iteration 188, loss = 1.24147276\n",
      "Iteration 189, loss = 1.24108687\n",
      "Iteration 190, loss = 1.24070157\n",
      "Iteration 191, loss = 1.24031687\n",
      "Iteration 192, loss = 1.23993275\n",
      "Iteration 193, loss = 1.23954923\n",
      "Iteration 194, loss = 1.23916630\n",
      "Iteration 195, loss = 1.23878396\n",
      "Iteration 196, loss = 1.23840221\n",
      "Iteration 197, loss = 1.23802106\n",
      "Iteration 198, loss = 1.23764049\n",
      "Iteration 199, loss = 1.23726053\n",
      "Iteration 200, loss = 1.23688115\n",
      "Iteration 201, loss = 1.23650237\n",
      "Iteration 202, loss = 1.23612418\n",
      "Iteration 203, loss = 1.23574659\n",
      "Iteration 204, loss = 1.23536959\n",
      "Iteration 205, loss = 1.23499319\n",
      "Iteration 206, loss = 1.23461738\n",
      "Iteration 207, loss = 1.23424217\n",
      "Iteration 208, loss = 1.23386755\n",
      "Iteration 209, loss = 1.23349353\n",
      "Iteration 210, loss = 1.23312011\n",
      "Iteration 211, loss = 1.23274728\n",
      "Iteration 212, loss = 1.23237504\n",
      "Iteration 213, loss = 1.23200341\n",
      "Iteration 214, loss = 1.23163237\n",
      "Iteration 215, loss = 1.23126193\n",
      "Iteration 216, loss = 1.23089208\n",
      "Iteration 217, loss = 1.23052283\n",
      "Iteration 218, loss = 1.23015418\n",
      "Iteration 219, loss = 1.22978613\n",
      "Iteration 220, loss = 1.22941868\n",
      "Iteration 221, loss = 1.22905182\n",
      "Iteration 222, loss = 1.22868557\n",
      "Iteration 223, loss = 1.22831991\n",
      "Iteration 224, loss = 1.22795485\n",
      "Iteration 225, loss = 1.22759039\n",
      "Iteration 226, loss = 1.22722653\n",
      "Iteration 227, loss = 1.22686327\n",
      "Iteration 228, loss = 1.22650061\n",
      "Iteration 229, loss = 1.22613855\n",
      "Iteration 230, loss = 1.22577708\n",
      "Iteration 231, loss = 1.22541622\n",
      "Iteration 232, loss = 1.22505596\n",
      "Iteration 233, loss = 1.22469630\n",
      "Iteration 234, loss = 1.22433724\n",
      "Iteration 235, loss = 1.22397878\n",
      "Iteration 236, loss = 1.22362092\n",
      "Iteration 237, loss = 1.22326367\n",
      "Iteration 238, loss = 1.22290701\n",
      "Iteration 239, loss = 1.22255096\n",
      "Iteration 240, loss = 1.22219550\n",
      "Iteration 241, loss = 1.22184065\n",
      "Iteration 242, loss = 1.22148640\n",
      "Iteration 243, loss = 1.22113276\n",
      "Iteration 244, loss = 1.22077971\n",
      "Iteration 245, loss = 1.22042727\n",
      "Iteration 246, loss = 1.22007543\n",
      "Iteration 247, loss = 1.21972419\n",
      "Iteration 248, loss = 1.21937356\n",
      "Iteration 249, loss = 1.21902353\n",
      "Iteration 250, loss = 1.21867410\n",
      "Iteration 251, loss = 1.21832527\n",
      "Iteration 252, loss = 1.21797705\n",
      "Iteration 253, loss = 1.21762943\n",
      "Iteration 254, loss = 1.21728241\n",
      "Iteration 255, loss = 1.21693600\n",
      "Iteration 256, loss = 1.21659019\n",
      "Iteration 257, loss = 1.21624498\n",
      "Iteration 258, loss = 1.21590038\n",
      "Iteration 259, loss = 1.21555638\n",
      "Iteration 260, loss = 1.21521298\n",
      "Iteration 261, loss = 1.21487019\n",
      "Iteration 262, loss = 1.21452801\n",
      "Iteration 263, loss = 1.21418642\n",
      "Iteration 264, loss = 1.21384544\n",
      "Iteration 265, loss = 1.21350507\n",
      "Iteration 266, loss = 1.21316530\n",
      "Iteration 267, loss = 1.21282613\n",
      "Iteration 268, loss = 1.21248757\n",
      "Iteration 269, loss = 1.21214961\n",
      "Iteration 270, loss = 1.21181226\n",
      "Iteration 271, loss = 1.21147551\n",
      "Iteration 272, loss = 1.21113937\n",
      "Iteration 273, loss = 1.21080383\n",
      "Iteration 274, loss = 1.21046889\n",
      "Iteration 275, loss = 1.21013456\n",
      "Iteration 276, loss = 1.20980084\n",
      "Iteration 277, loss = 1.20946772\n",
      "Iteration 278, loss = 1.20913520\n",
      "Iteration 279, loss = 1.20880329\n",
      "Iteration 280, loss = 1.20847198\n",
      "Iteration 281, loss = 1.20814128\n",
      "Iteration 282, loss = 1.20781119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 283, loss = 1.20748169\n",
      "Iteration 284, loss = 1.20715281\n",
      "Iteration 285, loss = 1.20682452\n",
      "Iteration 286, loss = 1.20649685\n",
      "Iteration 287, loss = 1.20616977\n",
      "Iteration 288, loss = 1.20584331\n",
      "Iteration 289, loss = 1.20551744\n",
      "Iteration 290, loss = 1.20519218\n",
      "Iteration 291, loss = 1.20486753\n",
      "Iteration 292, loss = 1.20454348\n",
      "Iteration 293, loss = 1.20422004\n",
      "Iteration 294, loss = 1.20389720\n",
      "Iteration 295, loss = 1.20357496\n",
      "Iteration 296, loss = 1.20325333\n",
      "Iteration 297, loss = 1.20293231\n",
      "Iteration 298, loss = 1.20261188\n",
      "Iteration 299, loss = 1.20229207\n",
      "Iteration 300, loss = 1.20197285\n",
      "Iteration 301, loss = 1.20165425\n",
      "Iteration 302, loss = 1.20133624\n",
      "Iteration 303, loss = 1.20101884\n",
      "Iteration 304, loss = 1.20070205\n",
      "Iteration 305, loss = 1.20038585\n",
      "Iteration 306, loss = 1.20007027\n",
      "Iteration 307, loss = 1.19975528\n",
      "Iteration 308, loss = 1.19944090\n",
      "Iteration 309, loss = 1.19912713\n",
      "Iteration 310, loss = 1.19881395\n",
      "Iteration 311, loss = 1.19850138\n",
      "Iteration 312, loss = 1.19818942\n",
      "Iteration 313, loss = 1.19787806\n",
      "Iteration 314, loss = 1.19756730\n",
      "Iteration 315, loss = 1.19725714\n",
      "Iteration 316, loss = 1.19694759\n",
      "Iteration 317, loss = 1.19663864\n",
      "Iteration 318, loss = 1.19633029\n",
      "Iteration 319, loss = 1.19602255\n",
      "Iteration 320, loss = 1.19571540\n",
      "Iteration 321, loss = 1.19540887\n",
      "Iteration 322, loss = 1.19510293\n",
      "Iteration 323, loss = 1.19479759\n",
      "Iteration 324, loss = 1.19449286\n",
      "Iteration 325, loss = 1.19418873\n",
      "Iteration 326, loss = 1.19388520\n",
      "Iteration 327, loss = 1.19358227\n",
      "Iteration 328, loss = 1.19327995\n",
      "Iteration 329, loss = 1.19297822\n",
      "Iteration 330, loss = 1.19267710\n",
      "Iteration 331, loss = 1.19237658\n",
      "Iteration 332, loss = 1.19207665\n",
      "Iteration 333, loss = 1.19177733\n",
      "Iteration 334, loss = 1.19147861\n",
      "Iteration 335, loss = 1.19118049\n",
      "Iteration 336, loss = 1.19088297\n",
      "Iteration 337, loss = 1.19058605\n",
      "Iteration 338, loss = 1.19028973\n",
      "Iteration 339, loss = 1.18999401\n",
      "Iteration 340, loss = 1.18969889\n",
      "Iteration 341, loss = 1.18940436\n",
      "Iteration 342, loss = 1.18911044\n",
      "Iteration 343, loss = 1.18881711\n",
      "Iteration 344, loss = 1.18852439\n",
      "Iteration 345, loss = 1.18823226\n",
      "Iteration 346, loss = 1.18794073\n",
      "Iteration 347, loss = 1.18764980\n",
      "Iteration 348, loss = 1.18735946\n",
      "Iteration 349, loss = 1.18706972\n",
      "Iteration 350, loss = 1.18678058\n",
      "Iteration 351, loss = 1.18649204\n",
      "Iteration 352, loss = 1.18620409\n",
      "Iteration 353, loss = 1.18591674\n",
      "Iteration 354, loss = 1.18562999\n",
      "Iteration 355, loss = 1.18534383\n",
      "Iteration 356, loss = 1.18505827\n",
      "Iteration 357, loss = 1.18477330\n",
      "Iteration 358, loss = 1.18449005\n",
      "Iteration 359, loss = 1.18420784\n",
      "Iteration 360, loss = 1.18392624\n",
      "Iteration 361, loss = 1.18364523\n",
      "Iteration 362, loss = 1.18336483\n",
      "Iteration 363, loss = 1.18308503\n",
      "Iteration 364, loss = 1.18280582\n",
      "Iteration 365, loss = 1.18252722\n",
      "Iteration 366, loss = 1.18224921\n",
      "Iteration 367, loss = 1.18197181\n",
      "Iteration 368, loss = 1.18169501\n",
      "Iteration 369, loss = 1.18141880\n",
      "Iteration 370, loss = 1.18114320\n",
      "Iteration 371, loss = 1.18086819\n",
      "Iteration 372, loss = 1.18059379\n",
      "Iteration 373, loss = 1.18031998\n",
      "Iteration 374, loss = 1.18004677\n",
      "Iteration 375, loss = 1.17977416\n",
      "Iteration 376, loss = 1.17950215\n",
      "Iteration 377, loss = 1.17923074\n",
      "Iteration 378, loss = 1.17895993\n",
      "Iteration 379, loss = 1.17868971\n",
      "Iteration 380, loss = 1.17842010\n",
      "Iteration 381, loss = 1.17815108\n",
      "Iteration 382, loss = 1.17788265\n",
      "Iteration 383, loss = 1.17761483\n",
      "Iteration 384, loss = 1.17734760\n",
      "Iteration 385, loss = 1.17708097\n",
      "Iteration 386, loss = 1.17681494\n",
      "Iteration 387, loss = 1.17654950\n",
      "Iteration 388, loss = 1.17628466\n",
      "Iteration 389, loss = 1.17602041\n",
      "Iteration 390, loss = 1.17575676\n",
      "Iteration 391, loss = 1.17549371\n",
      "Iteration 392, loss = 1.17523125\n",
      "Iteration 393, loss = 1.17496939\n",
      "Iteration 394, loss = 1.17470812\n",
      "Iteration 395, loss = 1.17444744\n",
      "Iteration 396, loss = 1.17418736\n",
      "Iteration 397, loss = 1.17392787\n",
      "Iteration 398, loss = 1.17366898\n",
      "Iteration 399, loss = 1.17341068\n",
      "Iteration 400, loss = 1.17315297\n",
      "Iteration 401, loss = 1.17289585\n",
      "Iteration 402, loss = 1.17263933\n",
      "Iteration 403, loss = 1.17238340\n",
      "Iteration 404, loss = 1.17212806\n",
      "Iteration 405, loss = 1.17187331\n",
      "Iteration 406, loss = 1.17161915\n",
      "Iteration 407, loss = 1.17136558\n",
      "Iteration 408, loss = 1.17111261\n",
      "Iteration 409, loss = 1.17086022\n",
      "Iteration 410, loss = 1.17060843\n",
      "Iteration 411, loss = 1.17035722\n",
      "Iteration 412, loss = 1.17010660\n",
      "Iteration 413, loss = 1.16985657\n",
      "Iteration 414, loss = 1.16960713\n",
      "Iteration 415, loss = 1.16935828\n",
      "Iteration 416, loss = 1.16911002\n",
      "Iteration 417, loss = 1.16886234\n",
      "Iteration 418, loss = 1.16861525\n",
      "Iteration 419, loss = 1.16836875\n",
      "Iteration 420, loss = 1.16812283\n",
      "Iteration 421, loss = 1.16787750\n",
      "Iteration 422, loss = 1.16763276\n",
      "Iteration 423, loss = 1.16738860\n",
      "Iteration 424, loss = 1.16714503\n",
      "Iteration 425, loss = 1.16690204\n",
      "Iteration 426, loss = 1.16665963\n",
      "Iteration 427, loss = 1.16641781\n",
      "Iteration 428, loss = 1.16617657\n",
      "Iteration 429, loss = 1.16593592\n",
      "Iteration 430, loss = 1.16569584\n",
      "Iteration 431, loss = 1.16545636\n",
      "Iteration 432, loss = 1.16521745\n",
      "Iteration 433, loss = 1.16497912\n",
      "Iteration 434, loss = 1.16474138\n",
      "Iteration 435, loss = 1.16450421\n",
      "Iteration 436, loss = 1.16426763\n",
      "Iteration 437, loss = 1.16403163\n",
      "Iteration 438, loss = 1.16379620\n",
      "Iteration 439, loss = 1.16356136\n",
      "Iteration 440, loss = 1.16332709\n",
      "Iteration 441, loss = 1.16309340\n",
      "Iteration 442, loss = 1.16286030\n",
      "Iteration 443, loss = 1.16262776\n",
      "Iteration 444, loss = 1.16239581\n",
      "Iteration 445, loss = 1.16216443\n",
      "Iteration 446, loss = 1.16193363\n",
      "Iteration 447, loss = 1.16170340\n",
      "Iteration 448, loss = 1.16147375\n",
      "Iteration 449, loss = 1.16124468\n",
      "Iteration 450, loss = 1.16101618\n",
      "Iteration 451, loss = 1.16078825\n",
      "Iteration 452, loss = 1.16056090\n",
      "Iteration 453, loss = 1.16033412\n",
      "Iteration 454, loss = 1.16010791\n",
      "Iteration 455, loss = 1.15988228\n",
      "Iteration 456, loss = 1.15965721\n",
      "Iteration 457, loss = 1.15943272\n",
      "Iteration 458, loss = 1.15920880\n",
      "Iteration 459, loss = 1.15898545\n",
      "Iteration 460, loss = 1.15876267\n",
      "Iteration 461, loss = 1.15854046\n",
      "Iteration 462, loss = 1.15831882\n",
      "Iteration 463, loss = 1.15809775\n",
      "Iteration 464, loss = 1.15787724\n",
      "Iteration 465, loss = 1.15765731\n",
      "Iteration 466, loss = 1.15743794\n",
      "Iteration 467, loss = 1.15721914\n",
      "Iteration 468, loss = 1.15700090\n",
      "Iteration 469, loss = 1.15678323\n",
      "Iteration 470, loss = 1.15656612\n",
      "Iteration 471, loss = 1.15634958\n",
      "Iteration 472, loss = 1.15613361\n",
      "Iteration 473, loss = 1.15591819\n",
      "Iteration 474, loss = 1.15570334\n",
      "Iteration 475, loss = 1.15548906\n",
      "Iteration 476, loss = 1.15527533\n",
      "Iteration 477, loss = 1.15506217\n",
      "Iteration 478, loss = 1.15484957\n",
      "Iteration 479, loss = 1.15463753\n",
      "Iteration 480, loss = 1.15442605\n",
      "Iteration 481, loss = 1.15421513\n",
      "Iteration 482, loss = 1.15400476\n",
      "Iteration 483, loss = 1.15379496\n",
      "Iteration 484, loss = 1.15358572\n",
      "Iteration 485, loss = 1.15337703\n",
      "Iteration 486, loss = 1.15316890\n",
      "Iteration 487, loss = 1.15296132\n",
      "Iteration 488, loss = 1.15275431\n",
      "Iteration 489, loss = 1.15254784\n",
      "Iteration 490, loss = 1.15234194\n",
      "Iteration 491, loss = 1.15213658\n",
      "Iteration 492, loss = 1.15193178\n",
      "Iteration 493, loss = 1.15172754\n",
      "Iteration 494, loss = 1.15152384\n",
      "Iteration 495, loss = 1.15132070\n",
      "Iteration 496, loss = 1.15111811\n",
      "Iteration 497, loss = 1.15091607\n",
      "Iteration 498, loss = 1.15071458\n",
      "Iteration 499, loss = 1.15051364\n",
      "Iteration 500, loss = 1.15031325\n",
      "Iteration 501, loss = 1.15011341\n",
      "Iteration 502, loss = 1.14991412\n",
      "Iteration 503, loss = 1.14971537\n",
      "Iteration 504, loss = 1.14951717\n",
      "Iteration 505, loss = 1.14931952\n",
      "Iteration 506, loss = 1.14912241\n",
      "Iteration 507, loss = 1.14892585\n",
      "Iteration 508, loss = 1.14872984\n",
      "Iteration 509, loss = 1.14853436\n",
      "Iteration 510, loss = 1.14833943\n",
      "Iteration 511, loss = 1.14814505\n",
      "Iteration 512, loss = 1.14795120\n",
      "Iteration 513, loss = 1.14775790\n",
      "Iteration 514, loss = 1.14756514\n",
      "Iteration 515, loss = 1.14737292\n",
      "Iteration 516, loss = 1.14718124\n",
      "Iteration 517, loss = 1.14699009\n",
      "Iteration 518, loss = 1.14679949\n",
      "Iteration 519, loss = 1.14660942\n",
      "Iteration 520, loss = 1.14641989\n",
      "Iteration 521, loss = 1.14623090\n",
      "Iteration 522, loss = 1.14604245\n",
      "Iteration 523, loss = 1.14585453\n",
      "Iteration 524, loss = 1.14566714\n",
      "Iteration 525, loss = 1.14548029\n",
      "Iteration 526, loss = 1.14529397\n",
      "Iteration 527, loss = 1.14510819\n",
      "Iteration 528, loss = 1.14492293\n",
      "Iteration 529, loss = 1.14473821\n",
      "Iteration 530, loss = 1.14455402\n",
      "Iteration 531, loss = 1.14437036\n",
      "Iteration 532, loss = 1.14418723\n",
      "Iteration 533, loss = 1.14400463\n",
      "Iteration 534, loss = 1.14382256\n",
      "Iteration 535, loss = 1.14364101\n",
      "Iteration 536, loss = 1.14345999\n",
      "Iteration 537, loss = 1.14327950\n",
      "Iteration 538, loss = 1.14309954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 539, loss = 1.14292010\n",
      "Iteration 540, loss = 1.14274118\n",
      "Iteration 541, loss = 1.14256279\n",
      "Iteration 542, loss = 1.14238492\n",
      "Iteration 543, loss = 1.14220757\n",
      "Iteration 544, loss = 1.14203074\n",
      "Iteration 545, loss = 1.14185444\n",
      "Iteration 546, loss = 1.14167866\n",
      "Iteration 547, loss = 1.14150339\n",
      "Iteration 548, loss = 1.14132865\n",
      "Iteration 549, loss = 1.14115442\n",
      "Iteration 550, loss = 1.14098071\n",
      "Iteration 551, loss = 1.14080752\n",
      "Iteration 552, loss = 1.14063484\n",
      "Iteration 553, loss = 1.14046268\n",
      "Iteration 554, loss = 1.14029103\n",
      "Iteration 555, loss = 1.14011990\n",
      "Iteration 556, loss = 1.13994928\n",
      "Iteration 557, loss = 1.13977917\n",
      "Iteration 558, loss = 1.13960958\n",
      "Iteration 559, loss = 1.13944050\n",
      "Iteration 560, loss = 1.13927192\n",
      "Iteration 561, loss = 1.13910386\n",
      "Iteration 562, loss = 1.13893631\n",
      "Iteration 563, loss = 1.13876926\n",
      "Iteration 564, loss = 1.13860273\n",
      "Iteration 565, loss = 1.13843670\n",
      "Iteration 566, loss = 1.13827117\n",
      "Iteration 567, loss = 1.13810615\n",
      "Iteration 568, loss = 1.13794164\n",
      "Iteration 569, loss = 1.13777763\n",
      "Iteration 570, loss = 1.13761413\n",
      "Iteration 571, loss = 1.13745112\n",
      "Iteration 572, loss = 1.13728862\n",
      "Iteration 573, loss = 1.13712662\n",
      "Iteration 574, loss = 1.13696512\n",
      "Iteration 575, loss = 1.13680412\n",
      "Iteration 576, loss = 1.13664362\n",
      "Iteration 577, loss = 1.13648361\n",
      "Iteration 578, loss = 1.13632411\n",
      "Iteration 579, loss = 1.13616510\n",
      "Iteration 580, loss = 1.13600659\n",
      "Iteration 581, loss = 1.13584857\n",
      "Iteration 582, loss = 1.13569104\n",
      "Iteration 583, loss = 1.13553401\n",
      "Iteration 584, loss = 1.13537748\n",
      "Iteration 585, loss = 1.13522143\n",
      "Iteration 586, loss = 1.13506588\n",
      "Iteration 587, loss = 1.13491081\n",
      "Iteration 588, loss = 1.13475624\n",
      "Iteration 589, loss = 1.13460216\n",
      "Iteration 590, loss = 1.13444856\n",
      "Iteration 591, loss = 1.13429545\n",
      "Iteration 592, loss = 1.13414283\n",
      "Iteration 593, loss = 1.13399070\n",
      "Iteration 594, loss = 1.13383905\n",
      "Iteration 595, loss = 1.13368788\n",
      "Iteration 596, loss = 1.13353720\n",
      "Iteration 597, loss = 1.13338700\n",
      "Iteration 598, loss = 1.13323728\n",
      "Iteration 599, loss = 1.13308804\n",
      "Iteration 600, loss = 1.13293929\n",
      "Iteration 601, loss = 1.13279101\n",
      "Iteration 602, loss = 1.13264322\n",
      "Iteration 603, loss = 1.13249590\n",
      "Iteration 604, loss = 1.13234906\n",
      "Iteration 605, loss = 1.13220269\n",
      "Iteration 606, loss = 1.13205680\n",
      "Iteration 607, loss = 1.13191139\n",
      "Iteration 608, loss = 1.13176645\n",
      "Iteration 609, loss = 1.13162198\n",
      "Iteration 610, loss = 1.13147799\n",
      "Iteration 611, loss = 1.13133446\n",
      "Iteration 612, loss = 1.13119141\n",
      "Iteration 613, loss = 1.13104883\n",
      "Iteration 614, loss = 1.13090672\n",
      "Iteration 615, loss = 1.13076508\n",
      "Iteration 616, loss = 1.13062390\n",
      "Iteration 617, loss = 1.13048319\n",
      "Iteration 618, loss = 1.13034295\n",
      "Iteration 619, loss = 1.13020317\n",
      "Iteration 620, loss = 1.13006386\n",
      "Iteration 621, loss = 1.12992501\n",
      "Iteration 622, loss = 1.12978662\n",
      "Iteration 623, loss = 1.12964869\n",
      "Iteration 624, loss = 1.12951123\n",
      "Iteration 625, loss = 1.12937423\n",
      "Iteration 626, loss = 1.12923768\n",
      "Iteration 627, loss = 1.12910160\n",
      "Iteration 628, loss = 1.12896597\n",
      "Iteration 629, loss = 1.12883080\n",
      "Iteration 630, loss = 1.12869608\n",
      "Iteration 631, loss = 1.12856182\n",
      "Iteration 632, loss = 1.12842802\n",
      "Iteration 633, loss = 1.12829467\n",
      "Iteration 634, loss = 1.12816177\n",
      "Iteration 635, loss = 1.12802932\n",
      "Iteration 636, loss = 1.12789733\n",
      "Iteration 637, loss = 1.12776578\n",
      "Iteration 638, loss = 1.12763468\n",
      "Iteration 639, loss = 1.12750404\n",
      "Iteration 640, loss = 1.12737384\n",
      "Iteration 641, loss = 1.12724409\n",
      "Iteration 642, loss = 1.12711478\n",
      "Iteration 643, loss = 1.12698592\n",
      "Iteration 644, loss = 1.12685750\n",
      "Iteration 645, loss = 1.12672953\n",
      "Iteration 646, loss = 1.12660199\n",
      "Iteration 647, loss = 1.12647491\n",
      "Iteration 648, loss = 1.12634826\n",
      "Iteration 649, loss = 1.12622205\n",
      "Iteration 650, loss = 1.12609628\n",
      "Iteration 651, loss = 1.12597095\n",
      "Iteration 652, loss = 1.12584605\n",
      "Iteration 653, loss = 1.12572160\n",
      "Iteration 654, loss = 1.12559758\n",
      "Iteration 655, loss = 1.12547399\n",
      "Iteration 656, loss = 1.12535084\n",
      "Iteration 657, loss = 1.12522812\n",
      "Iteration 658, loss = 1.12510583\n",
      "Iteration 659, loss = 1.12498398\n",
      "Iteration 660, loss = 1.12486255\n",
      "Iteration 661, loss = 1.12474156\n",
      "Iteration 662, loss = 1.12462099\n",
      "Iteration 663, loss = 1.12450085\n",
      "Iteration 664, loss = 1.12438114\n",
      "Iteration 665, loss = 1.12426185\n",
      "Iteration 666, loss = 1.12414299\n",
      "Iteration 667, loss = 1.12402456\n",
      "Iteration 668, loss = 1.12390654\n",
      "Iteration 669, loss = 1.12378895\n",
      "Iteration 670, loss = 1.12367179\n",
      "Iteration 671, loss = 1.12355504\n",
      "Iteration 672, loss = 1.12343871\n",
      "Iteration 673, loss = 1.12332280\n",
      "Iteration 674, loss = 1.12320731\n",
      "Iteration 675, loss = 1.12309224\n",
      "Iteration 676, loss = 1.12297759\n",
      "Iteration 677, loss = 1.12286335\n",
      "Iteration 678, loss = 1.12274952\n",
      "Iteration 679, loss = 1.12263611\n",
      "Iteration 680, loss = 1.12252311\n",
      "Iteration 681, loss = 1.12241052\n",
      "Iteration 682, loss = 1.12229835\n",
      "Iteration 683, loss = 1.12218658\n",
      "Iteration 684, loss = 1.12207522\n",
      "Iteration 685, loss = 1.12196428\n",
      "Iteration 686, loss = 1.12185374\n",
      "Iteration 687, loss = 1.12174360\n",
      "Iteration 688, loss = 1.12163387\n",
      "Iteration 689, loss = 1.12152455\n",
      "Iteration 690, loss = 1.12141563\n",
      "Iteration 691, loss = 1.12130712\n",
      "Iteration 692, loss = 1.12119900\n",
      "Iteration 693, loss = 1.12109129\n",
      "Iteration 694, loss = 1.12098398\n",
      "Iteration 695, loss = 1.12087707\n",
      "Iteration 696, loss = 1.12077055\n",
      "Iteration 697, loss = 1.12066444\n",
      "Iteration 698, loss = 1.12055872\n",
      "Iteration 699, loss = 1.12045339\n",
      "Iteration 700, loss = 1.12034846\n",
      "Iteration 701, loss = 1.12024393\n",
      "Iteration 702, loss = 1.12013979\n",
      "Iteration 703, loss = 1.12003604\n",
      "Iteration 704, loss = 1.11993268\n",
      "Iteration 705, loss = 1.11982971\n",
      "Iteration 706, loss = 1.11972713\n",
      "Iteration 707, loss = 1.11962495\n",
      "Iteration 708, loss = 1.11952314\n",
      "Iteration 709, loss = 1.11942173\n",
      "Iteration 710, loss = 1.11932070\n",
      "Iteration 711, loss = 1.11922006\n",
      "Iteration 712, loss = 1.11911980\n",
      "Iteration 713, loss = 1.11901992\n",
      "Iteration 714, loss = 1.11892043\n",
      "Iteration 715, loss = 1.11882131\n",
      "Iteration 716, loss = 1.11872258\n",
      "Iteration 717, loss = 1.11862423\n",
      "Iteration 718, loss = 1.11852626\n",
      "Iteration 719, loss = 1.11842866\n",
      "Iteration 720, loss = 1.11833144\n",
      "Iteration 721, loss = 1.11823460\n",
      "Iteration 722, loss = 1.11813813\n",
      "Iteration 723, loss = 1.11804203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33079619\n",
      "Iteration 2, loss = 1.32987013\n",
      "Iteration 3, loss = 1.32897063\n",
      "Iteration 4, loss = 1.32809730\n",
      "Iteration 5, loss = 1.32719006\n",
      "Iteration 6, loss = 1.32622973\n",
      "Iteration 7, loss = 1.32531552\n",
      "Iteration 8, loss = 1.32443275\n",
      "Iteration 9, loss = 1.32356898\n",
      "Iteration 10, loss = 1.32272075\n",
      "Iteration 11, loss = 1.32191310\n",
      "Iteration 12, loss = 1.32111992\n",
      "Iteration 13, loss = 1.32033814\n",
      "Iteration 14, loss = 1.31958436\n",
      "Iteration 15, loss = 1.31885168\n",
      "Iteration 16, loss = 1.31815387\n",
      "Iteration 17, loss = 1.31746701\n",
      "Iteration 18, loss = 1.31678757\n",
      "Iteration 19, loss = 1.31613107\n",
      "Iteration 20, loss = 1.31550013\n",
      "Iteration 21, loss = 1.31488287\n",
      "Iteration 22, loss = 1.31427177\n",
      "Iteration 23, loss = 1.31366598\n",
      "Iteration 24, loss = 1.31307308\n",
      "Iteration 25, loss = 1.31249945\n",
      "Iteration 26, loss = 1.31195475\n",
      "Iteration 27, loss = 1.31142149\n",
      "Iteration 28, loss = 1.31089186\n",
      "Iteration 29, loss = 1.31037275\n",
      "Iteration 30, loss = 1.30985750\n",
      "Iteration 31, loss = 1.30934494\n",
      "Iteration 32, loss = 1.30883785\n",
      "Iteration 33, loss = 1.30833703\n",
      "Iteration 34, loss = 1.30783822\n",
      "Iteration 35, loss = 1.30734439\n",
      "Iteration 36, loss = 1.30685496\n",
      "Iteration 37, loss = 1.30636705\n",
      "Iteration 38, loss = 1.30588056\n",
      "Iteration 39, loss = 1.30539672\n",
      "Iteration 40, loss = 1.30491993\n",
      "Iteration 41, loss = 1.30444561\n",
      "Iteration 42, loss = 1.30397223\n",
      "Iteration 43, loss = 1.30349973\n",
      "Iteration 44, loss = 1.30302810\n",
      "Iteration 45, loss = 1.30255729\n",
      "Iteration 46, loss = 1.30208730\n",
      "Iteration 47, loss = 1.30161808\n",
      "Iteration 48, loss = 1.30114963\n",
      "Iteration 49, loss = 1.30068193\n",
      "Iteration 50, loss = 1.30021495\n",
      "Iteration 51, loss = 1.29974868\n",
      "Iteration 52, loss = 1.29928310\n",
      "Iteration 53, loss = 1.29881821\n",
      "Iteration 54, loss = 1.29835400\n",
      "Iteration 55, loss = 1.29789044\n",
      "Iteration 56, loss = 1.29742753\n",
      "Iteration 57, loss = 1.29696527\n",
      "Iteration 58, loss = 1.29650364\n",
      "Iteration 59, loss = 1.29604264\n",
      "Iteration 60, loss = 1.29558227\n",
      "Iteration 61, loss = 1.29512250\n",
      "Iteration 62, loss = 1.29466335\n",
      "Iteration 63, loss = 1.29420480\n",
      "Iteration 64, loss = 1.29374684\n",
      "Iteration 65, loss = 1.29328949\n",
      "Iteration 66, loss = 1.29283273\n",
      "Iteration 67, loss = 1.29237655\n",
      "Iteration 68, loss = 1.29192096\n",
      "Iteration 69, loss = 1.29146596\n",
      "Iteration 70, loss = 1.29101154\n",
      "Iteration 71, loss = 1.29055769\n",
      "Iteration 72, loss = 1.29010442\n",
      "Iteration 73, loss = 1.28965173\n",
      "Iteration 74, loss = 1.28919961\n",
      "Iteration 75, loss = 1.28874806\n",
      "Iteration 76, loss = 1.28829708\n",
      "Iteration 77, loss = 1.28784667\n",
      "Iteration 78, loss = 1.28739683\n",
      "Iteration 79, loss = 1.28694755\n",
      "Iteration 80, loss = 1.28649884\n",
      "Iteration 81, loss = 1.28605070\n",
      "Iteration 82, loss = 1.28560313\n",
      "Iteration 83, loss = 1.28515612\n",
      "Iteration 84, loss = 1.28470967\n",
      "Iteration 85, loss = 1.28426379\n",
      "Iteration 86, loss = 1.28381847\n",
      "Iteration 87, loss = 1.28337372\n",
      "Iteration 88, loss = 1.28292953\n",
      "Iteration 89, loss = 1.28248590\n",
      "Iteration 90, loss = 1.28204283\n",
      "Iteration 91, loss = 1.28160033\n",
      "Iteration 92, loss = 1.28115840\n",
      "Iteration 93, loss = 1.28071702\n",
      "Iteration 94, loss = 1.28027621\n",
      "Iteration 95, loss = 1.27983596\n",
      "Iteration 96, loss = 1.27939628\n",
      "Iteration 97, loss = 1.27895716\n",
      "Iteration 98, loss = 1.27851860\n",
      "Iteration 99, loss = 1.27808061\n",
      "Iteration 100, loss = 1.27764318\n",
      "Iteration 101, loss = 1.27720632\n",
      "Iteration 102, loss = 1.27677002\n",
      "Iteration 103, loss = 1.27633428\n",
      "Iteration 104, loss = 1.27589911\n",
      "Iteration 105, loss = 1.27546451\n",
      "Iteration 106, loss = 1.27503047\n",
      "Iteration 107, loss = 1.27459700\n",
      "Iteration 108, loss = 1.27416409\n",
      "Iteration 109, loss = 1.27373175\n",
      "Iteration 110, loss = 1.27329998\n",
      "Iteration 111, loss = 1.27286878\n",
      "Iteration 112, loss = 1.27243814\n",
      "Iteration 113, loss = 1.27200807\n",
      "Iteration 114, loss = 1.27157857\n",
      "Iteration 115, loss = 1.27114964\n",
      "Iteration 116, loss = 1.27072127\n",
      "Iteration 117, loss = 1.27029348\n",
      "Iteration 118, loss = 1.26986626\n",
      "Iteration 119, loss = 1.26943960\n",
      "Iteration 120, loss = 1.26901352\n",
      "Iteration 121, loss = 1.26858800\n",
      "Iteration 122, loss = 1.26816306\n",
      "Iteration 123, loss = 1.26773869\n",
      "Iteration 124, loss = 1.26731489\n",
      "Iteration 125, loss = 1.26689167\n",
      "Iteration 126, loss = 1.26646901\n",
      "Iteration 127, loss = 1.26604693\n",
      "Iteration 128, loss = 1.26562542\n",
      "Iteration 129, loss = 1.26520449\n",
      "Iteration 130, loss = 1.26478413\n",
      "Iteration 131, loss = 1.26436434\n",
      "Iteration 132, loss = 1.26394513\n",
      "Iteration 133, loss = 1.26352650\n",
      "Iteration 134, loss = 1.26310844\n",
      "Iteration 135, loss = 1.26269095\n",
      "Iteration 136, loss = 1.26227404\n",
      "Iteration 137, loss = 1.26185771\n",
      "Iteration 138, loss = 1.26144196\n",
      "Iteration 139, loss = 1.26102678\n",
      "Iteration 140, loss = 1.26061218\n",
      "Iteration 141, loss = 1.26019816\n",
      "Iteration 142, loss = 1.25978472\n",
      "Iteration 143, loss = 1.25937186\n",
      "Iteration 144, loss = 1.25895957\n",
      "Iteration 145, loss = 1.25854787\n",
      "Iteration 146, loss = 1.25813674\n",
      "Iteration 147, loss = 1.25772620\n",
      "Iteration 148, loss = 1.25731623\n",
      "Iteration 149, loss = 1.25690685\n",
      "Iteration 150, loss = 1.25649805\n",
      "Iteration 151, loss = 1.25608983\n",
      "Iteration 152, loss = 1.25568219\n",
      "Iteration 153, loss = 1.25527514\n",
      "Iteration 154, loss = 1.25486866\n",
      "Iteration 155, loss = 1.25446278\n",
      "Iteration 156, loss = 1.25405747\n",
      "Iteration 157, loss = 1.25365275\n",
      "Iteration 158, loss = 1.25324861\n",
      "Iteration 159, loss = 1.25284506\n",
      "Iteration 160, loss = 1.25244209\n",
      "Iteration 161, loss = 1.25203971\n",
      "Iteration 162, loss = 1.25163791\n",
      "Iteration 163, loss = 1.25123670\n",
      "Iteration 164, loss = 1.25083607\n",
      "Iteration 165, loss = 1.25043603\n",
      "Iteration 166, loss = 1.25003658\n",
      "Iteration 167, loss = 1.24963771\n",
      "Iteration 168, loss = 1.24923943\n",
      "Iteration 169, loss = 1.24884174\n",
      "Iteration 170, loss = 1.24844464\n",
      "Iteration 171, loss = 1.24804813\n",
      "Iteration 172, loss = 1.24765220\n",
      "Iteration 173, loss = 1.24725687\n",
      "Iteration 174, loss = 1.24686212\n",
      "Iteration 175, loss = 1.24646797\n",
      "Iteration 176, loss = 1.24607440\n",
      "Iteration 177, loss = 1.24568142\n",
      "Iteration 178, loss = 1.24528904\n",
      "Iteration 179, loss = 1.24489724\n",
      "Iteration 180, loss = 1.24450604\n",
      "Iteration 181, loss = 1.24411542\n",
      "Iteration 182, loss = 1.24372540\n",
      "Iteration 183, loss = 1.24333597\n",
      "Iteration 184, loss = 1.24294714\n",
      "Iteration 185, loss = 1.24255889\n",
      "Iteration 186, loss = 1.24217124\n",
      "Iteration 187, loss = 1.24178418\n",
      "Iteration 188, loss = 1.24139772\n",
      "Iteration 189, loss = 1.24101185\n",
      "Iteration 190, loss = 1.24062657\n",
      "Iteration 191, loss = 1.24024188\n",
      "Iteration 192, loss = 1.23985780\n",
      "Iteration 193, loss = 1.23947430\n",
      "Iteration 194, loss = 1.23909140\n",
      "Iteration 195, loss = 1.23870910\n",
      "Iteration 196, loss = 1.23832739\n",
      "Iteration 197, loss = 1.23794628\n",
      "Iteration 198, loss = 1.23756576\n",
      "Iteration 199, loss = 1.23718584\n",
      "Iteration 200, loss = 1.23680651\n",
      "Iteration 201, loss = 1.23642779\n",
      "Iteration 202, loss = 1.23604966\n",
      "Iteration 203, loss = 1.23567212\n",
      "Iteration 204, loss = 1.23529519\n",
      "Iteration 205, loss = 1.23491885\n",
      "Iteration 206, loss = 1.23454311\n",
      "Iteration 207, loss = 1.23416796\n",
      "Iteration 208, loss = 1.23379342\n",
      "Iteration 209, loss = 1.23341947\n",
      "Iteration 210, loss = 1.23304613\n",
      "Iteration 211, loss = 1.23267338\n",
      "Iteration 212, loss = 1.23230123\n",
      "Iteration 213, loss = 1.23192968\n",
      "Iteration 214, loss = 1.23155873\n",
      "Iteration 215, loss = 1.23118838\n",
      "Iteration 216, loss = 1.23081863\n",
      "Iteration 217, loss = 1.23044948\n",
      "Iteration 218, loss = 1.23008093\n",
      "Iteration 219, loss = 1.22971298\n",
      "Iteration 220, loss = 1.22934563\n",
      "Iteration 221, loss = 1.22897888\n",
      "Iteration 222, loss = 1.22861273\n",
      "Iteration 223, loss = 1.22824718\n",
      "Iteration 224, loss = 1.22788224\n",
      "Iteration 225, loss = 1.22751790\n",
      "Iteration 226, loss = 1.22715415\n",
      "Iteration 227, loss = 1.22679102\n",
      "Iteration 228, loss = 1.22642848\n",
      "Iteration 229, loss = 1.22606654\n",
      "Iteration 230, loss = 1.22570521\n",
      "Iteration 231, loss = 1.22534448\n",
      "Iteration 232, loss = 1.22498435\n",
      "Iteration 233, loss = 1.22462482\n",
      "Iteration 234, loss = 1.22426590\n",
      "Iteration 235, loss = 1.22390758\n",
      "Iteration 236, loss = 1.22354987\n",
      "Iteration 237, loss = 1.22319275\n",
      "Iteration 238, loss = 1.22283624\n",
      "Iteration 239, loss = 1.22248034\n",
      "Iteration 240, loss = 1.22212504\n",
      "Iteration 241, loss = 1.22177034\n",
      "Iteration 242, loss = 1.22141625\n",
      "Iteration 243, loss = 1.22106275\n",
      "Iteration 244, loss = 1.22070987\n",
      "Iteration 245, loss = 1.22035759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 246, loss = 1.22000591\n",
      "Iteration 247, loss = 1.21965484\n",
      "Iteration 248, loss = 1.21930437\n",
      "Iteration 249, loss = 1.21895451\n",
      "Iteration 250, loss = 1.21860525\n",
      "Iteration 251, loss = 1.21825659\n",
      "Iteration 252, loss = 1.21790855\n",
      "Iteration 253, loss = 1.21756110\n",
      "Iteration 254, loss = 1.21721426\n",
      "Iteration 255, loss = 1.21686803\n",
      "Iteration 256, loss = 1.21652240\n",
      "Iteration 257, loss = 1.21617738\n",
      "Iteration 258, loss = 1.21583296\n",
      "Iteration 259, loss = 1.21548915\n",
      "Iteration 260, loss = 1.21514594\n",
      "Iteration 261, loss = 1.21480334\n",
      "Iteration 262, loss = 1.21446135\n",
      "Iteration 263, loss = 1.21411996\n",
      "Iteration 264, loss = 1.21377918\n",
      "Iteration 265, loss = 1.21343900\n",
      "Iteration 266, loss = 1.21309943\n",
      "Iteration 267, loss = 1.21276046\n",
      "Iteration 268, loss = 1.21242210\n",
      "Iteration 269, loss = 1.21208435\n",
      "Iteration 270, loss = 1.21174720\n",
      "Iteration 271, loss = 1.21141066\n",
      "Iteration 272, loss = 1.21107472\n",
      "Iteration 273, loss = 1.21073939\n",
      "Iteration 274, loss = 1.21040467\n",
      "Iteration 275, loss = 1.21007055\n",
      "Iteration 276, loss = 1.20973704\n",
      "Iteration 277, loss = 1.20940414\n",
      "Iteration 278, loss = 1.20907184\n",
      "Iteration 279, loss = 1.20874014\n",
      "Iteration 280, loss = 1.20840906\n",
      "Iteration 281, loss = 1.20807857\n",
      "Iteration 282, loss = 1.20774870\n",
      "Iteration 283, loss = 1.20741943\n",
      "Iteration 284, loss = 1.20709077\n",
      "Iteration 285, loss = 1.20676271\n",
      "Iteration 286, loss = 1.20643526\n",
      "Iteration 287, loss = 1.20610842\n",
      "Iteration 288, loss = 1.20578218\n",
      "Iteration 289, loss = 1.20545655\n",
      "Iteration 290, loss = 1.20513152\n",
      "Iteration 291, loss = 1.20480710\n",
      "Iteration 292, loss = 1.20448328\n",
      "Iteration 293, loss = 1.20416007\n",
      "Iteration 294, loss = 1.20383747\n",
      "Iteration 295, loss = 1.20351547\n",
      "Iteration 296, loss = 1.20319408\n",
      "Iteration 297, loss = 1.20287329\n",
      "Iteration 298, loss = 1.20255311\n",
      "Iteration 299, loss = 1.20223353\n",
      "Iteration 300, loss = 1.20191456\n",
      "Iteration 301, loss = 1.20159620\n",
      "Iteration 302, loss = 1.20127844\n",
      "Iteration 303, loss = 1.20096128\n",
      "Iteration 304, loss = 1.20064473\n",
      "Iteration 305, loss = 1.20032878\n",
      "Iteration 306, loss = 1.20001344\n",
      "Iteration 307, loss = 1.19969871\n",
      "Iteration 308, loss = 1.19938458\n",
      "Iteration 309, loss = 1.19907105\n",
      "Iteration 310, loss = 1.19875813\n",
      "Iteration 311, loss = 1.19844581\n",
      "Iteration 312, loss = 1.19813409\n",
      "Iteration 313, loss = 1.19782298\n",
      "Iteration 314, loss = 1.19751248\n",
      "Iteration 315, loss = 1.19720258\n",
      "Iteration 316, loss = 1.19689328\n",
      "Iteration 317, loss = 1.19658458\n",
      "Iteration 318, loss = 1.19627649\n",
      "Iteration 319, loss = 1.19596900\n",
      "Iteration 320, loss = 1.19566211\n",
      "Iteration 321, loss = 1.19535583\n",
      "Iteration 322, loss = 1.19505015\n",
      "Iteration 323, loss = 1.19474507\n",
      "Iteration 324, loss = 1.19444060\n",
      "Iteration 325, loss = 1.19413673\n",
      "Iteration 326, loss = 1.19383346\n",
      "Iteration 327, loss = 1.19353079\n",
      "Iteration 328, loss = 1.19322872\n",
      "Iteration 329, loss = 1.19292726\n",
      "Iteration 330, loss = 1.19262639\n",
      "Iteration 331, loss = 1.19232613\n",
      "Iteration 332, loss = 1.19202647\n",
      "Iteration 333, loss = 1.19172741\n",
      "Iteration 334, loss = 1.19142895\n",
      "Iteration 335, loss = 1.19113109\n",
      "Iteration 336, loss = 1.19083383\n",
      "Iteration 337, loss = 1.19053717\n",
      "Iteration 338, loss = 1.19024111\n",
      "Iteration 339, loss = 1.18994565\n",
      "Iteration 340, loss = 1.18965079\n",
      "Iteration 341, loss = 1.18935653\n",
      "Iteration 342, loss = 1.18906287\n",
      "Iteration 343, loss = 1.18876980\n",
      "Iteration 344, loss = 1.18847734\n",
      "Iteration 345, loss = 1.18818547\n",
      "Iteration 346, loss = 1.18789420\n",
      "Iteration 347, loss = 1.18760353\n",
      "Iteration 348, loss = 1.18731346\n",
      "Iteration 349, loss = 1.18702518\n",
      "Iteration 350, loss = 1.18673760\n",
      "Iteration 351, loss = 1.18645062\n",
      "Iteration 352, loss = 1.18616425\n",
      "Iteration 353, loss = 1.18587847\n",
      "Iteration 354, loss = 1.18559330\n",
      "Iteration 355, loss = 1.18530873\n",
      "Iteration 356, loss = 1.18502476\n",
      "Iteration 357, loss = 1.18474139\n",
      "Iteration 358, loss = 1.18445862\n",
      "Iteration 359, loss = 1.18417646\n",
      "Iteration 360, loss = 1.18389489\n",
      "Iteration 361, loss = 1.18361393\n",
      "Iteration 362, loss = 1.18333357\n",
      "Iteration 363, loss = 1.18305381\n",
      "Iteration 364, loss = 1.18277465\n",
      "Iteration 365, loss = 1.18249610\n",
      "Iteration 366, loss = 1.18221814\n",
      "Iteration 367, loss = 1.18194078\n",
      "Iteration 368, loss = 1.18166403\n",
      "Iteration 369, loss = 1.18138788\n",
      "Iteration 370, loss = 1.18111232\n",
      "Iteration 371, loss = 1.18083737\n",
      "Iteration 372, loss = 1.18056301\n",
      "Iteration 373, loss = 1.18028926\n",
      "Iteration 374, loss = 1.18001610\n",
      "Iteration 375, loss = 1.17974355\n",
      "Iteration 376, loss = 1.17947159\n",
      "Iteration 377, loss = 1.17920023\n",
      "Iteration 378, loss = 1.17892947\n",
      "Iteration 379, loss = 1.17865931\n",
      "Iteration 380, loss = 1.17838975\n",
      "Iteration 381, loss = 1.17812078\n",
      "Iteration 382, loss = 1.17785242\n",
      "Iteration 383, loss = 1.17758465\n",
      "Iteration 384, loss = 1.17731747\n",
      "Iteration 385, loss = 1.17705090\n",
      "Iteration 386, loss = 1.17678492\n",
      "Iteration 387, loss = 1.17651954\n",
      "Iteration 388, loss = 1.17625475\n",
      "Iteration 389, loss = 1.17599056\n",
      "Iteration 390, loss = 1.17572696\n",
      "Iteration 391, loss = 1.17546397\n",
      "Iteration 392, loss = 1.17520156\n",
      "Iteration 393, loss = 1.17493975\n",
      "Iteration 394, loss = 1.17467854\n",
      "Iteration 395, loss = 1.17441792\n",
      "Iteration 396, loss = 1.17415789\n",
      "Iteration 397, loss = 1.17389846\n",
      "Iteration 398, loss = 1.17363962\n",
      "Iteration 399, loss = 1.17338138\n",
      "Iteration 400, loss = 1.17312372\n",
      "Iteration 401, loss = 1.17286666\n",
      "Iteration 402, loss = 1.17261020\n",
      "Iteration 403, loss = 1.17235432\n",
      "Iteration 404, loss = 1.17209904\n",
      "Iteration 405, loss = 1.17184435\n",
      "Iteration 406, loss = 1.17159024\n",
      "Iteration 407, loss = 1.17133673\n",
      "Iteration 408, loss = 1.17108381\n",
      "Iteration 409, loss = 1.17083148\n",
      "Iteration 410, loss = 1.17057974\n",
      "Iteration 411, loss = 1.17032859\n",
      "Iteration 412, loss = 1.17007803\n",
      "Iteration 413, loss = 1.16982806\n",
      "Iteration 414, loss = 1.16957868\n",
      "Iteration 415, loss = 1.16932988\n",
      "Iteration 416, loss = 1.16908167\n",
      "Iteration 417, loss = 1.16883405\n",
      "Iteration 418, loss = 1.16858702\n",
      "Iteration 419, loss = 1.16834057\n",
      "Iteration 420, loss = 1.16809471\n",
      "Iteration 421, loss = 1.16784944\n",
      "Iteration 422, loss = 1.16760475\n",
      "Iteration 423, loss = 1.16736065\n",
      "Iteration 424, loss = 1.16711713\n",
      "Iteration 425, loss = 1.16687420\n",
      "Iteration 426, loss = 1.16663185\n",
      "Iteration 427, loss = 1.16639008\n",
      "Iteration 428, loss = 1.16614890\n",
      "Iteration 429, loss = 1.16590830\n",
      "Iteration 430, loss = 1.16566829\n",
      "Iteration 431, loss = 1.16542886\n",
      "Iteration 432, loss = 1.16519000\n",
      "Iteration 433, loss = 1.16495173\n",
      "Iteration 434, loss = 1.16471405\n",
      "Iteration 435, loss = 1.16447694\n",
      "Iteration 436, loss = 1.16424041\n",
      "Iteration 437, loss = 1.16400447\n",
      "Iteration 438, loss = 1.16376910\n",
      "Iteration 439, loss = 1.16353431\n",
      "Iteration 440, loss = 1.16330010\n",
      "Iteration 441, loss = 1.16306647\n",
      "Iteration 442, loss = 1.16283342\n",
      "Iteration 443, loss = 1.16260094\n",
      "Iteration 444, loss = 1.16236904\n",
      "Iteration 445, loss = 1.16213772\n",
      "Iteration 446, loss = 1.16190698\n",
      "Iteration 447, loss = 1.16167681\n",
      "Iteration 448, loss = 1.16144721\n",
      "Iteration 449, loss = 1.16121819\n",
      "Iteration 450, loss = 1.16098975\n",
      "Iteration 451, loss = 1.16076188\n",
      "Iteration 452, loss = 1.16053458\n",
      "Iteration 453, loss = 1.16030786\n",
      "Iteration 454, loss = 1.16008171\n",
      "Iteration 455, loss = 1.15985613\n",
      "Iteration 456, loss = 1.15963112\n",
      "Iteration 457, loss = 1.15940669\n",
      "Iteration 458, loss = 1.15918282\n",
      "Iteration 459, loss = 1.15895953\n",
      "Iteration 460, loss = 1.15873681\n",
      "Iteration 461, loss = 1.15851465\n",
      "Iteration 462, loss = 1.15829307\n",
      "Iteration 463, loss = 1.15807205\n",
      "Iteration 464, loss = 1.15785161\n",
      "Iteration 465, loss = 1.15763173\n",
      "Iteration 466, loss = 1.15741241\n",
      "Iteration 467, loss = 1.15719367\n",
      "Iteration 468, loss = 1.15697549\n",
      "Iteration 469, loss = 1.15675787\n",
      "Iteration 470, loss = 1.15654082\n",
      "Iteration 471, loss = 1.15632434\n",
      "Iteration 472, loss = 1.15610842\n",
      "Iteration 473, loss = 1.15589306\n",
      "Iteration 474, loss = 1.15567827\n",
      "Iteration 475, loss = 1.15546404\n",
      "Iteration 476, loss = 1.15525037\n",
      "Iteration 477, loss = 1.15503726\n",
      "Iteration 478, loss = 1.15482472\n",
      "Iteration 479, loss = 1.15461273\n",
      "Iteration 480, loss = 1.15440131\n",
      "Iteration 481, loss = 1.15419044\n",
      "Iteration 482, loss = 1.15398014\n",
      "Iteration 483, loss = 1.15377039\n",
      "Iteration 484, loss = 1.15356120\n",
      "Iteration 485, loss = 1.15335257\n",
      "Iteration 486, loss = 1.15314450\n",
      "Iteration 487, loss = 1.15293698\n",
      "Iteration 488, loss = 1.15273002\n",
      "Iteration 489, loss = 1.15252361\n",
      "Iteration 490, loss = 1.15231776\n",
      "Iteration 491, loss = 1.15211246\n",
      "Iteration 492, loss = 1.15190772\n",
      "Iteration 493, loss = 1.15170352\n",
      "Iteration 494, loss = 1.15149989\n",
      "Iteration 495, loss = 1.15129680\n",
      "Iteration 496, loss = 1.15109427\n",
      "Iteration 497, loss = 1.15089228\n",
      "Iteration 498, loss = 1.15069085\n",
      "Iteration 499, loss = 1.15048997\n",
      "Iteration 500, loss = 1.15028963\n",
      "Iteration 501, loss = 1.15008985\n",
      "Iteration 502, loss = 1.14989061\n",
      "Iteration 503, loss = 1.14969192\n",
      "Iteration 504, loss = 1.14949378\n",
      "Iteration 505, loss = 1.14929618\n",
      "Iteration 506, loss = 1.14909913\n",
      "Iteration 507, loss = 1.14890262\n",
      "Iteration 508, loss = 1.14870666\n",
      "Iteration 509, loss = 1.14851124\n",
      "Iteration 510, loss = 1.14831637\n",
      "Iteration 511, loss = 1.14812204\n",
      "Iteration 512, loss = 1.14792825\n",
      "Iteration 513, loss = 1.14773500\n",
      "Iteration 514, loss = 1.14754230\n",
      "Iteration 515, loss = 1.14735013\n",
      "Iteration 516, loss = 1.14715851\n",
      "Iteration 517, loss = 1.14696742\n",
      "Iteration 518, loss = 1.14677687\n",
      "Iteration 519, loss = 1.14658686\n",
      "Iteration 520, loss = 1.14639739\n",
      "Iteration 521, loss = 1.14620845\n",
      "Iteration 522, loss = 1.14602005\n",
      "Iteration 523, loss = 1.14583218\n",
      "Iteration 524, loss = 1.14564485\n",
      "Iteration 525, loss = 1.14545806\n",
      "Iteration 526, loss = 1.14527179\n",
      "Iteration 527, loss = 1.14508606\n",
      "Iteration 528, loss = 1.14490087\n",
      "Iteration 529, loss = 1.14471620\n",
      "Iteration 530, loss = 1.14453206\n",
      "Iteration 531, loss = 1.14434846\n",
      "Iteration 532, loss = 1.14416538\n",
      "Iteration 533, loss = 1.14398284\n",
      "Iteration 534, loss = 1.14380082\n",
      "Iteration 535, loss = 1.14361933\n",
      "Iteration 536, loss = 1.14343837\n",
      "Iteration 537, loss = 1.14325793\n",
      "Iteration 538, loss = 1.14307802\n",
      "Iteration 539, loss = 1.14289863\n",
      "Iteration 540, loss = 1.14271977\n",
      "Iteration 541, loss = 1.14254143\n",
      "Iteration 542, loss = 1.14236362\n",
      "Iteration 543, loss = 1.14218632\n",
      "Iteration 544, loss = 1.14200955\n",
      "Iteration 545, loss = 1.14183330\n",
      "Iteration 546, loss = 1.14165757\n",
      "Iteration 547, loss = 1.14148236\n",
      "Iteration 548, loss = 1.14130767\n",
      "Iteration 549, loss = 1.14113350\n",
      "Iteration 550, loss = 1.14095984\n",
      "Iteration 551, loss = 1.14078670\n",
      "Iteration 552, loss = 1.14061408\n",
      "Iteration 553, loss = 1.14044198\n",
      "Iteration 554, loss = 1.14027038\n",
      "Iteration 555, loss = 1.14009931\n",
      "Iteration 556, loss = 1.13992874\n",
      "Iteration 557, loss = 1.13975869\n",
      "Iteration 558, loss = 1.13958915\n",
      "Iteration 559, loss = 1.13942012\n",
      "Iteration 560, loss = 1.13925160\n",
      "Iteration 561, loss = 1.13908359\n",
      "Iteration 562, loss = 1.13891609\n",
      "Iteration 563, loss = 1.13874910\n",
      "Iteration 564, loss = 1.13858262\n",
      "Iteration 565, loss = 1.13841664\n",
      "Iteration 566, loss = 1.13825117\n",
      "Iteration 567, loss = 1.13808621\n",
      "Iteration 568, loss = 1.13792175\n",
      "Iteration 569, loss = 1.13775779\n",
      "Iteration 570, loss = 1.13759434\n",
      "Iteration 571, loss = 1.13743139\n",
      "Iteration 572, loss = 1.13726894\n",
      "Iteration 573, loss = 1.13710699\n",
      "Iteration 574, loss = 1.13694555\n",
      "Iteration 575, loss = 1.13678460\n",
      "Iteration 576, loss = 1.13662415\n",
      "Iteration 577, loss = 1.13646420\n",
      "Iteration 578, loss = 1.13630475\n",
      "Iteration 579, loss = 1.13614579\n",
      "Iteration 580, loss = 1.13598733\n",
      "Iteration 581, loss = 1.13582937\n",
      "Iteration 582, loss = 1.13567190\n",
      "Iteration 583, loss = 1.13551492\n",
      "Iteration 584, loss = 1.13535843\n",
      "Iteration 585, loss = 1.13520244\n",
      "Iteration 586, loss = 1.13504694\n",
      "Iteration 587, loss = 1.13489193\n",
      "Iteration 588, loss = 1.13473741\n",
      "Iteration 589, loss = 1.13458338\n",
      "Iteration 590, loss = 1.13442983\n",
      "Iteration 591, loss = 1.13427678\n",
      "Iteration 592, loss = 1.13412421\n",
      "Iteration 593, loss = 1.13397213\n",
      "Iteration 594, loss = 1.13382053\n",
      "Iteration 595, loss = 1.13366941\n",
      "Iteration 596, loss = 1.13351878\n",
      "Iteration 597, loss = 1.13336864\n",
      "Iteration 598, loss = 1.13321897\n",
      "Iteration 599, loss = 1.13306979\n",
      "Iteration 600, loss = 1.13292108\n",
      "Iteration 601, loss = 1.13277286\n",
      "Iteration 602, loss = 1.13262512\n",
      "Iteration 603, loss = 1.13247785\n",
      "Iteration 604, loss = 1.13233106\n",
      "Iteration 605, loss = 1.13218475\n",
      "Iteration 606, loss = 1.13203891\n",
      "Iteration 607, loss = 1.13189355\n",
      "Iteration 608, loss = 1.13174866\n",
      "Iteration 609, loss = 1.13160424\n",
      "Iteration 610, loss = 1.13146030\n",
      "Iteration 611, loss = 1.13131683\n",
      "Iteration 612, loss = 1.13117383\n",
      "Iteration 613, loss = 1.13103130\n",
      "Iteration 614, loss = 1.13088924\n",
      "Iteration 615, loss = 1.13074764\n",
      "Iteration 616, loss = 1.13060652\n",
      "Iteration 617, loss = 1.13046586\n",
      "Iteration 618, loss = 1.13032567\n",
      "Iteration 619, loss = 1.13018594\n",
      "Iteration 620, loss = 1.13004668\n",
      "Iteration 621, loss = 1.12990788\n",
      "Iteration 622, loss = 1.12976954\n",
      "Iteration 623, loss = 1.12963167\n",
      "Iteration 624, loss = 1.12949426\n",
      "Iteration 625, loss = 1.12935730\n",
      "Iteration 626, loss = 1.12922081\n",
      "Iteration 627, loss = 1.12908477\n",
      "Iteration 628, loss = 1.12894920\n",
      "Iteration 629, loss = 1.12881408\n",
      "Iteration 630, loss = 1.12867941\n",
      "Iteration 631, loss = 1.12854520\n",
      "Iteration 632, loss = 1.12841145\n",
      "Iteration 633, loss = 1.12827815\n",
      "Iteration 634, loss = 1.12814530\n",
      "Iteration 635, loss = 1.12801290\n",
      "Iteration 636, loss = 1.12788095\n",
      "Iteration 637, loss = 1.12774946\n",
      "Iteration 638, loss = 1.12761841\n",
      "Iteration 639, loss = 1.12748782\n",
      "Iteration 640, loss = 1.12735767\n",
      "Iteration 641, loss = 1.12722796\n",
      "Iteration 642, loss = 1.12709870\n",
      "Iteration 643, loss = 1.12696989\n",
      "Iteration 644, loss = 1.12684152\n",
      "Iteration 645, loss = 1.12671360\n",
      "Iteration 646, loss = 1.12658612\n",
      "Iteration 647, loss = 1.12645908\n",
      "Iteration 648, loss = 1.12633248\n",
      "Iteration 649, loss = 1.12620632\n",
      "Iteration 650, loss = 1.12608060\n",
      "Iteration 651, loss = 1.12595532\n",
      "Iteration 652, loss = 1.12583047\n",
      "Iteration 653, loss = 1.12570606\n",
      "Iteration 654, loss = 1.12558209\n",
      "Iteration 655, loss = 1.12545855\n",
      "Iteration 656, loss = 1.12533545\n",
      "Iteration 657, loss = 1.12521278\n",
      "Iteration 658, loss = 1.12509054\n",
      "Iteration 659, loss = 1.12496873\n",
      "Iteration 660, loss = 1.12484736\n",
      "Iteration 661, loss = 1.12472641\n",
      "Iteration 662, loss = 1.12460589\n",
      "Iteration 663, loss = 1.12448580\n",
      "Iteration 664, loss = 1.12436614\n",
      "Iteration 665, loss = 1.12424690\n",
      "Iteration 666, loss = 1.12412809\n",
      "Iteration 667, loss = 1.12400970\n",
      "Iteration 668, loss = 1.12389173\n",
      "Iteration 669, loss = 1.12377419\n",
      "Iteration 670, loss = 1.12365707\n",
      "Iteration 671, loss = 1.12354037\n",
      "Iteration 672, loss = 1.12342409\n",
      "Iteration 673, loss = 1.12330823\n",
      "Iteration 674, loss = 1.12319279\n",
      "Iteration 675, loss = 1.12307776\n",
      "Iteration 676, loss = 1.12296315\n",
      "Iteration 677, loss = 1.12284896\n",
      "Iteration 678, loss = 1.12273518\n",
      "Iteration 679, loss = 1.12262182\n",
      "Iteration 680, loss = 1.12250886\n",
      "Iteration 681, loss = 1.12239632\n",
      "Iteration 682, loss = 1.12228419\n",
      "Iteration 683, loss = 1.12217248\n",
      "Iteration 684, loss = 1.12206117\n",
      "Iteration 685, loss = 1.12195026\n",
      "Iteration 686, loss = 1.12183977\n",
      "Iteration 687, loss = 1.12172968\n",
      "Iteration 688, loss = 1.12162000\n",
      "Iteration 689, loss = 1.12151072\n",
      "Iteration 690, loss = 1.12140185\n",
      "Iteration 691, loss = 1.12129338\n",
      "Iteration 692, loss = 1.12118531\n",
      "Iteration 693, loss = 1.12107765\n",
      "Iteration 694, loss = 1.12097038\n",
      "Iteration 695, loss = 1.12086351\n",
      "Iteration 696, loss = 1.12075705\n",
      "Iteration 697, loss = 1.12065097\n",
      "Iteration 698, loss = 1.12054530\n",
      "Iteration 699, loss = 1.12044002\n",
      "Iteration 700, loss = 1.12033514\n",
      "Iteration 701, loss = 1.12023065\n",
      "Iteration 702, loss = 1.12012655\n",
      "Iteration 703, loss = 1.12002285\n",
      "Iteration 704, loss = 1.11991954\n",
      "Iteration 705, loss = 1.11981661\n",
      "Iteration 706, loss = 1.11971408\n",
      "Iteration 707, loss = 1.11961194\n",
      "Iteration 708, loss = 1.11951018\n",
      "Iteration 709, loss = 1.11940881\n",
      "Iteration 710, loss = 1.11930782\n",
      "Iteration 711, loss = 1.11920723\n",
      "Iteration 712, loss = 1.11910701\n",
      "Iteration 713, loss = 1.11900718\n",
      "Iteration 714, loss = 1.11890773\n",
      "Iteration 715, loss = 1.11880866\n",
      "Iteration 716, loss = 1.11870997\n",
      "Iteration 717, loss = 1.11861166\n",
      "Iteration 718, loss = 1.11851373\n",
      "Iteration 719, loss = 1.11841618\n",
      "Iteration 720, loss = 1.11831900\n",
      "Iteration 721, loss = 1.11822220\n",
      "Iteration 722, loss = 1.11812578\n",
      "Iteration 723, loss = 1.11802973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.32935999\n",
      "Iteration 2, loss = 1.32847759\n",
      "Iteration 3, loss = 1.32762885\n",
      "Iteration 4, loss = 1.32680770\n",
      "Iteration 5, loss = 1.32596623\n",
      "Iteration 6, loss = 1.32508144\n",
      "Iteration 7, loss = 1.32425157\n",
      "Iteration 8, loss = 1.32346703\n",
      "Iteration 9, loss = 1.32270944\n",
      "Iteration 10, loss = 1.32196380\n",
      "Iteration 11, loss = 1.32123697\n",
      "Iteration 12, loss = 1.32052338\n",
      "Iteration 13, loss = 1.31981825\n",
      "Iteration 14, loss = 1.31913371\n",
      "Iteration 15, loss = 1.31846455\n",
      "Iteration 16, loss = 1.31781392\n",
      "Iteration 17, loss = 1.31718742\n",
      "Iteration 18, loss = 1.31657648\n",
      "Iteration 19, loss = 1.31597068\n",
      "Iteration 20, loss = 1.31538335\n",
      "Iteration 21, loss = 1.31481089\n",
      "Iteration 22, loss = 1.31424710\n",
      "Iteration 23, loss = 1.31368729\n",
      "Iteration 24, loss = 1.31313111\n",
      "Iteration 25, loss = 1.31257826\n",
      "Iteration 26, loss = 1.31203379\n",
      "Iteration 27, loss = 1.31149822\n",
      "Iteration 28, loss = 1.31096923\n",
      "Iteration 29, loss = 1.31044291\n",
      "Iteration 30, loss = 1.30992457\n",
      "Iteration 31, loss = 1.30941092\n",
      "Iteration 32, loss = 1.30889946\n",
      "Iteration 33, loss = 1.30839004\n",
      "Iteration 34, loss = 1.30788251\n",
      "Iteration 35, loss = 1.30737781\n",
      "Iteration 36, loss = 1.30688211\n",
      "Iteration 37, loss = 1.30639249\n",
      "Iteration 38, loss = 1.30590433\n",
      "Iteration 39, loss = 1.30541754\n",
      "Iteration 40, loss = 1.30493520\n",
      "Iteration 41, loss = 1.30445866\n",
      "Iteration 42, loss = 1.30398476\n",
      "Iteration 43, loss = 1.30351181\n",
      "Iteration 44, loss = 1.30303975\n",
      "Iteration 45, loss = 1.30256857\n",
      "Iteration 46, loss = 1.30209823\n",
      "Iteration 47, loss = 1.30162870\n",
      "Iteration 48, loss = 1.30115996\n",
      "Iteration 49, loss = 1.30069198\n",
      "Iteration 50, loss = 1.30022476\n",
      "Iteration 51, loss = 1.29975826\n",
      "Iteration 52, loss = 1.29929247\n",
      "Iteration 53, loss = 1.29882739\n",
      "Iteration 54, loss = 1.29836298\n",
      "Iteration 55, loss = 1.29789925\n",
      "Iteration 56, loss = 1.29743618\n",
      "Iteration 57, loss = 1.29697376\n",
      "Iteration 58, loss = 1.29651198\n",
      "Iteration 59, loss = 1.29605083\n",
      "Iteration 60, loss = 1.29559031\n",
      "Iteration 61, loss = 1.29513041\n",
      "Iteration 62, loss = 1.29467112\n",
      "Iteration 63, loss = 1.29421244\n",
      "Iteration 64, loss = 1.29375436\n",
      "Iteration 65, loss = 1.29329688\n",
      "Iteration 66, loss = 1.29283999\n",
      "Iteration 67, loss = 1.29238369\n",
      "Iteration 68, loss = 1.29192798\n",
      "Iteration 69, loss = 1.29147285\n",
      "Iteration 70, loss = 1.29101831\n",
      "Iteration 71, loss = 1.29056434\n",
      "Iteration 72, loss = 1.29011095\n",
      "Iteration 73, loss = 1.28965813\n",
      "Iteration 74, loss = 1.28920589\n",
      "Iteration 75, loss = 1.28875421\n",
      "Iteration 76, loss = 1.28830311\n",
      "Iteration 77, loss = 1.28785258\n",
      "Iteration 78, loss = 1.28740261\n",
      "Iteration 79, loss = 1.28695321\n",
      "Iteration 80, loss = 1.28650438\n",
      "Iteration 81, loss = 1.28605611\n",
      "Iteration 82, loss = 1.28560841\n",
      "Iteration 83, loss = 1.28516127\n",
      "Iteration 84, loss = 1.28471470\n",
      "Iteration 85, loss = 1.28426868\n",
      "Iteration 86, loss = 1.28382324\n",
      "Iteration 87, loss = 1.28337835\n",
      "Iteration 88, loss = 1.28293403\n",
      "Iteration 89, loss = 1.28249027\n",
      "Iteration 90, loss = 1.28204708\n",
      "Iteration 91, loss = 1.28160445\n",
      "Iteration 92, loss = 1.28116238\n",
      "Iteration 93, loss = 1.28072087\n",
      "Iteration 94, loss = 1.28027993\n",
      "Iteration 95, loss = 1.27983955\n",
      "Iteration 96, loss = 1.27939973\n",
      "Iteration 97, loss = 1.27896048\n",
      "Iteration 98, loss = 1.27852179\n",
      "Iteration 99, loss = 1.27808367\n",
      "Iteration 100, loss = 1.27764611\n",
      "Iteration 101, loss = 1.27720911\n",
      "Iteration 102, loss = 1.27677268\n",
      "Iteration 103, loss = 1.27633682\n",
      "Iteration 104, loss = 1.27590152\n",
      "Iteration 105, loss = 1.27546679\n",
      "Iteration 106, loss = 1.27503262\n",
      "Iteration 107, loss = 1.27459902\n",
      "Iteration 108, loss = 1.27416599\n",
      "Iteration 109, loss = 1.27373352\n",
      "Iteration 110, loss = 1.27330162\n",
      "Iteration 111, loss = 1.27287029\n",
      "Iteration 112, loss = 1.27243953\n",
      "Iteration 113, loss = 1.27200933\n",
      "Iteration 114, loss = 1.27157971\n",
      "Iteration 115, loss = 1.27115065\n",
      "Iteration 116, loss = 1.27072217\n",
      "Iteration 117, loss = 1.27029426\n",
      "Iteration 118, loss = 1.26986691\n",
      "Iteration 119, loss = 1.26944014\n",
      "Iteration 120, loss = 1.26901394\n",
      "Iteration 121, loss = 1.26858831\n",
      "Iteration 122, loss = 1.26816325\n",
      "Iteration 123, loss = 1.26773876\n",
      "Iteration 124, loss = 1.26731485\n",
      "Iteration 125, loss = 1.26689151\n",
      "Iteration 126, loss = 1.26646875\n",
      "Iteration 127, loss = 1.26604656\n",
      "Iteration 128, loss = 1.26562494\n",
      "Iteration 129, loss = 1.26520390\n",
      "Iteration 130, loss = 1.26478344\n",
      "Iteration 131, loss = 1.26436355\n",
      "Iteration 132, loss = 1.26394424\n",
      "Iteration 133, loss = 1.26352550\n",
      "Iteration 134, loss = 1.26310734\n",
      "Iteration 135, loss = 1.26268976\n",
      "Iteration 136, loss = 1.26227275\n",
      "Iteration 137, loss = 1.26185633\n",
      "Iteration 138, loss = 1.26144048\n",
      "Iteration 139, loss = 1.26102521\n",
      "Iteration 140, loss = 1.26061052\n",
      "Iteration 141, loss = 1.26019641\n",
      "Iteration 142, loss = 1.25978288\n",
      "Iteration 143, loss = 1.25936993\n",
      "Iteration 144, loss = 1.25895756\n",
      "Iteration 145, loss = 1.25854578\n",
      "Iteration 146, loss = 1.25813457\n",
      "Iteration 147, loss = 1.25772395\n",
      "Iteration 148, loss = 1.25731390\n",
      "Iteration 149, loss = 1.25690445\n",
      "Iteration 150, loss = 1.25649557\n",
      "Iteration 151, loss = 1.25608728\n",
      "Iteration 152, loss = 1.25567957\n",
      "Iteration 153, loss = 1.25527244\n",
      "Iteration 154, loss = 1.25486590\n",
      "Iteration 155, loss = 1.25445995\n",
      "Iteration 156, loss = 1.25405457\n",
      "Iteration 157, loss = 1.25364979\n",
      "Iteration 158, loss = 1.25324559\n",
      "Iteration 159, loss = 1.25284198\n",
      "Iteration 160, loss = 1.25243895\n",
      "Iteration 161, loss = 1.25203651\n",
      "Iteration 162, loss = 1.25163465\n",
      "Iteration 163, loss = 1.25123339\n",
      "Iteration 164, loss = 1.25083271\n",
      "Iteration 165, loss = 1.25043262\n",
      "Iteration 166, loss = 1.25003312\n",
      "Iteration 167, loss = 1.24963420\n",
      "Iteration 168, loss = 1.24923588\n",
      "Iteration 169, loss = 1.24883815\n",
      "Iteration 170, loss = 1.24844100\n",
      "Iteration 171, loss = 1.24804444\n",
      "Iteration 172, loss = 1.24764848\n",
      "Iteration 173, loss = 1.24725311\n",
      "Iteration 174, loss = 1.24685832\n",
      "Iteration 175, loss = 1.24646413\n",
      "Iteration 176, loss = 1.24607053\n",
      "Iteration 177, loss = 1.24567752\n",
      "Iteration 178, loss = 1.24528510\n",
      "Iteration 179, loss = 1.24489328\n",
      "Iteration 180, loss = 1.24450204\n",
      "Iteration 181, loss = 1.24411140\n",
      "Iteration 182, loss = 1.24372136\n",
      "Iteration 183, loss = 1.24333190\n",
      "Iteration 184, loss = 1.24294304\n",
      "Iteration 185, loss = 1.24255478\n",
      "Iteration 186, loss = 1.24216710\n",
      "Iteration 187, loss = 1.24178003\n",
      "Iteration 188, loss = 1.24139354\n",
      "Iteration 189, loss = 1.24100766\n",
      "Iteration 190, loss = 1.24062236\n",
      "Iteration 191, loss = 1.24023767\n",
      "Iteration 192, loss = 1.23985356\n",
      "Iteration 193, loss = 1.23947006\n",
      "Iteration 194, loss = 1.23908715\n",
      "Iteration 195, loss = 1.23870483\n",
      "Iteration 196, loss = 1.23832312\n",
      "Iteration 197, loss = 1.23794200\n",
      "Iteration 198, loss = 1.23756147\n",
      "Iteration 199, loss = 1.23718155\n",
      "Iteration 200, loss = 1.23680222\n",
      "Iteration 201, loss = 1.23642349\n",
      "Iteration 202, loss = 1.23604536\n",
      "Iteration 203, loss = 1.23566782\n",
      "Iteration 204, loss = 1.23529089\n",
      "Iteration 205, loss = 1.23491455\n",
      "Iteration 206, loss = 1.23453881\n",
      "Iteration 207, loss = 1.23416367\n",
      "Iteration 208, loss = 1.23378913\n",
      "Iteration 209, loss = 1.23341519\n",
      "Iteration 210, loss = 1.23304185\n",
      "Iteration 211, loss = 1.23266910\n",
      "Iteration 212, loss = 1.23229696\n",
      "Iteration 213, loss = 1.23192542\n",
      "Iteration 214, loss = 1.23155448\n",
      "Iteration 215, loss = 1.23118414\n",
      "Iteration 216, loss = 1.23081439\n",
      "Iteration 217, loss = 1.23044525\n",
      "Iteration 218, loss = 1.23007671\n",
      "Iteration 219, loss = 1.22970878\n",
      "Iteration 220, loss = 1.22934144\n",
      "Iteration 221, loss = 1.22897470\n",
      "Iteration 222, loss = 1.22860857\n",
      "Iteration 223, loss = 1.22824304\n",
      "Iteration 224, loss = 1.22787811\n",
      "Iteration 225, loss = 1.22751378\n",
      "Iteration 226, loss = 1.22715005\n",
      "Iteration 227, loss = 1.22678693\n",
      "Iteration 228, loss = 1.22642440\n",
      "Iteration 229, loss = 1.22606248\n",
      "Iteration 230, loss = 1.22570117\n",
      "Iteration 231, loss = 1.22534045\n",
      "Iteration 232, loss = 1.22498034\n",
      "Iteration 233, loss = 1.22462083\n",
      "Iteration 234, loss = 1.22426193\n",
      "Iteration 235, loss = 1.22390363\n",
      "Iteration 236, loss = 1.22354593\n",
      "Iteration 237, loss = 1.22318883\n",
      "Iteration 238, loss = 1.22283234\n",
      "Iteration 239, loss = 1.22247645\n",
      "Iteration 240, loss = 1.22212117\n",
      "Iteration 241, loss = 1.22176649\n",
      "Iteration 242, loss = 1.22141241\n",
      "Iteration 243, loss = 1.22105894\n",
      "Iteration 244, loss = 1.22070607\n",
      "Iteration 245, loss = 1.22035381\n",
      "Iteration 246, loss = 1.22000215\n",
      "Iteration 247, loss = 1.21965109\n",
      "Iteration 248, loss = 1.21930064\n",
      "Iteration 249, loss = 1.21895079\n",
      "Iteration 250, loss = 1.21860155\n",
      "Iteration 251, loss = 1.21825291\n",
      "Iteration 252, loss = 1.21790488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 1.21755745\n",
      "Iteration 254, loss = 1.21721062\n",
      "Iteration 255, loss = 1.21686440\n",
      "Iteration 256, loss = 1.21651879\n",
      "Iteration 257, loss = 1.21617378\n",
      "Iteration 258, loss = 1.21582937\n",
      "Iteration 259, loss = 1.21548557\n",
      "Iteration 260, loss = 1.21514238\n",
      "Iteration 261, loss = 1.21479978\n",
      "Iteration 262, loss = 1.21445780\n",
      "Iteration 263, loss = 1.21411642\n",
      "Iteration 264, loss = 1.21377564\n",
      "Iteration 265, loss = 1.21343547\n",
      "Iteration 266, loss = 1.21309591\n",
      "Iteration 267, loss = 1.21275694\n",
      "Iteration 268, loss = 1.21241859\n",
      "Iteration 269, loss = 1.21208084\n",
      "Iteration 270, loss = 1.21174369\n",
      "Iteration 271, loss = 1.21140715\n",
      "Iteration 272, loss = 1.21107121\n",
      "Iteration 273, loss = 1.21073588\n",
      "Iteration 274, loss = 1.21040115\n",
      "Iteration 275, loss = 1.21006703\n",
      "Iteration 276, loss = 1.20973352\n",
      "Iteration 277, loss = 1.20940060\n",
      "Iteration 278, loss = 1.20906830\n",
      "Iteration 279, loss = 1.20873659\n",
      "Iteration 280, loss = 1.20840549\n",
      "Iteration 281, loss = 1.20807500\n",
      "Iteration 282, loss = 1.20774511\n",
      "Iteration 283, loss = 1.20741583\n",
      "Iteration 284, loss = 1.20708715\n",
      "Iteration 285, loss = 1.20675907\n",
      "Iteration 286, loss = 1.20643160\n",
      "Iteration 287, loss = 1.20610473\n",
      "Iteration 288, loss = 1.20577847\n",
      "Iteration 289, loss = 1.20545281\n",
      "Iteration 290, loss = 1.20512775\n",
      "Iteration 291, loss = 1.20480330\n",
      "Iteration 292, loss = 1.20447946\n",
      "Iteration 293, loss = 1.20415621\n",
      "Iteration 294, loss = 1.20383357\n",
      "Iteration 295, loss = 1.20351154\n",
      "Iteration 296, loss = 1.20319010\n",
      "Iteration 297, loss = 1.20286927\n",
      "Iteration 298, loss = 1.20254905\n",
      "Iteration 299, loss = 1.20222942\n",
      "Iteration 300, loss = 1.20191040\n",
      "Iteration 301, loss = 1.20159198\n",
      "Iteration 302, loss = 1.20127417\n",
      "Iteration 303, loss = 1.20095695\n",
      "Iteration 304, loss = 1.20064034\n",
      "Iteration 305, loss = 1.20032434\n",
      "Iteration 306, loss = 1.20000893\n",
      "Iteration 307, loss = 1.19969413\n",
      "Iteration 308, loss = 1.19937992\n",
      "Iteration 309, loss = 1.19906632\n",
      "Iteration 310, loss = 1.19875332\n",
      "Iteration 311, loss = 1.19844092\n",
      "Iteration 312, loss = 1.19812913\n",
      "Iteration 313, loss = 1.19781793\n",
      "Iteration 314, loss = 1.19750734\n",
      "Iteration 315, loss = 1.19719734\n",
      "Iteration 316, loss = 1.19688795\n",
      "Iteration 317, loss = 1.19657915\n",
      "Iteration 318, loss = 1.19627169\n",
      "Iteration 319, loss = 1.19596536\n",
      "Iteration 320, loss = 1.19565963\n",
      "Iteration 321, loss = 1.19535451\n",
      "Iteration 322, loss = 1.19504999\n",
      "Iteration 323, loss = 1.19474608\n",
      "Iteration 324, loss = 1.19444277\n",
      "Iteration 325, loss = 1.19414007\n",
      "Iteration 326, loss = 1.19383797\n",
      "Iteration 327, loss = 1.19353648\n",
      "Iteration 328, loss = 1.19323559\n",
      "Iteration 329, loss = 1.19293530\n",
      "Iteration 330, loss = 1.19263563\n",
      "Iteration 331, loss = 1.19233655\n",
      "Iteration 332, loss = 1.19203808\n",
      "Iteration 333, loss = 1.19174022\n",
      "Iteration 334, loss = 1.19144296\n",
      "Iteration 335, loss = 1.19114631\n",
      "Iteration 336, loss = 1.19085026\n",
      "Iteration 337, loss = 1.19055482\n",
      "Iteration 338, loss = 1.19025998\n",
      "Iteration 339, loss = 1.18996575\n",
      "Iteration 340, loss = 1.18967212\n",
      "Iteration 341, loss = 1.18937910\n",
      "Iteration 342, loss = 1.18908668\n",
      "Iteration 343, loss = 1.18879486\n",
      "Iteration 344, loss = 1.18850365\n",
      "Iteration 345, loss = 1.18821305\n",
      "Iteration 346, loss = 1.18792304\n",
      "Iteration 347, loss = 1.18763365\n",
      "Iteration 348, loss = 1.18734485\n",
      "Iteration 349, loss = 1.18705666\n",
      "Iteration 350, loss = 1.18676907\n",
      "Iteration 351, loss = 1.18648209\n",
      "Iteration 352, loss = 1.18619571\n",
      "Iteration 353, loss = 1.18590993\n",
      "Iteration 354, loss = 1.18562475\n",
      "Iteration 355, loss = 1.18534018\n",
      "Iteration 356, loss = 1.18505621\n",
      "Iteration 357, loss = 1.18477284\n",
      "Iteration 358, loss = 1.18449008\n",
      "Iteration 359, loss = 1.18420792\n",
      "Iteration 360, loss = 1.18392635\n",
      "Iteration 361, loss = 1.18364539\n",
      "Iteration 362, loss = 1.18336504\n",
      "Iteration 363, loss = 1.18308528\n",
      "Iteration 364, loss = 1.18280613\n",
      "Iteration 365, loss = 1.18252757\n",
      "Iteration 366, loss = 1.18224962\n",
      "Iteration 367, loss = 1.18197227\n",
      "Iteration 368, loss = 1.18169551\n",
      "Iteration 369, loss = 1.18141936\n",
      "Iteration 370, loss = 1.18114381\n",
      "Iteration 371, loss = 1.18086886\n",
      "Iteration 372, loss = 1.18059451\n",
      "Iteration 373, loss = 1.18032075\n",
      "Iteration 374, loss = 1.18004760\n",
      "Iteration 375, loss = 1.17977504\n",
      "Iteration 376, loss = 1.17950309\n",
      "Iteration 377, loss = 1.17923173\n",
      "Iteration 378, loss = 1.17896097\n",
      "Iteration 379, loss = 1.17869081\n",
      "Iteration 380, loss = 1.17842125\n",
      "Iteration 381, loss = 1.17815228\n",
      "Iteration 382, loss = 1.17788391\n",
      "Iteration 383, loss = 1.17761614\n",
      "Iteration 384, loss = 1.17734897\n",
      "Iteration 385, loss = 1.17708239\n",
      "Iteration 386, loss = 1.17681641\n",
      "Iteration 387, loss = 1.17655102\n",
      "Iteration 388, loss = 1.17628623\n",
      "Iteration 389, loss = 1.17602204\n",
      "Iteration 390, loss = 1.17575844\n",
      "Iteration 391, loss = 1.17549544\n",
      "Iteration 392, loss = 1.17523303\n",
      "Iteration 393, loss = 1.17497121\n",
      "Iteration 394, loss = 1.17470999\n",
      "Iteration 395, loss = 1.17444937\n",
      "Iteration 396, loss = 1.17418934\n",
      "Iteration 397, loss = 1.17392990\n",
      "Iteration 398, loss = 1.17367105\n",
      "Iteration 399, loss = 1.17341280\n",
      "Iteration 400, loss = 1.17315514\n",
      "Iteration 401, loss = 1.17289807\n",
      "Iteration 402, loss = 1.17264160\n",
      "Iteration 403, loss = 1.17238571\n",
      "Iteration 404, loss = 1.17213042\n",
      "Iteration 405, loss = 1.17187572\n",
      "Iteration 406, loss = 1.17162160\n",
      "Iteration 407, loss = 1.17136808\n",
      "Iteration 408, loss = 1.17111515\n",
      "Iteration 409, loss = 1.17086281\n",
      "Iteration 410, loss = 1.17061106\n",
      "Iteration 411, loss = 1.17035990\n",
      "Iteration 412, loss = 1.17010933\n",
      "Iteration 413, loss = 1.16985934\n",
      "Iteration 414, loss = 1.16960994\n",
      "Iteration 415, loss = 1.16936113\n",
      "Iteration 416, loss = 1.16911291\n",
      "Iteration 417, loss = 1.16886528\n",
      "Iteration 418, loss = 1.16861823\n",
      "Iteration 419, loss = 1.16837177\n",
      "Iteration 420, loss = 1.16812590\n",
      "Iteration 421, loss = 1.16788061\n",
      "Iteration 422, loss = 1.16763590\n",
      "Iteration 423, loss = 1.16739178\n",
      "Iteration 424, loss = 1.16714825\n",
      "Iteration 425, loss = 1.16690530\n",
      "Iteration 426, loss = 1.16666293\n",
      "Iteration 427, loss = 1.16642115\n",
      "Iteration 428, loss = 1.16617995\n",
      "Iteration 429, loss = 1.16593934\n",
      "Iteration 430, loss = 1.16569930\n",
      "Iteration 431, loss = 1.16545985\n",
      "Iteration 432, loss = 1.16522098\n",
      "Iteration 433, loss = 1.16498269\n",
      "Iteration 434, loss = 1.16474498\n",
      "Iteration 435, loss = 1.16450786\n",
      "Iteration 436, loss = 1.16427131\n",
      "Iteration 437, loss = 1.16403534\n",
      "Iteration 438, loss = 1.16379995\n",
      "Iteration 439, loss = 1.16356514\n",
      "Iteration 440, loss = 1.16333091\n",
      "Iteration 441, loss = 1.16309726\n",
      "Iteration 442, loss = 1.16286418\n",
      "Iteration 443, loss = 1.16263169\n",
      "Iteration 444, loss = 1.16239976\n",
      "Iteration 445, loss = 1.16216842\n",
      "Iteration 446, loss = 1.16193765\n",
      "Iteration 447, loss = 1.16170746\n",
      "Iteration 448, loss = 1.16147784\n",
      "Iteration 449, loss = 1.16124880\n",
      "Iteration 450, loss = 1.16102033\n",
      "Iteration 451, loss = 1.16079243\n",
      "Iteration 452, loss = 1.16056511\n",
      "Iteration 453, loss = 1.16033836\n",
      "Iteration 454, loss = 1.16011219\n",
      "Iteration 455, loss = 1.15988658\n",
      "Iteration 456, loss = 1.15966155\n",
      "Iteration 457, loss = 1.15943709\n",
      "Iteration 458, loss = 1.15921320\n",
      "Iteration 459, loss = 1.15898988\n",
      "Iteration 460, loss = 1.15876713\n",
      "Iteration 461, loss = 1.15854494\n",
      "Iteration 462, loss = 1.15832333\n",
      "Iteration 463, loss = 1.15810229\n",
      "Iteration 464, loss = 1.15788181\n",
      "Iteration 465, loss = 1.15766190\n",
      "Iteration 466, loss = 1.15744256\n",
      "Iteration 467, loss = 1.15722378\n",
      "Iteration 468, loss = 1.15700557\n",
      "Iteration 469, loss = 1.15678793\n",
      "Iteration 470, loss = 1.15657085\n",
      "Iteration 471, loss = 1.15635434\n",
      "Iteration 472, loss = 1.15613839\n",
      "Iteration 473, loss = 1.15592300\n",
      "Iteration 474, loss = 1.15570817\n",
      "Iteration 475, loss = 1.15549391\n",
      "Iteration 476, loss = 1.15528021\n",
      "Iteration 477, loss = 1.15506707\n",
      "Iteration 478, loss = 1.15485450\n",
      "Iteration 479, loss = 1.15464248\n",
      "Iteration 480, loss = 1.15443102\n",
      "Iteration 481, loss = 1.15422013\n",
      "Iteration 482, loss = 1.15400979\n",
      "Iteration 483, loss = 1.15380001\n",
      "Iteration 484, loss = 1.15359078\n",
      "Iteration 485, loss = 1.15338212\n",
      "Iteration 486, loss = 1.15317401\n",
      "Iteration 487, loss = 1.15296646\n",
      "Iteration 488, loss = 1.15275946\n",
      "Iteration 489, loss = 1.15255302\n",
      "Iteration 490, loss = 1.15234713\n",
      "Iteration 491, loss = 1.15214180\n",
      "Iteration 492, loss = 1.15193702\n",
      "Iteration 493, loss = 1.15173280\n",
      "Iteration 494, loss = 1.15152912\n",
      "Iteration 495, loss = 1.15132600\n",
      "Iteration 496, loss = 1.15112343\n",
      "Iteration 497, loss = 1.15092141\n",
      "Iteration 498, loss = 1.15071994\n",
      "Iteration 499, loss = 1.15051902\n",
      "Iteration 500, loss = 1.15031865\n",
      "Iteration 501, loss = 1.15011883\n",
      "Iteration 502, loss = 1.14991955\n",
      "Iteration 503, loss = 1.14972082\n",
      "Iteration 504, loss = 1.14952264\n",
      "Iteration 505, loss = 1.14932501\n",
      "Iteration 506, loss = 1.14912792\n",
      "Iteration 507, loss = 1.14893138\n",
      "Iteration 508, loss = 1.14873538\n",
      "Iteration 509, loss = 1.14853992\n",
      "Iteration 510, loss = 1.14834501\n",
      "Iteration 511, loss = 1.14815064\n",
      "Iteration 512, loss = 1.14795681\n",
      "Iteration 513, loss = 1.14776352\n",
      "Iteration 514, loss = 1.14757078\n",
      "Iteration 515, loss = 1.14737857\n",
      "Iteration 516, loss = 1.14718690\n",
      "Iteration 517, loss = 1.14699578\n",
      "Iteration 518, loss = 1.14680519\n",
      "Iteration 519, loss = 1.14661514\n",
      "Iteration 520, loss = 1.14642562\n",
      "Iteration 521, loss = 1.14623664\n",
      "Iteration 522, loss = 1.14604820\n",
      "Iteration 523, loss = 1.14586030\n",
      "Iteration 524, loss = 1.14567292\n",
      "Iteration 525, loss = 1.14548609\n",
      "Iteration 526, loss = 1.14529978\n",
      "Iteration 527, loss = 1.14511401\n",
      "Iteration 528, loss = 1.14492877\n",
      "Iteration 529, loss = 1.14474406\n",
      "Iteration 530, loss = 1.14455988\n",
      "Iteration 531, loss = 1.14437624\n",
      "Iteration 532, loss = 1.14419312\n",
      "Iteration 533, loss = 1.14401053\n",
      "Iteration 534, loss = 1.14382847\n",
      "Iteration 535, loss = 1.14364693\n",
      "Iteration 536, loss = 1.14346593\n",
      "Iteration 537, loss = 1.14328545\n",
      "Iteration 538, loss = 1.14310549\n",
      "Iteration 539, loss = 1.14292606\n",
      "Iteration 540, loss = 1.14274715\n",
      "Iteration 541, loss = 1.14256877\n",
      "Iteration 542, loss = 1.14239091\n",
      "Iteration 543, loss = 1.14221357\n",
      "Iteration 544, loss = 1.14203676\n",
      "Iteration 545, loss = 1.14186046\n",
      "Iteration 546, loss = 1.14168469\n",
      "Iteration 547, loss = 1.14150943\n",
      "Iteration 548, loss = 1.14133470\n",
      "Iteration 549, loss = 1.14116048\n",
      "Iteration 550, loss = 1.14098678\n",
      "Iteration 551, loss = 1.14081359\n",
      "Iteration 552, loss = 1.14064092\n",
      "Iteration 553, loss = 1.14046877\n",
      "Iteration 554, loss = 1.14029713\n",
      "Iteration 555, loss = 1.14012601\n",
      "Iteration 556, loss = 1.13995540\n",
      "Iteration 557, loss = 1.13978530\n",
      "Iteration 558, loss = 1.13961571\n",
      "Iteration 559, loss = 1.13944664\n",
      "Iteration 560, loss = 1.13927807\n",
      "Iteration 561, loss = 1.13911002\n",
      "Iteration 562, loss = 1.13894247\n",
      "Iteration 563, loss = 1.13877543\n",
      "Iteration 564, loss = 1.13860890\n",
      "Iteration 565, loss = 1.13844288\n",
      "Iteration 566, loss = 1.13827736\n",
      "Iteration 567, loss = 1.13811235\n",
      "Iteration 568, loss = 1.13794784\n",
      "Iteration 569, loss = 1.13778383\n",
      "Iteration 570, loss = 1.13762033\n",
      "Iteration 571, loss = 1.13745733\n",
      "Iteration 572, loss = 1.13729484\n",
      "Iteration 573, loss = 1.13713284\n",
      "Iteration 574, loss = 1.13697135\n",
      "Iteration 575, loss = 1.13681035\n",
      "Iteration 576, loss = 1.13664985\n",
      "Iteration 577, loss = 1.13648985\n",
      "Iteration 578, loss = 1.13633035\n",
      "Iteration 579, loss = 1.13617135\n",
      "Iteration 580, loss = 1.13601284\n",
      "Iteration 581, loss = 1.13585482\n",
      "Iteration 582, loss = 1.13569730\n",
      "Iteration 583, loss = 1.13554028\n",
      "Iteration 584, loss = 1.13538374\n",
      "Iteration 585, loss = 1.13522770\n",
      "Iteration 586, loss = 1.13507215\n",
      "Iteration 587, loss = 1.13491709\n",
      "Iteration 588, loss = 1.13476252\n",
      "Iteration 589, loss = 1.13460844\n",
      "Iteration 590, loss = 1.13445484\n",
      "Iteration 591, loss = 1.13430174\n",
      "Iteration 592, loss = 1.13414912\n",
      "Iteration 593, loss = 1.13399698\n",
      "Iteration 594, loss = 1.13384534\n",
      "Iteration 595, loss = 1.13369417\n",
      "Iteration 596, loss = 1.13354349\n",
      "Iteration 597, loss = 1.13339329\n",
      "Iteration 598, loss = 1.13324358\n",
      "Iteration 599, loss = 1.13309434\n",
      "Iteration 600, loss = 1.13294559\n",
      "Iteration 601, loss = 1.13279731\n",
      "Iteration 602, loss = 1.13264951\n",
      "Iteration 603, loss = 1.13250220\n",
      "Iteration 604, loss = 1.13235536\n",
      "Iteration 605, loss = 1.13220899\n",
      "Iteration 606, loss = 1.13206310\n",
      "Iteration 607, loss = 1.13191769\n",
      "Iteration 608, loss = 1.13177275\n",
      "Iteration 609, loss = 1.13162828\n",
      "Iteration 610, loss = 1.13148429\n",
      "Iteration 611, loss = 1.13134076\n",
      "Iteration 612, loss = 1.13119771\n",
      "Iteration 613, loss = 1.13105513\n",
      "Iteration 614, loss = 1.13091302\n",
      "Iteration 615, loss = 1.13077137\n",
      "Iteration 616, loss = 1.13063019\n",
      "Iteration 617, loss = 1.13048948\n",
      "Iteration 618, loss = 1.13034924\n",
      "Iteration 619, loss = 1.13020946\n",
      "Iteration 620, loss = 1.13007015\n",
      "Iteration 621, loss = 1.12993129\n",
      "Iteration 622, loss = 1.12979290\n",
      "Iteration 623, loss = 1.12965498\n",
      "Iteration 624, loss = 1.12951751\n",
      "Iteration 625, loss = 1.12938051\n",
      "Iteration 626, loss = 1.12924396\n",
      "Iteration 627, loss = 1.12910787\n",
      "Iteration 628, loss = 1.12897224\n",
      "Iteration 629, loss = 1.12883707\n",
      "Iteration 630, loss = 1.12870235\n",
      "Iteration 631, loss = 1.12856809\n",
      "Iteration 632, loss = 1.12843428\n",
      "Iteration 633, loss = 1.12830093\n",
      "Iteration 634, loss = 1.12816802\n",
      "Iteration 635, loss = 1.12803557\n",
      "Iteration 636, loss = 1.12790357\n",
      "Iteration 637, loss = 1.12777203\n",
      "Iteration 638, loss = 1.12764093\n",
      "Iteration 639, loss = 1.12751028\n",
      "Iteration 640, loss = 1.12738007\n",
      "Iteration 641, loss = 1.12725032\n",
      "Iteration 642, loss = 1.12712101\n",
      "Iteration 643, loss = 1.12699214\n",
      "Iteration 644, loss = 1.12686372\n",
      "Iteration 645, loss = 1.12673574\n",
      "Iteration 646, loss = 1.12660820\n",
      "Iteration 647, loss = 1.12648111\n",
      "Iteration 648, loss = 1.12635446\n",
      "Iteration 649, loss = 1.12622824\n",
      "Iteration 650, loss = 1.12610247\n",
      "Iteration 651, loss = 1.12597713\n",
      "Iteration 652, loss = 1.12585224\n",
      "Iteration 653, loss = 1.12572777\n",
      "Iteration 654, loss = 1.12560375\n",
      "Iteration 655, loss = 1.12548016\n",
      "Iteration 656, loss = 1.12535700\n",
      "Iteration 657, loss = 1.12523427\n",
      "Iteration 658, loss = 1.12511198\n",
      "Iteration 659, loss = 1.12499012\n",
      "Iteration 660, loss = 1.12486869\n",
      "Iteration 661, loss = 1.12474769\n",
      "Iteration 662, loss = 1.12462712\n",
      "Iteration 663, loss = 1.12450697\n",
      "Iteration 664, loss = 1.12438725\n",
      "Iteration 665, loss = 1.12426796\n",
      "Iteration 666, loss = 1.12414909\n",
      "Iteration 667, loss = 1.12403065\n",
      "Iteration 668, loss = 1.12391263\n",
      "Iteration 669, loss = 1.12379504\n",
      "Iteration 670, loss = 1.12367786\n",
      "Iteration 671, loss = 1.12356111\n",
      "Iteration 672, loss = 1.12344478\n",
      "Iteration 673, loss = 1.12332886\n",
      "Iteration 674, loss = 1.12321336\n",
      "Iteration 675, loss = 1.12309828\n",
      "Iteration 676, loss = 1.12298362\n",
      "Iteration 677, loss = 1.12286937\n",
      "Iteration 678, loss = 1.12275554\n",
      "Iteration 679, loss = 1.12264212\n",
      "Iteration 680, loss = 1.12252912\n",
      "Iteration 681, loss = 1.12241652\n",
      "Iteration 682, loss = 1.12230434\n",
      "Iteration 683, loss = 1.12219257\n",
      "Iteration 684, loss = 1.12208120\n",
      "Iteration 685, loss = 1.12197025\n",
      "Iteration 686, loss = 1.12185970\n",
      "Iteration 687, loss = 1.12174956\n",
      "Iteration 688, loss = 1.12163982\n",
      "Iteration 689, loss = 1.12153049\n",
      "Iteration 690, loss = 1.12142156\n",
      "Iteration 691, loss = 1.12131304\n",
      "Iteration 692, loss = 1.12120492\n",
      "Iteration 693, loss = 1.12109720\n",
      "Iteration 694, loss = 1.12098988\n",
      "Iteration 695, loss = 1.12088295\n",
      "Iteration 696, loss = 1.12077643\n",
      "Iteration 697, loss = 1.12067031\n",
      "Iteration 698, loss = 1.12056458\n",
      "Iteration 699, loss = 1.12045925\n",
      "Iteration 700, loss = 1.12035431\n",
      "Iteration 701, loss = 1.12024977\n",
      "Iteration 702, loss = 1.12014562\n",
      "Iteration 703, loss = 1.12004186\n",
      "Iteration 704, loss = 1.11993849\n",
      "Iteration 705, loss = 1.11983551\n",
      "Iteration 706, loss = 1.11973293\n",
      "Iteration 707, loss = 1.11963073\n",
      "Iteration 708, loss = 1.11952892\n",
      "Iteration 709, loss = 1.11942749\n",
      "Iteration 710, loss = 1.11932646\n",
      "Iteration 711, loss = 1.11922580\n",
      "Iteration 712, loss = 1.11912553\n",
      "Iteration 713, loss = 1.11902565\n",
      "Iteration 714, loss = 1.11892615\n",
      "Iteration 715, loss = 1.11882702\n",
      "Iteration 716, loss = 1.11872828\n",
      "Iteration 717, loss = 1.11862992\n",
      "Iteration 718, loss = 1.11853193\n",
      "Iteration 719, loss = 1.11843433\n",
      "Iteration 720, loss = 1.11833710\n",
      "Iteration 721, loss = 1.11824025\n",
      "Iteration 722, loss = 1.11814377\n",
      "Iteration 723, loss = 1.11804766\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33009363\n",
      "Iteration 2, loss = 1.32914552\n",
      "Iteration 3, loss = 1.32823401\n",
      "Iteration 4, loss = 1.32735076\n",
      "Iteration 5, loss = 1.32642421\n",
      "Iteration 6, loss = 1.32545554\n",
      "Iteration 7, loss = 1.32453271\n",
      "Iteration 8, loss = 1.32365317\n",
      "Iteration 9, loss = 1.32280139\n",
      "Iteration 10, loss = 1.32196593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 1.32117032\n",
      "Iteration 12, loss = 1.32038331\n",
      "Iteration 13, loss = 1.31960402\n",
      "Iteration 14, loss = 1.31884869\n",
      "Iteration 15, loss = 1.31812134\n",
      "Iteration 16, loss = 1.31742867\n",
      "Iteration 17, loss = 1.31675554\n",
      "Iteration 18, loss = 1.31609612\n",
      "Iteration 19, loss = 1.31545733\n",
      "Iteration 20, loss = 1.31484476\n",
      "Iteration 21, loss = 1.31424522\n",
      "Iteration 22, loss = 1.31365244\n",
      "Iteration 23, loss = 1.31306458\n",
      "Iteration 24, loss = 1.31248752\n",
      "Iteration 25, loss = 1.31192284\n",
      "Iteration 26, loss = 1.31138892\n",
      "Iteration 27, loss = 1.31086959\n",
      "Iteration 28, loss = 1.31035356\n",
      "Iteration 29, loss = 1.30984034\n",
      "Iteration 30, loss = 1.30932968\n",
      "Iteration 31, loss = 1.30882138\n",
      "Iteration 32, loss = 1.30831523\n",
      "Iteration 33, loss = 1.30781517\n",
      "Iteration 34, loss = 1.30731940\n",
      "Iteration 35, loss = 1.30682805\n",
      "Iteration 36, loss = 1.30634405\n",
      "Iteration 37, loss = 1.30586420\n",
      "Iteration 38, loss = 1.30538546\n",
      "Iteration 39, loss = 1.30490779\n",
      "Iteration 40, loss = 1.30443113\n",
      "Iteration 41, loss = 1.30395768\n",
      "Iteration 42, loss = 1.30348891\n",
      "Iteration 43, loss = 1.30302153\n",
      "Iteration 44, loss = 1.30255467\n",
      "Iteration 45, loss = 1.30208835\n",
      "Iteration 46, loss = 1.30162257\n",
      "Iteration 47, loss = 1.30115732\n",
      "Iteration 48, loss = 1.30069261\n",
      "Iteration 49, loss = 1.30022845\n",
      "Iteration 50, loss = 1.29976483\n",
      "Iteration 51, loss = 1.29930176\n",
      "Iteration 52, loss = 1.29883924\n",
      "Iteration 53, loss = 1.29837727\n",
      "Iteration 54, loss = 1.29791585\n",
      "Iteration 55, loss = 1.29745498\n",
      "Iteration 56, loss = 1.29699467\n",
      "Iteration 57, loss = 1.29653491\n",
      "Iteration 58, loss = 1.29607570\n",
      "Iteration 59, loss = 1.29561705\n",
      "Iteration 60, loss = 1.29515896\n",
      "Iteration 61, loss = 1.29470143\n",
      "Iteration 62, loss = 1.29424445\n",
      "Iteration 63, loss = 1.29378803\n",
      "Iteration 64, loss = 1.29333217\n",
      "Iteration 65, loss = 1.29287686\n",
      "Iteration 66, loss = 1.29242212\n",
      "Iteration 67, loss = 1.29196793\n",
      "Iteration 68, loss = 1.29151431\n",
      "Iteration 69, loss = 1.29106124\n",
      "Iteration 70, loss = 1.29060874\n",
      "Iteration 71, loss = 1.29015679\n",
      "Iteration 72, loss = 1.28970541\n",
      "Iteration 73, loss = 1.28925458\n",
      "Iteration 74, loss = 1.28880431\n",
      "Iteration 75, loss = 1.28835461\n",
      "Iteration 76, loss = 1.28790547\n",
      "Iteration 77, loss = 1.28745689\n",
      "Iteration 78, loss = 1.28700886\n",
      "Iteration 79, loss = 1.28656141\n",
      "Iteration 80, loss = 1.28611451\n",
      "Iteration 81, loss = 1.28566817\n",
      "Iteration 82, loss = 1.28522240\n",
      "Iteration 83, loss = 1.28477718\n",
      "Iteration 84, loss = 1.28433253\n",
      "Iteration 85, loss = 1.28388844\n",
      "Iteration 86, loss = 1.28344492\n",
      "Iteration 87, loss = 1.28300195\n",
      "Iteration 88, loss = 1.28255955\n",
      "Iteration 89, loss = 1.28211772\n",
      "Iteration 90, loss = 1.28167644\n",
      "Iteration 91, loss = 1.28123573\n",
      "Iteration 92, loss = 1.28079558\n",
      "Iteration 93, loss = 1.28035600\n",
      "Iteration 94, loss = 1.27991698\n",
      "Iteration 95, loss = 1.27947852\n",
      "Iteration 96, loss = 1.27904063\n",
      "Iteration 97, loss = 1.27860330\n",
      "Iteration 98, loss = 1.27816654\n",
      "Iteration 99, loss = 1.27773034\n",
      "Iteration 100, loss = 1.27729471\n",
      "Iteration 101, loss = 1.27685964\n",
      "Iteration 102, loss = 1.27642513\n",
      "Iteration 103, loss = 1.27599120\n",
      "Iteration 104, loss = 1.27555783\n",
      "Iteration 105, loss = 1.27512502\n",
      "Iteration 106, loss = 1.27469279\n",
      "Iteration 107, loss = 1.27426111\n",
      "Iteration 108, loss = 1.27383001\n",
      "Iteration 109, loss = 1.27339947\n",
      "Iteration 110, loss = 1.27296950\n",
      "Iteration 111, loss = 1.27254010\n",
      "Iteration 112, loss = 1.27211127\n",
      "Iteration 113, loss = 1.27168300\n",
      "Iteration 114, loss = 1.27125531\n",
      "Iteration 115, loss = 1.27082818\n",
      "Iteration 116, loss = 1.27040162\n",
      "Iteration 117, loss = 1.26997563\n",
      "Iteration 118, loss = 1.26955021\n",
      "Iteration 119, loss = 1.26912536\n",
      "Iteration 120, loss = 1.26870108\n",
      "Iteration 121, loss = 1.26827737\n",
      "Iteration 122, loss = 1.26785424\n",
      "Iteration 123, loss = 1.26743167\n",
      "Iteration 124, loss = 1.26700967\n",
      "Iteration 125, loss = 1.26658825\n",
      "Iteration 126, loss = 1.26616740\n",
      "Iteration 127, loss = 1.26574712\n",
      "Iteration 128, loss = 1.26532741\n",
      "Iteration 129, loss = 1.26490827\n",
      "Iteration 130, loss = 1.26448971\n",
      "Iteration 131, loss = 1.26407172\n",
      "Iteration 132, loss = 1.26365431\n",
      "Iteration 133, loss = 1.26323747\n",
      "Iteration 134, loss = 1.26282120\n",
      "Iteration 135, loss = 1.26240551\n",
      "Iteration 136, loss = 1.26199040\n",
      "Iteration 137, loss = 1.26157585\n",
      "Iteration 138, loss = 1.26116189\n",
      "Iteration 139, loss = 1.26074850\n",
      "Iteration 140, loss = 1.26033568\n",
      "Iteration 141, loss = 1.25992345\n",
      "Iteration 142, loss = 1.25951179\n",
      "Iteration 143, loss = 1.25910070\n",
      "Iteration 144, loss = 1.25869020\n",
      "Iteration 145, loss = 1.25828027\n",
      "Iteration 146, loss = 1.25787092\n",
      "Iteration 147, loss = 1.25746214\n",
      "Iteration 148, loss = 1.25705395\n",
      "Iteration 149, loss = 1.25664633\n",
      "Iteration 150, loss = 1.25623930\n",
      "Iteration 151, loss = 1.25583284\n",
      "Iteration 152, loss = 1.25542696\n",
      "Iteration 153, loss = 1.25502167\n",
      "Iteration 154, loss = 1.25461695\n",
      "Iteration 155, loss = 1.25421281\n",
      "Iteration 156, loss = 1.25380926\n",
      "Iteration 157, loss = 1.25340628\n",
      "Iteration 158, loss = 1.25300389\n",
      "Iteration 159, loss = 1.25260208\n",
      "Iteration 160, loss = 1.25220085\n",
      "Iteration 161, loss = 1.25180020\n",
      "Iteration 162, loss = 1.25140014\n",
      "Iteration 163, loss = 1.25100066\n",
      "Iteration 164, loss = 1.25060176\n",
      "Iteration 165, loss = 1.25020345\n",
      "Iteration 166, loss = 1.24980572\n",
      "Iteration 167, loss = 1.24940857\n",
      "Iteration 168, loss = 1.24901201\n",
      "Iteration 169, loss = 1.24861603\n",
      "Iteration 170, loss = 1.24822064\n",
      "Iteration 171, loss = 1.24782583\n",
      "Iteration 172, loss = 1.24743161\n",
      "Iteration 173, loss = 1.24703797\n",
      "Iteration 174, loss = 1.24664492\n",
      "Iteration 175, loss = 1.24625246\n",
      "Iteration 176, loss = 1.24586058\n",
      "Iteration 177, loss = 1.24546929\n",
      "Iteration 178, loss = 1.24507859\n",
      "Iteration 179, loss = 1.24468848\n",
      "Iteration 180, loss = 1.24429895\n",
      "Iteration 181, loss = 1.24391001\n",
      "Iteration 182, loss = 1.24352166\n",
      "Iteration 183, loss = 1.24313390\n",
      "Iteration 184, loss = 1.24274672\n",
      "Iteration 185, loss = 1.24236014\n",
      "Iteration 186, loss = 1.24197414\n",
      "Iteration 187, loss = 1.24158874\n",
      "Iteration 188, loss = 1.24120392\n",
      "Iteration 189, loss = 1.24081970\n",
      "Iteration 190, loss = 1.24043606\n",
      "Iteration 191, loss = 1.24005301\n",
      "Iteration 192, loss = 1.23967056\n",
      "Iteration 193, loss = 1.23928870\n",
      "Iteration 194, loss = 1.23890743\n",
      "Iteration 195, loss = 1.23852675\n",
      "Iteration 196, loss = 1.23814666\n",
      "Iteration 197, loss = 1.23776716\n",
      "Iteration 198, loss = 1.23738826\n",
      "Iteration 199, loss = 1.23700995\n",
      "Iteration 200, loss = 1.23663223\n",
      "Iteration 201, loss = 1.23625510\n",
      "Iteration 202, loss = 1.23587857\n",
      "Iteration 203, loss = 1.23550263\n",
      "Iteration 204, loss = 1.23512729\n",
      "Iteration 205, loss = 1.23475254\n",
      "Iteration 206, loss = 1.23437838\n",
      "Iteration 207, loss = 1.23400482\n",
      "Iteration 208, loss = 1.23363185\n",
      "Iteration 209, loss = 1.23325948\n",
      "Iteration 210, loss = 1.23288770\n",
      "Iteration 211, loss = 1.23251652\n",
      "Iteration 212, loss = 1.23214593\n",
      "Iteration 213, loss = 1.23177594\n",
      "Iteration 214, loss = 1.23140655\n",
      "Iteration 215, loss = 1.23103775\n",
      "Iteration 216, loss = 1.23066955\n",
      "Iteration 217, loss = 1.23030194\n",
      "Iteration 218, loss = 1.22993493\n",
      "Iteration 219, loss = 1.22956852\n",
      "Iteration 220, loss = 1.22920271\n",
      "Iteration 221, loss = 1.22883749\n",
      "Iteration 222, loss = 1.22847287\n",
      "Iteration 223, loss = 1.22810885\n",
      "Iteration 224, loss = 1.22774542\n",
      "Iteration 225, loss = 1.22738260\n",
      "Iteration 226, loss = 1.22702037\n",
      "Iteration 227, loss = 1.22665874\n",
      "Iteration 228, loss = 1.22629771\n",
      "Iteration 229, loss = 1.22593728\n",
      "Iteration 230, loss = 1.22557745\n",
      "Iteration 231, loss = 1.22521821\n",
      "Iteration 232, loss = 1.22485958\n",
      "Iteration 233, loss = 1.22450155\n",
      "Iteration 234, loss = 1.22414411\n",
      "Iteration 235, loss = 1.22378728\n",
      "Iteration 236, loss = 1.22343104\n",
      "Iteration 237, loss = 1.22307541\n",
      "Iteration 238, loss = 1.22272037\n",
      "Iteration 239, loss = 1.22236594\n",
      "Iteration 240, loss = 1.22201211\n",
      "Iteration 241, loss = 1.22165888\n",
      "Iteration 242, loss = 1.22130625\n",
      "Iteration 243, loss = 1.22095422\n",
      "Iteration 244, loss = 1.22060279\n",
      "Iteration 245, loss = 1.22025196\n",
      "Iteration 246, loss = 1.21990174\n",
      "Iteration 247, loss = 1.21955212\n",
      "Iteration 248, loss = 1.21920309\n",
      "Iteration 249, loss = 1.21885467\n",
      "Iteration 250, loss = 1.21850686\n",
      "Iteration 251, loss = 1.21815964\n",
      "Iteration 252, loss = 1.21781303\n",
      "Iteration 253, loss = 1.21746702\n",
      "Iteration 254, loss = 1.21712161\n",
      "Iteration 255, loss = 1.21677681\n",
      "Iteration 256, loss = 1.21643261\n",
      "Iteration 257, loss = 1.21608901\n",
      "Iteration 258, loss = 1.21574601\n",
      "Iteration 259, loss = 1.21540362\n",
      "Iteration 260, loss = 1.21506183\n",
      "Iteration 261, loss = 1.21472065\n",
      "Iteration 262, loss = 1.21438006\n",
      "Iteration 263, loss = 1.21404009\n",
      "Iteration 264, loss = 1.21370071\n",
      "Iteration 265, loss = 1.21336194\n",
      "Iteration 266, loss = 1.21302377\n",
      "Iteration 267, loss = 1.21268621\n",
      "Iteration 268, loss = 1.21234925\n",
      "Iteration 269, loss = 1.21201290\n",
      "Iteration 270, loss = 1.21167715\n",
      "Iteration 271, loss = 1.21134200\n",
      "Iteration 272, loss = 1.21100746\n",
      "Iteration 273, loss = 1.21067352\n",
      "Iteration 274, loss = 1.21034019\n",
      "Iteration 275, loss = 1.21000746\n",
      "Iteration 276, loss = 1.20967534\n",
      "Iteration 277, loss = 1.20934382\n",
      "Iteration 278, loss = 1.20901291\n",
      "Iteration 279, loss = 1.20868260\n",
      "Iteration 280, loss = 1.20835289\n",
      "Iteration 281, loss = 1.20802379\n",
      "Iteration 282, loss = 1.20769530\n",
      "Iteration 283, loss = 1.20736741\n",
      "Iteration 284, loss = 1.20704013\n",
      "Iteration 285, loss = 1.20671345\n",
      "Iteration 286, loss = 1.20638738\n",
      "Iteration 287, loss = 1.20606191\n",
      "Iteration 288, loss = 1.20573705\n",
      "Iteration 289, loss = 1.20541279\n",
      "Iteration 290, loss = 1.20508914\n",
      "Iteration 291, loss = 1.20476609\n",
      "Iteration 292, loss = 1.20444365\n",
      "Iteration 293, loss = 1.20412181\n",
      "Iteration 294, loss = 1.20380058\n",
      "Iteration 295, loss = 1.20347996\n",
      "Iteration 296, loss = 1.20315994\n",
      "Iteration 297, loss = 1.20284053\n",
      "Iteration 298, loss = 1.20252172\n",
      "Iteration 299, loss = 1.20220351\n",
      "Iteration 300, loss = 1.20188592\n",
      "Iteration 301, loss = 1.20156893\n",
      "Iteration 302, loss = 1.20125254\n",
      "Iteration 303, loss = 1.20093676\n",
      "Iteration 304, loss = 1.20062158\n",
      "Iteration 305, loss = 1.20030701\n",
      "Iteration 306, loss = 1.19999305\n",
      "Iteration 307, loss = 1.19967969\n",
      "Iteration 308, loss = 1.19936694\n",
      "Iteration 309, loss = 1.19905479\n",
      "Iteration 310, loss = 1.19874324\n",
      "Iteration 311, loss = 1.19843231\n",
      "Iteration 312, loss = 1.19812198\n",
      "Iteration 313, loss = 1.19781225\n",
      "Iteration 314, loss = 1.19750313\n",
      "Iteration 315, loss = 1.19719461\n",
      "Iteration 316, loss = 1.19688670\n",
      "Iteration 317, loss = 1.19657939\n",
      "Iteration 318, loss = 1.19627269\n",
      "Iteration 319, loss = 1.19596660\n",
      "Iteration 320, loss = 1.19566110\n",
      "Iteration 321, loss = 1.19535622\n",
      "Iteration 322, loss = 1.19505194\n",
      "Iteration 323, loss = 1.19474826\n",
      "Iteration 324, loss = 1.19444519\n",
      "Iteration 325, loss = 1.19414272\n",
      "Iteration 326, loss = 1.19384086\n",
      "Iteration 327, loss = 1.19353960\n",
      "Iteration 328, loss = 1.19323895\n",
      "Iteration 329, loss = 1.19293890\n",
      "Iteration 330, loss = 1.19263945\n",
      "Iteration 331, loss = 1.19234061\n",
      "Iteration 332, loss = 1.19204238\n",
      "Iteration 333, loss = 1.19174474\n",
      "Iteration 334, loss = 1.19144771\n",
      "Iteration 335, loss = 1.19115129\n",
      "Iteration 336, loss = 1.19085547\n",
      "Iteration 337, loss = 1.19056025\n",
      "Iteration 338, loss = 1.19026564\n",
      "Iteration 339, loss = 1.18997163\n",
      "Iteration 340, loss = 1.18967822\n",
      "Iteration 341, loss = 1.18938541\n",
      "Iteration 342, loss = 1.18909321\n",
      "Iteration 343, loss = 1.18880161\n",
      "Iteration 344, loss = 1.18851062\n",
      "Iteration 345, loss = 1.18822023\n",
      "Iteration 346, loss = 1.18793044\n",
      "Iteration 347, loss = 1.18764125\n",
      "Iteration 348, loss = 1.18735266\n",
      "Iteration 349, loss = 1.18706468\n",
      "Iteration 350, loss = 1.18677730\n",
      "Iteration 351, loss = 1.18649052\n",
      "Iteration 352, loss = 1.18620434\n",
      "Iteration 353, loss = 1.18591877\n",
      "Iteration 354, loss = 1.18563379\n",
      "Iteration 355, loss = 1.18534942\n",
      "Iteration 356, loss = 1.18506565\n",
      "Iteration 357, loss = 1.18478248\n",
      "Iteration 358, loss = 1.18449991\n",
      "Iteration 359, loss = 1.18421794\n",
      "Iteration 360, loss = 1.18393657\n",
      "Iteration 361, loss = 1.18365580\n",
      "Iteration 362, loss = 1.18337563\n",
      "Iteration 363, loss = 1.18309606\n",
      "Iteration 364, loss = 1.18281709\n",
      "Iteration 365, loss = 1.18253872\n",
      "Iteration 366, loss = 1.18226095\n",
      "Iteration 367, loss = 1.18198378\n",
      "Iteration 368, loss = 1.18170721\n",
      "Iteration 369, loss = 1.18143124\n",
      "Iteration 370, loss = 1.18115587\n",
      "Iteration 371, loss = 1.18088109\n",
      "Iteration 372, loss = 1.18060691\n",
      "Iteration 373, loss = 1.18033333\n",
      "Iteration 374, loss = 1.18006035\n",
      "Iteration 375, loss = 1.17978797\n",
      "Iteration 376, loss = 1.17951618\n",
      "Iteration 377, loss = 1.17924499\n",
      "Iteration 378, loss = 1.17897440\n",
      "Iteration 379, loss = 1.17870440\n",
      "Iteration 380, loss = 1.17843500\n",
      "Iteration 381, loss = 1.17816620\n",
      "Iteration 382, loss = 1.17789799\n",
      "Iteration 383, loss = 1.17763038\n",
      "Iteration 384, loss = 1.17736337\n",
      "Iteration 385, loss = 1.17709695\n",
      "Iteration 386, loss = 1.17683112\n",
      "Iteration 387, loss = 1.17656589\n",
      "Iteration 388, loss = 1.17630126\n",
      "Iteration 389, loss = 1.17603722\n",
      "Iteration 390, loss = 1.17577377\n",
      "Iteration 391, loss = 1.17551091\n",
      "Iteration 392, loss = 1.17524865\n",
      "Iteration 393, loss = 1.17498699\n",
      "Iteration 394, loss = 1.17472591\n",
      "Iteration 395, loss = 1.17446543\n",
      "Iteration 396, loss = 1.17420554\n",
      "Iteration 397, loss = 1.17394625\n",
      "Iteration 398, loss = 1.17368754\n",
      "Iteration 399, loss = 1.17342943\n",
      "Iteration 400, loss = 1.17317191\n",
      "Iteration 401, loss = 1.17291498\n",
      "Iteration 402, loss = 1.17265864\n",
      "Iteration 403, loss = 1.17240289\n",
      "Iteration 404, loss = 1.17214773\n",
      "Iteration 405, loss = 1.17189316\n",
      "Iteration 406, loss = 1.17163918\n",
      "Iteration 407, loss = 1.17138579\n",
      "Iteration 408, loss = 1.17113299\n",
      "Iteration 409, loss = 1.17088078\n",
      "Iteration 410, loss = 1.17062915\n",
      "Iteration 411, loss = 1.17037812\n",
      "Iteration 412, loss = 1.17012767\n",
      "Iteration 413, loss = 1.16987781\n",
      "Iteration 414, loss = 1.16962853\n",
      "Iteration 415, loss = 1.16937984\n",
      "Iteration 416, loss = 1.16913174\n",
      "Iteration 417, loss = 1.16888423\n",
      "Iteration 418, loss = 1.16863730\n",
      "Iteration 419, loss = 1.16839095\n",
      "Iteration 420, loss = 1.16814519\n",
      "Iteration 421, loss = 1.16790002\n",
      "Iteration 422, loss = 1.16765543\n",
      "Iteration 423, loss = 1.16741142\n",
      "Iteration 424, loss = 1.16716800\n",
      "Iteration 425, loss = 1.16692516\n",
      "Iteration 426, loss = 1.16668290\n",
      "Iteration 427, loss = 1.16644123\n",
      "Iteration 428, loss = 1.16620014\n",
      "Iteration 429, loss = 1.16595963\n",
      "Iteration 430, loss = 1.16571970\n",
      "Iteration 431, loss = 1.16548035\n",
      "Iteration 432, loss = 1.16524158\n",
      "Iteration 433, loss = 1.16500340\n",
      "Iteration 434, loss = 1.16476579\n",
      "Iteration 435, loss = 1.16452876\n",
      "Iteration 436, loss = 1.16429231\n",
      "Iteration 437, loss = 1.16405644\n",
      "Iteration 438, loss = 1.16382115\n",
      "Iteration 439, loss = 1.16358644\n",
      "Iteration 440, loss = 1.16335230\n",
      "Iteration 441, loss = 1.16311874\n",
      "Iteration 442, loss = 1.16288576\n",
      "Iteration 443, loss = 1.16265335\n",
      "Iteration 444, loss = 1.16242152\n",
      "Iteration 445, loss = 1.16219027\n",
      "Iteration 446, loss = 1.16195959\n",
      "Iteration 447, loss = 1.16172948\n",
      "Iteration 448, loss = 1.16149995\n",
      "Iteration 449, loss = 1.16127100\n",
      "Iteration 450, loss = 1.16104261\n",
      "Iteration 451, loss = 1.16081480\n",
      "Iteration 452, loss = 1.16058756\n",
      "Iteration 453, loss = 1.16036090\n",
      "Iteration 454, loss = 1.16013480\n",
      "Iteration 455, loss = 1.15990928\n",
      "Iteration 456, loss = 1.15968432\n",
      "Iteration 457, loss = 1.15945994\n",
      "Iteration 458, loss = 1.15923613\n",
      "Iteration 459, loss = 1.15901288\n",
      "Iteration 460, loss = 1.15879021\n",
      "Iteration 461, loss = 1.15856810\n",
      "Iteration 462, loss = 1.15834656\n",
      "Iteration 463, loss = 1.15812559\n",
      "Iteration 464, loss = 1.15790519\n",
      "Iteration 465, loss = 1.15768535\n",
      "Iteration 466, loss = 1.15746608\n",
      "Iteration 467, loss = 1.15724737\n",
      "Iteration 468, loss = 1.15702923\n",
      "Iteration 469, loss = 1.15681166\n",
      "Iteration 470, loss = 1.15659464\n",
      "Iteration 471, loss = 1.15637820\n",
      "Iteration 472, loss = 1.15616231\n",
      "Iteration 473, loss = 1.15594699\n",
      "Iteration 474, loss = 1.15573223\n",
      "Iteration 475, loss = 1.15551803\n",
      "Iteration 476, loss = 1.15530439\n",
      "Iteration 477, loss = 1.15509131\n",
      "Iteration 478, loss = 1.15487880\n",
      "Iteration 479, loss = 1.15466684\n",
      "Iteration 480, loss = 1.15445544\n",
      "Iteration 481, loss = 1.15424460\n",
      "Iteration 482, loss = 1.15403432\n",
      "Iteration 483, loss = 1.15382460\n",
      "Iteration 484, loss = 1.15361543\n",
      "Iteration 485, loss = 1.15340682\n",
      "Iteration 486, loss = 1.15319876\n",
      "Iteration 487, loss = 1.15299126\n",
      "Iteration 488, loss = 1.15278432\n",
      "Iteration 489, loss = 1.15257793\n",
      "Iteration 490, loss = 1.15237209\n",
      "Iteration 491, loss = 1.15216681\n",
      "Iteration 492, loss = 1.15196208\n",
      "Iteration 493, loss = 1.15175790\n",
      "Iteration 494, loss = 1.15155428\n",
      "Iteration 495, loss = 1.15135120\n",
      "Iteration 496, loss = 1.15114868\n",
      "Iteration 497, loss = 1.15094670\n",
      "Iteration 498, loss = 1.15074528\n",
      "Iteration 499, loss = 1.15054440\n",
      "Iteration 500, loss = 1.15034408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 501, loss = 1.15014430\n",
      "Iteration 502, loss = 1.14994506\n",
      "Iteration 503, loss = 1.14974638\n",
      "Iteration 504, loss = 1.14954824\n",
      "Iteration 505, loss = 1.14935064\n",
      "Iteration 506, loss = 1.14915359\n",
      "Iteration 507, loss = 1.14895708\n",
      "Iteration 508, loss = 1.14876112\n",
      "Iteration 509, loss = 1.14856570\n",
      "Iteration 510, loss = 1.14837083\n",
      "Iteration 511, loss = 1.14817649\n",
      "Iteration 512, loss = 1.14798270\n",
      "Iteration 513, loss = 1.14778944\n",
      "Iteration 514, loss = 1.14759673\n",
      "Iteration 515, loss = 1.14740456\n",
      "Iteration 516, loss = 1.14721292\n",
      "Iteration 517, loss = 1.14702183\n",
      "Iteration 518, loss = 1.14683127\n",
      "Iteration 519, loss = 1.14664125\n",
      "Iteration 520, loss = 1.14645176\n",
      "Iteration 521, loss = 1.14626281\n",
      "Iteration 522, loss = 1.14607440\n",
      "Iteration 523, loss = 1.14588652\n",
      "Iteration 524, loss = 1.14569918\n",
      "Iteration 525, loss = 1.14551236\n",
      "Iteration 526, loss = 1.14532608\n",
      "Iteration 527, loss = 1.14514034\n",
      "Iteration 528, loss = 1.14495512\n",
      "Iteration 529, loss = 1.14477044\n",
      "Iteration 530, loss = 1.14458628\n",
      "Iteration 531, loss = 1.14440265\n",
      "Iteration 532, loss = 1.14421956\n",
      "Iteration 533, loss = 1.14403699\n",
      "Iteration 534, loss = 1.14385495\n",
      "Iteration 535, loss = 1.14367343\n",
      "Iteration 536, loss = 1.14349245\n",
      "Iteration 537, loss = 1.14331198\n",
      "Iteration 538, loss = 1.14313205\n",
      "Iteration 539, loss = 1.14295263\n",
      "Iteration 540, loss = 1.14277374\n",
      "Iteration 541, loss = 1.14259538\n",
      "Iteration 542, loss = 1.14241753\n",
      "Iteration 543, loss = 1.14224021\n",
      "Iteration 544, loss = 1.14206341\n",
      "Iteration 545, loss = 1.14188713\n",
      "Iteration 546, loss = 1.14171137\n",
      "Iteration 547, loss = 1.14153612\n",
      "Iteration 548, loss = 1.14136140\n",
      "Iteration 549, loss = 1.14118719\n",
      "Iteration 550, loss = 1.14101350\n",
      "Iteration 551, loss = 1.14084033\n",
      "Iteration 552, loss = 1.14066767\n",
      "Iteration 553, loss = 1.14049553\n",
      "Iteration 554, loss = 1.14032390\n",
      "Iteration 555, loss = 1.14015278\n",
      "Iteration 556, loss = 1.13998218\n",
      "Iteration 557, loss = 1.13981208\n",
      "Iteration 558, loss = 1.13964250\n",
      "Iteration 559, loss = 1.13947343\n",
      "Iteration 560, loss = 1.13930487\n",
      "Iteration 561, loss = 1.13913682\n",
      "Iteration 562, loss = 1.13896928\n",
      "Iteration 563, loss = 1.13880225\n",
      "Iteration 564, loss = 1.13863572\n",
      "Iteration 565, loss = 1.13846970\n",
      "Iteration 566, loss = 1.13830418\n",
      "Iteration 567, loss = 1.13813917\n",
      "Iteration 568, loss = 1.13797466\n",
      "Iteration 569, loss = 1.13781066\n",
      "Iteration 570, loss = 1.13764716\n",
      "Iteration 571, loss = 1.13748416\n",
      "Iteration 572, loss = 1.13732166\n",
      "Iteration 573, loss = 1.13715967\n",
      "Iteration 574, loss = 1.13699817\n",
      "Iteration 575, loss = 1.13683717\n",
      "Iteration 576, loss = 1.13667667\n",
      "Iteration 577, loss = 1.13651667\n",
      "Iteration 578, loss = 1.13635716\n",
      "Iteration 579, loss = 1.13619816\n",
      "Iteration 580, loss = 1.13603964\n",
      "Iteration 581, loss = 1.13588162\n",
      "Iteration 582, loss = 1.13572410\n",
      "Iteration 583, loss = 1.13556706\n",
      "Iteration 584, loss = 1.13541052\n",
      "Iteration 585, loss = 1.13525447\n",
      "Iteration 586, loss = 1.13509891\n",
      "Iteration 587, loss = 1.13494385\n",
      "Iteration 588, loss = 1.13478927\n",
      "Iteration 589, loss = 1.13463518\n",
      "Iteration 590, loss = 1.13448157\n",
      "Iteration 591, loss = 1.13432846\n",
      "Iteration 592, loss = 1.13417583\n",
      "Iteration 593, loss = 1.13402368\n",
      "Iteration 594, loss = 1.13387203\n",
      "Iteration 595, loss = 1.13372085\n",
      "Iteration 596, loss = 1.13357016\n",
      "Iteration 597, loss = 1.13341995\n",
      "Iteration 598, loss = 1.13327022\n",
      "Iteration 599, loss = 1.13312097\n",
      "Iteration 600, loss = 1.13297220\n",
      "Iteration 601, loss = 1.13282391\n",
      "Iteration 602, loss = 1.13267610\n",
      "Iteration 603, loss = 1.13252877\n",
      "Iteration 604, loss = 1.13238191\n",
      "Iteration 605, loss = 1.13223553\n",
      "Iteration 606, loss = 1.13208963\n",
      "Iteration 607, loss = 1.13194420\n",
      "Iteration 608, loss = 1.13179924\n",
      "Iteration 609, loss = 1.13165476\n",
      "Iteration 610, loss = 1.13151074\n",
      "Iteration 611, loss = 1.13136720\n",
      "Iteration 612, loss = 1.13122413\n",
      "Iteration 613, loss = 1.13108153\n",
      "Iteration 614, loss = 1.13093940\n",
      "Iteration 615, loss = 1.13079774\n",
      "Iteration 616, loss = 1.13065654\n",
      "Iteration 617, loss = 1.13051581\n",
      "Iteration 618, loss = 1.13037554\n",
      "Iteration 619, loss = 1.13023574\n",
      "Iteration 620, loss = 1.13009641\n",
      "Iteration 621, loss = 1.12995753\n",
      "Iteration 622, loss = 1.12981912\n",
      "Iteration 623, loss = 1.12968117\n",
      "Iteration 624, loss = 1.12954368\n",
      "Iteration 625, loss = 1.12940665\n",
      "Iteration 626, loss = 1.12927008\n",
      "Iteration 627, loss = 1.12913397\n",
      "Iteration 628, loss = 1.12899832\n",
      "Iteration 629, loss = 1.12886312\n",
      "Iteration 630, loss = 1.12872838\n",
      "Iteration 631, loss = 1.12859409\n",
      "Iteration 632, loss = 1.12846026\n",
      "Iteration 633, loss = 1.12832688\n",
      "Iteration 634, loss = 1.12819395\n",
      "Iteration 635, loss = 1.12806147\n",
      "Iteration 636, loss = 1.12792945\n",
      "Iteration 637, loss = 1.12779787\n",
      "Iteration 638, loss = 1.12766674\n",
      "Iteration 639, loss = 1.12753607\n",
      "Iteration 640, loss = 1.12740583\n",
      "Iteration 641, loss = 1.12727605\n",
      "Iteration 642, loss = 1.12714671\n",
      "Iteration 643, loss = 1.12701782\n",
      "Iteration 644, loss = 1.12688936\n",
      "Iteration 645, loss = 1.12676136\n",
      "Iteration 646, loss = 1.12663379\n",
      "Iteration 647, loss = 1.12650667\n",
      "Iteration 648, loss = 1.12637998\n",
      "Iteration 649, loss = 1.12625374\n",
      "Iteration 650, loss = 1.12612793\n",
      "Iteration 651, loss = 1.12600257\n",
      "Iteration 652, loss = 1.12587764\n",
      "Iteration 653, loss = 1.12575314\n",
      "Iteration 654, loss = 1.12562908\n",
      "Iteration 655, loss = 1.12550546\n",
      "Iteration 656, loss = 1.12538227\n",
      "Iteration 657, loss = 1.12525951\n",
      "Iteration 658, loss = 1.12513719\n",
      "Iteration 659, loss = 1.12501529\n",
      "Iteration 660, loss = 1.12489383\n",
      "Iteration 661, loss = 1.12477279\n",
      "Iteration 662, loss = 1.12465218\n",
      "Iteration 663, loss = 1.12453201\n",
      "Iteration 664, loss = 1.12441225\n",
      "Iteration 665, loss = 1.12429293\n",
      "Iteration 666, loss = 1.12417402\n",
      "Iteration 667, loss = 1.12405555\n",
      "Iteration 668, loss = 1.12393749\n",
      "Iteration 669, loss = 1.12381986\n",
      "Iteration 670, loss = 1.12370265\n",
      "Iteration 671, loss = 1.12358586\n",
      "Iteration 672, loss = 1.12346949\n",
      "Iteration 673, loss = 1.12335353\n",
      "Iteration 674, loss = 1.12323800\n",
      "Iteration 675, loss = 1.12312288\n",
      "Iteration 676, loss = 1.12300818\n",
      "Iteration 677, loss = 1.12289390\n",
      "Iteration 678, loss = 1.12278003\n",
      "Iteration 679, loss = 1.12266657\n",
      "Iteration 680, loss = 1.12255352\n",
      "Iteration 681, loss = 1.12244089\n",
      "Iteration 682, loss = 1.12232867\n",
      "Iteration 683, loss = 1.12221685\n",
      "Iteration 684, loss = 1.12210545\n",
      "Iteration 685, loss = 1.12199446\n",
      "Iteration 686, loss = 1.12188387\n",
      "Iteration 687, loss = 1.12177369\n",
      "Iteration 688, loss = 1.12166391\n",
      "Iteration 689, loss = 1.12155454\n",
      "Iteration 690, loss = 1.12144557\n",
      "Iteration 691, loss = 1.12133700\n",
      "Iteration 692, loss = 1.12122884\n",
      "Iteration 693, loss = 1.12112108\n",
      "Iteration 694, loss = 1.12101372\n",
      "Iteration 695, loss = 1.12090675\n",
      "Iteration 696, loss = 1.12080019\n",
      "Iteration 697, loss = 1.12069402\n",
      "Iteration 698, loss = 1.12058825\n",
      "Iteration 699, loss = 1.12048288\n",
      "Iteration 700, loss = 1.12037790\n",
      "Iteration 701, loss = 1.12027331\n",
      "Iteration 702, loss = 1.12016912\n",
      "Iteration 703, loss = 1.12006531\n",
      "Iteration 704, loss = 1.11996190\n",
      "Iteration 705, loss = 1.11985888\n",
      "Iteration 706, loss = 1.11975625\n",
      "Iteration 707, loss = 1.11965401\n",
      "Iteration 708, loss = 1.11955216\n",
      "Iteration 709, loss = 1.11945069\n",
      "Iteration 710, loss = 1.11934960\n",
      "Iteration 711, loss = 1.11924891\n",
      "Iteration 712, loss = 1.11914859\n",
      "Iteration 713, loss = 1.11904866\n",
      "Iteration 714, loss = 1.11894911\n",
      "Iteration 715, loss = 1.11884995\n",
      "Iteration 716, loss = 1.11875116\n",
      "Iteration 717, loss = 1.11865275\n",
      "Iteration 718, loss = 1.11855472\n",
      "Iteration 719, loss = 1.11845707\n",
      "Iteration 720, loss = 1.11835979\n",
      "Iteration 721, loss = 1.11826289\n",
      "Iteration 722, loss = 1.11816637\n",
      "Iteration 723, loss = 1.11807022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33048782\n",
      "Iteration 2, loss = 1.33019556\n",
      "Iteration 3, loss = 1.32978646\n",
      "Iteration 4, loss = 1.32928475\n",
      "Iteration 5, loss = 1.32869964\n",
      "Iteration 6, loss = 1.32804623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.32733465\n",
      "Iteration 8, loss = 1.32658809\n",
      "Iteration 9, loss = 1.32577694\n",
      "Iteration 10, loss = 1.32485449\n",
      "Iteration 11, loss = 1.32392447\n",
      "Iteration 12, loss = 1.32299100\n",
      "Iteration 13, loss = 1.32202819\n",
      "Iteration 14, loss = 1.32104915\n",
      "Iteration 15, loss = 1.32005798\n",
      "Iteration 16, loss = 1.31905479\n",
      "Iteration 17, loss = 1.31804483\n",
      "Iteration 18, loss = 1.31702322\n",
      "Iteration 19, loss = 1.31603018\n",
      "Iteration 20, loss = 1.31508552\n",
      "Iteration 21, loss = 1.31415028\n",
      "Iteration 22, loss = 1.31322275\n",
      "Iteration 23, loss = 1.31231060\n",
      "Iteration 24, loss = 1.31140019\n",
      "Iteration 25, loss = 1.31048938\n",
      "Iteration 26, loss = 1.30957843\n",
      "Iteration 27, loss = 1.30866756\n",
      "Iteration 28, loss = 1.30776867\n",
      "Iteration 29, loss = 1.30690251\n",
      "Iteration 30, loss = 1.30603752\n",
      "Iteration 31, loss = 1.30517377\n",
      "Iteration 32, loss = 1.30431541\n",
      "Iteration 33, loss = 1.30346711\n",
      "Iteration 34, loss = 1.30262145\n",
      "Iteration 35, loss = 1.30178078\n",
      "Iteration 36, loss = 1.30094689\n",
      "Iteration 37, loss = 1.30011740\n",
      "Iteration 38, loss = 1.29928951\n",
      "Iteration 39, loss = 1.29847446\n",
      "Iteration 40, loss = 1.29766248\n",
      "Iteration 41, loss = 1.29685193\n",
      "Iteration 42, loss = 1.29604288\n",
      "Iteration 43, loss = 1.29523540\n",
      "Iteration 44, loss = 1.29442954\n",
      "Iteration 45, loss = 1.29362538\n",
      "Iteration 46, loss = 1.29282295\n",
      "Iteration 47, loss = 1.29202231\n",
      "Iteration 48, loss = 1.29122349\n",
      "Iteration 49, loss = 1.29042654\n",
      "Iteration 50, loss = 1.28963149\n",
      "Iteration 51, loss = 1.28883837\n",
      "Iteration 52, loss = 1.28804721\n",
      "Iteration 53, loss = 1.28725803\n",
      "Iteration 54, loss = 1.28647086\n",
      "Iteration 55, loss = 1.28568571\n",
      "Iteration 56, loss = 1.28490261\n",
      "Iteration 57, loss = 1.28412157\n",
      "Iteration 58, loss = 1.28334260\n",
      "Iteration 59, loss = 1.28256573\n",
      "Iteration 60, loss = 1.28179095\n",
      "Iteration 61, loss = 1.28101829\n",
      "Iteration 62, loss = 1.28024775\n",
      "Iteration 63, loss = 1.27947934\n",
      "Iteration 64, loss = 1.27871307\n",
      "Iteration 65, loss = 1.27794894\n",
      "Iteration 66, loss = 1.27718696\n",
      "Iteration 67, loss = 1.27642714\n",
      "Iteration 68, loss = 1.27566948\n",
      "Iteration 69, loss = 1.27491398\n",
      "Iteration 70, loss = 1.27416065\n",
      "Iteration 71, loss = 1.27340952\n",
      "Iteration 72, loss = 1.27266160\n",
      "Iteration 73, loss = 1.27191590\n",
      "Iteration 74, loss = 1.27117242\n",
      "Iteration 75, loss = 1.27043117\n",
      "Iteration 76, loss = 1.26969214\n",
      "Iteration 77, loss = 1.26895533\n",
      "Iteration 78, loss = 1.26822074\n",
      "Iteration 79, loss = 1.26748837\n",
      "Iteration 80, loss = 1.26675822\n",
      "Iteration 81, loss = 1.26603028\n",
      "Iteration 82, loss = 1.26530456\n",
      "Iteration 83, loss = 1.26458105\n",
      "Iteration 84, loss = 1.26385975\n",
      "Iteration 85, loss = 1.26314066\n",
      "Iteration 86, loss = 1.26242379\n",
      "Iteration 87, loss = 1.26170912\n",
      "Iteration 88, loss = 1.26099666\n",
      "Iteration 89, loss = 1.26028641\n",
      "Iteration 90, loss = 1.25957836\n",
      "Iteration 91, loss = 1.25887251\n",
      "Iteration 92, loss = 1.25816887\n",
      "Iteration 93, loss = 1.25746742\n",
      "Iteration 94, loss = 1.25676818\n",
      "Iteration 95, loss = 1.25607113\n",
      "Iteration 96, loss = 1.25537628\n",
      "Iteration 97, loss = 1.25468362\n",
      "Iteration 98, loss = 1.25399316\n",
      "Iteration 99, loss = 1.25330488\n",
      "Iteration 100, loss = 1.25261880\n",
      "Iteration 101, loss = 1.25193491\n",
      "Iteration 102, loss = 1.25125320\n",
      "Iteration 103, loss = 1.25057368\n",
      "Iteration 104, loss = 1.24989633\n",
      "Iteration 105, loss = 1.24922118\n",
      "Iteration 106, loss = 1.24854820\n",
      "Iteration 107, loss = 1.24787739\n",
      "Iteration 108, loss = 1.24720877\n",
      "Iteration 109, loss = 1.24654232\n",
      "Iteration 110, loss = 1.24587804\n",
      "Iteration 111, loss = 1.24521593\n",
      "Iteration 112, loss = 1.24455599\n",
      "Iteration 113, loss = 1.24389821\n",
      "Iteration 114, loss = 1.24324261\n",
      "Iteration 115, loss = 1.24258916\n",
      "Iteration 116, loss = 1.24193787\n",
      "Iteration 117, loss = 1.24128874\n",
      "Iteration 118, loss = 1.24064177\n",
      "Iteration 119, loss = 1.23999695\n",
      "Iteration 120, loss = 1.23935429\n",
      "Iteration 121, loss = 1.23871377\n",
      "Iteration 122, loss = 1.23807540\n",
      "Iteration 123, loss = 1.23743918\n",
      "Iteration 124, loss = 1.23680510\n",
      "Iteration 125, loss = 1.23617316\n",
      "Iteration 126, loss = 1.23554336\n",
      "Iteration 127, loss = 1.23491569\n",
      "Iteration 128, loss = 1.23429016\n",
      "Iteration 129, loss = 1.23366676\n",
      "Iteration 130, loss = 1.23304549\n",
      "Iteration 131, loss = 1.23242635\n",
      "Iteration 132, loss = 1.23180933\n",
      "Iteration 133, loss = 1.23119443\n",
      "Iteration 134, loss = 1.23058165\n",
      "Iteration 135, loss = 1.22997099\n",
      "Iteration 136, loss = 1.22936244\n",
      "Iteration 137, loss = 1.22875600\n",
      "Iteration 138, loss = 1.22815168\n",
      "Iteration 139, loss = 1.22754945\n",
      "Iteration 140, loss = 1.22694933\n",
      "Iteration 141, loss = 1.22635132\n",
      "Iteration 142, loss = 1.22575540\n",
      "Iteration 143, loss = 1.22516157\n",
      "Iteration 144, loss = 1.22456984\n",
      "Iteration 145, loss = 1.22398019\n",
      "Iteration 146, loss = 1.22339264\n",
      "Iteration 147, loss = 1.22280717\n",
      "Iteration 148, loss = 1.22222378\n",
      "Iteration 149, loss = 1.22164246\n",
      "Iteration 150, loss = 1.22106322\n",
      "Iteration 151, loss = 1.22048606\n",
      "Iteration 152, loss = 1.21991096\n",
      "Iteration 153, loss = 1.21933793\n",
      "Iteration 154, loss = 1.21876697\n",
      "Iteration 155, loss = 1.21819806\n",
      "Iteration 156, loss = 1.21763121\n",
      "Iteration 157, loss = 1.21706642\n",
      "Iteration 158, loss = 1.21650367\n",
      "Iteration 159, loss = 1.21594298\n",
      "Iteration 160, loss = 1.21538433\n",
      "Iteration 161, loss = 1.21482772\n",
      "Iteration 162, loss = 1.21427315\n",
      "Iteration 163, loss = 1.21372061\n",
      "Iteration 164, loss = 1.21317011\n",
      "Iteration 165, loss = 1.21262163\n",
      "Iteration 166, loss = 1.21207518\n",
      "Iteration 167, loss = 1.21153075\n",
      "Iteration 168, loss = 1.21098834\n",
      "Iteration 169, loss = 1.21044795\n",
      "Iteration 170, loss = 1.20990956\n",
      "Iteration 171, loss = 1.20937319\n",
      "Iteration 172, loss = 1.20883882\n",
      "Iteration 173, loss = 1.20830645\n",
      "Iteration 174, loss = 1.20777608\n",
      "Iteration 175, loss = 1.20724771\n",
      "Iteration 176, loss = 1.20672132\n",
      "Iteration 177, loss = 1.20619693\n",
      "Iteration 178, loss = 1.20567451\n",
      "Iteration 179, loss = 1.20515408\n",
      "Iteration 180, loss = 1.20463563\n",
      "Iteration 181, loss = 1.20411915\n",
      "Iteration 182, loss = 1.20360463\n",
      "Iteration 183, loss = 1.20309209\n",
      "Iteration 184, loss = 1.20258150\n",
      "Iteration 185, loss = 1.20207288\n",
      "Iteration 186, loss = 1.20156621\n",
      "Iteration 187, loss = 1.20106149\n",
      "Iteration 188, loss = 1.20055872\n",
      "Iteration 189, loss = 1.20005789\n",
      "Iteration 190, loss = 1.19955900\n",
      "Iteration 191, loss = 1.19906205\n",
      "Iteration 192, loss = 1.19856703\n",
      "Iteration 193, loss = 1.19807394\n",
      "Iteration 194, loss = 1.19758278\n",
      "Iteration 195, loss = 1.19709353\n",
      "Iteration 196, loss = 1.19660620\n",
      "Iteration 197, loss = 1.19612079\n",
      "Iteration 198, loss = 1.19563728\n",
      "Iteration 199, loss = 1.19515568\n",
      "Iteration 200, loss = 1.19467598\n",
      "Iteration 201, loss = 1.19419818\n",
      "Iteration 202, loss = 1.19372227\n",
      "Iteration 203, loss = 1.19324825\n",
      "Iteration 204, loss = 1.19277611\n",
      "Iteration 205, loss = 1.19230586\n",
      "Iteration 206, loss = 1.19183748\n",
      "Iteration 207, loss = 1.19137097\n",
      "Iteration 208, loss = 1.19090634\n",
      "Iteration 209, loss = 1.19044357\n",
      "Iteration 210, loss = 1.18998266\n",
      "Iteration 211, loss = 1.18952360\n",
      "Iteration 212, loss = 1.18906640\n",
      "Iteration 213, loss = 1.18861105\n",
      "Iteration 214, loss = 1.18815754\n",
      "Iteration 215, loss = 1.18770587\n",
      "Iteration 216, loss = 1.18725604\n",
      "Iteration 217, loss = 1.18680804\n",
      "Iteration 218, loss = 1.18636186\n",
      "Iteration 219, loss = 1.18591751\n",
      "Iteration 220, loss = 1.18547498\n",
      "Iteration 221, loss = 1.18503426\n",
      "Iteration 222, loss = 1.18459535\n",
      "Iteration 223, loss = 1.18415825\n",
      "Iteration 224, loss = 1.18372295\n",
      "Iteration 225, loss = 1.18328945\n",
      "Iteration 226, loss = 1.18285773\n",
      "Iteration 227, loss = 1.18242781\n",
      "Iteration 228, loss = 1.18199968\n",
      "Iteration 229, loss = 1.18157332\n",
      "Iteration 230, loss = 1.18114873\n",
      "Iteration 231, loss = 1.18072592\n",
      "Iteration 232, loss = 1.18030488\n",
      "Iteration 233, loss = 1.17988560\n",
      "Iteration 234, loss = 1.17946807\n",
      "Iteration 235, loss = 1.17905230\n",
      "Iteration 236, loss = 1.17863828\n",
      "Iteration 237, loss = 1.17822600\n",
      "Iteration 238, loss = 1.17781546\n",
      "Iteration 239, loss = 1.17740666\n",
      "Iteration 240, loss = 1.17699959\n",
      "Iteration 241, loss = 1.17659424\n",
      "Iteration 242, loss = 1.17619062\n",
      "Iteration 243, loss = 1.17578871\n",
      "Iteration 244, loss = 1.17538852\n",
      "Iteration 245, loss = 1.17499004\n",
      "Iteration 246, loss = 1.17459326\n",
      "Iteration 247, loss = 1.17419818\n",
      "Iteration 248, loss = 1.17380479\n",
      "Iteration 249, loss = 1.17341309\n",
      "Iteration 250, loss = 1.17302308\n",
      "Iteration 251, loss = 1.17263475\n",
      "Iteration 252, loss = 1.17224810\n",
      "Iteration 253, loss = 1.17186312\n",
      "Iteration 254, loss = 1.17147980\n",
      "Iteration 255, loss = 1.17109815\n",
      "Iteration 256, loss = 1.17071816\n",
      "Iteration 257, loss = 1.17033982\n",
      "Iteration 258, loss = 1.16996313\n",
      "Iteration 259, loss = 1.16958808\n",
      "Iteration 260, loss = 1.16921467\n",
      "Iteration 261, loss = 1.16884290\n",
      "Iteration 262, loss = 1.16847276\n",
      "Iteration 263, loss = 1.16810424\n",
      "Iteration 264, loss = 1.16773734\n",
      "Iteration 265, loss = 1.16737206\n",
      "Iteration 266, loss = 1.16700839\n",
      "Iteration 267, loss = 1.16664632\n",
      "Iteration 268, loss = 1.16628586\n",
      "Iteration 269, loss = 1.16592700\n",
      "Iteration 270, loss = 1.16556973\n",
      "Iteration 271, loss = 1.16521404\n",
      "Iteration 272, loss = 1.16485994\n",
      "Iteration 273, loss = 1.16450742\n",
      "Iteration 274, loss = 1.16415647\n",
      "Iteration 275, loss = 1.16380709\n",
      "Iteration 276, loss = 1.16345927\n",
      "Iteration 277, loss = 1.16311302\n",
      "Iteration 278, loss = 1.16276831\n",
      "Iteration 279, loss = 1.16242516\n",
      "Iteration 280, loss = 1.16208355\n",
      "Iteration 281, loss = 1.16174349\n",
      "Iteration 282, loss = 1.16140496\n",
      "Iteration 283, loss = 1.16106796\n",
      "Iteration 284, loss = 1.16073248\n",
      "Iteration 285, loss = 1.16039853\n",
      "Iteration 286, loss = 1.16006609\n",
      "Iteration 287, loss = 1.15973517\n",
      "Iteration 288, loss = 1.15940575\n",
      "Iteration 289, loss = 1.15907783\n",
      "Iteration 290, loss = 1.15875142\n",
      "Iteration 291, loss = 1.15842649\n",
      "Iteration 292, loss = 1.15810305\n",
      "Iteration 293, loss = 1.15778110\n",
      "Iteration 294, loss = 1.15746062\n",
      "Iteration 295, loss = 1.15714162\n",
      "Iteration 296, loss = 1.15682408\n",
      "Iteration 297, loss = 1.15650801\n",
      "Iteration 298, loss = 1.15619340\n",
      "Iteration 299, loss = 1.15588025\n",
      "Iteration 300, loss = 1.15556854\n",
      "Iteration 301, loss = 1.15525828\n",
      "Iteration 302, loss = 1.15494946\n",
      "Iteration 303, loss = 1.15464207\n",
      "Iteration 304, loss = 1.15433611\n",
      "Iteration 305, loss = 1.15403158\n",
      "Iteration 306, loss = 1.15372847\n",
      "Iteration 307, loss = 1.15342678\n",
      "Iteration 308, loss = 1.15312649\n",
      "Iteration 309, loss = 1.15282762\n",
      "Iteration 310, loss = 1.15253014\n",
      "Iteration 311, loss = 1.15223406\n",
      "Iteration 312, loss = 1.15193938\n",
      "Iteration 313, loss = 1.15164608\n",
      "Iteration 314, loss = 1.15135416\n",
      "Iteration 315, loss = 1.15106363\n",
      "Iteration 316, loss = 1.15077446\n",
      "Iteration 317, loss = 1.15048666\n",
      "Iteration 318, loss = 1.15020023\n",
      "Iteration 319, loss = 1.14991515\n",
      "Iteration 320, loss = 1.14963143\n",
      "Iteration 321, loss = 1.14934906\n",
      "Iteration 322, loss = 1.14906803\n",
      "Iteration 323, loss = 1.14878835\n",
      "Iteration 324, loss = 1.14850999\n",
      "Iteration 325, loss = 1.14823297\n",
      "Iteration 326, loss = 1.14795727\n",
      "Iteration 327, loss = 1.14768289\n",
      "Iteration 328, loss = 1.14740983\n",
      "Iteration 329, loss = 1.14713808\n",
      "Iteration 330, loss = 1.14686764\n",
      "Iteration 331, loss = 1.14659850\n",
      "Iteration 332, loss = 1.14633065\n",
      "Iteration 333, loss = 1.14606410\n",
      "Iteration 334, loss = 1.14579883\n",
      "Iteration 335, loss = 1.14553485\n",
      "Iteration 336, loss = 1.14527214\n",
      "Iteration 337, loss = 1.14501071\n",
      "Iteration 338, loss = 1.14475055\n",
      "Iteration 339, loss = 1.14449165\n",
      "Iteration 340, loss = 1.14423401\n",
      "Iteration 341, loss = 1.14397762\n",
      "Iteration 342, loss = 1.14372249\n",
      "Iteration 343, loss = 1.14346860\n",
      "Iteration 344, loss = 1.14321595\n",
      "Iteration 345, loss = 1.14296453\n",
      "Iteration 346, loss = 1.14271435\n",
      "Iteration 347, loss = 1.14246539\n",
      "Iteration 348, loss = 1.14221765\n",
      "Iteration 349, loss = 1.14197114\n",
      "Iteration 350, loss = 1.14172583\n",
      "Iteration 351, loss = 1.14148173\n",
      "Iteration 352, loss = 1.14123884\n",
      "Iteration 353, loss = 1.14099714\n",
      "Iteration 354, loss = 1.14075664\n",
      "Iteration 355, loss = 1.14051732\n",
      "Iteration 356, loss = 1.14027920\n",
      "Iteration 357, loss = 1.14004225\n",
      "Iteration 358, loss = 1.13980647\n",
      "Iteration 359, loss = 1.13957187\n",
      "Iteration 360, loss = 1.13933844\n",
      "Iteration 361, loss = 1.13910616\n",
      "Iteration 362, loss = 1.13887505\n",
      "Iteration 363, loss = 1.13864508\n",
      "Iteration 364, loss = 1.13841627\n",
      "Iteration 365, loss = 1.13818859\n",
      "Iteration 366, loss = 1.13796206\n",
      "Iteration 367, loss = 1.13773666\n",
      "Iteration 368, loss = 1.13751239\n",
      "Iteration 369, loss = 1.13728925\n",
      "Iteration 370, loss = 1.13706722\n",
      "Iteration 371, loss = 1.13684632\n",
      "Iteration 372, loss = 1.13662652\n",
      "Iteration 373, loss = 1.13640784\n",
      "Iteration 374, loss = 1.13619025\n",
      "Iteration 375, loss = 1.13597377\n",
      "Iteration 376, loss = 1.13575837\n",
      "Iteration 377, loss = 1.13554407\n",
      "Iteration 378, loss = 1.13533085\n",
      "Iteration 379, loss = 1.13511872\n",
      "Iteration 380, loss = 1.13490766\n",
      "Iteration 381, loss = 1.13469767\n",
      "Iteration 382, loss = 1.13448875\n",
      "Iteration 383, loss = 1.13428089\n",
      "Iteration 384, loss = 1.13407409\n",
      "Iteration 385, loss = 1.13386834\n",
      "Iteration 386, loss = 1.13366365\n",
      "Iteration 387, loss = 1.13346000\n",
      "Iteration 388, loss = 1.13325739\n",
      "Iteration 389, loss = 1.13305581\n",
      "Iteration 390, loss = 1.13285527\n",
      "Iteration 391, loss = 1.13265576\n",
      "Iteration 392, loss = 1.13245727\n",
      "Iteration 393, loss = 1.13225980\n",
      "Iteration 394, loss = 1.13206335\n",
      "Iteration 395, loss = 1.13186790\n",
      "Iteration 396, loss = 1.13167347\n",
      "Iteration 397, loss = 1.13148003\n",
      "Iteration 398, loss = 1.13128760\n",
      "Iteration 399, loss = 1.13109615\n",
      "Iteration 400, loss = 1.13090570\n",
      "Iteration 401, loss = 1.13071623\n",
      "Iteration 402, loss = 1.13052774\n",
      "Iteration 403, loss = 1.13034023\n",
      "Iteration 404, loss = 1.13015369\n",
      "Iteration 405, loss = 1.12996812\n",
      "Iteration 406, loss = 1.12978352\n",
      "Iteration 407, loss = 1.12959987\n",
      "Iteration 408, loss = 1.12941718\n",
      "Iteration 409, loss = 1.12923544\n",
      "Iteration 410, loss = 1.12905465\n",
      "Iteration 411, loss = 1.12887480\n",
      "Iteration 412, loss = 1.12869589\n",
      "Iteration 413, loss = 1.12851792\n",
      "Iteration 414, loss = 1.12834087\n",
      "Iteration 415, loss = 1.12816476\n",
      "Iteration 416, loss = 1.12798956\n",
      "Iteration 417, loss = 1.12781529\n",
      "Iteration 418, loss = 1.12764193\n",
      "Iteration 419, loss = 1.12746948\n",
      "Iteration 420, loss = 1.12729794\n",
      "Iteration 421, loss = 1.12712730\n",
      "Iteration 422, loss = 1.12695755\n",
      "Iteration 423, loss = 1.12678871\n",
      "Iteration 424, loss = 1.12662075\n",
      "Iteration 425, loss = 1.12645368\n",
      "Iteration 426, loss = 1.12628749\n",
      "Iteration 427, loss = 1.12612218\n",
      "Iteration 428, loss = 1.12595774\n",
      "Iteration 429, loss = 1.12579418\n",
      "Iteration 430, loss = 1.12563148\n",
      "Iteration 431, loss = 1.12546964\n",
      "Iteration 432, loss = 1.12530867\n",
      "Iteration 433, loss = 1.12514855\n",
      "Iteration 434, loss = 1.12498927\n",
      "Iteration 435, loss = 1.12483085\n",
      "Iteration 436, loss = 1.12467327\n",
      "Iteration 437, loss = 1.12451653\n",
      "Iteration 438, loss = 1.12436062\n",
      "Iteration 439, loss = 1.12420555\n",
      "Iteration 440, loss = 1.12405130\n",
      "Iteration 441, loss = 1.12389788\n",
      "Iteration 442, loss = 1.12374528\n",
      "Iteration 443, loss = 1.12359349\n",
      "Iteration 444, loss = 1.12344252\n",
      "Iteration 445, loss = 1.12329236\n",
      "Iteration 446, loss = 1.12314300\n",
      "Iteration 447, loss = 1.12299444\n",
      "Iteration 448, loss = 1.12284668\n",
      "Iteration 449, loss = 1.12269971\n",
      "Iteration 450, loss = 1.12255354\n",
      "Iteration 451, loss = 1.12240815\n",
      "Iteration 452, loss = 1.12226354\n",
      "Iteration 453, loss = 1.12211971\n",
      "Iteration 454, loss = 1.12197666\n",
      "Iteration 455, loss = 1.12183438\n",
      "Iteration 456, loss = 1.12169287\n",
      "Iteration 457, loss = 1.12155212\n",
      "Iteration 458, loss = 1.12141214\n",
      "Iteration 459, loss = 1.12127291\n",
      "Iteration 460, loss = 1.12113443\n",
      "Iteration 461, loss = 1.12099671\n",
      "Iteration 462, loss = 1.12085973\n",
      "Iteration 463, loss = 1.12072349\n",
      "Iteration 464, loss = 1.12058800\n",
      "Iteration 465, loss = 1.12045324\n",
      "Iteration 466, loss = 1.12031921\n",
      "Iteration 467, loss = 1.12018591\n",
      "Iteration 468, loss = 1.12005334\n",
      "Iteration 469, loss = 1.11992148\n",
      "Iteration 470, loss = 1.11979035\n",
      "Iteration 471, loss = 1.11965993\n",
      "Iteration 472, loss = 1.11953023\n",
      "Iteration 473, loss = 1.11940123\n",
      "Iteration 474, loss = 1.11927293\n",
      "Iteration 475, loss = 1.11914534\n",
      "Iteration 476, loss = 1.11901844\n",
      "Iteration 477, loss = 1.11889224\n",
      "Iteration 478, loss = 1.11876673\n",
      "Iteration 479, loss = 1.11864191\n",
      "Iteration 480, loss = 1.11851841\n",
      "Iteration 481, loss = 1.11839752\n",
      "Iteration 482, loss = 1.11827731\n",
      "Iteration 483, loss = 1.11815777\n",
      "Iteration 484, loss = 1.11803891\n",
      "Iteration 485, loss = 1.11792072\n",
      "Iteration 486, loss = 1.11780320\n",
      "Iteration 487, loss = 1.11768635\n",
      "Iteration 488, loss = 1.11757015\n",
      "Iteration 489, loss = 1.11745462\n",
      "Iteration 490, loss = 1.11733974\n",
      "Iteration 491, loss = 1.11722552\n",
      "Iteration 492, loss = 1.11711194\n",
      "Iteration 493, loss = 1.11699901\n",
      "Iteration 494, loss = 1.11688672\n",
      "Iteration 495, loss = 1.11677507\n",
      "Iteration 496, loss = 1.11666406\n",
      "Iteration 497, loss = 1.11655369\n",
      "Iteration 498, loss = 1.11644394\n",
      "Iteration 499, loss = 1.11633482\n",
      "Iteration 500, loss = 1.11622633\n",
      "Iteration 501, loss = 1.11611845\n",
      "Iteration 502, loss = 1.11601119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 1.11590455\n",
      "Iteration 504, loss = 1.11579852\n",
      "Iteration 505, loss = 1.11569310\n",
      "Iteration 506, loss = 1.11558828\n",
      "Iteration 507, loss = 1.11548407\n",
      "Iteration 508, loss = 1.11538046\n",
      "Iteration 509, loss = 1.11527744\n",
      "Iteration 510, loss = 1.11517501\n",
      "Iteration 511, loss = 1.11507318\n",
      "Iteration 512, loss = 1.11497193\n",
      "Iteration 513, loss = 1.11487127\n",
      "Iteration 514, loss = 1.11477118\n",
      "Iteration 515, loss = 1.11467168\n",
      "Iteration 516, loss = 1.11457275\n",
      "Iteration 517, loss = 1.11447440\n",
      "Iteration 518, loss = 1.11437661\n",
      "Iteration 519, loss = 1.11427939\n",
      "Iteration 520, loss = 1.11418273\n",
      "Iteration 521, loss = 1.11408663\n",
      "Iteration 522, loss = 1.11399110\n",
      "Iteration 523, loss = 1.11389611\n",
      "Iteration 524, loss = 1.11380168\n",
      "Iteration 525, loss = 1.11370780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33104251\n",
      "Iteration 2, loss = 1.33067874\n",
      "Iteration 3, loss = 1.33017169\n",
      "Iteration 4, loss = 1.32954013\n",
      "Iteration 5, loss = 1.32879997\n",
      "Iteration 6, loss = 1.32795976\n",
      "Iteration 7, loss = 1.32706109\n",
      "Iteration 8, loss = 1.32611775\n",
      "Iteration 9, loss = 1.32515935\n",
      "Iteration 10, loss = 1.32420205\n",
      "Iteration 11, loss = 1.32322054\n",
      "Iteration 12, loss = 1.32222494\n",
      "Iteration 13, loss = 1.32123651\n",
      "Iteration 14, loss = 1.32023831\n",
      "Iteration 15, loss = 1.31923610\n",
      "Iteration 16, loss = 1.31823522\n",
      "Iteration 17, loss = 1.31727419\n",
      "Iteration 18, loss = 1.31635346\n",
      "Iteration 19, loss = 1.31543741\n",
      "Iteration 20, loss = 1.31453155\n",
      "Iteration 21, loss = 1.31363646\n",
      "Iteration 22, loss = 1.31274175\n",
      "Iteration 23, loss = 1.31184537\n",
      "Iteration 24, loss = 1.31094770\n",
      "Iteration 25, loss = 1.31004906\n",
      "Iteration 26, loss = 1.30918086\n",
      "Iteration 27, loss = 1.30833045\n",
      "Iteration 28, loss = 1.30748012\n",
      "Iteration 29, loss = 1.30663360\n",
      "Iteration 30, loss = 1.30579132\n",
      "Iteration 31, loss = 1.30495577\n",
      "Iteration 32, loss = 1.30412079\n",
      "Iteration 33, loss = 1.30329147\n",
      "Iteration 34, loss = 1.30246571\n",
      "Iteration 35, loss = 1.30164081\n",
      "Iteration 36, loss = 1.30081740\n",
      "Iteration 37, loss = 1.30000080\n",
      "Iteration 38, loss = 1.29918525\n",
      "Iteration 39, loss = 1.29837087\n",
      "Iteration 40, loss = 1.29755776\n",
      "Iteration 41, loss = 1.29674601\n",
      "Iteration 42, loss = 1.29593570\n",
      "Iteration 43, loss = 1.29512692\n",
      "Iteration 44, loss = 1.29432154\n",
      "Iteration 45, loss = 1.29351970\n",
      "Iteration 46, loss = 1.29271950\n",
      "Iteration 47, loss = 1.29192099\n",
      "Iteration 48, loss = 1.29112423\n",
      "Iteration 49, loss = 1.29032927\n",
      "Iteration 50, loss = 1.28953614\n",
      "Iteration 51, loss = 1.28874490\n",
      "Iteration 52, loss = 1.28795557\n",
      "Iteration 53, loss = 1.28716818\n",
      "Iteration 54, loss = 1.28638277\n",
      "Iteration 55, loss = 1.28559936\n",
      "Iteration 56, loss = 1.28481797\n",
      "Iteration 57, loss = 1.28403863\n",
      "Iteration 58, loss = 1.28326134\n",
      "Iteration 59, loss = 1.28248613\n",
      "Iteration 60, loss = 1.28171302\n",
      "Iteration 61, loss = 1.28094201\n",
      "Iteration 62, loss = 1.28017312\n",
      "Iteration 63, loss = 1.27940635\n",
      "Iteration 64, loss = 1.27864173\n",
      "Iteration 65, loss = 1.27787925\n",
      "Iteration 66, loss = 1.27711892\n",
      "Iteration 67, loss = 1.27636076\n",
      "Iteration 68, loss = 1.27560476\n",
      "Iteration 69, loss = 1.27485093\n",
      "Iteration 70, loss = 1.27409928\n",
      "Iteration 71, loss = 1.27334981\n",
      "Iteration 72, loss = 1.27260253\n",
      "Iteration 73, loss = 1.27185743\n",
      "Iteration 74, loss = 1.27111452\n",
      "Iteration 75, loss = 1.27037380\n",
      "Iteration 76, loss = 1.26963527\n",
      "Iteration 77, loss = 1.26889894\n",
      "Iteration 78, loss = 1.26816481\n",
      "Iteration 79, loss = 1.26743288\n",
      "Iteration 80, loss = 1.26670314\n",
      "Iteration 81, loss = 1.26597561\n",
      "Iteration 82, loss = 1.26525027\n",
      "Iteration 83, loss = 1.26452714\n",
      "Iteration 84, loss = 1.26380620\n",
      "Iteration 85, loss = 1.26308747\n",
      "Iteration 86, loss = 1.26237093\n",
      "Iteration 87, loss = 1.26165660\n",
      "Iteration 88, loss = 1.26094446\n",
      "Iteration 89, loss = 1.26023452\n",
      "Iteration 90, loss = 1.25952678\n",
      "Iteration 91, loss = 1.25882124\n",
      "Iteration 92, loss = 1.25811789\n",
      "Iteration 93, loss = 1.25741674\n",
      "Iteration 94, loss = 1.25671779\n",
      "Iteration 95, loss = 1.25602103\n",
      "Iteration 96, loss = 1.25532646\n",
      "Iteration 97, loss = 1.25463408\n",
      "Iteration 98, loss = 1.25394389\n",
      "Iteration 99, loss = 1.25325589\n",
      "Iteration 100, loss = 1.25257008\n",
      "Iteration 101, loss = 1.25188645\n",
      "Iteration 102, loss = 1.25120501\n",
      "Iteration 103, loss = 1.25052575\n",
      "Iteration 104, loss = 1.24984867\n",
      "Iteration 105, loss = 1.24917377\n",
      "Iteration 106, loss = 1.24850105\n",
      "Iteration 107, loss = 1.24783051\n",
      "Iteration 108, loss = 1.24716214\n",
      "Iteration 109, loss = 1.24649594\n",
      "Iteration 110, loss = 1.24583192\n",
      "Iteration 111, loss = 1.24517006\n",
      "Iteration 112, loss = 1.24451038\n",
      "Iteration 113, loss = 1.24385285\n",
      "Iteration 114, loss = 1.24319749\n",
      "Iteration 115, loss = 1.24254430\n",
      "Iteration 116, loss = 1.24189326\n",
      "Iteration 117, loss = 1.24124438\n",
      "Iteration 118, loss = 1.24059766\n",
      "Iteration 119, loss = 1.23995309\n",
      "Iteration 120, loss = 1.23931067\n",
      "Iteration 121, loss = 1.23867040\n",
      "Iteration 122, loss = 1.23803227\n",
      "Iteration 123, loss = 1.23739630\n",
      "Iteration 124, loss = 1.23676246\n",
      "Iteration 125, loss = 1.23613076\n",
      "Iteration 126, loss = 1.23550121\n",
      "Iteration 127, loss = 1.23487379\n",
      "Iteration 128, loss = 1.23424850\n",
      "Iteration 129, loss = 1.23362534\n",
      "Iteration 130, loss = 1.23300431\n",
      "Iteration 131, loss = 1.23238541\n",
      "Iteration 132, loss = 1.23176863\n",
      "Iteration 133, loss = 1.23115397\n",
      "Iteration 134, loss = 1.23054143\n",
      "Iteration 135, loss = 1.22993101\n",
      "Iteration 136, loss = 1.22932270\n",
      "Iteration 137, loss = 1.22871650\n",
      "Iteration 138, loss = 1.22811241\n",
      "Iteration 139, loss = 1.22751043\n",
      "Iteration 140, loss = 1.22691055\n",
      "Iteration 141, loss = 1.22631277\n",
      "Iteration 142, loss = 1.22571708\n",
      "Iteration 143, loss = 1.22512350\n",
      "Iteration 144, loss = 1.22453200\n",
      "Iteration 145, loss = 1.22394259\n",
      "Iteration 146, loss = 1.22335527\n",
      "Iteration 147, loss = 1.22277003\n",
      "Iteration 148, loss = 1.22218688\n",
      "Iteration 149, loss = 1.22160580\n",
      "Iteration 150, loss = 1.22102679\n",
      "Iteration 151, loss = 1.22044986\n",
      "Iteration 152, loss = 1.21987500\n",
      "Iteration 153, loss = 1.21930220\n",
      "Iteration 154, loss = 1.21873146\n",
      "Iteration 155, loss = 1.21816279\n",
      "Iteration 156, loss = 1.21759617\n",
      "Iteration 157, loss = 1.21703161\n",
      "Iteration 158, loss = 1.21646909\n",
      "Iteration 159, loss = 1.21590862\n",
      "Iteration 160, loss = 1.21535020\n",
      "Iteration 161, loss = 1.21479382\n",
      "Iteration 162, loss = 1.21423948\n",
      "Iteration 163, loss = 1.21368717\n",
      "Iteration 164, loss = 1.21313689\n",
      "Iteration 165, loss = 1.21258864\n",
      "Iteration 166, loss = 1.21204242\n",
      "Iteration 167, loss = 1.21149822\n",
      "Iteration 168, loss = 1.21095603\n",
      "Iteration 169, loss = 1.21041586\n",
      "Iteration 170, loss = 1.20987770\n",
      "Iteration 171, loss = 1.20934155\n",
      "Iteration 172, loss = 1.20880741\n",
      "Iteration 173, loss = 1.20827526\n",
      "Iteration 174, loss = 1.20774511\n",
      "Iteration 175, loss = 1.20721696\n",
      "Iteration 176, loss = 1.20669080\n",
      "Iteration 177, loss = 1.20616662\n",
      "Iteration 178, loss = 1.20564443\n",
      "Iteration 179, loss = 1.20512422\n",
      "Iteration 180, loss = 1.20460598\n",
      "Iteration 181, loss = 1.20408972\n",
      "Iteration 182, loss = 1.20357543\n",
      "Iteration 183, loss = 1.20306310\n",
      "Iteration 184, loss = 1.20255273\n",
      "Iteration 185, loss = 1.20204432\n",
      "Iteration 186, loss = 1.20153787\n",
      "Iteration 187, loss = 1.20103337\n",
      "Iteration 188, loss = 1.20053081\n",
      "Iteration 189, loss = 1.20003020\n",
      "Iteration 190, loss = 1.19953153\n",
      "Iteration 191, loss = 1.19903479\n",
      "Iteration 192, loss = 1.19853998\n",
      "Iteration 193, loss = 1.19804710\n",
      "Iteration 194, loss = 1.19755615\n",
      "Iteration 195, loss = 1.19706712\n",
      "Iteration 196, loss = 1.19658000\n",
      "Iteration 197, loss = 1.19609480\n",
      "Iteration 198, loss = 1.19561150\n",
      "Iteration 199, loss = 1.19513011\n",
      "Iteration 200, loss = 1.19465062\n",
      "Iteration 201, loss = 1.19417302\n",
      "Iteration 202, loss = 1.19369732\n",
      "Iteration 203, loss = 1.19322351\n",
      "Iteration 204, loss = 1.19275158\n",
      "Iteration 205, loss = 1.19228153\n",
      "Iteration 206, loss = 1.19181336\n",
      "Iteration 207, loss = 1.19134706\n",
      "Iteration 208, loss = 1.19088263\n",
      "Iteration 209, loss = 1.19042006\n",
      "Iteration 210, loss = 1.18995935\n",
      "Iteration 211, loss = 1.18950050\n",
      "Iteration 212, loss = 1.18904351\n",
      "Iteration 213, loss = 1.18858835\n",
      "Iteration 214, loss = 1.18813505\n",
      "Iteration 215, loss = 1.18768358\n",
      "Iteration 216, loss = 1.18723395\n",
      "Iteration 217, loss = 1.18678614\n",
      "Iteration 218, loss = 1.18634017\n",
      "Iteration 219, loss = 1.18589602\n",
      "Iteration 220, loss = 1.18545368\n",
      "Iteration 221, loss = 1.18501316\n",
      "Iteration 222, loss = 1.18457445\n",
      "Iteration 223, loss = 1.18413754\n",
      "Iteration 224, loss = 1.18370244\n",
      "Iteration 225, loss = 1.18326913\n",
      "Iteration 226, loss = 1.18283761\n",
      "Iteration 227, loss = 1.18240789\n",
      "Iteration 228, loss = 1.18197994\n",
      "Iteration 229, loss = 1.18155378\n",
      "Iteration 230, loss = 1.18112939\n",
      "Iteration 231, loss = 1.18070677\n",
      "Iteration 232, loss = 1.18028591\n",
      "Iteration 233, loss = 1.17986682\n",
      "Iteration 234, loss = 1.17944949\n",
      "Iteration 235, loss = 1.17903390\n",
      "Iteration 236, loss = 1.17862007\n",
      "Iteration 237, loss = 1.17820798\n",
      "Iteration 238, loss = 1.17779763\n",
      "Iteration 239, loss = 1.17738901\n",
      "Iteration 240, loss = 1.17698213\n",
      "Iteration 241, loss = 1.17657697\n",
      "Iteration 242, loss = 1.17617353\n",
      "Iteration 243, loss = 1.17577181\n",
      "Iteration 244, loss = 1.17537180\n",
      "Iteration 245, loss = 1.17497350\n",
      "Iteration 246, loss = 1.17457690\n",
      "Iteration 247, loss = 1.17418201\n",
      "Iteration 248, loss = 1.17378880\n",
      "Iteration 249, loss = 1.17339729\n",
      "Iteration 250, loss = 1.17300746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251, loss = 1.17261931\n",
      "Iteration 252, loss = 1.17223283\n",
      "Iteration 253, loss = 1.17184803\n",
      "Iteration 254, loss = 1.17146489\n",
      "Iteration 255, loss = 1.17108342\n",
      "Iteration 256, loss = 1.17070360\n",
      "Iteration 257, loss = 1.17032544\n",
      "Iteration 258, loss = 1.16994892\n",
      "Iteration 259, loss = 1.16957405\n",
      "Iteration 260, loss = 1.16920082\n",
      "Iteration 261, loss = 1.16882922\n",
      "Iteration 262, loss = 1.16845925\n",
      "Iteration 263, loss = 1.16809090\n",
      "Iteration 264, loss = 1.16772418\n",
      "Iteration 265, loss = 1.16735907\n",
      "Iteration 266, loss = 1.16699557\n",
      "Iteration 267, loss = 1.16663367\n",
      "Iteration 268, loss = 1.16627338\n",
      "Iteration 269, loss = 1.16591469\n",
      "Iteration 270, loss = 1.16555758\n",
      "Iteration 271, loss = 1.16520207\n",
      "Iteration 272, loss = 1.16484813\n",
      "Iteration 273, loss = 1.16449578\n",
      "Iteration 274, loss = 1.16414499\n",
      "Iteration 275, loss = 1.16379578\n",
      "Iteration 276, loss = 1.16344813\n",
      "Iteration 277, loss = 1.16310204\n",
      "Iteration 278, loss = 1.16275750\n",
      "Iteration 279, loss = 1.16241451\n",
      "Iteration 280, loss = 1.16207306\n",
      "Iteration 281, loss = 1.16173316\n",
      "Iteration 282, loss = 1.16139479\n",
      "Iteration 283, loss = 1.16105795\n",
      "Iteration 284, loss = 1.16072263\n",
      "Iteration 285, loss = 1.16038884\n",
      "Iteration 286, loss = 1.16005656\n",
      "Iteration 287, loss = 1.15972580\n",
      "Iteration 288, loss = 1.15939654\n",
      "Iteration 289, loss = 1.15906878\n",
      "Iteration 290, loss = 1.15874252\n",
      "Iteration 291, loss = 1.15841775\n",
      "Iteration 292, loss = 1.15809446\n",
      "Iteration 293, loss = 1.15777266\n",
      "Iteration 294, loss = 1.15745234\n",
      "Iteration 295, loss = 1.15713349\n",
      "Iteration 296, loss = 1.15681611\n",
      "Iteration 297, loss = 1.15650019\n",
      "Iteration 298, loss = 1.15618573\n",
      "Iteration 299, loss = 1.15587273\n",
      "Iteration 300, loss = 1.15556117\n",
      "Iteration 301, loss = 1.15525106\n",
      "Iteration 302, loss = 1.15494239\n",
      "Iteration 303, loss = 1.15463515\n",
      "Iteration 304, loss = 1.15432934\n",
      "Iteration 305, loss = 1.15402495\n",
      "Iteration 306, loss = 1.15372199\n",
      "Iteration 307, loss = 1.15342044\n",
      "Iteration 308, loss = 1.15312031\n",
      "Iteration 309, loss = 1.15282157\n",
      "Iteration 310, loss = 1.15252424\n",
      "Iteration 311, loss = 1.15222831\n",
      "Iteration 312, loss = 1.15193377\n",
      "Iteration 313, loss = 1.15164061\n",
      "Iteration 314, loss = 1.15134884\n",
      "Iteration 315, loss = 1.15105844\n",
      "Iteration 316, loss = 1.15076942\n",
      "Iteration 317, loss = 1.15048176\n",
      "Iteration 318, loss = 1.15019546\n",
      "Iteration 319, loss = 1.14991053\n",
      "Iteration 320, loss = 1.14962695\n",
      "Iteration 321, loss = 1.14934471\n",
      "Iteration 322, loss = 1.14906382\n",
      "Iteration 323, loss = 1.14878427\n",
      "Iteration 324, loss = 1.14850605\n",
      "Iteration 325, loss = 1.14822917\n",
      "Iteration 326, loss = 1.14795360\n",
      "Iteration 327, loss = 1.14767936\n",
      "Iteration 328, loss = 1.14740643\n",
      "Iteration 329, loss = 1.14713482\n",
      "Iteration 330, loss = 1.14686450\n",
      "Iteration 331, loss = 1.14659549\n",
      "Iteration 332, loss = 1.14632778\n",
      "Iteration 333, loss = 1.14606136\n",
      "Iteration 334, loss = 1.14579622\n",
      "Iteration 335, loss = 1.14553237\n",
      "Iteration 336, loss = 1.14526979\n",
      "Iteration 337, loss = 1.14500849\n",
      "Iteration 338, loss = 1.14474845\n",
      "Iteration 339, loss = 1.14448968\n",
      "Iteration 340, loss = 1.14423217\n",
      "Iteration 341, loss = 1.14397591\n",
      "Iteration 342, loss = 1.14372090\n",
      "Iteration 343, loss = 1.14346713\n",
      "Iteration 344, loss = 1.14321461\n",
      "Iteration 345, loss = 1.14296332\n",
      "Iteration 346, loss = 1.14271325\n",
      "Iteration 347, loss = 1.14246442\n",
      "Iteration 348, loss = 1.14221681\n",
      "Iteration 349, loss = 1.14197041\n",
      "Iteration 350, loss = 1.14172523\n",
      "Iteration 351, loss = 1.14148125\n",
      "Iteration 352, loss = 1.14123847\n",
      "Iteration 353, loss = 1.14099690\n",
      "Iteration 354, loss = 1.14075651\n",
      "Iteration 355, loss = 1.14051732\n",
      "Iteration 356, loss = 1.14027931\n",
      "Iteration 357, loss = 1.14004248\n",
      "Iteration 358, loss = 1.13980682\n",
      "Iteration 359, loss = 1.13957233\n",
      "Iteration 360, loss = 1.13933901\n",
      "Iteration 361, loss = 1.13910685\n",
      "Iteration 362, loss = 1.13887585\n",
      "Iteration 363, loss = 1.13864600\n",
      "Iteration 364, loss = 1.13841730\n",
      "Iteration 365, loss = 1.13818974\n",
      "Iteration 366, loss = 1.13796332\n",
      "Iteration 367, loss = 1.13773803\n",
      "Iteration 368, loss = 1.13751387\n",
      "Iteration 369, loss = 1.13729084\n",
      "Iteration 370, loss = 1.13706893\n",
      "Iteration 371, loss = 1.13684813\n",
      "Iteration 372, loss = 1.13662844\n",
      "Iteration 373, loss = 1.13640986\n",
      "Iteration 374, loss = 1.13619239\n",
      "Iteration 375, loss = 1.13597601\n",
      "Iteration 376, loss = 1.13576072\n",
      "Iteration 377, loss = 1.13554653\n",
      "Iteration 378, loss = 1.13533342\n",
      "Iteration 379, loss = 1.13512139\n",
      "Iteration 380, loss = 1.13491043\n",
      "Iteration 381, loss = 1.13470055\n",
      "Iteration 382, loss = 1.13449173\n",
      "Iteration 383, loss = 1.13428397\n",
      "Iteration 384, loss = 1.13407728\n",
      "Iteration 385, loss = 1.13387163\n",
      "Iteration 386, loss = 1.13366704\n",
      "Iteration 387, loss = 1.13346349\n",
      "Iteration 388, loss = 1.13326098\n",
      "Iteration 389, loss = 1.13305951\n",
      "Iteration 390, loss = 1.13285906\n",
      "Iteration 391, loss = 1.13265965\n",
      "Iteration 392, loss = 1.13246126\n",
      "Iteration 393, loss = 1.13226389\n",
      "Iteration 394, loss = 1.13206753\n",
      "Iteration 395, loss = 1.13187219\n",
      "Iteration 396, loss = 1.13167785\n",
      "Iteration 397, loss = 1.13148451\n",
      "Iteration 398, loss = 1.13129217\n",
      "Iteration 399, loss = 1.13110082\n",
      "Iteration 400, loss = 1.13091046\n",
      "Iteration 401, loss = 1.13072108\n",
      "Iteration 402, loss = 1.13053269\n",
      "Iteration 403, loss = 1.13034527\n",
      "Iteration 404, loss = 1.13015882\n",
      "Iteration 405, loss = 1.12997335\n",
      "Iteration 406, loss = 1.12978883\n",
      "Iteration 407, loss = 1.12960528\n",
      "Iteration 408, loss = 1.12942268\n",
      "Iteration 409, loss = 1.12924103\n",
      "Iteration 410, loss = 1.12906032\n",
      "Iteration 411, loss = 1.12888057\n",
      "Iteration 412, loss = 1.12870175\n",
      "Iteration 413, loss = 1.12852386\n",
      "Iteration 414, loss = 1.12834690\n",
      "Iteration 415, loss = 1.12817087\n",
      "Iteration 416, loss = 1.12799577\n",
      "Iteration 417, loss = 1.12782158\n",
      "Iteration 418, loss = 1.12764831\n",
      "Iteration 419, loss = 1.12747594\n",
      "Iteration 420, loss = 1.12730448\n",
      "Iteration 421, loss = 1.12713393\n",
      "Iteration 422, loss = 1.12696427\n",
      "Iteration 423, loss = 1.12679550\n",
      "Iteration 424, loss = 1.12662763\n",
      "Iteration 425, loss = 1.12646064\n",
      "Iteration 426, loss = 1.12629453\n",
      "Iteration 427, loss = 1.12612931\n",
      "Iteration 428, loss = 1.12596495\n",
      "Iteration 429, loss = 1.12580147\n",
      "Iteration 430, loss = 1.12563885\n",
      "Iteration 431, loss = 1.12547709\n",
      "Iteration 432, loss = 1.12531620\n",
      "Iteration 433, loss = 1.12515615\n",
      "Iteration 434, loss = 1.12499696\n",
      "Iteration 435, loss = 1.12483861\n",
      "Iteration 436, loss = 1.12468111\n",
      "Iteration 437, loss = 1.12452445\n",
      "Iteration 438, loss = 1.12436862\n",
      "Iteration 439, loss = 1.12421362\n",
      "Iteration 440, loss = 1.12405945\n",
      "Iteration 441, loss = 1.12390610\n",
      "Iteration 442, loss = 1.12375357\n",
      "Iteration 443, loss = 1.12360186\n",
      "Iteration 444, loss = 1.12345096\n",
      "Iteration 445, loss = 1.12330087\n",
      "Iteration 446, loss = 1.12315159\n",
      "Iteration 447, loss = 1.12300310\n",
      "Iteration 448, loss = 1.12285541\n",
      "Iteration 449, loss = 1.12270852\n",
      "Iteration 450, loss = 1.12256242\n",
      "Iteration 451, loss = 1.12241710\n",
      "Iteration 452, loss = 1.12227256\n",
      "Iteration 453, loss = 1.12212880\n",
      "Iteration 454, loss = 1.12198582\n",
      "Iteration 455, loss = 1.12184361\n",
      "Iteration 456, loss = 1.12170217\n",
      "Iteration 457, loss = 1.12156149\n",
      "Iteration 458, loss = 1.12142157\n",
      "Iteration 459, loss = 1.12128241\n",
      "Iteration 460, loss = 1.12114400\n",
      "Iteration 461, loss = 1.12100634\n",
      "Iteration 462, loss = 1.12086943\n",
      "Iteration 463, loss = 1.12073326\n",
      "Iteration 464, loss = 1.12059783\n",
      "Iteration 465, loss = 1.12046314\n",
      "Iteration 466, loss = 1.12032917\n",
      "Iteration 467, loss = 1.12019594\n",
      "Iteration 468, loss = 1.12006343\n",
      "Iteration 469, loss = 1.11993164\n",
      "Iteration 470, loss = 1.11980057\n",
      "Iteration 471, loss = 1.11967022\n",
      "Iteration 472, loss = 1.11954057\n",
      "Iteration 473, loss = 1.11941163\n",
      "Iteration 474, loss = 1.11928340\n",
      "Iteration 475, loss = 1.11915587\n",
      "Iteration 476, loss = 1.11902904\n",
      "Iteration 477, loss = 1.11890290\n",
      "Iteration 478, loss = 1.11877745\n",
      "Iteration 479, loss = 1.11865268\n",
      "Iteration 480, loss = 1.11852861\n",
      "Iteration 481, loss = 1.11840521\n",
      "Iteration 482, loss = 1.11828249\n",
      "Iteration 483, loss = 1.11816044\n",
      "Iteration 484, loss = 1.11803906\n",
      "Iteration 485, loss = 1.11791836\n",
      "Iteration 486, loss = 1.11779831\n",
      "Iteration 487, loss = 1.11767956\n",
      "Iteration 488, loss = 1.11756341\n",
      "Iteration 489, loss = 1.11744793\n",
      "Iteration 490, loss = 1.11733309\n",
      "Iteration 491, loss = 1.11721891\n",
      "Iteration 492, loss = 1.11710538\n",
      "Iteration 493, loss = 1.11699249\n",
      "Iteration 494, loss = 1.11688025\n",
      "Iteration 495, loss = 1.11676864\n",
      "Iteration 496, loss = 1.11665767\n",
      "Iteration 497, loss = 1.11654734\n",
      "Iteration 498, loss = 1.11643763\n",
      "Iteration 499, loss = 1.11632855\n",
      "Iteration 500, loss = 1.11622009\n",
      "Iteration 501, loss = 1.11611225\n",
      "Iteration 502, loss = 1.11600504\n",
      "Iteration 503, loss = 1.11589843\n",
      "Iteration 504, loss = 1.11579244\n",
      "Iteration 505, loss = 1.11568705\n",
      "Iteration 506, loss = 1.11558227\n",
      "Iteration 507, loss = 1.11547809\n",
      "Iteration 508, loss = 1.11537451\n",
      "Iteration 509, loss = 1.11527153\n",
      "Iteration 510, loss = 1.11516914\n",
      "Iteration 511, loss = 1.11506734\n",
      "Iteration 512, loss = 1.11496613\n",
      "Iteration 513, loss = 1.11486550\n",
      "Iteration 514, loss = 1.11476545\n",
      "Iteration 515, loss = 1.11466598\n",
      "Iteration 516, loss = 1.11456709\n",
      "Iteration 517, loss = 1.11446876\n",
      "Iteration 518, loss = 1.11437101\n",
      "Iteration 519, loss = 1.11427382\n",
      "Iteration 520, loss = 1.11417720\n",
      "Iteration 521, loss = 1.11408113\n",
      "Iteration 522, loss = 1.11398563\n",
      "Iteration 523, loss = 1.11389067\n",
      "Iteration 524, loss = 1.11379627\n",
      "Iteration 525, loss = 1.11370242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33079619\n",
      "Iteration 2, loss = 1.33050646\n",
      "Iteration 3, loss = 1.33010074\n",
      "Iteration 4, loss = 1.32959972\n",
      "Iteration 5, loss = 1.32901158\n",
      "Iteration 6, loss = 1.32835061\n",
      "Iteration 7, loss = 1.32763012\n",
      "Iteration 8, loss = 1.32687028\n",
      "Iteration 9, loss = 1.32603734\n",
      "Iteration 10, loss = 1.32509032\n",
      "Iteration 11, loss = 1.32414362\n",
      "Iteration 12, loss = 1.32317380\n",
      "Iteration 13, loss = 1.32217237\n",
      "Iteration 14, loss = 1.32117072\n",
      "Iteration 15, loss = 1.32016430\n",
      "Iteration 16, loss = 1.31914999\n",
      "Iteration 17, loss = 1.31813724\n",
      "Iteration 18, loss = 1.31712368\n",
      "Iteration 19, loss = 1.31614901\n",
      "Iteration 20, loss = 1.31518318\n",
      "Iteration 21, loss = 1.31421430\n",
      "Iteration 22, loss = 1.31326388\n",
      "Iteration 23, loss = 1.31233654\n",
      "Iteration 24, loss = 1.31140861\n",
      "Iteration 25, loss = 1.31048037\n",
      "Iteration 26, loss = 1.30955208\n",
      "Iteration 27, loss = 1.30862475\n",
      "Iteration 28, loss = 1.30775358\n",
      "Iteration 29, loss = 1.30688638\n",
      "Iteration 30, loss = 1.30602050\n",
      "Iteration 31, loss = 1.30515906\n",
      "Iteration 32, loss = 1.30430890\n",
      "Iteration 33, loss = 1.30346190\n",
      "Iteration 34, loss = 1.30262086\n",
      "Iteration 35, loss = 1.30178579\n",
      "Iteration 36, loss = 1.30095527\n",
      "Iteration 37, loss = 1.30012679\n",
      "Iteration 38, loss = 1.29930621\n",
      "Iteration 39, loss = 1.29848710\n",
      "Iteration 40, loss = 1.29766952\n",
      "Iteration 41, loss = 1.29685355\n",
      "Iteration 42, loss = 1.29603923\n",
      "Iteration 43, loss = 1.29522677\n",
      "Iteration 44, loss = 1.29442070\n",
      "Iteration 45, loss = 1.29361634\n",
      "Iteration 46, loss = 1.29281374\n",
      "Iteration 47, loss = 1.29201292\n",
      "Iteration 48, loss = 1.29121395\n",
      "Iteration 49, loss = 1.29041685\n",
      "Iteration 50, loss = 1.28962165\n",
      "Iteration 51, loss = 1.28882840\n",
      "Iteration 52, loss = 1.28803711\n",
      "Iteration 53, loss = 1.28724780\n",
      "Iteration 54, loss = 1.28646051\n",
      "Iteration 55, loss = 1.28567525\n",
      "Iteration 56, loss = 1.28489204\n",
      "Iteration 57, loss = 1.28411090\n",
      "Iteration 58, loss = 1.28333183\n",
      "Iteration 59, loss = 1.28255486\n",
      "Iteration 60, loss = 1.28177999\n",
      "Iteration 61, loss = 1.28100724\n",
      "Iteration 62, loss = 1.28023661\n",
      "Iteration 63, loss = 1.27946811\n",
      "Iteration 64, loss = 1.27870175\n",
      "Iteration 65, loss = 1.27793754\n",
      "Iteration 66, loss = 1.27717548\n",
      "Iteration 67, loss = 1.27641596\n",
      "Iteration 68, loss = 1.27565930\n",
      "Iteration 69, loss = 1.27490484\n",
      "Iteration 70, loss = 1.27415260\n",
      "Iteration 71, loss = 1.27340256\n",
      "Iteration 72, loss = 1.27265474\n",
      "Iteration 73, loss = 1.27190913\n",
      "Iteration 74, loss = 1.27116573\n",
      "Iteration 75, loss = 1.27042454\n",
      "Iteration 76, loss = 1.26968556\n",
      "Iteration 77, loss = 1.26894880\n",
      "Iteration 78, loss = 1.26821425\n",
      "Iteration 79, loss = 1.26748190\n",
      "Iteration 80, loss = 1.26675177\n",
      "Iteration 81, loss = 1.26602385\n",
      "Iteration 82, loss = 1.26529814\n",
      "Iteration 83, loss = 1.26457464\n",
      "Iteration 84, loss = 1.26385334\n",
      "Iteration 85, loss = 1.26313426\n",
      "Iteration 86, loss = 1.26241738\n",
      "Iteration 87, loss = 1.26170270\n",
      "Iteration 88, loss = 1.26099023\n",
      "Iteration 89, loss = 1.26027997\n",
      "Iteration 90, loss = 1.25957190\n",
      "Iteration 91, loss = 1.25886604\n",
      "Iteration 92, loss = 1.25816237\n",
      "Iteration 93, loss = 1.25746091\n",
      "Iteration 94, loss = 1.25676164\n",
      "Iteration 95, loss = 1.25606457\n",
      "Iteration 96, loss = 1.25536970\n",
      "Iteration 97, loss = 1.25467702\n",
      "Iteration 98, loss = 1.25398653\n",
      "Iteration 99, loss = 1.25329823\n",
      "Iteration 100, loss = 1.25261212\n",
      "Iteration 101, loss = 1.25192819\n",
      "Iteration 102, loss = 1.25124646\n",
      "Iteration 103, loss = 1.25056691\n",
      "Iteration 104, loss = 1.24988954\n",
      "Iteration 105, loss = 1.24921435\n",
      "Iteration 106, loss = 1.24854134\n",
      "Iteration 107, loss = 1.24787050\n",
      "Iteration 108, loss = 1.24720185\n",
      "Iteration 109, loss = 1.24653537\n",
      "Iteration 110, loss = 1.24587106\n",
      "Iteration 111, loss = 1.24520892\n",
      "Iteration 112, loss = 1.24454894\n",
      "Iteration 113, loss = 1.24389114\n",
      "Iteration 114, loss = 1.24323550\n",
      "Iteration 115, loss = 1.24258202\n",
      "Iteration 116, loss = 1.24193070\n",
      "Iteration 117, loss = 1.24128154\n",
      "Iteration 118, loss = 1.24063454\n",
      "Iteration 119, loss = 1.23998969\n",
      "Iteration 120, loss = 1.23934699\n",
      "Iteration 121, loss = 1.23870644\n",
      "Iteration 122, loss = 1.23806804\n",
      "Iteration 123, loss = 1.23743179\n",
      "Iteration 124, loss = 1.23679767\n",
      "Iteration 125, loss = 1.23616570\n",
      "Iteration 126, loss = 1.23553587\n",
      "Iteration 127, loss = 1.23490818\n",
      "Iteration 128, loss = 1.23428261\n",
      "Iteration 129, loss = 1.23365918\n",
      "Iteration 130, loss = 1.23303788\n",
      "Iteration 131, loss = 1.23241871\n",
      "Iteration 132, loss = 1.23180166\n",
      "Iteration 133, loss = 1.23118673\n",
      "Iteration 134, loss = 1.23057392\n",
      "Iteration 135, loss = 1.22996323\n",
      "Iteration 136, loss = 1.22935465\n",
      "Iteration 137, loss = 1.22874818\n",
      "Iteration 138, loss = 1.22814382\n",
      "Iteration 139, loss = 1.22754157\n",
      "Iteration 140, loss = 1.22694142\n",
      "Iteration 141, loss = 1.22634337\n",
      "Iteration 142, loss = 1.22574742\n",
      "Iteration 143, loss = 1.22515357\n",
      "Iteration 144, loss = 1.22456181\n",
      "Iteration 145, loss = 1.22397213\n",
      "Iteration 146, loss = 1.22338455\n",
      "Iteration 147, loss = 1.22279905\n",
      "Iteration 148, loss = 1.22221563\n",
      "Iteration 149, loss = 1.22163429\n",
      "Iteration 150, loss = 1.22105502\n",
      "Iteration 151, loss = 1.22047783\n",
      "Iteration 152, loss = 1.21990270\n",
      "Iteration 153, loss = 1.21932964\n",
      "Iteration 154, loss = 1.21875865\n",
      "Iteration 155, loss = 1.21818971\n",
      "Iteration 156, loss = 1.21762284\n",
      "Iteration 157, loss = 1.21705802\n",
      "Iteration 158, loss = 1.21649524\n",
      "Iteration 159, loss = 1.21593452\n",
      "Iteration 160, loss = 1.21537584\n",
      "Iteration 161, loss = 1.21481920\n",
      "Iteration 162, loss = 1.21426461\n",
      "Iteration 163, loss = 1.21371204\n",
      "Iteration 164, loss = 1.21316151\n",
      "Iteration 165, loss = 1.21261301\n",
      "Iteration 166, loss = 1.21206653\n",
      "Iteration 167, loss = 1.21152208\n",
      "Iteration 168, loss = 1.21097964\n",
      "Iteration 169, loss = 1.21043922\n",
      "Iteration 170, loss = 1.20990081\n",
      "Iteration 171, loss = 1.20936441\n",
      "Iteration 172, loss = 1.20883002\n",
      "Iteration 173, loss = 1.20829762\n",
      "Iteration 174, loss = 1.20776723\n",
      "Iteration 175, loss = 1.20723883\n",
      "Iteration 176, loss = 1.20671242\n",
      "Iteration 177, loss = 1.20618800\n",
      "Iteration 178, loss = 1.20566556\n",
      "Iteration 179, loss = 1.20514510\n",
      "Iteration 180, loss = 1.20462662\n",
      "Iteration 181, loss = 1.20411012\n",
      "Iteration 182, loss = 1.20359558\n",
      "Iteration 183, loss = 1.20308301\n",
      "Iteration 184, loss = 1.20257240\n",
      "Iteration 185, loss = 1.20206375\n",
      "Iteration 186, loss = 1.20155706\n",
      "Iteration 187, loss = 1.20105232\n",
      "Iteration 188, loss = 1.20054952\n",
      "Iteration 189, loss = 1.20004867\n",
      "Iteration 190, loss = 1.19954976\n",
      "Iteration 191, loss = 1.19905279\n",
      "Iteration 192, loss = 1.19855774\n",
      "Iteration 193, loss = 1.19806463\n",
      "Iteration 194, loss = 1.19757344\n",
      "Iteration 195, loss = 1.19708417\n",
      "Iteration 196, loss = 1.19659682\n",
      "Iteration 197, loss = 1.19611139\n",
      "Iteration 198, loss = 1.19562786\n",
      "Iteration 199, loss = 1.19514623\n",
      "Iteration 200, loss = 1.19466651\n",
      "Iteration 201, loss = 1.19418869\n",
      "Iteration 202, loss = 1.19371276\n",
      "Iteration 203, loss = 1.19323871\n",
      "Iteration 204, loss = 1.19276656\n",
      "Iteration 205, loss = 1.19229628\n",
      "Iteration 206, loss = 1.19182788\n",
      "Iteration 207, loss = 1.19136136\n",
      "Iteration 208, loss = 1.19089670\n",
      "Iteration 209, loss = 1.19043391\n",
      "Iteration 210, loss = 1.18997298\n",
      "Iteration 211, loss = 1.18951391\n",
      "Iteration 212, loss = 1.18905668\n",
      "Iteration 213, loss = 1.18860131\n",
      "Iteration 214, loss = 1.18814778\n",
      "Iteration 215, loss = 1.18769609\n",
      "Iteration 216, loss = 1.18724624\n",
      "Iteration 217, loss = 1.18679822\n",
      "Iteration 218, loss = 1.18635203\n",
      "Iteration 219, loss = 1.18590766\n",
      "Iteration 220, loss = 1.18546510\n",
      "Iteration 221, loss = 1.18502437\n",
      "Iteration 222, loss = 1.18458544\n",
      "Iteration 223, loss = 1.18414832\n",
      "Iteration 224, loss = 1.18371300\n",
      "Iteration 225, loss = 1.18327948\n",
      "Iteration 226, loss = 1.18284775\n",
      "Iteration 227, loss = 1.18241781\n",
      "Iteration 228, loss = 1.18198965\n",
      "Iteration 229, loss = 1.18156328\n",
      "Iteration 230, loss = 1.18113868\n",
      "Iteration 231, loss = 1.18071585\n",
      "Iteration 232, loss = 1.18029479\n",
      "Iteration 233, loss = 1.17987549\n",
      "Iteration 234, loss = 1.17945794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 235, loss = 1.17904216\n",
      "Iteration 236, loss = 1.17862812\n",
      "Iteration 237, loss = 1.17821582\n",
      "Iteration 238, loss = 1.17780527\n",
      "Iteration 239, loss = 1.17739645\n",
      "Iteration 240, loss = 1.17698936\n",
      "Iteration 241, loss = 1.17658400\n",
      "Iteration 242, loss = 1.17618036\n",
      "Iteration 243, loss = 1.17577844\n",
      "Iteration 244, loss = 1.17537823\n",
      "Iteration 245, loss = 1.17497973\n",
      "Iteration 246, loss = 1.17458293\n",
      "Iteration 247, loss = 1.17418784\n",
      "Iteration 248, loss = 1.17379444\n",
      "Iteration 249, loss = 1.17340273\n",
      "Iteration 250, loss = 1.17301270\n",
      "Iteration 251, loss = 1.17262436\n",
      "Iteration 252, loss = 1.17223769\n",
      "Iteration 253, loss = 1.17185269\n",
      "Iteration 254, loss = 1.17146936\n",
      "Iteration 255, loss = 1.17108770\n",
      "Iteration 256, loss = 1.17070769\n",
      "Iteration 257, loss = 1.17032934\n",
      "Iteration 258, loss = 1.16995263\n",
      "Iteration 259, loss = 1.16957757\n",
      "Iteration 260, loss = 1.16920415\n",
      "Iteration 261, loss = 1.16883236\n",
      "Iteration 262, loss = 1.16846221\n",
      "Iteration 263, loss = 1.16809367\n",
      "Iteration 264, loss = 1.16772676\n",
      "Iteration 265, loss = 1.16736147\n",
      "Iteration 266, loss = 1.16699779\n",
      "Iteration 267, loss = 1.16663571\n",
      "Iteration 268, loss = 1.16627524\n",
      "Iteration 269, loss = 1.16591636\n",
      "Iteration 270, loss = 1.16555908\n",
      "Iteration 271, loss = 1.16520338\n",
      "Iteration 272, loss = 1.16484927\n",
      "Iteration 273, loss = 1.16449673\n",
      "Iteration 274, loss = 1.16414577\n",
      "Iteration 275, loss = 1.16379638\n",
      "Iteration 276, loss = 1.16344855\n",
      "Iteration 277, loss = 1.16310228\n",
      "Iteration 278, loss = 1.16275757\n",
      "Iteration 279, loss = 1.16241441\n",
      "Iteration 280, loss = 1.16207279\n",
      "Iteration 281, loss = 1.16173271\n",
      "Iteration 282, loss = 1.16139417\n",
      "Iteration 283, loss = 1.16105716\n",
      "Iteration 284, loss = 1.16072167\n",
      "Iteration 285, loss = 1.16038771\n",
      "Iteration 286, loss = 1.16005526\n",
      "Iteration 287, loss = 1.15972433\n",
      "Iteration 288, loss = 1.15939490\n",
      "Iteration 289, loss = 1.15906698\n",
      "Iteration 290, loss = 1.15874055\n",
      "Iteration 291, loss = 1.15841561\n",
      "Iteration 292, loss = 1.15809217\n",
      "Iteration 293, loss = 1.15777020\n",
      "Iteration 294, loss = 1.15744972\n",
      "Iteration 295, loss = 1.15713071\n",
      "Iteration 296, loss = 1.15681316\n",
      "Iteration 297, loss = 1.15649708\n",
      "Iteration 298, loss = 1.15618246\n",
      "Iteration 299, loss = 1.15586930\n",
      "Iteration 300, loss = 1.15555758\n",
      "Iteration 301, loss = 1.15524731\n",
      "Iteration 302, loss = 1.15493848\n",
      "Iteration 303, loss = 1.15463109\n",
      "Iteration 304, loss = 1.15432512\n",
      "Iteration 305, loss = 1.15402059\n",
      "Iteration 306, loss = 1.15371747\n",
      "Iteration 307, loss = 1.15341577\n",
      "Iteration 308, loss = 1.15311548\n",
      "Iteration 309, loss = 1.15281659\n",
      "Iteration 310, loss = 1.15251911\n",
      "Iteration 311, loss = 1.15222303\n",
      "Iteration 312, loss = 1.15192833\n",
      "Iteration 313, loss = 1.15163503\n",
      "Iteration 314, loss = 1.15134310\n",
      "Iteration 315, loss = 1.15105256\n",
      "Iteration 316, loss = 1.15076339\n",
      "Iteration 317, loss = 1.15047558\n",
      "Iteration 318, loss = 1.15018915\n",
      "Iteration 319, loss = 1.14990406\n",
      "Iteration 320, loss = 1.14962034\n",
      "Iteration 321, loss = 1.14933796\n",
      "Iteration 322, loss = 1.14905693\n",
      "Iteration 323, loss = 1.14877723\n",
      "Iteration 324, loss = 1.14849887\n",
      "Iteration 325, loss = 1.14822185\n",
      "Iteration 326, loss = 1.14794614\n",
      "Iteration 327, loss = 1.14767176\n",
      "Iteration 328, loss = 1.14739869\n",
      "Iteration 329, loss = 1.14712694\n",
      "Iteration 330, loss = 1.14685649\n",
      "Iteration 331, loss = 1.14658734\n",
      "Iteration 332, loss = 1.14631949\n",
      "Iteration 333, loss = 1.14605294\n",
      "Iteration 334, loss = 1.14578767\n",
      "Iteration 335, loss = 1.14552368\n",
      "Iteration 336, loss = 1.14526097\n",
      "Iteration 337, loss = 1.14499953\n",
      "Iteration 338, loss = 1.14473937\n",
      "Iteration 339, loss = 1.14448046\n",
      "Iteration 340, loss = 1.14422282\n",
      "Iteration 341, loss = 1.14396643\n",
      "Iteration 342, loss = 1.14371129\n",
      "Iteration 343, loss = 1.14345740\n",
      "Iteration 344, loss = 1.14320474\n",
      "Iteration 345, loss = 1.14295332\n",
      "Iteration 346, loss = 1.14270314\n",
      "Iteration 347, loss = 1.14245418\n",
      "Iteration 348, loss = 1.14220644\n",
      "Iteration 349, loss = 1.14195992\n",
      "Iteration 350, loss = 1.14171461\n",
      "Iteration 351, loss = 1.14147051\n",
      "Iteration 352, loss = 1.14122761\n",
      "Iteration 353, loss = 1.14098591\n",
      "Iteration 354, loss = 1.14074541\n",
      "Iteration 355, loss = 1.14050609\n",
      "Iteration 356, loss = 1.14026796\n",
      "Iteration 357, loss = 1.14003101\n",
      "Iteration 358, loss = 1.13979524\n",
      "Iteration 359, loss = 1.13956063\n",
      "Iteration 360, loss = 1.13932720\n",
      "Iteration 361, loss = 1.13909492\n",
      "Iteration 362, loss = 1.13886380\n",
      "Iteration 363, loss = 1.13863384\n",
      "Iteration 364, loss = 1.13840502\n",
      "Iteration 365, loss = 1.13817735\n",
      "Iteration 366, loss = 1.13795081\n",
      "Iteration 367, loss = 1.13772541\n",
      "Iteration 368, loss = 1.13750114\n",
      "Iteration 369, loss = 1.13727800\n",
      "Iteration 370, loss = 1.13705597\n",
      "Iteration 371, loss = 1.13683507\n",
      "Iteration 372, loss = 1.13661527\n",
      "Iteration 373, loss = 1.13639658\n",
      "Iteration 374, loss = 1.13617900\n",
      "Iteration 375, loss = 1.13596251\n",
      "Iteration 376, loss = 1.13574712\n",
      "Iteration 377, loss = 1.13553282\n",
      "Iteration 378, loss = 1.13531960\n",
      "Iteration 379, loss = 1.13510747\n",
      "Iteration 380, loss = 1.13489641\n",
      "Iteration 381, loss = 1.13468642\n",
      "Iteration 382, loss = 1.13447750\n",
      "Iteration 383, loss = 1.13426964\n",
      "Iteration 384, loss = 1.13406284\n",
      "Iteration 385, loss = 1.13385709\n",
      "Iteration 386, loss = 1.13365240\n",
      "Iteration 387, loss = 1.13344875\n",
      "Iteration 388, loss = 1.13324614\n",
      "Iteration 389, loss = 1.13304457\n",
      "Iteration 390, loss = 1.13284403\n",
      "Iteration 391, loss = 1.13264452\n",
      "Iteration 392, loss = 1.13244603\n",
      "Iteration 393, loss = 1.13224856\n",
      "Iteration 394, loss = 1.13205211\n",
      "Iteration 395, loss = 1.13185667\n",
      "Iteration 396, loss = 1.13166223\n",
      "Iteration 397, loss = 1.13146880\n",
      "Iteration 398, loss = 1.13127637\n",
      "Iteration 399, loss = 1.13108492\n",
      "Iteration 400, loss = 1.13089447\n",
      "Iteration 401, loss = 1.13070501\n",
      "Iteration 402, loss = 1.13051652\n",
      "Iteration 403, loss = 1.13032901\n",
      "Iteration 404, loss = 1.13014248\n",
      "Iteration 405, loss = 1.12995691\n",
      "Iteration 406, loss = 1.12977230\n",
      "Iteration 407, loss = 1.12958866\n",
      "Iteration 408, loss = 1.12940597\n",
      "Iteration 409, loss = 1.12922424\n",
      "Iteration 410, loss = 1.12904345\n",
      "Iteration 411, loss = 1.12886360\n",
      "Iteration 412, loss = 1.12868470\n",
      "Iteration 413, loss = 1.12850673\n",
      "Iteration 414, loss = 1.12832969\n",
      "Iteration 415, loss = 1.12815357\n",
      "Iteration 416, loss = 1.12797838\n",
      "Iteration 417, loss = 1.12780411\n",
      "Iteration 418, loss = 1.12763076\n",
      "Iteration 419, loss = 1.12745831\n",
      "Iteration 420, loss = 1.12728677\n",
      "Iteration 421, loss = 1.12711613\n",
      "Iteration 422, loss = 1.12694639\n",
      "Iteration 423, loss = 1.12677755\n",
      "Iteration 424, loss = 1.12660960\n",
      "Iteration 425, loss = 1.12644253\n",
      "Iteration 426, loss = 1.12627635\n",
      "Iteration 427, loss = 1.12611104\n",
      "Iteration 428, loss = 1.12594661\n",
      "Iteration 429, loss = 1.12578305\n",
      "Iteration 430, loss = 1.12562035\n",
      "Iteration 431, loss = 1.12545852\n",
      "Iteration 432, loss = 1.12529755\n",
      "Iteration 433, loss = 1.12513743\n",
      "Iteration 434, loss = 1.12497817\n",
      "Iteration 435, loss = 1.12481975\n",
      "Iteration 436, loss = 1.12466217\n",
      "Iteration 437, loss = 1.12450544\n",
      "Iteration 438, loss = 1.12434953\n",
      "Iteration 439, loss = 1.12419447\n",
      "Iteration 440, loss = 1.12404022\n",
      "Iteration 441, loss = 1.12388681\n",
      "Iteration 442, loss = 1.12373421\n",
      "Iteration 443, loss = 1.12358243\n",
      "Iteration 444, loss = 1.12343146\n",
      "Iteration 445, loss = 1.12328130\n",
      "Iteration 446, loss = 1.12313195\n",
      "Iteration 447, loss = 1.12298340\n",
      "Iteration 448, loss = 1.12283564\n",
      "Iteration 449, loss = 1.12268868\n",
      "Iteration 450, loss = 1.12254251\n",
      "Iteration 451, loss = 1.12239713\n",
      "Iteration 452, loss = 1.12225253\n",
      "Iteration 453, loss = 1.12210871\n",
      "Iteration 454, loss = 1.12196566\n",
      "Iteration 455, loss = 1.12182339\n",
      "Iteration 456, loss = 1.12168188\n",
      "Iteration 457, loss = 1.12154114\n",
      "Iteration 458, loss = 1.12140116\n",
      "Iteration 459, loss = 1.12126194\n",
      "Iteration 460, loss = 1.12112347\n",
      "Iteration 461, loss = 1.12098575\n",
      "Iteration 462, loss = 1.12084878\n",
      "Iteration 463, loss = 1.12071255\n",
      "Iteration 464, loss = 1.12057706\n",
      "Iteration 465, loss = 1.12044230\n",
      "Iteration 466, loss = 1.12030828\n",
      "Iteration 467, loss = 1.12017499\n",
      "Iteration 468, loss = 1.12004242\n",
      "Iteration 469, loss = 1.11991058\n",
      "Iteration 470, loss = 1.11977945\n",
      "Iteration 471, loss = 1.11964904\n",
      "Iteration 472, loss = 1.11951934\n",
      "Iteration 473, loss = 1.11939035\n",
      "Iteration 474, loss = 1.11926206\n",
      "Iteration 475, loss = 1.11913448\n",
      "Iteration 476, loss = 1.11900759\n",
      "Iteration 477, loss = 1.11888365\n",
      "Iteration 478, loss = 1.11876070\n",
      "Iteration 479, loss = 1.11863843\n",
      "Iteration 480, loss = 1.11851686\n",
      "Iteration 481, loss = 1.11839596\n",
      "Iteration 482, loss = 1.11827575\n",
      "Iteration 483, loss = 1.11815622\n",
      "Iteration 484, loss = 1.11803737\n",
      "Iteration 485, loss = 1.11791918\n",
      "Iteration 486, loss = 1.11780167\n",
      "Iteration 487, loss = 1.11768482\n",
      "Iteration 488, loss = 1.11756863\n",
      "Iteration 489, loss = 1.11745310\n",
      "Iteration 490, loss = 1.11733823\n",
      "Iteration 491, loss = 1.11722401\n",
      "Iteration 492, loss = 1.11711044\n",
      "Iteration 493, loss = 1.11699752\n",
      "Iteration 494, loss = 1.11688524\n",
      "Iteration 495, loss = 1.11677360\n",
      "Iteration 496, loss = 1.11666260\n",
      "Iteration 497, loss = 1.11655223\n",
      "Iteration 498, loss = 1.11644249\n",
      "Iteration 499, loss = 1.11633338\n",
      "Iteration 500, loss = 1.11622489\n",
      "Iteration 501, loss = 1.11611702\n",
      "Iteration 502, loss = 1.11600977\n",
      "Iteration 503, loss = 1.11590314\n",
      "Iteration 504, loss = 1.11579712\n",
      "Iteration 505, loss = 1.11569170\n",
      "Iteration 506, loss = 1.11558689\n",
      "Iteration 507, loss = 1.11548269\n",
      "Iteration 508, loss = 1.11537908\n",
      "Iteration 509, loss = 1.11527607\n",
      "Iteration 510, loss = 1.11517365\n",
      "Iteration 511, loss = 1.11507182\n",
      "Iteration 512, loss = 1.11497058\n",
      "Iteration 513, loss = 1.11486993\n",
      "Iteration 514, loss = 1.11476985\n",
      "Iteration 515, loss = 1.11467036\n",
      "Iteration 516, loss = 1.11457144\n",
      "Iteration 517, loss = 1.11447309\n",
      "Iteration 518, loss = 1.11437531\n",
      "Iteration 519, loss = 1.11427810\n",
      "Iteration 520, loss = 1.11418145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 521, loss = 1.11408536\n",
      "Iteration 522, loss = 1.11398983\n",
      "Iteration 523, loss = 1.11389485\n",
      "Iteration 524, loss = 1.11380042\n",
      "Iteration 525, loss = 1.11370655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32935999\n",
      "Iteration 2, loss = 1.32909223\n",
      "Iteration 3, loss = 1.32871574\n",
      "Iteration 4, loss = 1.32824894\n",
      "Iteration 5, loss = 1.32770464\n",
      "Iteration 6, loss = 1.32709326\n",
      "Iteration 7, loss = 1.32642866\n",
      "Iteration 8, loss = 1.32571743\n",
      "Iteration 9, loss = 1.32498989\n",
      "Iteration 10, loss = 1.32417606\n",
      "Iteration 11, loss = 1.32330799\n",
      "Iteration 12, loss = 1.32245297\n",
      "Iteration 13, loss = 1.32159334\n",
      "Iteration 14, loss = 1.32071111\n",
      "Iteration 15, loss = 1.31981436\n",
      "Iteration 16, loss = 1.31891590\n",
      "Iteration 17, loss = 1.31800357\n",
      "Iteration 18, loss = 1.31708076\n",
      "Iteration 19, loss = 1.31615947\n",
      "Iteration 20, loss = 1.31523608\n",
      "Iteration 21, loss = 1.31431238\n",
      "Iteration 22, loss = 1.31341434\n",
      "Iteration 23, loss = 1.31252769\n",
      "Iteration 24, loss = 1.31164100\n",
      "Iteration 25, loss = 1.31075249\n",
      "Iteration 26, loss = 1.30987436\n",
      "Iteration 27, loss = 1.30900480\n",
      "Iteration 28, loss = 1.30813499\n",
      "Iteration 29, loss = 1.30726516\n",
      "Iteration 30, loss = 1.30639550\n",
      "Iteration 31, loss = 1.30552621\n",
      "Iteration 32, loss = 1.30465746\n",
      "Iteration 33, loss = 1.30379605\n",
      "Iteration 34, loss = 1.30294288\n",
      "Iteration 35, loss = 1.30209101\n",
      "Iteration 36, loss = 1.30124052\n",
      "Iteration 37, loss = 1.30039149\n",
      "Iteration 38, loss = 1.29954398\n",
      "Iteration 39, loss = 1.29870385\n",
      "Iteration 40, loss = 1.29786718\n",
      "Iteration 41, loss = 1.29703239\n",
      "Iteration 42, loss = 1.29619950\n",
      "Iteration 43, loss = 1.29536853\n",
      "Iteration 44, loss = 1.29454130\n",
      "Iteration 45, loss = 1.29371698\n",
      "Iteration 46, loss = 1.29289934\n",
      "Iteration 47, loss = 1.29209114\n",
      "Iteration 48, loss = 1.29128516\n",
      "Iteration 49, loss = 1.29048139\n",
      "Iteration 50, loss = 1.28967982\n",
      "Iteration 51, loss = 1.28888046\n",
      "Iteration 52, loss = 1.28808329\n",
      "Iteration 53, loss = 1.28728833\n",
      "Iteration 54, loss = 1.28649833\n",
      "Iteration 55, loss = 1.28571170\n",
      "Iteration 56, loss = 1.28492729\n",
      "Iteration 57, loss = 1.28414509\n",
      "Iteration 58, loss = 1.28336509\n",
      "Iteration 59, loss = 1.28258730\n",
      "Iteration 60, loss = 1.28181172\n",
      "Iteration 61, loss = 1.28103834\n",
      "Iteration 62, loss = 1.28026717\n",
      "Iteration 63, loss = 1.27949821\n",
      "Iteration 64, loss = 1.27873145\n",
      "Iteration 65, loss = 1.27796690\n",
      "Iteration 66, loss = 1.27720455\n",
      "Iteration 67, loss = 1.27644441\n",
      "Iteration 68, loss = 1.27568648\n",
      "Iteration 69, loss = 1.27493074\n",
      "Iteration 70, loss = 1.27417722\n",
      "Iteration 71, loss = 1.27342589\n",
      "Iteration 72, loss = 1.27267677\n",
      "Iteration 73, loss = 1.27192985\n",
      "Iteration 74, loss = 1.27118514\n",
      "Iteration 75, loss = 1.27044262\n",
      "Iteration 76, loss = 1.26970230\n",
      "Iteration 77, loss = 1.26896419\n",
      "Iteration 78, loss = 1.26822827\n",
      "Iteration 79, loss = 1.26749455\n",
      "Iteration 80, loss = 1.26676303\n",
      "Iteration 81, loss = 1.26603413\n",
      "Iteration 82, loss = 1.26530812\n",
      "Iteration 83, loss = 1.26458436\n",
      "Iteration 84, loss = 1.26386284\n",
      "Iteration 85, loss = 1.26314357\n",
      "Iteration 86, loss = 1.26242653\n",
      "Iteration 87, loss = 1.26171172\n",
      "Iteration 88, loss = 1.26099914\n",
      "Iteration 89, loss = 1.26028878\n",
      "Iteration 90, loss = 1.25958064\n",
      "Iteration 91, loss = 1.25887472\n",
      "Iteration 92, loss = 1.25817102\n",
      "Iteration 93, loss = 1.25746953\n",
      "Iteration 94, loss = 1.25677025\n",
      "Iteration 95, loss = 1.25607317\n",
      "Iteration 96, loss = 1.25537831\n",
      "Iteration 97, loss = 1.25468564\n",
      "Iteration 98, loss = 1.25399517\n",
      "Iteration 99, loss = 1.25330690\n",
      "Iteration 100, loss = 1.25262083\n",
      "Iteration 101, loss = 1.25193695\n",
      "Iteration 102, loss = 1.25125526\n",
      "Iteration 103, loss = 1.25057576\n",
      "Iteration 104, loss = 1.24989845\n",
      "Iteration 105, loss = 1.24922332\n",
      "Iteration 106, loss = 1.24855037\n",
      "Iteration 107, loss = 1.24787960\n",
      "Iteration 108, loss = 1.24721102\n",
      "Iteration 109, loss = 1.24654460\n",
      "Iteration 110, loss = 1.24588037\n",
      "Iteration 111, loss = 1.24521830\n",
      "Iteration 112, loss = 1.24455841\n",
      "Iteration 113, loss = 1.24390068\n",
      "Iteration 114, loss = 1.24324512\n",
      "Iteration 115, loss = 1.24259172\n",
      "Iteration 116, loss = 1.24194048\n",
      "Iteration 117, loss = 1.24129140\n",
      "Iteration 118, loss = 1.24064448\n",
      "Iteration 119, loss = 1.23999972\n",
      "Iteration 120, loss = 1.23935710\n",
      "Iteration 121, loss = 1.23871664\n",
      "Iteration 122, loss = 1.23807832\n",
      "Iteration 123, loss = 1.23744215\n",
      "Iteration 124, loss = 1.23680813\n",
      "Iteration 125, loss = 1.23617624\n",
      "Iteration 126, loss = 1.23554650\n",
      "Iteration 127, loss = 1.23491889\n",
      "Iteration 128, loss = 1.23429341\n",
      "Iteration 129, loss = 1.23367007\n",
      "Iteration 130, loss = 1.23304885\n",
      "Iteration 131, loss = 1.23242976\n",
      "Iteration 132, loss = 1.23181280\n",
      "Iteration 133, loss = 1.23119795\n",
      "Iteration 134, loss = 1.23058523\n",
      "Iteration 135, loss = 1.22997462\n",
      "Iteration 136, loss = 1.22936613\n",
      "Iteration 137, loss = 1.22875975\n",
      "Iteration 138, loss = 1.22815547\n",
      "Iteration 139, loss = 1.22755331\n",
      "Iteration 140, loss = 1.22695324\n",
      "Iteration 141, loss = 1.22635528\n",
      "Iteration 142, loss = 1.22575941\n",
      "Iteration 143, loss = 1.22516564\n",
      "Iteration 144, loss = 1.22457397\n",
      "Iteration 145, loss = 1.22398438\n",
      "Iteration 146, loss = 1.22339688\n",
      "Iteration 147, loss = 1.22281146\n",
      "Iteration 148, loss = 1.22222812\n",
      "Iteration 149, loss = 1.22164686\n",
      "Iteration 150, loss = 1.22106768\n",
      "Iteration 151, loss = 1.22049057\n",
      "Iteration 152, loss = 1.21991552\n",
      "Iteration 153, loss = 1.21934255\n",
      "Iteration 154, loss = 1.21877163\n",
      "Iteration 155, loss = 1.21820278\n",
      "Iteration 156, loss = 1.21763598\n",
      "Iteration 157, loss = 1.21707124\n",
      "Iteration 158, loss = 1.21650855\n",
      "Iteration 159, loss = 1.21594791\n",
      "Iteration 160, loss = 1.21538931\n",
      "Iteration 161, loss = 1.21483275\n",
      "Iteration 162, loss = 1.21427823\n",
      "Iteration 163, loss = 1.21372575\n",
      "Iteration 164, loss = 1.21317530\n",
      "Iteration 165, loss = 1.21262687\n",
      "Iteration 166, loss = 1.21208047\n",
      "Iteration 167, loss = 1.21153609\n",
      "Iteration 168, loss = 1.21099374\n",
      "Iteration 169, loss = 1.21045339\n",
      "Iteration 170, loss = 1.20991506\n",
      "Iteration 171, loss = 1.20937873\n",
      "Iteration 172, loss = 1.20884442\n",
      "Iteration 173, loss = 1.20831210\n",
      "Iteration 174, loss = 1.20778178\n",
      "Iteration 175, loss = 1.20725345\n",
      "Iteration 176, loss = 1.20672712\n",
      "Iteration 177, loss = 1.20620277\n",
      "Iteration 178, loss = 1.20568041\n",
      "Iteration 179, loss = 1.20516003\n",
      "Iteration 180, loss = 1.20464162\n",
      "Iteration 181, loss = 1.20412519\n",
      "Iteration 182, loss = 1.20361072\n",
      "Iteration 183, loss = 1.20309823\n",
      "Iteration 184, loss = 1.20258769\n",
      "Iteration 185, loss = 1.20207911\n",
      "Iteration 186, loss = 1.20157249\n",
      "Iteration 187, loss = 1.20106782\n",
      "Iteration 188, loss = 1.20056510\n",
      "Iteration 189, loss = 1.20006432\n",
      "Iteration 190, loss = 1.19956548\n",
      "Iteration 191, loss = 1.19906857\n",
      "Iteration 192, loss = 1.19857360\n",
      "Iteration 193, loss = 1.19808055\n",
      "Iteration 194, loss = 1.19758943\n",
      "Iteration 195, loss = 1.19710023\n",
      "Iteration 196, loss = 1.19661295\n",
      "Iteration 197, loss = 1.19612758\n",
      "Iteration 198, loss = 1.19564412\n",
      "Iteration 199, loss = 1.19516256\n",
      "Iteration 200, loss = 1.19468291\n",
      "Iteration 201, loss = 1.19420515\n",
      "Iteration 202, loss = 1.19372929\n",
      "Iteration 203, loss = 1.19325531\n",
      "Iteration 204, loss = 1.19278322\n",
      "Iteration 205, loss = 1.19231301\n",
      "Iteration 206, loss = 1.19184468\n",
      "Iteration 207, loss = 1.19137821\n",
      "Iteration 208, loss = 1.19091362\n",
      "Iteration 209, loss = 1.19045090\n",
      "Iteration 210, loss = 1.18999003\n",
      "Iteration 211, loss = 1.18953102\n",
      "Iteration 212, loss = 1.18907386\n",
      "Iteration 213, loss = 1.18861855\n",
      "Iteration 214, loss = 1.18816508\n",
      "Iteration 215, loss = 1.18771346\n",
      "Iteration 216, loss = 1.18726367\n",
      "Iteration 217, loss = 1.18681571\n",
      "Iteration 218, loss = 1.18636957\n",
      "Iteration 219, loss = 1.18592526\n",
      "Iteration 220, loss = 1.18548277\n",
      "Iteration 221, loss = 1.18504209\n",
      "Iteration 222, loss = 1.18460323\n",
      "Iteration 223, loss = 1.18416616\n",
      "Iteration 224, loss = 1.18373090\n",
      "Iteration 225, loss = 1.18329744\n",
      "Iteration 226, loss = 1.18286577\n",
      "Iteration 227, loss = 1.18243589\n",
      "Iteration 228, loss = 1.18200779\n",
      "Iteration 229, loss = 1.18158147\n",
      "Iteration 230, loss = 1.18115693\n",
      "Iteration 231, loss = 1.18073416\n",
      "Iteration 232, loss = 1.18031315\n",
      "Iteration 233, loss = 1.17989391\n",
      "Iteration 234, loss = 1.17947642\n",
      "Iteration 235, loss = 1.17906069\n",
      "Iteration 236, loss = 1.17864670\n",
      "Iteration 237, loss = 1.17823446\n",
      "Iteration 238, loss = 1.17782396\n",
      "Iteration 239, loss = 1.17741520\n",
      "Iteration 240, loss = 1.17700816\n",
      "Iteration 241, loss = 1.17660286\n",
      "Iteration 242, loss = 1.17619927\n",
      "Iteration 243, loss = 1.17579740\n",
      "Iteration 244, loss = 1.17539725\n",
      "Iteration 245, loss = 1.17499880\n",
      "Iteration 246, loss = 1.17460205\n",
      "Iteration 247, loss = 1.17420701\n",
      "Iteration 248, loss = 1.17381366\n",
      "Iteration 249, loss = 1.17342200\n",
      "Iteration 250, loss = 1.17303202\n",
      "Iteration 251, loss = 1.17264373\n",
      "Iteration 252, loss = 1.17225711\n",
      "Iteration 253, loss = 1.17187217\n",
      "Iteration 254, loss = 1.17148889\n",
      "Iteration 255, loss = 1.17110727\n",
      "Iteration 256, loss = 1.17072731\n",
      "Iteration 257, loss = 1.17034900\n",
      "Iteration 258, loss = 1.16997235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259, loss = 1.16959733\n",
      "Iteration 260, loss = 1.16922396\n",
      "Iteration 261, loss = 1.16885222\n",
      "Iteration 262, loss = 1.16848211\n",
      "Iteration 263, loss = 1.16811362\n",
      "Iteration 264, loss = 1.16774676\n",
      "Iteration 265, loss = 1.16738151\n",
      "Iteration 266, loss = 1.16701787\n",
      "Iteration 267, loss = 1.16665584\n",
      "Iteration 268, loss = 1.16629541\n",
      "Iteration 269, loss = 1.16593658\n",
      "Iteration 270, loss = 1.16557934\n",
      "Iteration 271, loss = 1.16522368\n",
      "Iteration 272, loss = 1.16486961\n",
      "Iteration 273, loss = 1.16451712\n",
      "Iteration 274, loss = 1.16416620\n",
      "Iteration 275, loss = 1.16381685\n",
      "Iteration 276, loss = 1.16346907\n",
      "Iteration 277, loss = 1.16312284\n",
      "Iteration 278, loss = 1.16277817\n",
      "Iteration 279, loss = 1.16243505\n",
      "Iteration 280, loss = 1.16209347\n",
      "Iteration 281, loss = 1.16175343\n",
      "Iteration 282, loss = 1.16141493\n",
      "Iteration 283, loss = 1.16107796\n",
      "Iteration 284, loss = 1.16074251\n",
      "Iteration 285, loss = 1.16040859\n",
      "Iteration 286, loss = 1.16007618\n",
      "Iteration 287, loss = 1.15974529\n",
      "Iteration 288, loss = 1.15941590\n",
      "Iteration 289, loss = 1.15908801\n",
      "Iteration 290, loss = 1.15876162\n",
      "Iteration 291, loss = 1.15843672\n",
      "Iteration 292, loss = 1.15811331\n",
      "Iteration 293, loss = 1.15779138\n",
      "Iteration 294, loss = 1.15747093\n",
      "Iteration 295, loss = 1.15715196\n",
      "Iteration 296, loss = 1.15683445\n",
      "Iteration 297, loss = 1.15651841\n",
      "Iteration 298, loss = 1.15620382\n",
      "Iteration 299, loss = 1.15589069\n",
      "Iteration 300, loss = 1.15557901\n",
      "Iteration 301, loss = 1.15526878\n",
      "Iteration 302, loss = 1.15495998\n",
      "Iteration 303, loss = 1.15465262\n",
      "Iteration 304, loss = 1.15434669\n",
      "Iteration 305, loss = 1.15404218\n",
      "Iteration 306, loss = 1.15373909\n",
      "Iteration 307, loss = 1.15343742\n",
      "Iteration 308, loss = 1.15313717\n",
      "Iteration 309, loss = 1.15283831\n",
      "Iteration 310, loss = 1.15254086\n",
      "Iteration 311, loss = 1.15224481\n",
      "Iteration 312, loss = 1.15195015\n",
      "Iteration 313, loss = 1.15165687\n",
      "Iteration 314, loss = 1.15136498\n",
      "Iteration 315, loss = 1.15107447\n",
      "Iteration 316, loss = 1.15078532\n",
      "Iteration 317, loss = 1.15049755\n",
      "Iteration 318, loss = 1.15021114\n",
      "Iteration 319, loss = 1.14992609\n",
      "Iteration 320, loss = 1.14964239\n",
      "Iteration 321, loss = 1.14936004\n",
      "Iteration 322, loss = 1.14907903\n",
      "Iteration 323, loss = 1.14879937\n",
      "Iteration 324, loss = 1.14852104\n",
      "Iteration 325, loss = 1.14824403\n",
      "Iteration 326, loss = 1.14796836\n",
      "Iteration 327, loss = 1.14769400\n",
      "Iteration 328, loss = 1.14742096\n",
      "Iteration 329, loss = 1.14714923\n",
      "Iteration 330, loss = 1.14687881\n",
      "Iteration 331, loss = 1.14660969\n",
      "Iteration 332, loss = 1.14634186\n",
      "Iteration 333, loss = 1.14607533\n",
      "Iteration 334, loss = 1.14581008\n",
      "Iteration 335, loss = 1.14554612\n",
      "Iteration 336, loss = 1.14528343\n",
      "Iteration 337, loss = 1.14502202\n",
      "Iteration 338, loss = 1.14476188\n",
      "Iteration 339, loss = 1.14450300\n",
      "Iteration 340, loss = 1.14424538\n",
      "Iteration 341, loss = 1.14398901\n",
      "Iteration 342, loss = 1.14373389\n",
      "Iteration 343, loss = 1.14348002\n",
      "Iteration 344, loss = 1.14322739\n",
      "Iteration 345, loss = 1.14297599\n",
      "Iteration 346, loss = 1.14272583\n",
      "Iteration 347, loss = 1.14247689\n",
      "Iteration 348, loss = 1.14222917\n",
      "Iteration 349, loss = 1.14198267\n",
      "Iteration 350, loss = 1.14173738\n",
      "Iteration 351, loss = 1.14149330\n",
      "Iteration 352, loss = 1.14125042\n",
      "Iteration 353, loss = 1.14100874\n",
      "Iteration 354, loss = 1.14076825\n",
      "Iteration 355, loss = 1.14052896\n",
      "Iteration 356, loss = 1.14029085\n",
      "Iteration 357, loss = 1.14005391\n",
      "Iteration 358, loss = 1.13981816\n",
      "Iteration 359, loss = 1.13958357\n",
      "Iteration 360, loss = 1.13935015\n",
      "Iteration 361, loss = 1.13911789\n",
      "Iteration 362, loss = 1.13888679\n",
      "Iteration 363, loss = 1.13865684\n",
      "Iteration 364, loss = 1.13842804\n",
      "Iteration 365, loss = 1.13820039\n",
      "Iteration 366, loss = 1.13797387\n",
      "Iteration 367, loss = 1.13774848\n",
      "Iteration 368, loss = 1.13752423\n",
      "Iteration 369, loss = 1.13730110\n",
      "Iteration 370, loss = 1.13707909\n",
      "Iteration 371, loss = 1.13685820\n",
      "Iteration 372, loss = 1.13663842\n",
      "Iteration 373, loss = 1.13641974\n",
      "Iteration 374, loss = 1.13620217\n",
      "Iteration 375, loss = 1.13598570\n",
      "Iteration 376, loss = 1.13577032\n",
      "Iteration 377, loss = 1.13555603\n",
      "Iteration 378, loss = 1.13534283\n",
      "Iteration 379, loss = 1.13513071\n",
      "Iteration 380, loss = 1.13491966\n",
      "Iteration 381, loss = 1.13470968\n",
      "Iteration 382, loss = 1.13450077\n",
      "Iteration 383, loss = 1.13429293\n",
      "Iteration 384, loss = 1.13408614\n",
      "Iteration 385, loss = 1.13388041\n",
      "Iteration 386, loss = 1.13367572\n",
      "Iteration 387, loss = 1.13347208\n",
      "Iteration 388, loss = 1.13326948\n",
      "Iteration 389, loss = 1.13306792\n",
      "Iteration 390, loss = 1.13286739\n",
      "Iteration 391, loss = 1.13266789\n",
      "Iteration 392, loss = 1.13246942\n",
      "Iteration 393, loss = 1.13227196\n",
      "Iteration 394, loss = 1.13207551\n",
      "Iteration 395, loss = 1.13188008\n",
      "Iteration 396, loss = 1.13168565\n",
      "Iteration 397, loss = 1.13149223\n",
      "Iteration 398, loss = 1.13129980\n",
      "Iteration 399, loss = 1.13110837\n",
      "Iteration 400, loss = 1.13091793\n",
      "Iteration 401, loss = 1.13072847\n",
      "Iteration 402, loss = 1.13053999\n",
      "Iteration 403, loss = 1.13035249\n",
      "Iteration 404, loss = 1.13016596\n",
      "Iteration 405, loss = 1.12998040\n",
      "Iteration 406, loss = 1.12979580\n",
      "Iteration 407, loss = 1.12961216\n",
      "Iteration 408, loss = 1.12942948\n",
      "Iteration 409, loss = 1.12924775\n",
      "Iteration 410, loss = 1.12906697\n",
      "Iteration 411, loss = 1.12888713\n",
      "Iteration 412, loss = 1.12870823\n",
      "Iteration 413, loss = 1.12853026\n",
      "Iteration 414, loss = 1.12835323\n",
      "Iteration 415, loss = 1.12817712\n",
      "Iteration 416, loss = 1.12800194\n",
      "Iteration 417, loss = 1.12782767\n",
      "Iteration 418, loss = 1.12765432\n",
      "Iteration 419, loss = 1.12748188\n",
      "Iteration 420, loss = 1.12731034\n",
      "Iteration 421, loss = 1.12713971\n",
      "Iteration 422, loss = 1.12696997\n",
      "Iteration 423, loss = 1.12680113\n",
      "Iteration 424, loss = 1.12663318\n",
      "Iteration 425, loss = 1.12646612\n",
      "Iteration 426, loss = 1.12629994\n",
      "Iteration 427, loss = 1.12613463\n",
      "Iteration 428, loss = 1.12597020\n",
      "Iteration 429, loss = 1.12580664\n",
      "Iteration 430, loss = 1.12564395\n",
      "Iteration 431, loss = 1.12548212\n",
      "Iteration 432, loss = 1.12532115\n",
      "Iteration 433, loss = 1.12516104\n",
      "Iteration 434, loss = 1.12500177\n",
      "Iteration 435, loss = 1.12484335\n",
      "Iteration 436, loss = 1.12468578\n",
      "Iteration 437, loss = 1.12452904\n",
      "Iteration 438, loss = 1.12437314\n",
      "Iteration 439, loss = 1.12421807\n",
      "Iteration 440, loss = 1.12406383\n",
      "Iteration 441, loss = 1.12391041\n",
      "Iteration 442, loss = 1.12375782\n",
      "Iteration 443, loss = 1.12360604\n",
      "Iteration 444, loss = 1.12345507\n",
      "Iteration 445, loss = 1.12330491\n",
      "Iteration 446, loss = 1.12315556\n",
      "Iteration 447, loss = 1.12300700\n",
      "Iteration 448, loss = 1.12285925\n",
      "Iteration 449, loss = 1.12271228\n",
      "Iteration 450, loss = 1.12256611\n",
      "Iteration 451, loss = 1.12242073\n",
      "Iteration 452, loss = 1.12227613\n",
      "Iteration 453, loss = 1.12213230\n",
      "Iteration 454, loss = 1.12198925\n",
      "Iteration 455, loss = 1.12184698\n",
      "Iteration 456, loss = 1.12170547\n",
      "Iteration 457, loss = 1.12156473\n",
      "Iteration 458, loss = 1.12142474\n",
      "Iteration 459, loss = 1.12128552\n",
      "Iteration 460, loss = 1.12114704\n",
      "Iteration 461, loss = 1.12100932\n",
      "Iteration 462, loss = 1.12087235\n",
      "Iteration 463, loss = 1.12073611\n",
      "Iteration 464, loss = 1.12060062\n",
      "Iteration 465, loss = 1.12046586\n",
      "Iteration 466, loss = 1.12033184\n",
      "Iteration 467, loss = 1.12019854\n",
      "Iteration 468, loss = 1.12006597\n",
      "Iteration 469, loss = 1.11993412\n",
      "Iteration 470, loss = 1.11980299\n",
      "Iteration 471, loss = 1.11967257\n",
      "Iteration 472, loss = 1.11954287\n",
      "Iteration 473, loss = 1.11941387\n",
      "Iteration 474, loss = 1.11928558\n",
      "Iteration 475, loss = 1.11915799\n",
      "Iteration 476, loss = 1.11903109\n",
      "Iteration 477, loss = 1.11890489\n",
      "Iteration 478, loss = 1.11877939\n",
      "Iteration 479, loss = 1.11865456\n",
      "Iteration 480, loss = 1.11853043\n",
      "Iteration 481, loss = 1.11840697\n",
      "Iteration 482, loss = 1.11828419\n",
      "Iteration 483, loss = 1.11816209\n",
      "Iteration 484, loss = 1.11804066\n",
      "Iteration 485, loss = 1.11792171\n",
      "Iteration 486, loss = 1.11780419\n",
      "Iteration 487, loss = 1.11768733\n",
      "Iteration 488, loss = 1.11757114\n",
      "Iteration 489, loss = 1.11745560\n",
      "Iteration 490, loss = 1.11734073\n",
      "Iteration 491, loss = 1.11722650\n",
      "Iteration 492, loss = 1.11711292\n",
      "Iteration 493, loss = 1.11699999\n",
      "Iteration 494, loss = 1.11688770\n",
      "Iteration 495, loss = 1.11677605\n",
      "Iteration 496, loss = 1.11666503\n",
      "Iteration 497, loss = 1.11655465\n",
      "Iteration 498, loss = 1.11644490\n",
      "Iteration 499, loss = 1.11633578\n",
      "Iteration 500, loss = 1.11622728\n",
      "Iteration 501, loss = 1.11611940\n",
      "Iteration 502, loss = 1.11601214\n",
      "Iteration 503, loss = 1.11590549\n",
      "Iteration 504, loss = 1.11579946\n",
      "Iteration 505, loss = 1.11569403\n",
      "Iteration 506, loss = 1.11558921\n",
      "Iteration 507, loss = 1.11548499\n",
      "Iteration 508, loss = 1.11538137\n",
      "Iteration 509, loss = 1.11527835\n",
      "Iteration 510, loss = 1.11517592\n",
      "Iteration 511, loss = 1.11507408\n",
      "Iteration 512, loss = 1.11497283\n",
      "Iteration 513, loss = 1.11487216\n",
      "Iteration 514, loss = 1.11477207\n",
      "Iteration 515, loss = 1.11467257\n",
      "Iteration 516, loss = 1.11457363\n",
      "Iteration 517, loss = 1.11447527\n",
      "Iteration 518, loss = 1.11437748\n",
      "Iteration 519, loss = 1.11428025\n",
      "Iteration 520, loss = 1.11418359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 521, loss = 1.11408749\n",
      "Iteration 522, loss = 1.11399195\n",
      "Iteration 523, loss = 1.11389696\n",
      "Iteration 524, loss = 1.11380252\n",
      "Iteration 525, loss = 1.11370863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33009363\n",
      "Iteration 2, loss = 1.32979279\n",
      "Iteration 3, loss = 1.32936676\n",
      "Iteration 4, loss = 1.32884585\n",
      "Iteration 5, loss = 1.32823655\n",
      "Iteration 6, loss = 1.32755681\n",
      "Iteration 7, loss = 1.32681363\n",
      "Iteration 8, loss = 1.32602180\n",
      "Iteration 9, loss = 1.32513868\n",
      "Iteration 10, loss = 1.32417021\n",
      "Iteration 11, loss = 1.32321121\n",
      "Iteration 12, loss = 1.32223683\n",
      "Iteration 13, loss = 1.32123465\n",
      "Iteration 14, loss = 1.32024742\n",
      "Iteration 15, loss = 1.31924576\n",
      "Iteration 16, loss = 1.31823327\n",
      "Iteration 17, loss = 1.31722276\n",
      "Iteration 18, loss = 1.31621363\n",
      "Iteration 19, loss = 1.31524359\n",
      "Iteration 20, loss = 1.31429633\n",
      "Iteration 21, loss = 1.31335161\n",
      "Iteration 22, loss = 1.31242792\n",
      "Iteration 23, loss = 1.31151868\n",
      "Iteration 24, loss = 1.31060881\n",
      "Iteration 25, loss = 1.30969855\n",
      "Iteration 26, loss = 1.30878816\n",
      "Iteration 27, loss = 1.30788460\n",
      "Iteration 28, loss = 1.30703088\n",
      "Iteration 29, loss = 1.30617780\n",
      "Iteration 30, loss = 1.30532547\n",
      "Iteration 31, loss = 1.30447624\n",
      "Iteration 32, loss = 1.30363027\n",
      "Iteration 33, loss = 1.30278533\n",
      "Iteration 34, loss = 1.30194215\n",
      "Iteration 35, loss = 1.30110775\n",
      "Iteration 36, loss = 1.30027569\n",
      "Iteration 37, loss = 1.29944670\n",
      "Iteration 38, loss = 1.29862304\n",
      "Iteration 39, loss = 1.29780912\n",
      "Iteration 40, loss = 1.29699639\n",
      "Iteration 41, loss = 1.29618496\n",
      "Iteration 42, loss = 1.29537492\n",
      "Iteration 43, loss = 1.29456635\n",
      "Iteration 44, loss = 1.29375931\n",
      "Iteration 45, loss = 1.29295389\n",
      "Iteration 46, loss = 1.29215235\n",
      "Iteration 47, loss = 1.29135358\n",
      "Iteration 48, loss = 1.29055644\n",
      "Iteration 49, loss = 1.28976100\n",
      "Iteration 50, loss = 1.28896730\n",
      "Iteration 51, loss = 1.28817540\n",
      "Iteration 52, loss = 1.28738534\n",
      "Iteration 53, loss = 1.28659716\n",
      "Iteration 54, loss = 1.28581089\n",
      "Iteration 55, loss = 1.28502657\n",
      "Iteration 56, loss = 1.28424422\n",
      "Iteration 57, loss = 1.28346388\n",
      "Iteration 58, loss = 1.28268555\n",
      "Iteration 59, loss = 1.28190928\n",
      "Iteration 60, loss = 1.28113506\n",
      "Iteration 61, loss = 1.28036293\n",
      "Iteration 62, loss = 1.27959289\n",
      "Iteration 63, loss = 1.27882495\n",
      "Iteration 64, loss = 1.27805914\n",
      "Iteration 65, loss = 1.27729546\n",
      "Iteration 66, loss = 1.27653392\n",
      "Iteration 67, loss = 1.27577452\n",
      "Iteration 68, loss = 1.27501729\n",
      "Iteration 69, loss = 1.27426221\n",
      "Iteration 70, loss = 1.27350931\n",
      "Iteration 71, loss = 1.27275858\n",
      "Iteration 72, loss = 1.27201003\n",
      "Iteration 73, loss = 1.27126366\n",
      "Iteration 74, loss = 1.27051948\n",
      "Iteration 75, loss = 1.26977749\n",
      "Iteration 76, loss = 1.26903770\n",
      "Iteration 77, loss = 1.26830009\n",
      "Iteration 78, loss = 1.26756469\n",
      "Iteration 79, loss = 1.26683148\n",
      "Iteration 80, loss = 1.26610047\n",
      "Iteration 81, loss = 1.26537167\n",
      "Iteration 82, loss = 1.26464506\n",
      "Iteration 83, loss = 1.26392066\n",
      "Iteration 84, loss = 1.26319846\n",
      "Iteration 85, loss = 1.26247846\n",
      "Iteration 86, loss = 1.26176067\n",
      "Iteration 87, loss = 1.26104508\n",
      "Iteration 88, loss = 1.26033169\n",
      "Iteration 89, loss = 1.25962050\n",
      "Iteration 90, loss = 1.25891152\n",
      "Iteration 91, loss = 1.25820474\n",
      "Iteration 92, loss = 1.25750016\n",
      "Iteration 93, loss = 1.25679778\n",
      "Iteration 94, loss = 1.25609760\n",
      "Iteration 95, loss = 1.25539961\n",
      "Iteration 96, loss = 1.25470383\n",
      "Iteration 97, loss = 1.25401024\n",
      "Iteration 98, loss = 1.25331884\n",
      "Iteration 99, loss = 1.25262964\n",
      "Iteration 100, loss = 1.25194263\n",
      "Iteration 101, loss = 1.25125781\n",
      "Iteration 102, loss = 1.25057518\n",
      "Iteration 103, loss = 1.24989474\n",
      "Iteration 104, loss = 1.24921649\n",
      "Iteration 105, loss = 1.24854042\n",
      "Iteration 106, loss = 1.24786654\n",
      "Iteration 107, loss = 1.24719484\n",
      "Iteration 108, loss = 1.24652532\n",
      "Iteration 109, loss = 1.24585797\n",
      "Iteration 110, loss = 1.24519281\n",
      "Iteration 111, loss = 1.24453018\n",
      "Iteration 112, loss = 1.24386977\n",
      "Iteration 113, loss = 1.24321155\n",
      "Iteration 114, loss = 1.24255549\n",
      "Iteration 115, loss = 1.24190161\n",
      "Iteration 116, loss = 1.24124990\n",
      "Iteration 117, loss = 1.24060036\n",
      "Iteration 118, loss = 1.23995298\n",
      "Iteration 119, loss = 1.23930777\n",
      "Iteration 120, loss = 1.23866472\n",
      "Iteration 121, loss = 1.23802384\n",
      "Iteration 122, loss = 1.23738511\n",
      "Iteration 123, loss = 1.23674854\n",
      "Iteration 124, loss = 1.23611413\n",
      "Iteration 125, loss = 1.23548186\n",
      "Iteration 126, loss = 1.23485175\n",
      "Iteration 127, loss = 1.23422378\n",
      "Iteration 128, loss = 1.23359797\n",
      "Iteration 129, loss = 1.23297429\n",
      "Iteration 130, loss = 1.23235276\n",
      "Iteration 131, loss = 1.23173336\n",
      "Iteration 132, loss = 1.23111610\n",
      "Iteration 133, loss = 1.23050097\n",
      "Iteration 134, loss = 1.22988798\n",
      "Iteration 135, loss = 1.22927711\n",
      "Iteration 136, loss = 1.22866837\n",
      "Iteration 137, loss = 1.22806176\n",
      "Iteration 138, loss = 1.22745726\n",
      "Iteration 139, loss = 1.22685489\n",
      "Iteration 140, loss = 1.22625463\n",
      "Iteration 141, loss = 1.22565648\n",
      "Iteration 142, loss = 1.22506044\n",
      "Iteration 143, loss = 1.22446652\n",
      "Iteration 144, loss = 1.22387469\n",
      "Iteration 145, loss = 1.22328497\n",
      "Iteration 146, loss = 1.22269735\n",
      "Iteration 147, loss = 1.22211182\n",
      "Iteration 148, loss = 1.22152839\n",
      "Iteration 149, loss = 1.22094705\n",
      "Iteration 150, loss = 1.22036780\n",
      "Iteration 151, loss = 1.21979063\n",
      "Iteration 152, loss = 1.21921555\n",
      "Iteration 153, loss = 1.21864254\n",
      "Iteration 154, loss = 1.21807161\n",
      "Iteration 155, loss = 1.21750275\n",
      "Iteration 156, loss = 1.21693596\n",
      "Iteration 157, loss = 1.21637124\n",
      "Iteration 158, loss = 1.21580859\n",
      "Iteration 159, loss = 1.21524799\n",
      "Iteration 160, loss = 1.21468945\n",
      "Iteration 161, loss = 1.21413297\n",
      "Iteration 162, loss = 1.21357854\n",
      "Iteration 163, loss = 1.21302615\n",
      "Iteration 164, loss = 1.21247582\n",
      "Iteration 165, loss = 1.21192752\n",
      "Iteration 166, loss = 1.21138126\n",
      "Iteration 167, loss = 1.21083703\n",
      "Iteration 168, loss = 1.21029484\n",
      "Iteration 169, loss = 1.20975467\n",
      "Iteration 170, loss = 1.20921653\n",
      "Iteration 171, loss = 1.20868041\n",
      "Iteration 172, loss = 1.20814631\n",
      "Iteration 173, loss = 1.20761423\n",
      "Iteration 174, loss = 1.20708415\n",
      "Iteration 175, loss = 1.20655609\n",
      "Iteration 176, loss = 1.20603002\n",
      "Iteration 177, loss = 1.20550596\n",
      "Iteration 178, loss = 1.20498389\n",
      "Iteration 179, loss = 1.20446382\n",
      "Iteration 180, loss = 1.20394574\n",
      "Iteration 181, loss = 1.20342964\n",
      "Iteration 182, loss = 1.20291553\n",
      "Iteration 183, loss = 1.20240340\n",
      "Iteration 184, loss = 1.20189324\n",
      "Iteration 185, loss = 1.20138505\n",
      "Iteration 186, loss = 1.20087883\n",
      "Iteration 187, loss = 1.20037458\n",
      "Iteration 188, loss = 1.19987228\n",
      "Iteration 189, loss = 1.19937195\n",
      "Iteration 190, loss = 1.19887356\n",
      "Iteration 191, loss = 1.19837713\n",
      "Iteration 192, loss = 1.19788264\n",
      "Iteration 193, loss = 1.19739009\n",
      "Iteration 194, loss = 1.19689948\n",
      "Iteration 195, loss = 1.19641080\n",
      "Iteration 196, loss = 1.19592405\n",
      "Iteration 197, loss = 1.19543923\n",
      "Iteration 198, loss = 1.19495633\n",
      "Iteration 199, loss = 1.19447535\n",
      "Iteration 200, loss = 1.19399628\n",
      "Iteration 201, loss = 1.19351913\n",
      "Iteration 202, loss = 1.19304388\n",
      "Iteration 203, loss = 1.19257053\n",
      "Iteration 204, loss = 1.19209908\n",
      "Iteration 205, loss = 1.19162952\n",
      "Iteration 206, loss = 1.19116185\n",
      "Iteration 207, loss = 1.19069607\n",
      "Iteration 208, loss = 1.19023217\n",
      "Iteration 209, loss = 1.18977015\n",
      "Iteration 210, loss = 1.18931001\n",
      "Iteration 211, loss = 1.18885173\n",
      "Iteration 212, loss = 1.18839531\n",
      "Iteration 213, loss = 1.18794076\n",
      "Iteration 214, loss = 1.18748807\n",
      "Iteration 215, loss = 1.18703723\n",
      "Iteration 216, loss = 1.18658823\n",
      "Iteration 217, loss = 1.18614108\n",
      "Iteration 218, loss = 1.18569577\n",
      "Iteration 219, loss = 1.18525230\n",
      "Iteration 220, loss = 1.18481066\n",
      "Iteration 221, loss = 1.18437084\n",
      "Iteration 222, loss = 1.18393285\n",
      "Iteration 223, loss = 1.18349668\n",
      "Iteration 224, loss = 1.18306232\n",
      "Iteration 225, loss = 1.18262977\n",
      "Iteration 226, loss = 1.18219903\n",
      "Iteration 227, loss = 1.18177009\n",
      "Iteration 228, loss = 1.18134294\n",
      "Iteration 229, loss = 1.18091759\n",
      "Iteration 230, loss = 1.18049402\n",
      "Iteration 231, loss = 1.18007224\n",
      "Iteration 232, loss = 1.17965224\n",
      "Iteration 233, loss = 1.17923401\n",
      "Iteration 234, loss = 1.17881756\n",
      "Iteration 235, loss = 1.17840287\n",
      "Iteration 236, loss = 1.17798994\n",
      "Iteration 237, loss = 1.17757877\n",
      "Iteration 238, loss = 1.17716935\n",
      "Iteration 239, loss = 1.17676167\n",
      "Iteration 240, loss = 1.17635574\n",
      "Iteration 241, loss = 1.17595155\n",
      "Iteration 242, loss = 1.17554910\n",
      "Iteration 243, loss = 1.17514837\n",
      "Iteration 244, loss = 1.17474937\n",
      "Iteration 245, loss = 1.17435209\n",
      "Iteration 246, loss = 1.17395653\n",
      "Iteration 247, loss = 1.17356268\n",
      "Iteration 248, loss = 1.17317053\n",
      "Iteration 249, loss = 1.17278009\n",
      "Iteration 250, loss = 1.17239135\n",
      "Iteration 251, loss = 1.17200430\n",
      "Iteration 252, loss = 1.17161893\n",
      "Iteration 253, loss = 1.17123525\n",
      "Iteration 254, loss = 1.17085325\n",
      "Iteration 255, loss = 1.17047293\n",
      "Iteration 256, loss = 1.17009427\n",
      "Iteration 257, loss = 1.16971728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 1.16934195\n",
      "Iteration 259, loss = 1.16896828\n",
      "Iteration 260, loss = 1.16859626\n",
      "Iteration 261, loss = 1.16822588\n",
      "Iteration 262, loss = 1.16785715\n",
      "Iteration 263, loss = 1.16749005\n",
      "Iteration 264, loss = 1.16712459\n",
      "Iteration 265, loss = 1.16676075\n",
      "Iteration 266, loss = 1.16639854\n",
      "Iteration 267, loss = 1.16603795\n",
      "Iteration 268, loss = 1.16567897\n",
      "Iteration 269, loss = 1.16532159\n",
      "Iteration 270, loss = 1.16496583\n",
      "Iteration 271, loss = 1.16461166\n",
      "Iteration 272, loss = 1.16425908\n",
      "Iteration 273, loss = 1.16390810\n",
      "Iteration 274, loss = 1.16355870\n",
      "Iteration 275, loss = 1.16321088\n",
      "Iteration 276, loss = 1.16286464\n",
      "Iteration 277, loss = 1.16251996\n",
      "Iteration 278, loss = 1.16217686\n",
      "Iteration 279, loss = 1.16183531\n",
      "Iteration 280, loss = 1.16149532\n",
      "Iteration 281, loss = 1.16115689\n",
      "Iteration 282, loss = 1.16082000\n",
      "Iteration 283, loss = 1.16048465\n",
      "Iteration 284, loss = 1.16015084\n",
      "Iteration 285, loss = 1.15981856\n",
      "Iteration 286, loss = 1.15948781\n",
      "Iteration 287, loss = 1.15915858\n",
      "Iteration 288, loss = 1.15883087\n",
      "Iteration 289, loss = 1.15850467\n",
      "Iteration 290, loss = 1.15817998\n",
      "Iteration 291, loss = 1.15785680\n",
      "Iteration 292, loss = 1.15753511\n",
      "Iteration 293, loss = 1.15721492\n",
      "Iteration 294, loss = 1.15689622\n",
      "Iteration 295, loss = 1.15657900\n",
      "Iteration 296, loss = 1.15626326\n",
      "Iteration 297, loss = 1.15594899\n",
      "Iteration 298, loss = 1.15563620\n",
      "Iteration 299, loss = 1.15532487\n",
      "Iteration 300, loss = 1.15501500\n",
      "Iteration 301, loss = 1.15470659\n",
      "Iteration 302, loss = 1.15439962\n",
      "Iteration 303, loss = 1.15409410\n",
      "Iteration 304, loss = 1.15379003\n",
      "Iteration 305, loss = 1.15348738\n",
      "Iteration 306, loss = 1.15318617\n",
      "Iteration 307, loss = 1.15288639\n",
      "Iteration 308, loss = 1.15258803\n",
      "Iteration 309, loss = 1.15229108\n",
      "Iteration 310, loss = 1.15199555\n",
      "Iteration 311, loss = 1.15170142\n",
      "Iteration 312, loss = 1.15140870\n",
      "Iteration 313, loss = 1.15111737\n",
      "Iteration 314, loss = 1.15082744\n",
      "Iteration 315, loss = 1.15053889\n",
      "Iteration 316, loss = 1.15025173\n",
      "Iteration 317, loss = 1.14996594\n",
      "Iteration 318, loss = 1.14968153\n",
      "Iteration 319, loss = 1.14939849\n",
      "Iteration 320, loss = 1.14911681\n",
      "Iteration 321, loss = 1.14883649\n",
      "Iteration 322, loss = 1.14855752\n",
      "Iteration 323, loss = 1.14827990\n",
      "Iteration 324, loss = 1.14800363\n",
      "Iteration 325, loss = 1.14772870\n",
      "Iteration 326, loss = 1.14745510\n",
      "Iteration 327, loss = 1.14718283\n",
      "Iteration 328, loss = 1.14691189\n",
      "Iteration 329, loss = 1.14664226\n",
      "Iteration 330, loss = 1.14637396\n",
      "Iteration 331, loss = 1.14610696\n",
      "Iteration 332, loss = 1.14584127\n",
      "Iteration 333, loss = 1.14557688\n",
      "Iteration 334, loss = 1.14531379\n",
      "Iteration 335, loss = 1.14505199\n",
      "Iteration 336, loss = 1.14479148\n",
      "Iteration 337, loss = 1.14453225\n",
      "Iteration 338, loss = 1.14427430\n",
      "Iteration 339, loss = 1.14401762\n",
      "Iteration 340, loss = 1.14376221\n",
      "Iteration 341, loss = 1.14350806\n",
      "Iteration 342, loss = 1.14325517\n",
      "Iteration 343, loss = 1.14300353\n",
      "Iteration 344, loss = 1.14275315\n",
      "Iteration 345, loss = 1.14250401\n",
      "Iteration 346, loss = 1.14225611\n",
      "Iteration 347, loss = 1.14200944\n",
      "Iteration 348, loss = 1.14176400\n",
      "Iteration 349, loss = 1.14151979\n",
      "Iteration 350, loss = 1.14127680\n",
      "Iteration 351, loss = 1.14103503\n",
      "Iteration 352, loss = 1.14079447\n",
      "Iteration 353, loss = 1.14055511\n",
      "Iteration 354, loss = 1.14031696\n",
      "Iteration 355, loss = 1.14008001\n",
      "Iteration 356, loss = 1.13984425\n",
      "Iteration 357, loss = 1.13960967\n",
      "Iteration 358, loss = 1.13937628\n",
      "Iteration 359, loss = 1.13914407\n",
      "Iteration 360, loss = 1.13891304\n",
      "Iteration 361, loss = 1.13868317\n",
      "Iteration 362, loss = 1.13845447\n",
      "Iteration 363, loss = 1.13822693\n",
      "Iteration 364, loss = 1.13800054\n",
      "Iteration 365, loss = 1.13777531\n",
      "Iteration 366, loss = 1.13755123\n",
      "Iteration 367, loss = 1.13732828\n",
      "Iteration 368, loss = 1.13710648\n",
      "Iteration 369, loss = 1.13688580\n",
      "Iteration 370, loss = 1.13666626\n",
      "Iteration 371, loss = 1.13644784\n",
      "Iteration 372, loss = 1.13623054\n",
      "Iteration 373, loss = 1.13601435\n",
      "Iteration 374, loss = 1.13579928\n",
      "Iteration 375, loss = 1.13558531\n",
      "Iteration 376, loss = 1.13537244\n",
      "Iteration 377, loss = 1.13516067\n",
      "Iteration 378, loss = 1.13494999\n",
      "Iteration 379, loss = 1.13474040\n",
      "Iteration 380, loss = 1.13453189\n",
      "Iteration 381, loss = 1.13432446\n",
      "Iteration 382, loss = 1.13411811\n",
      "Iteration 383, loss = 1.13391282\n",
      "Iteration 384, loss = 1.13370860\n",
      "Iteration 385, loss = 1.13350545\n",
      "Iteration 386, loss = 1.13330334\n",
      "Iteration 387, loss = 1.13310230\n",
      "Iteration 388, loss = 1.13290230\n",
      "Iteration 389, loss = 1.13270334\n",
      "Iteration 390, loss = 1.13250542\n",
      "Iteration 391, loss = 1.13230854\n",
      "Iteration 392, loss = 1.13211269\n",
      "Iteration 393, loss = 1.13191786\n",
      "Iteration 394, loss = 1.13172405\n",
      "Iteration 395, loss = 1.13153127\n",
      "Iteration 396, loss = 1.13133949\n",
      "Iteration 397, loss = 1.13114873\n",
      "Iteration 398, loss = 1.13095897\n",
      "Iteration 399, loss = 1.13077020\n",
      "Iteration 400, loss = 1.13058244\n",
      "Iteration 401, loss = 1.13039566\n",
      "Iteration 402, loss = 1.13020988\n",
      "Iteration 403, loss = 1.13002507\n",
      "Iteration 404, loss = 1.12984125\n",
      "Iteration 405, loss = 1.12965840\n",
      "Iteration 406, loss = 1.12947652\n",
      "Iteration 407, loss = 1.12929560\n",
      "Iteration 408, loss = 1.12911565\n",
      "Iteration 409, loss = 1.12893665\n",
      "Iteration 410, loss = 1.12875861\n",
      "Iteration 411, loss = 1.12858152\n",
      "Iteration 412, loss = 1.12840537\n",
      "Iteration 413, loss = 1.12823016\n",
      "Iteration 414, loss = 1.12805589\n",
      "Iteration 415, loss = 1.12788256\n",
      "Iteration 416, loss = 1.12771015\n",
      "Iteration 417, loss = 1.12753866\n",
      "Iteration 418, loss = 1.12736810\n",
      "Iteration 419, loss = 1.12719845\n",
      "Iteration 420, loss = 1.12702971\n",
      "Iteration 421, loss = 1.12686188\n",
      "Iteration 422, loss = 1.12669496\n",
      "Iteration 423, loss = 1.12652893\n",
      "Iteration 424, loss = 1.12636380\n",
      "Iteration 425, loss = 1.12619957\n",
      "Iteration 426, loss = 1.12603622\n",
      "Iteration 427, loss = 1.12587375\n",
      "Iteration 428, loss = 1.12571216\n",
      "Iteration 429, loss = 1.12555145\n",
      "Iteration 430, loss = 1.12539161\n",
      "Iteration 431, loss = 1.12523264\n",
      "Iteration 432, loss = 1.12507453\n",
      "Iteration 433, loss = 1.12491728\n",
      "Iteration 434, loss = 1.12476089\n",
      "Iteration 435, loss = 1.12460535\n",
      "Iteration 436, loss = 1.12445066\n",
      "Iteration 437, loss = 1.12429681\n",
      "Iteration 438, loss = 1.12414380\n",
      "Iteration 439, loss = 1.12399163\n",
      "Iteration 440, loss = 1.12384029\n",
      "Iteration 441, loss = 1.12368978\n",
      "Iteration 442, loss = 1.12354010\n",
      "Iteration 443, loss = 1.12339123\n",
      "Iteration 444, loss = 1.12324318\n",
      "Iteration 445, loss = 1.12309595\n",
      "Iteration 446, loss = 1.12294953\n",
      "Iteration 447, loss = 1.12280391\n",
      "Iteration 448, loss = 1.12265909\n",
      "Iteration 449, loss = 1.12251507\n",
      "Iteration 450, loss = 1.12237185\n",
      "Iteration 451, loss = 1.12222941\n",
      "Iteration 452, loss = 1.12208777\n",
      "Iteration 453, loss = 1.12194690\n",
      "Iteration 454, loss = 1.12180682\n",
      "Iteration 455, loss = 1.12166751\n",
      "Iteration 456, loss = 1.12152898\n",
      "Iteration 457, loss = 1.12139121\n",
      "Iteration 458, loss = 1.12125421\n",
      "Iteration 459, loss = 1.12111797\n",
      "Iteration 460, loss = 1.12098248\n",
      "Iteration 461, loss = 1.12084775\n",
      "Iteration 462, loss = 1.12071377\n",
      "Iteration 463, loss = 1.12058054\n",
      "Iteration 464, loss = 1.12044805\n",
      "Iteration 465, loss = 1.12031630\n",
      "Iteration 466, loss = 1.12018529\n",
      "Iteration 467, loss = 1.12005501\n",
      "Iteration 468, loss = 1.11992545\n",
      "Iteration 469, loss = 1.11979663\n",
      "Iteration 470, loss = 1.11966852\n",
      "Iteration 471, loss = 1.11954113\n",
      "Iteration 472, loss = 1.11941446\n",
      "Iteration 473, loss = 1.11928850\n",
      "Iteration 474, loss = 1.11916325\n",
      "Iteration 475, loss = 1.11903870\n",
      "Iteration 476, loss = 1.11891485\n",
      "Iteration 477, loss = 1.11879171\n",
      "Iteration 478, loss = 1.11866925\n",
      "Iteration 479, loss = 1.11854749\n",
      "Iteration 480, loss = 1.11842641\n",
      "Iteration 481, loss = 1.11830602\n",
      "Iteration 482, loss = 1.11818630\n",
      "Iteration 483, loss = 1.11806727\n",
      "Iteration 484, loss = 1.11794891\n",
      "Iteration 485, loss = 1.11783121\n",
      "Iteration 486, loss = 1.11771419\n",
      "Iteration 487, loss = 1.11759783\n",
      "Iteration 488, loss = 1.11748213\n",
      "Iteration 489, loss = 1.11736709\n",
      "Iteration 490, loss = 1.11725270\n",
      "Iteration 491, loss = 1.11713897\n",
      "Iteration 492, loss = 1.11702588\n",
      "Iteration 493, loss = 1.11691343\n",
      "Iteration 494, loss = 1.11680163\n",
      "Iteration 495, loss = 1.11669046\n",
      "Iteration 496, loss = 1.11657993\n",
      "Iteration 497, loss = 1.11647003\n",
      "Iteration 498, loss = 1.11636076\n",
      "Iteration 499, loss = 1.11625211\n",
      "Iteration 500, loss = 1.11614409\n",
      "Iteration 501, loss = 1.11603668\n",
      "Iteration 502, loss = 1.11592989\n",
      "Iteration 503, loss = 1.11582371\n",
      "Iteration 504, loss = 1.11571815\n",
      "Iteration 505, loss = 1.11561318\n",
      "Iteration 506, loss = 1.11550883\n",
      "Iteration 507, loss = 1.11540507\n",
      "Iteration 508, loss = 1.11530191\n",
      "Iteration 509, loss = 1.11519934\n",
      "Iteration 510, loss = 1.11509736\n",
      "Iteration 511, loss = 1.11499598\n",
      "Iteration 512, loss = 1.11489517\n",
      "Iteration 513, loss = 1.11479495\n",
      "Iteration 514, loss = 1.11469531\n",
      "Iteration 515, loss = 1.11459624\n",
      "Iteration 516, loss = 1.11449775\n",
      "Iteration 517, loss = 1.11439983\n",
      "Iteration 518, loss = 1.11430247\n",
      "Iteration 519, loss = 1.11420568\n",
      "Iteration 520, loss = 1.11410945\n",
      "Iteration 521, loss = 1.11401378\n",
      "Iteration 522, loss = 1.11391866\n",
      "Iteration 523, loss = 1.11382410\n",
      "Iteration 524, loss = 1.11373009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13218766\n",
      "Iteration 2, loss = 1.14933933\n",
      "Iteration 3, loss = 1.27870341\n",
      "Iteration 4, loss = 1.17704877\n",
      "Iteration 5, loss = 1.04555397\n",
      "Iteration 6, loss = 0.99979567\n",
      "Iteration 7, loss = 0.96526300\n",
      "Iteration 8, loss = 0.89311396\n",
      "Iteration 9, loss = 0.85254097\n",
      "Iteration 10, loss = 0.82853592\n",
      "Iteration 11, loss = 0.76467767\n",
      "Iteration 12, loss = 0.68419456\n",
      "Iteration 13, loss = 0.61567605\n",
      "Iteration 14, loss = 0.57308126\n",
      "Iteration 15, loss = 0.49668149\n",
      "Iteration 16, loss = 0.44818718\n",
      "Iteration 17, loss = 0.42903512\n",
      "Iteration 18, loss = 0.41675004\n",
      "Iteration 19, loss = 0.40616241\n",
      "Iteration 20, loss = 0.40038800\n",
      "Iteration 21, loss = 0.39717738\n",
      "Iteration 22, loss = 0.39404089\n",
      "Iteration 23, loss = 0.38912367\n",
      "Iteration 24, loss = 0.38384784\n",
      "Iteration 25, loss = 0.38040380\n",
      "Iteration 26, loss = 0.37740999\n",
      "Iteration 27, loss = 0.37247027\n",
      "Iteration 28, loss = 0.36700490\n",
      "Iteration 29, loss = 0.36290943\n",
      "Iteration 30, loss = 0.35864187\n",
      "Iteration 31, loss = 0.35279941\n",
      "Iteration 32, loss = 0.34670317\n",
      "Iteration 33, loss = 0.34119586\n",
      "Iteration 34, loss = 0.33500845\n",
      "Iteration 35, loss = 0.32768861\n",
      "Iteration 36, loss = 0.32036314\n",
      "Iteration 37, loss = 0.31316020\n",
      "Iteration 38, loss = 0.30503905\n",
      "Iteration 39, loss = 0.29576543\n",
      "Iteration 40, loss = 0.28603270\n",
      "Iteration 41, loss = 0.27610849\n",
      "Iteration 42, loss = 0.26577393\n",
      "Iteration 43, loss = 0.25537424\n",
      "Iteration 44, loss = 0.24552268\n",
      "Iteration 45, loss = 0.23588739\n",
      "Iteration 46, loss = 0.22610050\n",
      "Iteration 47, loss = 0.21715293\n",
      "Iteration 48, loss = 0.20926291\n",
      "Iteration 49, loss = 0.20177814\n",
      "Iteration 50, loss = 0.19525591\n",
      "Iteration 51, loss = 0.18920441\n",
      "Iteration 52, loss = 0.18321216\n",
      "Iteration 53, loss = 0.17775933\n",
      "Iteration 54, loss = 0.17211262\n",
      "Iteration 55, loss = 0.16701616\n",
      "Iteration 56, loss = 0.16194562\n",
      "Iteration 57, loss = 0.15706567\n",
      "Iteration 58, loss = 0.15253700\n",
      "Iteration 59, loss = 0.14792799\n",
      "Iteration 60, loss = 0.14378374\n",
      "Iteration 61, loss = 0.13951728\n",
      "Iteration 62, loss = 0.13567688\n",
      "Iteration 63, loss = 0.13182345\n",
      "Iteration 64, loss = 0.12825693\n",
      "Iteration 65, loss = 0.12485338\n",
      "Iteration 66, loss = 0.12148110\n",
      "Iteration 67, loss = 0.11846056\n",
      "Iteration 68, loss = 0.11547508\n",
      "Iteration 69, loss = 0.11279062\n",
      "Iteration 70, loss = 0.11017873\n",
      "Iteration 71, loss = 0.10779023\n",
      "Iteration 72, loss = 0.10551832\n",
      "Iteration 73, loss = 0.10336145\n",
      "Iteration 74, loss = 0.10138622\n",
      "Iteration 75, loss = 0.09946991\n",
      "Iteration 76, loss = 0.09773469\n",
      "Iteration 77, loss = 0.09604173\n",
      "Iteration 78, loss = 0.09448998\n",
      "Iteration 79, loss = 0.09299305\n",
      "Iteration 80, loss = 0.09160032\n",
      "Iteration 81, loss = 0.09027537\n",
      "Iteration 82, loss = 0.08902775\n",
      "Iteration 83, loss = 0.08784998\n",
      "Iteration 84, loss = 0.08672950\n",
      "Iteration 85, loss = 0.08567731\n",
      "Iteration 86, loss = 0.08466898\n",
      "Iteration 87, loss = 0.08371622\n",
      "Iteration 88, loss = 0.08280218\n",
      "Iteration 89, loss = 0.08194095\n",
      "Iteration 90, loss = 0.08110885\n",
      "Iteration 91, loss = 0.08031712\n",
      "Iteration 92, loss = 0.07955675\n",
      "Iteration 93, loss = 0.07883305\n",
      "Iteration 94, loss = 0.07813799\n",
      "Iteration 95, loss = 0.07747171\n",
      "Iteration 96, loss = 0.07683411\n",
      "Iteration 97, loss = 0.07622313\n",
      "Iteration 98, loss = 0.07563625\n",
      "Iteration 99, loss = 0.07506796\n",
      "Iteration 100, loss = 0.07452288\n",
      "Iteration 101, loss = 0.07399280\n",
      "Iteration 102, loss = 0.07348767\n",
      "Iteration 103, loss = 0.07299825\n",
      "Iteration 104, loss = 0.07253097\n",
      "Iteration 105, loss = 0.07207674\n",
      "Iteration 106, loss = 0.07163936\n",
      "Iteration 107, loss = 0.07121795\n",
      "Iteration 108, loss = 0.07081065\n",
      "Iteration 109, loss = 0.07041663\n",
      "Iteration 110, loss = 0.07003612\n",
      "Iteration 111, loss = 0.06966671\n",
      "Iteration 112, loss = 0.06930890\n",
      "Iteration 113, loss = 0.06896199\n",
      "Iteration 114, loss = 0.06862570\n",
      "Iteration 115, loss = 0.06829927\n",
      "Iteration 116, loss = 0.06798327\n",
      "Iteration 117, loss = 0.06767744\n",
      "Iteration 118, loss = 0.06738097\n",
      "Iteration 119, loss = 0.06709411\n",
      "Iteration 120, loss = 0.06681562\n",
      "Iteration 121, loss = 0.06654608\n",
      "Iteration 122, loss = 0.06628265\n",
      "Iteration 123, loss = 0.06602776\n",
      "Iteration 124, loss = 0.06577942\n",
      "Iteration 125, loss = 0.06553897\n",
      "Iteration 126, loss = 0.06530468\n",
      "Iteration 127, loss = 0.06507745\n",
      "Iteration 128, loss = 0.06485613\n",
      "Iteration 129, loss = 0.06464098\n",
      "Iteration 130, loss = 0.06443163\n",
      "Iteration 131, loss = 0.06422763\n",
      "Iteration 132, loss = 0.06402920\n",
      "Iteration 133, loss = 0.06383555\n",
      "Iteration 134, loss = 0.06364709\n",
      "Iteration 135, loss = 0.06346304\n",
      "Iteration 136, loss = 0.06328367\n",
      "Iteration 137, loss = 0.06311118\n",
      "Iteration 138, loss = 0.06294093\n",
      "Iteration 139, loss = 0.06277278\n",
      "Iteration 140, loss = 0.06260889\n",
      "Iteration 141, loss = 0.06245006\n",
      "Iteration 142, loss = 0.06229412\n",
      "Iteration 143, loss = 0.06214157\n",
      "Iteration 144, loss = 0.06199217\n",
      "Iteration 145, loss = 0.06184590\n",
      "Iteration 146, loss = 0.06170260\n",
      "Iteration 147, loss = 0.06156216\n",
      "Iteration 148, loss = 0.06142453\n",
      "Iteration 149, loss = 0.06128955\n",
      "Iteration 150, loss = 0.06115694\n",
      "Iteration 151, loss = 0.06102668\n",
      "Iteration 152, loss = 0.06089877\n",
      "Iteration 153, loss = 0.06077328\n",
      "Iteration 154, loss = 0.06064999\n",
      "Iteration 155, loss = 0.06052893\n",
      "Iteration 156, loss = 0.06041001\n",
      "Iteration 157, loss = 0.06029458\n",
      "Iteration 158, loss = 0.06018033\n",
      "Iteration 159, loss = 0.06006699\n",
      "Iteration 160, loss = 0.05995581\n",
      "Iteration 161, loss = 0.05984729\n",
      "Iteration 162, loss = 0.05974044\n",
      "Iteration 163, loss = 0.05963529\n",
      "Iteration 164, loss = 0.05953174\n",
      "Iteration 165, loss = 0.05942976\n",
      "Iteration 166, loss = 0.05932932\n",
      "Iteration 167, loss = 0.05923036\n",
      "Iteration 168, loss = 0.05913285\n",
      "Iteration 169, loss = 0.05903676\n",
      "Iteration 170, loss = 0.05894204\n",
      "Iteration 171, loss = 0.05884932\n",
      "Iteration 172, loss = 0.05875777\n",
      "Iteration 173, loss = 0.05866687\n",
      "Iteration 174, loss = 0.05857772\n",
      "Iteration 175, loss = 0.05849003\n",
      "Iteration 176, loss = 0.05840345\n",
      "Iteration 177, loss = 0.05831798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14020736\n",
      "Iteration 2, loss = 1.15104323\n",
      "Iteration 3, loss = 1.28874816\n",
      "Iteration 4, loss = 1.18350696\n",
      "Iteration 5, loss = 1.04674976\n",
      "Iteration 6, loss = 0.99587033\n",
      "Iteration 7, loss = 0.99357002\n",
      "Iteration 8, loss = 0.89686519\n",
      "Iteration 9, loss = 0.84827333\n",
      "Iteration 10, loss = 0.82722709\n",
      "Iteration 11, loss = 0.77130963\n",
      "Iteration 12, loss = 0.68982788\n",
      "Iteration 13, loss = 0.61591127\n",
      "Iteration 14, loss = 0.57916555\n",
      "Iteration 15, loss = 0.50327875\n",
      "Iteration 16, loss = 0.45165500\n",
      "Iteration 17, loss = 0.43001663\n",
      "Iteration 18, loss = 0.41859197\n",
      "Iteration 19, loss = 0.40916570\n",
      "Iteration 20, loss = 0.40185059\n",
      "Iteration 21, loss = 0.39806824\n",
      "Iteration 22, loss = 0.39563359\n",
      "Iteration 23, loss = 0.39164583\n",
      "Iteration 24, loss = 0.38601688\n",
      "Iteration 25, loss = 0.38160818\n",
      "Iteration 26, loss = 0.37902709\n",
      "Iteration 27, loss = 0.37525631\n",
      "Iteration 28, loss = 0.36953559\n",
      "Iteration 29, loss = 0.36454682\n",
      "Iteration 30, loss = 0.36077368\n",
      "Iteration 31, loss = 0.35590735\n",
      "Iteration 32, loss = 0.34956721\n",
      "Iteration 33, loss = 0.34347414\n",
      "Iteration 34, loss = 0.33778223\n",
      "Iteration 35, loss = 0.33104733\n",
      "Iteration 36, loss = 0.32328469\n",
      "Iteration 37, loss = 0.31568773\n",
      "Iteration 38, loss = 0.30813271\n",
      "Iteration 39, loss = 0.29953459\n",
      "Iteration 40, loss = 0.28974811\n",
      "Iteration 41, loss = 0.27955772\n",
      "Iteration 42, loss = 0.26931554\n",
      "Iteration 43, loss = 0.25882006\n",
      "Iteration 44, loss = 0.24841821\n",
      "Iteration 45, loss = 0.23870469\n",
      "Iteration 46, loss = 0.22924392\n",
      "Iteration 47, loss = 0.21969921\n",
      "Iteration 48, loss = 0.21107865\n",
      "Iteration 49, loss = 0.20338145\n",
      "Iteration 50, loss = 0.19597648\n",
      "Iteration 51, loss = 0.18943829\n",
      "Iteration 52, loss = 0.18307468\n",
      "Iteration 53, loss = 0.17671148\n",
      "Iteration 54, loss = 0.17080844\n",
      "Iteration 55, loss = 0.16502645\n",
      "Iteration 56, loss = 0.15967785\n",
      "Iteration 57, loss = 0.15449787\n",
      "Iteration 58, loss = 0.14950861\n",
      "Iteration 59, loss = 0.14482492\n",
      "Iteration 60, loss = 0.14021229\n",
      "Iteration 61, loss = 0.13592626\n",
      "Iteration 62, loss = 0.13171703\n",
      "Iteration 63, loss = 0.12776717\n",
      "Iteration 64, loss = 0.12385421\n",
      "Iteration 65, loss = 0.12021592\n",
      "Iteration 66, loss = 0.11678406\n",
      "Iteration 67, loss = 0.11348869\n",
      "Iteration 68, loss = 0.11049818\n",
      "Iteration 69, loss = 0.10757084\n",
      "Iteration 70, loss = 0.10490471\n",
      "Iteration 71, loss = 0.10235558\n",
      "Iteration 72, loss = 0.09999116\n",
      "Iteration 73, loss = 0.09778978\n",
      "Iteration 74, loss = 0.09570233\n",
      "Iteration 75, loss = 0.09379393\n",
      "Iteration 76, loss = 0.09196323\n",
      "Iteration 77, loss = 0.09029318\n",
      "Iteration 78, loss = 0.08869769\n",
      "Iteration 79, loss = 0.08722351\n",
      "Iteration 80, loss = 0.08582562\n",
      "Iteration 81, loss = 0.08450663\n",
      "Iteration 82, loss = 0.08327013\n",
      "Iteration 83, loss = 0.08210401\n",
      "Iteration 84, loss = 0.08101071\n",
      "Iteration 85, loss = 0.07997732\n",
      "Iteration 86, loss = 0.07900618\n",
      "Iteration 87, loss = 0.07808795\n",
      "Iteration 88, loss = 0.07722176\n",
      "Iteration 89, loss = 0.07640306\n",
      "Iteration 90, loss = 0.07562767\n",
      "Iteration 91, loss = 0.07489475\n",
      "Iteration 92, loss = 0.07419735\n",
      "Iteration 93, loss = 0.07353470\n",
      "Iteration 94, loss = 0.07290324\n",
      "Iteration 95, loss = 0.07230222\n",
      "Iteration 96, loss = 0.07172687\n",
      "Iteration 97, loss = 0.07118069\n",
      "Iteration 98, loss = 0.07065915\n",
      "Iteration 99, loss = 0.07015986\n",
      "Iteration 100, loss = 0.06968165\n",
      "Iteration 101, loss = 0.06922208\n",
      "Iteration 102, loss = 0.06878327\n",
      "Iteration 103, loss = 0.06836083\n",
      "Iteration 104, loss = 0.06795699\n",
      "Iteration 105, loss = 0.06756611\n",
      "Iteration 106, loss = 0.06719372\n",
      "Iteration 107, loss = 0.06683231\n",
      "Iteration 108, loss = 0.06648589\n",
      "Iteration 109, loss = 0.06615216\n",
      "Iteration 110, loss = 0.06583118\n",
      "Iteration 111, loss = 0.06552101\n",
      "Iteration 112, loss = 0.06522094\n",
      "Iteration 113, loss = 0.06493276\n",
      "Iteration 114, loss = 0.06465368\n",
      "Iteration 115, loss = 0.06438531\n",
      "Iteration 116, loss = 0.06412525\n",
      "Iteration 117, loss = 0.06387471\n",
      "Iteration 118, loss = 0.06363188\n",
      "Iteration 119, loss = 0.06339641\n",
      "Iteration 120, loss = 0.06316818\n",
      "Iteration 121, loss = 0.06294693\n",
      "Iteration 122, loss = 0.06273271\n",
      "Iteration 123, loss = 0.06252478\n",
      "Iteration 124, loss = 0.06232329\n",
      "Iteration 125, loss = 0.06212776\n",
      "Iteration 126, loss = 0.06193788\n",
      "Iteration 127, loss = 0.06175343\n",
      "Iteration 128, loss = 0.06157432\n",
      "Iteration 129, loss = 0.06140005\n",
      "Iteration 130, loss = 0.06123070\n",
      "Iteration 131, loss = 0.06106579\n",
      "Iteration 132, loss = 0.06090531\n",
      "Iteration 133, loss = 0.06074878\n",
      "Iteration 134, loss = 0.06059629\n",
      "Iteration 135, loss = 0.06044746\n",
      "Iteration 136, loss = 0.06030232\n",
      "Iteration 137, loss = 0.06016024\n",
      "Iteration 138, loss = 0.06002138\n",
      "Iteration 139, loss = 0.05988574\n",
      "Iteration 140, loss = 0.05975312\n",
      "Iteration 141, loss = 0.05962348\n",
      "Iteration 142, loss = 0.05949740\n",
      "Iteration 143, loss = 0.05937314\n",
      "Iteration 144, loss = 0.05925192\n",
      "Iteration 145, loss = 0.05913350\n",
      "Iteration 146, loss = 0.05901746\n",
      "Iteration 147, loss = 0.05890376\n",
      "Iteration 148, loss = 0.05879226\n",
      "Iteration 149, loss = 0.05868293\n",
      "Iteration 150, loss = 0.05857566\n",
      "Iteration 151, loss = 0.05847024\n",
      "Iteration 152, loss = 0.05836665\n",
      "Iteration 153, loss = 0.05826492\n",
      "Iteration 154, loss = 0.05816501\n",
      "Iteration 155, loss = 0.05806711\n",
      "Iteration 156, loss = 0.05797093\n",
      "Iteration 157, loss = 0.05787645\n",
      "Iteration 158, loss = 0.05778375\n",
      "Iteration 159, loss = 0.05769257\n",
      "Iteration 160, loss = 0.05760285\n",
      "Iteration 161, loss = 0.05751448\n",
      "Iteration 162, loss = 0.05742735\n",
      "Iteration 163, loss = 0.05734158\n",
      "Iteration 164, loss = 0.05725728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13352491\n",
      "Iteration 2, loss = 1.14848288\n",
      "Iteration 3, loss = 1.27328245\n",
      "Iteration 4, loss = 1.17572099\n",
      "Iteration 5, loss = 1.04077468\n",
      "Iteration 6, loss = 0.98633750\n",
      "Iteration 7, loss = 0.95755756\n",
      "Iteration 8, loss = 0.88884846\n",
      "Iteration 9, loss = 0.83925213\n",
      "Iteration 10, loss = 0.81385199\n",
      "Iteration 11, loss = 0.76193378\n",
      "Iteration 12, loss = 0.68244683\n",
      "Iteration 13, loss = 0.60405403\n",
      "Iteration 14, loss = 0.56851022\n",
      "Iteration 15, loss = 0.49510110\n",
      "Iteration 16, loss = 0.44104677\n",
      "Iteration 17, loss = 0.42165175\n",
      "Iteration 18, loss = 0.41094354\n",
      "Iteration 19, loss = 0.39846604\n",
      "Iteration 20, loss = 0.38727141\n",
      "Iteration 21, loss = 0.38007634\n",
      "Iteration 22, loss = 0.37159915\n",
      "Iteration 23, loss = 0.35700477\n",
      "Iteration 24, loss = 0.33735409\n",
      "Iteration 25, loss = 0.31434633\n",
      "Iteration 26, loss = 0.28754833\n",
      "Iteration 27, loss = 0.25734959\n",
      "Iteration 28, loss = 0.22527175\n",
      "Iteration 29, loss = 0.19332371\n",
      "Iteration 30, loss = 0.16422316\n",
      "Iteration 31, loss = 0.14225451\n",
      "Iteration 32, loss = 0.14107938\n",
      "Iteration 33, loss = 0.10863435\n",
      "Iteration 34, loss = 0.10130256\n",
      "Iteration 35, loss = 0.09444080\n",
      "Iteration 36, loss = 0.07958183\n",
      "Iteration 37, loss = 0.08087179\n",
      "Iteration 38, loss = 0.06642853\n",
      "Iteration 39, loss = 0.06978749\n",
      "Iteration 40, loss = 0.05987583\n",
      "Iteration 41, loss = 0.06209004\n",
      "Iteration 42, loss = 0.05486566\n",
      "Iteration 43, loss = 0.05607960\n",
      "Iteration 44, loss = 0.05254877\n",
      "Iteration 45, loss = 0.05138161\n",
      "Iteration 46, loss = 0.05050415\n",
      "Iteration 47, loss = 0.04839334\n",
      "Iteration 48, loss = 0.04919396\n",
      "Iteration 49, loss = 0.04603782\n",
      "Iteration 50, loss = 0.04789708\n",
      "Iteration 51, loss = 0.04483404\n",
      "Iteration 52, loss = 0.04632124\n",
      "Iteration 53, loss = 0.04422316\n",
      "Iteration 54, loss = 0.04489714\n",
      "Iteration 55, loss = 0.04375160\n",
      "Iteration 56, loss = 0.04366742\n",
      "Iteration 57, loss = 0.04350027\n",
      "Iteration 58, loss = 0.04263897\n",
      "Iteration 59, loss = 0.04309684\n",
      "Iteration 60, loss = 0.04202343\n",
      "Iteration 61, loss = 0.04256108\n",
      "Iteration 62, loss = 0.04159680\n",
      "Iteration 63, loss = 0.04200151\n",
      "Iteration 64, loss = 0.04129690\n",
      "Iteration 65, loss = 0.04142516\n",
      "Iteration 66, loss = 0.04108785\n",
      "Iteration 67, loss = 0.04090912\n",
      "Iteration 68, loss = 0.04084453\n",
      "Iteration 69, loss = 0.04050886\n",
      "Iteration 70, loss = 0.04058407\n",
      "Iteration 71, loss = 0.04017817\n",
      "Iteration 72, loss = 0.04030373\n",
      "Iteration 73, loss = 0.03992647\n",
      "Iteration 74, loss = 0.04000676\n",
      "Iteration 75, loss = 0.03972098\n",
      "Iteration 76, loss = 0.03972630\n",
      "Iteration 77, loss = 0.03953260\n",
      "Iteration 78, loss = 0.03946880\n",
      "Iteration 79, loss = 0.03935990\n",
      "Iteration 80, loss = 0.03923839\n",
      "Iteration 81, loss = 0.03918716\n",
      "Iteration 82, loss = 0.03904062\n",
      "Iteration 83, loss = 0.03901703\n",
      "Iteration 84, loss = 0.03886494\n",
      "Iteration 85, loss = 0.03885168\n",
      "Iteration 86, loss = 0.03870986\n",
      "Iteration 87, loss = 0.03869117\n",
      "Iteration 88, loss = 0.03856862\n",
      "Iteration 89, loss = 0.03854002\n",
      "Iteration 90, loss = 0.03843687\n",
      "Iteration 91, loss = 0.03839724\n",
      "Iteration 92, loss = 0.03831297\n",
      "Iteration 93, loss = 0.03826376\n",
      "Iteration 94, loss = 0.03819394\n",
      "Iteration 95, loss = 0.03813879\n",
      "Iteration 96, loss = 0.03807985\n",
      "Iteration 97, loss = 0.03802107\n",
      "Iteration 98, loss = 0.03796959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 0.03790999\n",
      "Iteration 100, loss = 0.03786319\n",
      "Iteration 101, loss = 0.03780412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13108299\n",
      "Iteration 2, loss = 1.14482298\n",
      "Iteration 3, loss = 1.27741210\n",
      "Iteration 4, loss = 1.17791337\n",
      "Iteration 5, loss = 1.04322361\n",
      "Iteration 6, loss = 0.98953740\n",
      "Iteration 7, loss = 0.96112826\n",
      "Iteration 8, loss = 0.89264711\n",
      "Iteration 9, loss = 0.84329583\n",
      "Iteration 10, loss = 0.81919384\n",
      "Iteration 11, loss = 0.76671080\n",
      "Iteration 12, loss = 0.68554232\n",
      "Iteration 13, loss = 0.60695245\n",
      "Iteration 14, loss = 0.56959239\n",
      "Iteration 15, loss = 0.49321903\n",
      "Iteration 16, loss = 0.44370511\n",
      "Iteration 17, loss = 0.42339556\n",
      "Iteration 18, loss = 0.41220880\n",
      "Iteration 19, loss = 0.40064583\n",
      "Iteration 20, loss = 0.38898905\n",
      "Iteration 21, loss = 0.38131850\n",
      "Iteration 22, loss = 0.37161426\n",
      "Iteration 23, loss = 0.35272122\n",
      "Iteration 24, loss = 0.34073889\n",
      "Iteration 25, loss = 0.31368546\n",
      "Iteration 26, loss = 0.29964031\n",
      "Iteration 27, loss = 0.26636748\n",
      "Iteration 28, loss = 0.24936294\n",
      "Iteration 29, loss = 0.21810756\n",
      "Iteration 30, loss = 0.19745453\n",
      "Iteration 31, loss = 0.18090257\n",
      "Iteration 32, loss = 0.15139534\n",
      "Iteration 33, loss = 0.14220752\n",
      "Iteration 34, loss = 0.12859467\n",
      "Iteration 35, loss = 0.10666873\n",
      "Iteration 36, loss = 0.10644343\n",
      "Iteration 37, loss = 0.09086218\n",
      "Iteration 38, loss = 0.08267712\n",
      "Iteration 39, loss = 0.07984311\n",
      "Iteration 40, loss = 0.06733014\n",
      "Iteration 41, loss = 0.06863115\n",
      "Iteration 42, loss = 0.05889539\n",
      "Iteration 43, loss = 0.05981322\n",
      "Iteration 44, loss = 0.05389631\n",
      "Iteration 45, loss = 0.05244051\n",
      "Iteration 46, loss = 0.04976304\n",
      "Iteration 47, loss = 0.04682910\n",
      "Iteration 48, loss = 0.04658192\n",
      "Iteration 49, loss = 0.04310558\n",
      "Iteration 50, loss = 0.04390245\n",
      "Iteration 51, loss = 0.04029091\n",
      "Iteration 52, loss = 0.04119313\n",
      "Iteration 53, loss = 0.03839610\n",
      "Iteration 54, loss = 0.03897055\n",
      "Iteration 55, loss = 0.03717000\n",
      "Iteration 56, loss = 0.03686130\n",
      "Iteration 57, loss = 0.03611268\n",
      "Iteration 58, loss = 0.03507227\n",
      "Iteration 59, loss = 0.03515878\n",
      "Iteration 60, loss = 0.03379360\n",
      "Iteration 61, loss = 0.03417279\n",
      "Iteration 62, loss = 0.03291403\n",
      "Iteration 63, loss = 0.03298450\n",
      "Iteration 64, loss = 0.03230397\n",
      "Iteration 65, loss = 0.03186615\n",
      "Iteration 66, loss = 0.03176843\n",
      "Iteration 67, loss = 0.03099767\n",
      "Iteration 68, loss = 0.03111104\n",
      "Iteration 69, loss = 0.03042903\n",
      "Iteration 70, loss = 0.03027614\n",
      "Iteration 71, loss = 0.03001066\n",
      "Iteration 72, loss = 0.02951839\n",
      "Iteration 73, loss = 0.02949831\n",
      "Iteration 74, loss = 0.02900379\n",
      "Iteration 75, loss = 0.02885555\n",
      "Iteration 76, loss = 0.02863367\n",
      "Iteration 77, loss = 0.02824724\n",
      "Iteration 78, loss = 0.02817925\n",
      "Iteration 79, loss = 0.02783247\n",
      "Iteration 80, loss = 0.02760498\n",
      "Iteration 81, loss = 0.02747222\n",
      "Iteration 82, loss = 0.02713588\n",
      "Iteration 83, loss = 0.02698633\n",
      "Iteration 84, loss = 0.02679274\n",
      "Iteration 85, loss = 0.02651231\n",
      "Iteration 86, loss = 0.02637883\n",
      "Iteration 87, loss = 0.02616387\n",
      "Iteration 88, loss = 0.02592931\n",
      "Iteration 89, loss = 0.02579132\n",
      "Iteration 90, loss = 0.02558039\n",
      "Iteration 91, loss = 0.02537225\n",
      "Iteration 92, loss = 0.02522885\n",
      "Iteration 93, loss = 0.02503209\n",
      "Iteration 94, loss = 0.02483737\n",
      "Iteration 95, loss = 0.02469110\n",
      "Iteration 96, loss = 0.02451154\n",
      "Iteration 97, loss = 0.02432469\n",
      "Iteration 98, loss = 0.02417579\n",
      "Iteration 99, loss = 0.02401340\n",
      "Iteration 100, loss = 0.02383471\n",
      "Iteration 101, loss = 0.02368137\n",
      "Iteration 102, loss = 0.02353285\n",
      "Iteration 103, loss = 0.02336651\n",
      "Iteration 104, loss = 0.02320857\n",
      "Iteration 105, loss = 0.02306622\n",
      "Iteration 106, loss = 0.02291570\n",
      "Iteration 107, loss = 0.02275887\n",
      "Iteration 108, loss = 0.02261430\n",
      "Iteration 109, loss = 0.02247556\n",
      "Iteration 110, loss = 0.02232924\n",
      "Iteration 111, loss = 0.02218262\n",
      "Iteration 112, loss = 0.02204502\n",
      "Iteration 113, loss = 0.02191035\n",
      "Iteration 114, loss = 0.02177144\n",
      "Iteration 115, loss = 0.02163242\n",
      "Iteration 116, loss = 0.02149938\n",
      "Iteration 117, loss = 0.02136975\n",
      "Iteration 118, loss = 0.02123833\n",
      "Iteration 119, loss = 0.02110597\n",
      "Iteration 120, loss = 0.02097662\n",
      "Iteration 121, loss = 0.02085107\n",
      "Iteration 122, loss = 0.02072655\n",
      "Iteration 123, loss = 0.02060117\n",
      "Iteration 124, loss = 0.02047619\n",
      "Iteration 125, loss = 0.02035355\n",
      "Iteration 126, loss = 0.02023345\n",
      "Iteration 127, loss = 0.02011460\n",
      "Iteration 128, loss = 0.01999588\n",
      "Iteration 129, loss = 0.01987747\n",
      "Iteration 130, loss = 0.01976022\n",
      "Iteration 131, loss = 0.01964474\n",
      "Iteration 132, loss = 0.01953090\n",
      "Iteration 133, loss = 0.01941817\n",
      "Iteration 134, loss = 0.01930613\n",
      "Iteration 135, loss = 0.01919465\n",
      "Iteration 136, loss = 0.01908394\n",
      "Iteration 137, loss = 0.01897424\n",
      "Iteration 138, loss = 0.01886570\n",
      "Iteration 139, loss = 0.01875835\n",
      "Iteration 140, loss = 0.01865208\n",
      "Iteration 141, loss = 0.01854678\n",
      "Iteration 142, loss = 0.01844237\n",
      "Iteration 143, loss = 0.01833880\n",
      "Iteration 144, loss = 0.01823606\n",
      "Iteration 145, loss = 0.01813418\n",
      "Iteration 146, loss = 0.01803317\n",
      "Iteration 147, loss = 0.01793309\n",
      "Iteration 148, loss = 0.01783400\n",
      "Iteration 149, loss = 0.01773611\n",
      "Iteration 150, loss = 0.01763961\n",
      "Iteration 151, loss = 0.01754521\n",
      "Iteration 152, loss = 0.01745385\n",
      "Iteration 153, loss = 0.01736841\n",
      "Iteration 154, loss = 0.01729304\n",
      "Iteration 155, loss = 0.01724184\n",
      "Iteration 156, loss = 0.01723323\n",
      "Iteration 157, loss = 0.01734623\n",
      "Iteration 158, loss = 0.01764782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15205734\n",
      "Iteration 2, loss = 1.14419193\n",
      "Iteration 3, loss = 1.27699650\n",
      "Iteration 4, loss = 1.19173933\n",
      "Iteration 5, loss = 1.05076625\n",
      "Iteration 6, loss = 0.98149315\n",
      "Iteration 7, loss = 0.96383159\n",
      "Iteration 8, loss = 0.90517196\n",
      "Iteration 9, loss = 0.84066355\n",
      "Iteration 10, loss = 0.80773611\n",
      "Iteration 11, loss = 0.77293340\n",
      "Iteration 12, loss = 0.69128866\n",
      "Iteration 13, loss = 0.59959020\n",
      "Iteration 14, loss = 0.56374840\n",
      "Iteration 15, loss = 0.49930600\n",
      "Iteration 16, loss = 0.44582922\n",
      "Iteration 17, loss = 0.42522843\n",
      "Iteration 18, loss = 0.41585981\n",
      "Iteration 19, loss = 0.40742047\n",
      "Iteration 20, loss = 0.39962644\n",
      "Iteration 21, loss = 0.39469270\n",
      "Iteration 22, loss = 0.39273622\n",
      "Iteration 23, loss = 0.38875212\n",
      "Iteration 24, loss = 0.37783610\n",
      "Iteration 25, loss = 0.36721317\n",
      "Iteration 26, loss = 0.35482202\n",
      "Iteration 27, loss = 0.33987118\n",
      "Iteration 28, loss = 0.32245067\n",
      "Iteration 29, loss = 0.30175823\n",
      "Iteration 30, loss = 0.27895496\n",
      "Iteration 31, loss = 0.25422458\n",
      "Iteration 32, loss = 0.22875253\n",
      "Iteration 33, loss = 0.20405434\n",
      "Iteration 34, loss = 0.18271097\n",
      "Iteration 35, loss = 0.17819746\n",
      "Iteration 36, loss = 0.15969609\n",
      "Iteration 37, loss = 0.13465518\n",
      "Iteration 38, loss = 0.13361182\n",
      "Iteration 39, loss = 0.12269918\n",
      "Iteration 40, loss = 0.10934806\n",
      "Iteration 41, loss = 0.11054156\n",
      "Iteration 42, loss = 0.09956585\n",
      "Iteration 43, loss = 0.09594302\n",
      "Iteration 44, loss = 0.09528399\n",
      "Iteration 45, loss = 0.08661667\n",
      "Iteration 46, loss = 0.08842565\n",
      "Iteration 47, loss = 0.08448564\n",
      "Iteration 48, loss = 0.08014935\n",
      "Iteration 49, loss = 0.08238949\n",
      "Iteration 50, loss = 0.07699392\n",
      "Iteration 51, loss = 0.07652065\n",
      "Iteration 52, loss = 0.07651087\n",
      "Iteration 53, loss = 0.07247894\n",
      "Iteration 54, loss = 0.07356390\n",
      "Iteration 55, loss = 0.07182350\n",
      "Iteration 56, loss = 0.07006480\n",
      "Iteration 57, loss = 0.07090788\n",
      "Iteration 58, loss = 0.06883963\n",
      "Iteration 59, loss = 0.06852513\n",
      "Iteration 60, loss = 0.06874620\n",
      "Iteration 61, loss = 0.06698641\n",
      "Iteration 62, loss = 0.06723270\n",
      "Iteration 63, loss = 0.06710032\n",
      "Iteration 64, loss = 0.06567973\n",
      "Iteration 65, loss = 0.06605166\n",
      "Iteration 66, loss = 0.06585769\n",
      "Iteration 67, loss = 0.06465069\n",
      "Iteration 68, loss = 0.06497028\n",
      "Iteration 69, loss = 0.06488566\n",
      "Iteration 70, loss = 0.06381203\n",
      "Iteration 71, loss = 0.06397783\n",
      "Iteration 72, loss = 0.06406273\n",
      "Iteration 73, loss = 0.06313673\n",
      "Iteration 74, loss = 0.06306858\n",
      "Iteration 75, loss = 0.06327986\n",
      "Iteration 76, loss = 0.06259718\n",
      "Iteration 77, loss = 0.06227604\n",
      "Iteration 78, loss = 0.06247114\n",
      "Iteration 79, loss = 0.06213020\n",
      "Iteration 80, loss = 0.06167229\n",
      "Iteration 81, loss = 0.06167135\n",
      "Iteration 82, loss = 0.06163119\n",
      "Iteration 83, loss = 0.06127687\n",
      "Iteration 84, loss = 0.06102229\n",
      "Iteration 85, loss = 0.06103131\n",
      "Iteration 86, loss = 0.06092761\n",
      "Iteration 87, loss = 0.06061470\n",
      "Iteration 88, loss = 0.06043973\n",
      "Iteration 89, loss = 0.06042030\n",
      "Iteration 90, loss = 0.06029171\n",
      "Iteration 91, loss = 0.06005460\n",
      "Iteration 92, loss = 0.05988750\n",
      "Iteration 93, loss = 0.05982955\n",
      "Iteration 94, loss = 0.05974317\n",
      "Iteration 95, loss = 0.05955929\n",
      "Iteration 96, loss = 0.05938345\n",
      "Iteration 97, loss = 0.05928385\n",
      "Iteration 98, loss = 0.05921182\n",
      "Iteration 99, loss = 0.05909864\n",
      "Iteration 100, loss = 0.05894094\n",
      "Iteration 101, loss = 0.05879974\n",
      "Iteration 102, loss = 0.05870154\n",
      "Iteration 103, loss = 0.05862064\n",
      "Iteration 104, loss = 0.05852251\n",
      "Iteration 105, loss = 0.05839667\n",
      "Iteration 106, loss = 0.05826746\n",
      "Iteration 107, loss = 0.05815506\n",
      "Iteration 108, loss = 0.05806269\n",
      "Iteration 109, loss = 0.05797813\n",
      "Iteration 110, loss = 0.05788681\n",
      "Iteration 111, loss = 0.05778568\n",
      "Iteration 112, loss = 0.05767689\n",
      "Iteration 113, loss = 0.05756934\n",
      "Iteration 114, loss = 0.05746780\n",
      "Iteration 115, loss = 0.05737395\n",
      "Iteration 116, loss = 0.05728628\n",
      "Iteration 117, loss = 0.05720214\n",
      "Iteration 118, loss = 0.05711981\n",
      "Iteration 119, loss = 0.05703762\n",
      "Iteration 120, loss = 0.05695681\n",
      "Iteration 121, loss = 0.05687659\n",
      "Iteration 122, loss = 0.05680079\n",
      "Iteration 123, loss = 0.05672882\n",
      "Iteration 124, loss = 0.05666903\n",
      "Iteration 125, loss = 0.05662222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13218766\n",
      "Iteration 2, loss = 1.28482475\n",
      "Iteration 3, loss = 1.19946847\n",
      "Iteration 4, loss = 1.18332591\n",
      "Iteration 5, loss = 1.16493729\n",
      "Iteration 6, loss = 1.14670535\n",
      "Iteration 7, loss = 1.13038506\n",
      "Iteration 8, loss = 1.11727792\n",
      "Iteration 9, loss = 1.10792222\n",
      "Iteration 10, loss = 1.10227983\n",
      "Iteration 11, loss = 1.09985889\n",
      "Iteration 12, loss = 1.09988604\n",
      "Iteration 13, loss = 1.10148410\n",
      "Iteration 14, loss = 1.10382066\n",
      "Iteration 15, loss = 1.10620981\n",
      "Iteration 16, loss = 1.10816363\n",
      "Iteration 17, loss = 1.10940094\n",
      "Iteration 18, loss = 1.10982508\n",
      "Iteration 19, loss = 1.10948367\n",
      "Iteration 20, loss = 1.10852121\n",
      "Iteration 21, loss = 1.10713310\n",
      "Iteration 22, loss = 1.10552643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14020736\n",
      "Iteration 2, loss = 1.29833436\n",
      "Iteration 3, loss = 1.19792533\n",
      "Iteration 4, loss = 1.18016631\n",
      "Iteration 5, loss = 1.15488782\n",
      "Iteration 6, loss = 1.11759895\n",
      "Iteration 7, loss = 1.08111808\n",
      "Iteration 8, loss = 1.03751140\n",
      "Iteration 9, loss = 0.99437874\n",
      "Iteration 10, loss = 0.95363847\n",
      "Iteration 11, loss = 0.91467278\n",
      "Iteration 12, loss = 0.87905572\n",
      "Iteration 13, loss = 0.84399240\n",
      "Iteration 14, loss = 0.80933383\n",
      "Iteration 15, loss = 0.77617639\n",
      "Iteration 16, loss = 0.74614370\n",
      "Iteration 17, loss = 0.71988314\n",
      "Iteration 18, loss = 0.69628636\n",
      "Iteration 19, loss = 0.67163491\n",
      "Iteration 20, loss = 0.64461134\n",
      "Iteration 21, loss = 0.61741459\n",
      "Iteration 22, loss = 0.59060784\n",
      "Iteration 23, loss = 0.56473573\n",
      "Iteration 24, loss = 0.54141738\n",
      "Iteration 25, loss = 0.51966294\n",
      "Iteration 26, loss = 0.49965394\n",
      "Iteration 27, loss = 0.48155365\n",
      "Iteration 28, loss = 0.46546035\n",
      "Iteration 29, loss = 0.45131186\n",
      "Iteration 30, loss = 0.43885168\n",
      "Iteration 31, loss = 0.42768640\n",
      "Iteration 32, loss = 0.41738443\n",
      "Iteration 33, loss = 0.40762033\n",
      "Iteration 34, loss = 0.39823293\n",
      "Iteration 35, loss = 0.38919548\n",
      "Iteration 36, loss = 0.38054973\n",
      "Iteration 37, loss = 0.37233917\n",
      "Iteration 38, loss = 0.36456889\n",
      "Iteration 39, loss = 0.35720003\n",
      "Iteration 40, loss = 0.35016771\n",
      "Iteration 41, loss = 0.34340483\n",
      "Iteration 42, loss = 0.33685922\n",
      "Iteration 43, loss = 0.33050036\n",
      "Iteration 44, loss = 0.32431741\n",
      "Iteration 45, loss = 0.31831218\n",
      "Iteration 46, loss = 0.31249106\n",
      "Iteration 47, loss = 0.30685863\n",
      "Iteration 48, loss = 0.30141452\n",
      "Iteration 49, loss = 0.29615353\n",
      "Iteration 50, loss = 0.29106763\n",
      "Iteration 51, loss = 0.28614856\n",
      "Iteration 52, loss = 0.28138960\n",
      "Iteration 53, loss = 0.27678629\n",
      "Iteration 54, loss = 0.27233603\n",
      "Iteration 55, loss = 0.26803722\n",
      "Iteration 56, loss = 0.26388826\n",
      "Iteration 57, loss = 0.25988695\n",
      "Iteration 58, loss = 0.25603011\n",
      "Iteration 59, loss = 0.25231372\n",
      "Iteration 60, loss = 0.24873308\n",
      "Iteration 61, loss = 0.24528317\n",
      "Iteration 62, loss = 0.24195882\n",
      "Iteration 63, loss = 0.23875494\n",
      "Iteration 64, loss = 0.23566650\n",
      "Iteration 65, loss = 0.23268860\n",
      "Iteration 66, loss = 0.22981638\n",
      "Iteration 67, loss = 0.22704503\n",
      "Iteration 68, loss = 0.22436981\n",
      "Iteration 69, loss = 0.22178602\n",
      "Iteration 70, loss = 0.21928906\n",
      "Iteration 71, loss = 0.21687446\n",
      "Iteration 72, loss = 0.21453796\n",
      "Iteration 73, loss = 0.21227546\n",
      "Iteration 74, loss = 0.21008312\n",
      "Iteration 75, loss = 0.20795729\n",
      "Iteration 76, loss = 0.20589455\n",
      "Iteration 77, loss = 0.20389169\n",
      "Iteration 78, loss = 0.20194568\n",
      "Iteration 79, loss = 0.20005369\n",
      "Iteration 80, loss = 0.19821306\n",
      "Iteration 81, loss = 0.19642129\n",
      "Iteration 82, loss = 0.19467605\n",
      "Iteration 83, loss = 0.19297515\n",
      "Iteration 84, loss = 0.19131658\n",
      "Iteration 85, loss = 0.18969843\n",
      "Iteration 86, loss = 0.18811895\n",
      "Iteration 87, loss = 0.18657649\n",
      "Iteration 88, loss = 0.18506953\n",
      "Iteration 89, loss = 0.18359663\n",
      "Iteration 90, loss = 0.18215647\n",
      "Iteration 91, loss = 0.18074778\n",
      "Iteration 92, loss = 0.17936941\n",
      "Iteration 93, loss = 0.17802025\n",
      "Iteration 94, loss = 0.17669927\n",
      "Iteration 95, loss = 0.17540550\n",
      "Iteration 96, loss = 0.17413802\n",
      "Iteration 97, loss = 0.17289596\n",
      "Iteration 98, loss = 0.17167850\n",
      "Iteration 99, loss = 0.17048486\n",
      "Iteration 100, loss = 0.16931430\n",
      "Iteration 101, loss = 0.16816611\n",
      "Iteration 102, loss = 0.16703964\n",
      "Iteration 103, loss = 0.16593423\n",
      "Iteration 104, loss = 0.16484927\n",
      "Iteration 105, loss = 0.16378417\n",
      "Iteration 106, loss = 0.16273838\n",
      "Iteration 107, loss = 0.16171136\n",
      "Iteration 108, loss = 0.16070258\n",
      "Iteration 109, loss = 0.15971156\n",
      "Iteration 110, loss = 0.15873781\n",
      "Iteration 111, loss = 0.15778087\n",
      "Iteration 112, loss = 0.15684031\n",
      "Iteration 113, loss = 0.15591569\n",
      "Iteration 114, loss = 0.15500660\n",
      "Iteration 115, loss = 0.15411265\n",
      "Iteration 116, loss = 0.15323346\n",
      "Iteration 117, loss = 0.15236865\n",
      "Iteration 118, loss = 0.15151787\n",
      "Iteration 119, loss = 0.15068077\n",
      "Iteration 120, loss = 0.14985703\n",
      "Iteration 121, loss = 0.14904632\n",
      "Iteration 122, loss = 0.14824832\n",
      "Iteration 123, loss = 0.14746275\n",
      "Iteration 124, loss = 0.14668930\n",
      "Iteration 125, loss = 0.14592770\n",
      "Iteration 126, loss = 0.14517767\n",
      "Iteration 127, loss = 0.14443895\n",
      "Iteration 128, loss = 0.14371128\n",
      "Iteration 129, loss = 0.14299442\n",
      "Iteration 130, loss = 0.14228813\n",
      "Iteration 131, loss = 0.14159216\n",
      "Iteration 132, loss = 0.14090630\n",
      "Iteration 133, loss = 0.14023033\n",
      "Iteration 134, loss = 0.13956404\n",
      "Iteration 135, loss = 0.13890721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 136, loss = 0.13825966\n",
      "Iteration 137, loss = 0.13762117\n",
      "Iteration 138, loss = 0.13699158\n",
      "Iteration 139, loss = 0.13637068\n",
      "Iteration 140, loss = 0.13575832\n",
      "Iteration 141, loss = 0.13515429\n",
      "Iteration 142, loss = 0.13455846\n",
      "Iteration 143, loss = 0.13397064\n",
      "Iteration 144, loss = 0.13339069\n",
      "Iteration 145, loss = 0.13281843\n",
      "Iteration 146, loss = 0.13225374\n",
      "Iteration 147, loss = 0.13169644\n",
      "Iteration 148, loss = 0.13114642\n",
      "Iteration 149, loss = 0.13060351\n",
      "Iteration 150, loss = 0.13006761\n",
      "Iteration 151, loss = 0.12953855\n",
      "Iteration 152, loss = 0.12901623\n",
      "Iteration 153, loss = 0.12850049\n",
      "Iteration 154, loss = 0.12799125\n",
      "Iteration 155, loss = 0.12748836\n",
      "Iteration 156, loss = 0.12699173\n",
      "Iteration 157, loss = 0.12650121\n",
      "Iteration 158, loss = 0.12601672\n",
      "Iteration 159, loss = 0.12553813\n",
      "Iteration 160, loss = 0.12506536\n",
      "Iteration 161, loss = 0.12459827\n",
      "Iteration 162, loss = 0.12413680\n",
      "Iteration 163, loss = 0.12368082\n",
      "Iteration 164, loss = 0.12323026\n",
      "Iteration 165, loss = 0.12278499\n",
      "Iteration 166, loss = 0.12234496\n",
      "Iteration 167, loss = 0.12191005\n",
      "Iteration 168, loss = 0.12148019\n",
      "Iteration 169, loss = 0.12105528\n",
      "Iteration 170, loss = 0.12063526\n",
      "Iteration 171, loss = 0.12022001\n",
      "Iteration 172, loss = 0.11980950\n",
      "Iteration 173, loss = 0.11940360\n",
      "Iteration 174, loss = 0.11900228\n",
      "Iteration 175, loss = 0.11860543\n",
      "Iteration 176, loss = 0.11821301\n",
      "Iteration 177, loss = 0.11782491\n",
      "Iteration 178, loss = 0.11744110\n",
      "Iteration 179, loss = 0.11706148\n",
      "Iteration 180, loss = 0.11668601\n",
      "Iteration 181, loss = 0.11631460\n",
      "Iteration 182, loss = 0.11594721\n",
      "Iteration 183, loss = 0.11558375\n",
      "Iteration 184, loss = 0.11522419\n",
      "Iteration 185, loss = 0.11486844\n",
      "Iteration 186, loss = 0.11451647\n",
      "Iteration 187, loss = 0.11416820\n",
      "Iteration 188, loss = 0.11382358\n",
      "Iteration 189, loss = 0.11348256\n",
      "Iteration 190, loss = 0.11314508\n",
      "Iteration 191, loss = 0.11281109\n",
      "Iteration 192, loss = 0.11248054\n",
      "Iteration 193, loss = 0.11215337\n",
      "Iteration 194, loss = 0.11182954\n",
      "Iteration 195, loss = 0.11150899\n",
      "Iteration 196, loss = 0.11119168\n",
      "Iteration 197, loss = 0.11087755\n",
      "Iteration 198, loss = 0.11056658\n",
      "Iteration 199, loss = 0.11025870\n",
      "Iteration 200, loss = 0.10995387\n",
      "Iteration 201, loss = 0.10965205\n",
      "Iteration 202, loss = 0.10935320\n",
      "Iteration 203, loss = 0.10905726\n",
      "Iteration 204, loss = 0.10876422\n",
      "Iteration 205, loss = 0.10847401\n",
      "Iteration 206, loss = 0.10818660\n",
      "Iteration 207, loss = 0.10790195\n",
      "Iteration 208, loss = 0.10762003\n",
      "Iteration 209, loss = 0.10734079\n",
      "Iteration 210, loss = 0.10706420\n",
      "Iteration 211, loss = 0.10679021\n",
      "Iteration 212, loss = 0.10651881\n",
      "Iteration 213, loss = 0.10624994\n",
      "Iteration 214, loss = 0.10598358\n",
      "Iteration 215, loss = 0.10571969\n",
      "Iteration 216, loss = 0.10545824\n",
      "Iteration 217, loss = 0.10519919\n",
      "Iteration 218, loss = 0.10494252\n",
      "Iteration 219, loss = 0.10468819\n",
      "Iteration 220, loss = 0.10443617\n",
      "Iteration 221, loss = 0.10418642\n",
      "Iteration 222, loss = 0.10393893\n",
      "Iteration 223, loss = 0.10369366\n",
      "Iteration 224, loss = 0.10345058\n",
      "Iteration 225, loss = 0.10320966\n",
      "Iteration 226, loss = 0.10297088\n",
      "Iteration 227, loss = 0.10273420\n",
      "Iteration 228, loss = 0.10249961\n",
      "Iteration 229, loss = 0.10226706\n",
      "Iteration 230, loss = 0.10203655\n",
      "Iteration 231, loss = 0.10180803\n",
      "Iteration 232, loss = 0.10158150\n",
      "Iteration 233, loss = 0.10135691\n",
      "Iteration 234, loss = 0.10113425\n",
      "Iteration 235, loss = 0.10091349\n",
      "Iteration 236, loss = 0.10069462\n",
      "Iteration 237, loss = 0.10047759\n",
      "Iteration 238, loss = 0.10026241\n",
      "Iteration 239, loss = 0.10004903\n",
      "Iteration 240, loss = 0.09983744\n",
      "Iteration 241, loss = 0.09962761\n",
      "Iteration 242, loss = 0.09941953\n",
      "Iteration 243, loss = 0.09921318\n",
      "Iteration 244, loss = 0.09900852\n",
      "Iteration 245, loss = 0.09880555\n",
      "Iteration 246, loss = 0.09860424\n",
      "Iteration 247, loss = 0.09840457\n",
      "Iteration 248, loss = 0.09820653\n",
      "Iteration 249, loss = 0.09801009\n",
      "Iteration 250, loss = 0.09781523\n",
      "Iteration 251, loss = 0.09762194\n",
      "Iteration 252, loss = 0.09743019\n",
      "Iteration 253, loss = 0.09723998\n",
      "Iteration 254, loss = 0.09705127\n",
      "Iteration 255, loss = 0.09686406\n",
      "Iteration 256, loss = 0.09667833\n",
      "Iteration 257, loss = 0.09649405\n",
      "Iteration 258, loss = 0.09631122\n",
      "Iteration 259, loss = 0.09612982\n",
      "Iteration 260, loss = 0.09594982\n",
      "Iteration 261, loss = 0.09577122\n",
      "Iteration 262, loss = 0.09559399\n",
      "Iteration 263, loss = 0.09541813\n",
      "Iteration 264, loss = 0.09524362\n",
      "Iteration 265, loss = 0.09507044\n",
      "Iteration 266, loss = 0.09489857\n",
      "Iteration 267, loss = 0.09472801\n",
      "Iteration 268, loss = 0.09455873\n",
      "Iteration 269, loss = 0.09439073\n",
      "Iteration 270, loss = 0.09422399\n",
      "Iteration 271, loss = 0.09405849\n",
      "Iteration 272, loss = 0.09389423\n",
      "Iteration 273, loss = 0.09373118\n",
      "Iteration 274, loss = 0.09356934\n",
      "Iteration 275, loss = 0.09340869\n",
      "Iteration 276, loss = 0.09324923\n",
      "Iteration 277, loss = 0.09309092\n",
      "Iteration 278, loss = 0.09293378\n",
      "Iteration 279, loss = 0.09277777\n",
      "Iteration 280, loss = 0.09262289\n",
      "Iteration 281, loss = 0.09246914\n",
      "Iteration 282, loss = 0.09231648\n",
      "Iteration 283, loss = 0.09216492\n",
      "Iteration 284, loss = 0.09201445\n",
      "Iteration 285, loss = 0.09186504\n",
      "Iteration 286, loss = 0.09171670\n",
      "Iteration 287, loss = 0.09156940\n",
      "Iteration 288, loss = 0.09142314\n",
      "Iteration 289, loss = 0.09127791\n",
      "Iteration 290, loss = 0.09113370\n",
      "Iteration 291, loss = 0.09099049\n",
      "Iteration 292, loss = 0.09084828\n",
      "Iteration 293, loss = 0.09070705\n",
      "Iteration 294, loss = 0.09056680\n",
      "Iteration 295, loss = 0.09042751\n",
      "Iteration 296, loss = 0.09028918\n",
      "Iteration 297, loss = 0.09015180\n",
      "Iteration 298, loss = 0.09001535\n",
      "Iteration 299, loss = 0.08987983\n",
      "Iteration 300, loss = 0.08974522\n",
      "Iteration 301, loss = 0.08961153\n",
      "Iteration 302, loss = 0.08947874\n",
      "Iteration 303, loss = 0.08934683\n",
      "Iteration 304, loss = 0.08921581\n",
      "Iteration 305, loss = 0.08908566\n",
      "Iteration 306, loss = 0.08895638\n",
      "Iteration 307, loss = 0.08882795\n",
      "Iteration 308, loss = 0.08870037\n",
      "Iteration 309, loss = 0.08857362\n",
      "Iteration 310, loss = 0.08844771\n",
      "Iteration 311, loss = 0.08832263\n",
      "Iteration 312, loss = 0.08819836\n",
      "Iteration 313, loss = 0.08807489\n",
      "Iteration 314, loss = 0.08795223\n",
      "Iteration 315, loss = 0.08783035\n",
      "Iteration 316, loss = 0.08770927\n",
      "Iteration 317, loss = 0.08758895\n",
      "Iteration 318, loss = 0.08746941\n",
      "Iteration 319, loss = 0.08735063\n",
      "Iteration 320, loss = 0.08723261\n",
      "Iteration 321, loss = 0.08711533\n",
      "Iteration 322, loss = 0.08699879\n",
      "Iteration 323, loss = 0.08688299\n",
      "Iteration 324, loss = 0.08676792\n",
      "Iteration 325, loss = 0.08665356\n",
      "Iteration 326, loss = 0.08653992\n",
      "Iteration 327, loss = 0.08642699\n",
      "Iteration 328, loss = 0.08631475\n",
      "Iteration 329, loss = 0.08620322\n",
      "Iteration 330, loss = 0.08609236\n",
      "Iteration 331, loss = 0.08598219\n",
      "Iteration 332, loss = 0.08587270\n",
      "Iteration 333, loss = 0.08576387\n",
      "Iteration 334, loss = 0.08565571\n",
      "Iteration 335, loss = 0.08554820\n",
      "Iteration 336, loss = 0.08544135\n",
      "Iteration 337, loss = 0.08533514\n",
      "Iteration 338, loss = 0.08522957\n",
      "Iteration 339, loss = 0.08512463\n",
      "Iteration 340, loss = 0.08502032\n",
      "Iteration 341, loss = 0.08491663\n",
      "Iteration 342, loss = 0.08481356\n",
      "Iteration 343, loss = 0.08471111\n",
      "Iteration 344, loss = 0.08460925\n",
      "Iteration 345, loss = 0.08450800\n",
      "Iteration 346, loss = 0.08440734\n",
      "Iteration 347, loss = 0.08430728\n",
      "Iteration 348, loss = 0.08420780\n",
      "Iteration 349, loss = 0.08410890\n",
      "Iteration 350, loss = 0.08401057\n",
      "Iteration 351, loss = 0.08391281\n",
      "Iteration 352, loss = 0.08381562\n",
      "Iteration 353, loss = 0.08371899\n",
      "Iteration 354, loss = 0.08362291\n",
      "Iteration 355, loss = 0.08352739\n",
      "Iteration 356, loss = 0.08343241\n",
      "Iteration 357, loss = 0.08333797\n",
      "Iteration 358, loss = 0.08324406\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13352491\n",
      "Iteration 2, loss = 1.29658765\n",
      "Iteration 3, loss = 1.19695420\n",
      "Iteration 4, loss = 1.17783052\n",
      "Iteration 5, loss = 1.14342566\n",
      "Iteration 6, loss = 1.11136356\n",
      "Iteration 7, loss = 1.06909833\n",
      "Iteration 8, loss = 1.01997092\n",
      "Iteration 9, loss = 0.97511819\n",
      "Iteration 10, loss = 0.93159095\n",
      "Iteration 11, loss = 0.89282062\n",
      "Iteration 12, loss = 0.85742195\n",
      "Iteration 13, loss = 0.82294899\n",
      "Iteration 14, loss = 0.78938232\n",
      "Iteration 15, loss = 0.75751800\n",
      "Iteration 16, loss = 0.72837930\n",
      "Iteration 17, loss = 0.70205384\n",
      "Iteration 18, loss = 0.67693190\n",
      "Iteration 19, loss = 0.65110572\n",
      "Iteration 20, loss = 0.62477443\n",
      "Iteration 21, loss = 0.59915021\n",
      "Iteration 22, loss = 0.57436311\n",
      "Iteration 23, loss = 0.55046415\n",
      "Iteration 24, loss = 0.52772488\n",
      "Iteration 25, loss = 0.50623438\n",
      "Iteration 26, loss = 0.48598593\n",
      "Iteration 27, loss = 0.46699704\n",
      "Iteration 28, loss = 0.44941582\n",
      "Iteration 29, loss = 0.43355369\n",
      "Iteration 30, loss = 0.41965760\n",
      "Iteration 31, loss = 0.40761884\n",
      "Iteration 32, loss = 0.39705216\n",
      "Iteration 33, loss = 0.38739107\n",
      "Iteration 34, loss = 0.37811166\n",
      "Iteration 35, loss = 0.36893791\n",
      "Iteration 36, loss = 0.35982925\n",
      "Iteration 37, loss = 0.35089438\n",
      "Iteration 38, loss = 0.34228044\n",
      "Iteration 39, loss = 0.33408695\n",
      "Iteration 40, loss = 0.32633202\n",
      "Iteration 41, loss = 0.31896706\n",
      "Iteration 42, loss = 0.31191466\n",
      "Iteration 43, loss = 0.30510354\n",
      "Iteration 44, loss = 0.29848787\n",
      "Iteration 45, loss = 0.29205067\n",
      "Iteration 46, loss = 0.28579670\n",
      "Iteration 47, loss = 0.27974108\n",
      "Iteration 48, loss = 0.27389875\n",
      "Iteration 49, loss = 0.26827774\n",
      "Iteration 50, loss = 0.26287725\n",
      "Iteration 51, loss = 0.25768942\n",
      "Iteration 52, loss = 0.25270293\n",
      "Iteration 53, loss = 0.24790650\n",
      "Iteration 54, loss = 0.24329103\n",
      "Iteration 55, loss = 0.23885013\n",
      "Iteration 56, loss = 0.23457957\n",
      "Iteration 57, loss = 0.23047602\n",
      "Iteration 58, loss = 0.22653600\n",
      "Iteration 59, loss = 0.22275518\n",
      "Iteration 60, loss = 0.21912811\n",
      "Iteration 61, loss = 0.21564840\n",
      "Iteration 62, loss = 0.21230911\n",
      "Iteration 63, loss = 0.20910309\n",
      "Iteration 64, loss = 0.20602331\n",
      "Iteration 65, loss = 0.20306305\n",
      "Iteration 66, loss = 0.20021595\n",
      "Iteration 67, loss = 0.19747599\n",
      "Iteration 68, loss = 0.19483747\n",
      "Iteration 69, loss = 0.19229493\n",
      "Iteration 70, loss = 0.18984311\n",
      "Iteration 71, loss = 0.18747697\n",
      "Iteration 72, loss = 0.18519167\n",
      "Iteration 73, loss = 0.18298263\n",
      "Iteration 74, loss = 0.18084553\n",
      "Iteration 75, loss = 0.17877633\n",
      "Iteration 76, loss = 0.17677127\n",
      "Iteration 77, loss = 0.17482688\n",
      "Iteration 78, loss = 0.17293995\n",
      "Iteration 79, loss = 0.17110750\n",
      "Iteration 80, loss = 0.16932679\n",
      "Iteration 81, loss = 0.16759527\n",
      "Iteration 82, loss = 0.16591056\n",
      "Iteration 83, loss = 0.16427046\n",
      "Iteration 84, loss = 0.16267291\n",
      "Iteration 85, loss = 0.16111599\n",
      "Iteration 86, loss = 0.15959792\n",
      "Iteration 87, loss = 0.15811704\n",
      "Iteration 88, loss = 0.15667179\n",
      "Iteration 89, loss = 0.15526073\n",
      "Iteration 90, loss = 0.15388250\n",
      "Iteration 91, loss = 0.15253583\n",
      "Iteration 92, loss = 0.15121954\n",
      "Iteration 93, loss = 0.14993250\n",
      "Iteration 94, loss = 0.14867366\n",
      "Iteration 95, loss = 0.14744203\n",
      "Iteration 96, loss = 0.14623664\n",
      "Iteration 97, loss = 0.14505663\n",
      "Iteration 98, loss = 0.14390112\n",
      "Iteration 99, loss = 0.14276931\n",
      "Iteration 100, loss = 0.14166044\n",
      "Iteration 101, loss = 0.14057377\n",
      "Iteration 102, loss = 0.13950859\n",
      "Iteration 103, loss = 0.13846426\n",
      "Iteration 104, loss = 0.13744011\n",
      "Iteration 105, loss = 0.13643555\n",
      "Iteration 106, loss = 0.13544998\n",
      "Iteration 107, loss = 0.13448285\n",
      "Iteration 108, loss = 0.13353362\n",
      "Iteration 109, loss = 0.13260177\n",
      "Iteration 110, loss = 0.13168680\n",
      "Iteration 111, loss = 0.13078824\n",
      "Iteration 112, loss = 0.12990562\n",
      "Iteration 113, loss = 0.12903851\n",
      "Iteration 114, loss = 0.12818648\n",
      "Iteration 115, loss = 0.12734913\n",
      "Iteration 116, loss = 0.12652606\n",
      "Iteration 117, loss = 0.12571689\n",
      "Iteration 118, loss = 0.12492125\n",
      "Iteration 119, loss = 0.12413881\n",
      "Iteration 120, loss = 0.12336921\n",
      "Iteration 121, loss = 0.12261213\n",
      "Iteration 122, loss = 0.12186726\n",
      "Iteration 123, loss = 0.12113429\n",
      "Iteration 124, loss = 0.12041293\n",
      "Iteration 125, loss = 0.11970289\n",
      "Iteration 126, loss = 0.11900390\n",
      "Iteration 127, loss = 0.11831571\n",
      "Iteration 128, loss = 0.11763804\n",
      "Iteration 129, loss = 0.11697066\n",
      "Iteration 130, loss = 0.11631332\n",
      "Iteration 131, loss = 0.11566580\n",
      "Iteration 132, loss = 0.11502787\n",
      "Iteration 133, loss = 0.11439932\n",
      "Iteration 134, loss = 0.11377993\n",
      "Iteration 135, loss = 0.11316950\n",
      "Iteration 136, loss = 0.11256784\n",
      "Iteration 137, loss = 0.11197476\n",
      "Iteration 138, loss = 0.11139006\n",
      "Iteration 139, loss = 0.11081358\n",
      "Iteration 140, loss = 0.11024513\n",
      "Iteration 141, loss = 0.10968456\n",
      "Iteration 142, loss = 0.10913169\n",
      "Iteration 143, loss = 0.10858636\n",
      "Iteration 144, loss = 0.10804843\n",
      "Iteration 145, loss = 0.10751774\n",
      "Iteration 146, loss = 0.10699414\n",
      "Iteration 147, loss = 0.10647750\n",
      "Iteration 148, loss = 0.10596768\n",
      "Iteration 149, loss = 0.10546454\n",
      "Iteration 150, loss = 0.10496795\n",
      "Iteration 151, loss = 0.10447779\n",
      "Iteration 152, loss = 0.10399393\n",
      "Iteration 153, loss = 0.10351626\n",
      "Iteration 154, loss = 0.10304465\n",
      "Iteration 155, loss = 0.10257899\n",
      "Iteration 156, loss = 0.10211918\n",
      "Iteration 157, loss = 0.10166509\n",
      "Iteration 158, loss = 0.10121664\n",
      "Iteration 159, loss = 0.10077370\n",
      "Iteration 160, loss = 0.10033620\n",
      "Iteration 161, loss = 0.09990402\n",
      "Iteration 162, loss = 0.09947707\n",
      "Iteration 163, loss = 0.09905526\n",
      "Iteration 164, loss = 0.09863850\n",
      "Iteration 165, loss = 0.09822670\n",
      "Iteration 166, loss = 0.09781977\n",
      "Iteration 167, loss = 0.09741763\n",
      "Iteration 168, loss = 0.09702020\n",
      "Iteration 169, loss = 0.09662739\n",
      "Iteration 170, loss = 0.09623913\n",
      "Iteration 171, loss = 0.09585534\n",
      "Iteration 172, loss = 0.09547595\n",
      "Iteration 173, loss = 0.09510088\n",
      "Iteration 174, loss = 0.09473006\n",
      "Iteration 175, loss = 0.09436342\n",
      "Iteration 176, loss = 0.09400089\n",
      "Iteration 177, loss = 0.09364241\n",
      "Iteration 178, loss = 0.09328791\n",
      "Iteration 179, loss = 0.09293732\n",
      "Iteration 180, loss = 0.09259059\n",
      "Iteration 181, loss = 0.09224765\n",
      "Iteration 182, loss = 0.09190844\n",
      "Iteration 183, loss = 0.09157290\n",
      "Iteration 184, loss = 0.09124097\n",
      "Iteration 185, loss = 0.09091261\n",
      "Iteration 186, loss = 0.09058775\n",
      "Iteration 187, loss = 0.09026634\n",
      "Iteration 188, loss = 0.08994832\n",
      "Iteration 189, loss = 0.08963365\n",
      "Iteration 190, loss = 0.08932227\n",
      "Iteration 191, loss = 0.08901414\n",
      "Iteration 192, loss = 0.08870920\n",
      "Iteration 193, loss = 0.08840741\n",
      "Iteration 194, loss = 0.08810872\n",
      "Iteration 195, loss = 0.08781309\n",
      "Iteration 196, loss = 0.08752046\n",
      "Iteration 197, loss = 0.08723080\n",
      "Iteration 198, loss = 0.08694406\n",
      "Iteration 199, loss = 0.08666021\n",
      "Iteration 200, loss = 0.08637919\n",
      "Iteration 201, loss = 0.08610096\n",
      "Iteration 202, loss = 0.08582550\n",
      "Iteration 203, loss = 0.08555275\n",
      "Iteration 204, loss = 0.08528269\n",
      "Iteration 205, loss = 0.08501526\n",
      "Iteration 206, loss = 0.08475044\n",
      "Iteration 207, loss = 0.08448819\n",
      "Iteration 208, loss = 0.08422847\n",
      "Iteration 209, loss = 0.08397124\n",
      "Iteration 210, loss = 0.08371648\n",
      "Iteration 211, loss = 0.08346415\n",
      "Iteration 212, loss = 0.08321421\n",
      "Iteration 213, loss = 0.08296664\n",
      "Iteration 214, loss = 0.08272140\n",
      "Iteration 215, loss = 0.08247846\n",
      "Iteration 216, loss = 0.08223778\n",
      "Iteration 217, loss = 0.08199934\n",
      "Iteration 218, loss = 0.08176312\n",
      "Iteration 219, loss = 0.08152907\n",
      "Iteration 220, loss = 0.08129717\n",
      "Iteration 221, loss = 0.08106739\n",
      "Iteration 222, loss = 0.08083970\n",
      "Iteration 223, loss = 0.08061409\n",
      "Iteration 224, loss = 0.08039051\n",
      "Iteration 225, loss = 0.08016895\n",
      "Iteration 226, loss = 0.07994937\n",
      "Iteration 227, loss = 0.07973175\n",
      "Iteration 228, loss = 0.07951607\n",
      "Iteration 229, loss = 0.07930231\n",
      "Iteration 230, loss = 0.07909043\n",
      "Iteration 231, loss = 0.07888041\n",
      "Iteration 232, loss = 0.07867224\n",
      "Iteration 233, loss = 0.07846588\n",
      "Iteration 234, loss = 0.07826131\n",
      "Iteration 235, loss = 0.07805852\n",
      "Iteration 236, loss = 0.07785748\n",
      "Iteration 237, loss = 0.07765817\n",
      "Iteration 238, loss = 0.07746056\n",
      "Iteration 239, loss = 0.07726464\n",
      "Iteration 240, loss = 0.07707038\n",
      "Iteration 241, loss = 0.07687778\n",
      "Iteration 242, loss = 0.07668679\n",
      "Iteration 243, loss = 0.07649741\n",
      "Iteration 244, loss = 0.07630962\n",
      "Iteration 245, loss = 0.07612339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 246, loss = 0.07593872\n",
      "Iteration 247, loss = 0.07575557\n",
      "Iteration 248, loss = 0.07557394\n",
      "Iteration 249, loss = 0.07539380\n",
      "Iteration 250, loss = 0.07521513\n",
      "Iteration 251, loss = 0.07503793\n",
      "Iteration 252, loss = 0.07486216\n",
      "Iteration 253, loss = 0.07468783\n",
      "Iteration 254, loss = 0.07451490\n",
      "Iteration 255, loss = 0.07434336\n",
      "Iteration 256, loss = 0.07417320\n",
      "Iteration 257, loss = 0.07400440\n",
      "Iteration 258, loss = 0.07383694\n",
      "Iteration 259, loss = 0.07367081\n",
      "Iteration 260, loss = 0.07350600\n",
      "Iteration 261, loss = 0.07334249\n",
      "Iteration 262, loss = 0.07318026\n",
      "Iteration 263, loss = 0.07301930\n",
      "Iteration 264, loss = 0.07285960\n",
      "Iteration 265, loss = 0.07270114\n",
      "Iteration 266, loss = 0.07254391\n",
      "Iteration 267, loss = 0.07238789\n",
      "Iteration 268, loss = 0.07223307\n",
      "Iteration 269, loss = 0.07207944\n",
      "Iteration 270, loss = 0.07192698\n",
      "Iteration 271, loss = 0.07177569\n",
      "Iteration 272, loss = 0.07162554\n",
      "Iteration 273, loss = 0.07147653\n",
      "Iteration 274, loss = 0.07132865\n",
      "Iteration 275, loss = 0.07118188\n",
      "Iteration 276, loss = 0.07103620\n",
      "Iteration 277, loss = 0.07089162\n",
      "Iteration 278, loss = 0.07074811\n",
      "Iteration 279, loss = 0.07060566\n",
      "Iteration 280, loss = 0.07046427\n",
      "Iteration 281, loss = 0.07032392\n",
      "Iteration 282, loss = 0.07018460\n",
      "Iteration 283, loss = 0.07004631\n",
      "Iteration 284, loss = 0.06990902\n",
      "Iteration 285, loss = 0.06977273\n",
      "Iteration 286, loss = 0.06963743\n",
      "Iteration 287, loss = 0.06950310\n",
      "Iteration 288, loss = 0.06936975\n",
      "Iteration 289, loss = 0.06923735\n",
      "Iteration 290, loss = 0.06910590\n",
      "Iteration 291, loss = 0.06897539\n",
      "Iteration 292, loss = 0.06884581\n",
      "Iteration 293, loss = 0.06871714\n",
      "Iteration 294, loss = 0.06858939\n",
      "Iteration 295, loss = 0.06846253\n",
      "Iteration 296, loss = 0.06833657\n",
      "Iteration 297, loss = 0.06821149\n",
      "Iteration 298, loss = 0.06808728\n",
      "Iteration 299, loss = 0.06796394\n",
      "Iteration 300, loss = 0.06784145\n",
      "Iteration 301, loss = 0.06771981\n",
      "Iteration 302, loss = 0.06759901\n",
      "Iteration 303, loss = 0.06747904\n",
      "Iteration 304, loss = 0.06735989\n",
      "Iteration 305, loss = 0.06724155\n",
      "Iteration 306, loss = 0.06712402\n",
      "Iteration 307, loss = 0.06700729\n",
      "Iteration 308, loss = 0.06689135\n",
      "Iteration 309, loss = 0.06677619\n",
      "Iteration 310, loss = 0.06666181\n",
      "Iteration 311, loss = 0.06654819\n",
      "Iteration 312, loss = 0.06643533\n",
      "Iteration 313, loss = 0.06632323\n",
      "Iteration 314, loss = 0.06621187\n",
      "Iteration 315, loss = 0.06610125\n",
      "Iteration 316, loss = 0.06599136\n",
      "Iteration 317, loss = 0.06588219\n",
      "Iteration 318, loss = 0.06577374\n",
      "Iteration 319, loss = 0.06566600\n",
      "Iteration 320, loss = 0.06555896\n",
      "Iteration 321, loss = 0.06545262\n",
      "Iteration 322, loss = 0.06534698\n",
      "Iteration 323, loss = 0.06524201\n",
      "Iteration 324, loss = 0.06513772\n",
      "Iteration 325, loss = 0.06503410\n",
      "Iteration 326, loss = 0.06493115\n",
      "Iteration 327, loss = 0.06482886\n",
      "Iteration 328, loss = 0.06472721\n",
      "Iteration 329, loss = 0.06462622\n",
      "Iteration 330, loss = 0.06452586\n",
      "Iteration 331, loss = 0.06442614\n",
      "Iteration 332, loss = 0.06432705\n",
      "Iteration 333, loss = 0.06422858\n",
      "Iteration 334, loss = 0.06413073\n",
      "Iteration 335, loss = 0.06403349\n",
      "Iteration 336, loss = 0.06393685\n",
      "Iteration 337, loss = 0.06384082\n",
      "Iteration 338, loss = 0.06374538\n",
      "Iteration 339, loss = 0.06365053\n",
      "Iteration 340, loss = 0.06355626\n",
      "Iteration 341, loss = 0.06346258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13108299\n",
      "Iteration 2, loss = 1.30051125\n",
      "Iteration 3, loss = 1.19711330\n",
      "Iteration 4, loss = 1.17888563\n",
      "Iteration 5, loss = 1.14507166\n",
      "Iteration 6, loss = 1.11238280\n",
      "Iteration 7, loss = 1.06912038\n",
      "Iteration 8, loss = 1.01972412\n",
      "Iteration 9, loss = 0.97452339\n",
      "Iteration 10, loss = 0.93114557\n",
      "Iteration 11, loss = 0.89219120\n",
      "Iteration 12, loss = 0.85664056\n",
      "Iteration 13, loss = 0.82209409\n",
      "Iteration 14, loss = 0.78863730\n",
      "Iteration 15, loss = 0.75722371\n",
      "Iteration 16, loss = 0.72893177\n",
      "Iteration 17, loss = 0.70357893\n",
      "Iteration 18, loss = 0.67903412\n",
      "Iteration 19, loss = 0.65416959\n",
      "Iteration 20, loss = 0.62794595\n",
      "Iteration 21, loss = 0.60123356\n",
      "Iteration 22, loss = 0.57498850\n",
      "Iteration 23, loss = 0.55067378\n",
      "Iteration 24, loss = 0.52753756\n",
      "Iteration 25, loss = 0.50580638\n",
      "Iteration 26, loss = 0.48571801\n",
      "Iteration 27, loss = 0.46752445\n",
      "Iteration 28, loss = 0.45142645\n",
      "Iteration 29, loss = 0.43736672\n",
      "Iteration 30, loss = 0.42502176\n",
      "Iteration 31, loss = 0.41389221\n",
      "Iteration 32, loss = 0.40347086\n",
      "Iteration 33, loss = 0.39343321\n",
      "Iteration 34, loss = 0.38366589\n",
      "Iteration 35, loss = 0.37420721\n",
      "Iteration 36, loss = 0.36515384\n",
      "Iteration 37, loss = 0.35657734\n",
      "Iteration 38, loss = 0.34848308\n",
      "Iteration 39, loss = 0.34081601\n",
      "Iteration 40, loss = 0.33349303\n",
      "Iteration 41, loss = 0.32643670\n",
      "Iteration 42, loss = 0.31959513\n",
      "Iteration 43, loss = 0.31294602\n",
      "Iteration 44, loss = 0.30648946\n",
      "Iteration 45, loss = 0.30023607\n",
      "Iteration 46, loss = 0.29419612\n",
      "Iteration 47, loss = 0.28837299\n",
      "Iteration 48, loss = 0.28276193\n",
      "Iteration 49, loss = 0.27735268\n",
      "Iteration 50, loss = 0.27213342\n",
      "Iteration 51, loss = 0.26709407\n",
      "Iteration 52, loss = 0.26222777\n",
      "Iteration 53, loss = 0.25753060\n",
      "Iteration 54, loss = 0.25300038\n",
      "Iteration 55, loss = 0.24863519\n",
      "Iteration 56, loss = 0.24443230\n",
      "Iteration 57, loss = 0.24038778\n",
      "Iteration 58, loss = 0.23649653\n",
      "Iteration 59, loss = 0.23275267\n",
      "Iteration 60, loss = 0.22914998\n",
      "Iteration 61, loss = 0.22568228\n",
      "Iteration 62, loss = 0.22234356\n",
      "Iteration 63, loss = 0.21912811\n",
      "Iteration 64, loss = 0.21603037\n",
      "Iteration 65, loss = 0.21304493\n",
      "Iteration 66, loss = 0.21016648\n",
      "Iteration 67, loss = 0.20738973\n",
      "Iteration 68, loss = 0.20470953\n",
      "Iteration 69, loss = 0.20212083\n",
      "Iteration 70, loss = 0.19961880\n",
      "Iteration 71, loss = 0.19719882\n",
      "Iteration 72, loss = 0.19485654\n",
      "Iteration 73, loss = 0.19258788\n",
      "Iteration 74, loss = 0.19038901\n",
      "Iteration 75, loss = 0.18825633\n",
      "Iteration 76, loss = 0.18618649\n",
      "Iteration 77, loss = 0.18417631\n",
      "Iteration 78, loss = 0.18222283\n",
      "Iteration 79, loss = 0.18032328\n",
      "Iteration 80, loss = 0.17847504\n",
      "Iteration 81, loss = 0.17667569\n",
      "Iteration 82, loss = 0.17492296\n",
      "Iteration 83, loss = 0.17321474\n",
      "Iteration 84, loss = 0.17154906\n",
      "Iteration 85, loss = 0.16992408\n",
      "Iteration 86, loss = 0.16833810\n",
      "Iteration 87, loss = 0.16678953\n",
      "Iteration 88, loss = 0.16527686\n",
      "Iteration 89, loss = 0.16379870\n",
      "Iteration 90, loss = 0.16235372\n",
      "Iteration 91, loss = 0.16094071\n",
      "Iteration 92, loss = 0.15955849\n",
      "Iteration 93, loss = 0.15820598\n",
      "Iteration 94, loss = 0.15688212\n",
      "Iteration 95, loss = 0.15558596\n",
      "Iteration 96, loss = 0.15431656\n",
      "Iteration 97, loss = 0.15307304\n",
      "Iteration 98, loss = 0.15185457\n",
      "Iteration 99, loss = 0.15066036\n",
      "Iteration 100, loss = 0.14948964\n",
      "Iteration 101, loss = 0.14834170\n",
      "Iteration 102, loss = 0.14721583\n",
      "Iteration 103, loss = 0.14611140\n",
      "Iteration 104, loss = 0.14502774\n",
      "Iteration 105, loss = 0.14396427\n",
      "Iteration 106, loss = 0.14292039\n",
      "Iteration 107, loss = 0.14189557\n",
      "Iteration 108, loss = 0.14088922\n",
      "Iteration 109, loss = 0.13990090\n",
      "Iteration 110, loss = 0.13893003\n",
      "Iteration 111, loss = 0.13797620\n",
      "Iteration 112, loss = 0.13703890\n",
      "Iteration 113, loss = 0.13611774\n",
      "Iteration 114, loss = 0.13521223\n",
      "Iteration 115, loss = 0.13432202\n",
      "Iteration 116, loss = 0.13344665\n",
      "Iteration 117, loss = 0.13258581\n",
      "Iteration 118, loss = 0.13173904\n",
      "Iteration 119, loss = 0.13090608\n",
      "Iteration 120, loss = 0.13008648\n",
      "Iteration 121, loss = 0.12928002\n",
      "Iteration 122, loss = 0.12848626\n",
      "Iteration 123, loss = 0.12770500\n",
      "Iteration 124, loss = 0.12693583\n",
      "Iteration 125, loss = 0.12617856\n",
      "Iteration 126, loss = 0.12543281\n",
      "Iteration 127, loss = 0.12469841\n",
      "Iteration 128, loss = 0.12397499\n",
      "Iteration 129, loss = 0.12326240\n",
      "Iteration 130, loss = 0.12256028\n",
      "Iteration 131, loss = 0.12186851\n",
      "Iteration 132, loss = 0.12118675\n",
      "Iteration 133, loss = 0.12051487\n",
      "Iteration 134, loss = 0.11985256\n",
      "Iteration 135, loss = 0.11919970\n",
      "Iteration 136, loss = 0.11855601\n",
      "Iteration 137, loss = 0.11792137\n",
      "Iteration 138, loss = 0.11729550\n",
      "Iteration 139, loss = 0.11667830\n",
      "Iteration 140, loss = 0.11606952\n",
      "Iteration 141, loss = 0.11546905\n",
      "Iteration 142, loss = 0.11487666\n",
      "Iteration 143, loss = 0.11429224\n",
      "Iteration 144, loss = 0.11371557\n",
      "Iteration 145, loss = 0.11314656\n",
      "Iteration 146, loss = 0.11258500\n",
      "Iteration 147, loss = 0.11203079\n",
      "Iteration 148, loss = 0.11148374\n",
      "Iteration 149, loss = 0.11094375\n",
      "Iteration 150, loss = 0.11041065\n",
      "Iteration 151, loss = 0.10988434\n",
      "Iteration 152, loss = 0.10936467\n",
      "Iteration 153, loss = 0.10885152\n",
      "Iteration 154, loss = 0.10834475\n",
      "Iteration 155, loss = 0.10784427\n",
      "Iteration 156, loss = 0.10734994\n",
      "Iteration 157, loss = 0.10686166\n",
      "Iteration 158, loss = 0.10637929\n",
      "Iteration 159, loss = 0.10590276\n",
      "Iteration 160, loss = 0.10543194\n",
      "Iteration 161, loss = 0.10496674\n",
      "Iteration 162, loss = 0.10450704\n",
      "Iteration 163, loss = 0.10405276\n",
      "Iteration 164, loss = 0.10360379\n",
      "Iteration 165, loss = 0.10316005\n",
      "Iteration 166, loss = 0.10272143\n",
      "Iteration 167, loss = 0.10228786\n",
      "Iteration 168, loss = 0.10185924\n",
      "Iteration 169, loss = 0.10143550\n",
      "Iteration 170, loss = 0.10101653\n",
      "Iteration 171, loss = 0.10060228\n",
      "Iteration 172, loss = 0.10019264\n",
      "Iteration 173, loss = 0.09978756\n",
      "Iteration 174, loss = 0.09938694\n",
      "Iteration 175, loss = 0.09899073\n",
      "Iteration 176, loss = 0.09859883\n",
      "Iteration 177, loss = 0.09821119\n",
      "Iteration 178, loss = 0.09782774\n",
      "Iteration 179, loss = 0.09744840\n",
      "Iteration 180, loss = 0.09707311\n",
      "Iteration 181, loss = 0.09670181\n",
      "Iteration 182, loss = 0.09633443\n",
      "Iteration 183, loss = 0.09597091\n",
      "Iteration 184, loss = 0.09561118\n",
      "Iteration 185, loss = 0.09525520\n",
      "Iteration 186, loss = 0.09490289\n",
      "Iteration 187, loss = 0.09455421\n",
      "Iteration 188, loss = 0.09420910\n",
      "Iteration 189, loss = 0.09386749\n",
      "Iteration 190, loss = 0.09352935\n",
      "Iteration 191, loss = 0.09319461\n",
      "Iteration 192, loss = 0.09286323\n",
      "Iteration 193, loss = 0.09253515\n",
      "Iteration 194, loss = 0.09221033\n",
      "Iteration 195, loss = 0.09188871\n",
      "Iteration 196, loss = 0.09157025\n",
      "Iteration 197, loss = 0.09125491\n",
      "Iteration 198, loss = 0.09094263\n",
      "Iteration 199, loss = 0.09063337\n",
      "Iteration 200, loss = 0.09032709\n",
      "Iteration 201, loss = 0.09002374\n",
      "Iteration 202, loss = 0.08972328\n",
      "Iteration 203, loss = 0.08942568\n",
      "Iteration 204, loss = 0.08913088\n",
      "Iteration 205, loss = 0.08883886\n",
      "Iteration 206, loss = 0.08854957\n",
      "Iteration 207, loss = 0.08826297\n",
      "Iteration 208, loss = 0.08797902\n",
      "Iteration 209, loss = 0.08769769\n",
      "Iteration 210, loss = 0.08741894\n",
      "Iteration 211, loss = 0.08714274\n",
      "Iteration 212, loss = 0.08686905\n",
      "Iteration 213, loss = 0.08659784\n",
      "Iteration 214, loss = 0.08632907\n",
      "Iteration 215, loss = 0.08606271\n",
      "Iteration 216, loss = 0.08579872\n",
      "Iteration 217, loss = 0.08553708\n",
      "Iteration 218, loss = 0.08527776\n",
      "Iteration 219, loss = 0.08502071\n",
      "Iteration 220, loss = 0.08476592\n",
      "Iteration 221, loss = 0.08451336\n",
      "Iteration 222, loss = 0.08426298\n",
      "Iteration 223, loss = 0.08401477\n",
      "Iteration 224, loss = 0.08376870\n",
      "Iteration 225, loss = 0.08352474\n",
      "Iteration 226, loss = 0.08328285\n",
      "Iteration 227, loss = 0.08304303\n",
      "Iteration 228, loss = 0.08280523\n",
      "Iteration 229, loss = 0.08256943\n",
      "Iteration 230, loss = 0.08233561\n",
      "Iteration 231, loss = 0.08210375\n",
      "Iteration 232, loss = 0.08187381\n",
      "Iteration 233, loss = 0.08164577\n",
      "Iteration 234, loss = 0.08141961\n",
      "Iteration 235, loss = 0.08119531\n",
      "Iteration 236, loss = 0.08097284\n",
      "Iteration 237, loss = 0.08075218\n",
      "Iteration 238, loss = 0.08053331\n",
      "Iteration 239, loss = 0.08031621\n",
      "Iteration 240, loss = 0.08010085\n",
      "Iteration 241, loss = 0.07988722\n",
      "Iteration 242, loss = 0.07967528\n",
      "Iteration 243, loss = 0.07946503\n",
      "Iteration 244, loss = 0.07925644\n",
      "Iteration 245, loss = 0.07904949\n",
      "Iteration 246, loss = 0.07884416\n",
      "Iteration 247, loss = 0.07864044\n",
      "Iteration 248, loss = 0.07843830\n",
      "Iteration 249, loss = 0.07823773\n",
      "Iteration 250, loss = 0.07803870\n",
      "Iteration 251, loss = 0.07784120\n",
      "Iteration 252, loss = 0.07764521\n",
      "Iteration 253, loss = 0.07745072\n",
      "Iteration 254, loss = 0.07725770\n",
      "Iteration 255, loss = 0.07706614\n",
      "Iteration 256, loss = 0.07687602\n",
      "Iteration 257, loss = 0.07668733\n",
      "Iteration 258, loss = 0.07650005\n",
      "Iteration 259, loss = 0.07631416\n",
      "Iteration 260, loss = 0.07612965\n",
      "Iteration 261, loss = 0.07594650\n",
      "Iteration 262, loss = 0.07576470\n",
      "Iteration 263, loss = 0.07558423\n",
      "Iteration 264, loss = 0.07540508\n",
      "Iteration 265, loss = 0.07522723\n",
      "Iteration 266, loss = 0.07505067\n",
      "Iteration 267, loss = 0.07487538\n",
      "Iteration 268, loss = 0.07470135\n",
      "Iteration 269, loss = 0.07452857\n",
      "Iteration 270, loss = 0.07435702\n",
      "Iteration 271, loss = 0.07418669\n",
      "Iteration 272, loss = 0.07401756\n",
      "Iteration 273, loss = 0.07384963\n",
      "Iteration 274, loss = 0.07368288\n",
      "Iteration 275, loss = 0.07351729\n",
      "Iteration 276, loss = 0.07335286\n",
      "Iteration 277, loss = 0.07318957\n",
      "Iteration 278, loss = 0.07302741\n",
      "Iteration 279, loss = 0.07286637\n",
      "Iteration 280, loss = 0.07270644\n",
      "Iteration 281, loss = 0.07254760\n",
      "Iteration 282, loss = 0.07238984\n",
      "Iteration 283, loss = 0.07223316\n",
      "Iteration 284, loss = 0.07207753\n",
      "Iteration 285, loss = 0.07192296\n",
      "Iteration 286, loss = 0.07176943\n",
      "Iteration 287, loss = 0.07161692\n",
      "Iteration 288, loss = 0.07146543\n",
      "Iteration 289, loss = 0.07131495\n",
      "Iteration 290, loss = 0.07116547\n",
      "Iteration 291, loss = 0.07101697\n",
      "Iteration 292, loss = 0.07086945\n",
      "Iteration 293, loss = 0.07072290\n",
      "Iteration 294, loss = 0.07057730\n",
      "Iteration 295, loss = 0.07043266\n",
      "Iteration 296, loss = 0.07028895\n",
      "Iteration 297, loss = 0.07014617\n",
      "Iteration 298, loss = 0.07000430\n",
      "Iteration 299, loss = 0.06986335\n",
      "Iteration 300, loss = 0.06972330\n",
      "Iteration 301, loss = 0.06958415\n",
      "Iteration 302, loss = 0.06944588\n",
      "Iteration 303, loss = 0.06930848\n",
      "Iteration 304, loss = 0.06917195\n",
      "Iteration 305, loss = 0.06903627\n",
      "Iteration 306, loss = 0.06890145\n",
      "Iteration 307, loss = 0.06876747\n",
      "Iteration 308, loss = 0.06863432\n",
      "Iteration 309, loss = 0.06850200\n",
      "Iteration 310, loss = 0.06837050\n",
      "Iteration 311, loss = 0.06823980\n",
      "Iteration 312, loss = 0.06810991\n",
      "Iteration 313, loss = 0.06798082\n",
      "Iteration 314, loss = 0.06785251\n",
      "Iteration 315, loss = 0.06772497\n",
      "Iteration 316, loss = 0.06759822\n",
      "Iteration 317, loss = 0.06747222\n",
      "Iteration 318, loss = 0.06734699\n",
      "Iteration 319, loss = 0.06722250\n",
      "Iteration 320, loss = 0.06709876\n",
      "Iteration 321, loss = 0.06697576\n",
      "Iteration 322, loss = 0.06685348\n",
      "Iteration 323, loss = 0.06673193\n",
      "Iteration 324, loss = 0.06661110\n",
      "Iteration 325, loss = 0.06649098\n",
      "Iteration 326, loss = 0.06637156\n",
      "Iteration 327, loss = 0.06625283\n",
      "Iteration 328, loss = 0.06613480\n",
      "Iteration 329, loss = 0.06601746\n",
      "Iteration 330, loss = 0.06590079\n",
      "Iteration 331, loss = 0.06578479\n",
      "Iteration 332, loss = 0.06566946\n",
      "Iteration 333, loss = 0.06555479\n",
      "Iteration 334, loss = 0.06544078\n",
      "Iteration 335, loss = 0.06532742\n",
      "Iteration 336, loss = 0.06521469\n",
      "Iteration 337, loss = 0.06510261\n",
      "Iteration 338, loss = 0.06499115\n",
      "Iteration 339, loss = 0.06488033\n",
      "Iteration 340, loss = 0.06477012\n",
      "Iteration 341, loss = 0.06466053\n",
      "Iteration 342, loss = 0.06455155\n",
      "Iteration 343, loss = 0.06444317\n",
      "Iteration 344, loss = 0.06433539\n",
      "Iteration 345, loss = 0.06422821\n",
      "Iteration 346, loss = 0.06412162\n",
      "Iteration 347, loss = 0.06401560\n",
      "Iteration 348, loss = 0.06391017\n",
      "Iteration 349, loss = 0.06380532\n",
      "Iteration 350, loss = 0.06370103\n",
      "Iteration 351, loss = 0.06359730\n",
      "Iteration 352, loss = 0.06349414\n",
      "Iteration 353, loss = 0.06339153\n",
      "Iteration 354, loss = 0.06328947\n",
      "Iteration 355, loss = 0.06318795\n",
      "Iteration 356, loss = 0.06308698\n",
      "Iteration 357, loss = 0.06298654\n",
      "Iteration 358, loss = 0.06288663\n",
      "Iteration 359, loss = 0.06278726\n",
      "Iteration 360, loss = 0.06268840\n",
      "Iteration 361, loss = 0.06259006\n",
      "Iteration 362, loss = 0.06249224\n",
      "Iteration 363, loss = 0.06239493\n",
      "Iteration 364, loss = 0.06229812\n",
      "Iteration 365, loss = 0.06220182\n",
      "Iteration 366, loss = 0.06210601\n",
      "Iteration 367, loss = 0.06201070\n",
      "Iteration 368, loss = 0.06191588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15205734\n",
      "Iteration 2, loss = 1.33226487\n",
      "Iteration 3, loss = 1.18681585\n",
      "Iteration 4, loss = 1.15324918\n",
      "Iteration 5, loss = 1.11211569\n",
      "Iteration 6, loss = 1.05064254\n",
      "Iteration 7, loss = 0.98311532\n",
      "Iteration 8, loss = 0.92898516\n",
      "Iteration 9, loss = 0.87387476\n",
      "Iteration 10, loss = 0.83251890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.79512586\n",
      "Iteration 12, loss = 0.76145112\n",
      "Iteration 13, loss = 0.73186991\n",
      "Iteration 14, loss = 0.70704803\n",
      "Iteration 15, loss = 0.68623125\n",
      "Iteration 16, loss = 0.66640312\n",
      "Iteration 17, loss = 0.64352951\n",
      "Iteration 18, loss = 0.61877708\n",
      "Iteration 19, loss = 0.59396352\n",
      "Iteration 20, loss = 0.56975665\n",
      "Iteration 21, loss = 0.54709813\n",
      "Iteration 22, loss = 0.52532803\n",
      "Iteration 23, loss = 0.50480564\n",
      "Iteration 24, loss = 0.48586389\n",
      "Iteration 25, loss = 0.46876770\n",
      "Iteration 26, loss = 0.45358999\n",
      "Iteration 27, loss = 0.44018879\n",
      "Iteration 28, loss = 0.42820568\n",
      "Iteration 29, loss = 0.41715095\n",
      "Iteration 30, loss = 0.40664360\n",
      "Iteration 31, loss = 0.39648592\n",
      "Iteration 32, loss = 0.38664423\n",
      "Iteration 33, loss = 0.37718320\n",
      "Iteration 34, loss = 0.36818397\n",
      "Iteration 35, loss = 0.35968403\n",
      "Iteration 36, loss = 0.35165872\n",
      "Iteration 37, loss = 0.34403922\n",
      "Iteration 38, loss = 0.33674554\n",
      "Iteration 39, loss = 0.32971397\n",
      "Iteration 40, loss = 0.32290903\n",
      "Iteration 41, loss = 0.31632119\n",
      "Iteration 42, loss = 0.30995655\n",
      "Iteration 43, loss = 0.30382481\n",
      "Iteration 44, loss = 0.29793075\n",
      "Iteration 45, loss = 0.29227095\n",
      "Iteration 46, loss = 0.28683532\n",
      "Iteration 47, loss = 0.28161097\n",
      "Iteration 48, loss = 0.27658593\n",
      "Iteration 49, loss = 0.27175125\n",
      "Iteration 50, loss = 0.26710123\n",
      "Iteration 51, loss = 0.26263222\n",
      "Iteration 52, loss = 0.25834120\n",
      "Iteration 53, loss = 0.25422448\n",
      "Iteration 54, loss = 0.25027724\n",
      "Iteration 55, loss = 0.24649338\n",
      "Iteration 56, loss = 0.24286620\n",
      "Iteration 57, loss = 0.23938842\n",
      "Iteration 58, loss = 0.23605322\n",
      "Iteration 59, loss = 0.23285360\n",
      "Iteration 60, loss = 0.22978358\n",
      "Iteration 61, loss = 0.22683670\n",
      "Iteration 62, loss = 0.22400829\n",
      "Iteration 63, loss = 0.22129273\n",
      "Iteration 64, loss = 0.21868868\n",
      "Iteration 65, loss = 0.21619467\n",
      "Iteration 66, loss = 0.21382300\n",
      "Iteration 67, loss = 0.21159162\n",
      "Iteration 68, loss = 0.20957614\n",
      "Iteration 69, loss = 0.20786822\n",
      "Iteration 70, loss = 0.20686329\n",
      "Iteration 71, loss = 0.20680161\n",
      "Iteration 72, loss = 0.20983731\n",
      "Iteration 73, loss = 0.21514197\n",
      "Iteration 74, loss = 0.23481571\n",
      "Iteration 75, loss = 0.25050440\n",
      "Iteration 76, loss = 0.31913944\n",
      "Iteration 77, loss = 0.31693141\n",
      "Iteration 78, loss = 0.37227172\n",
      "Iteration 79, loss = 0.32020116\n",
      "Iteration 80, loss = 0.25846238\n",
      "Iteration 81, loss = 0.23341244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13218766\n",
      "Iteration 2, loss = 1.98082716\n",
      "Iteration 3, loss = 1.82988175\n",
      "Iteration 4, loss = 1.68469027\n",
      "Iteration 5, loss = 1.55288544\n",
      "Iteration 6, loss = 1.43717683\n",
      "Iteration 7, loss = 1.33963201\n",
      "Iteration 8, loss = 1.26103037\n",
      "Iteration 9, loss = 1.20135909\n",
      "Iteration 10, loss = 1.15965338\n",
      "Iteration 11, loss = 1.13247116\n",
      "Iteration 12, loss = 1.11674112\n",
      "Iteration 13, loss = 1.10866104\n",
      "Iteration 14, loss = 1.10503948\n",
      "Iteration 15, loss = 1.10283241\n",
      "Iteration 16, loss = 1.09970519\n",
      "Iteration 17, loss = 1.09382398\n",
      "Iteration 18, loss = 1.08457223\n",
      "Iteration 19, loss = 1.07188592\n",
      "Iteration 20, loss = 1.05599566\n",
      "Iteration 21, loss = 1.03737208\n",
      "Iteration 22, loss = 1.01665728\n",
      "Iteration 23, loss = 0.99455600\n",
      "Iteration 24, loss = 0.97180515\n",
      "Iteration 25, loss = 0.94946928\n",
      "Iteration 26, loss = 0.92858604\n",
      "Iteration 27, loss = 0.90960881\n",
      "Iteration 28, loss = 0.89292738\n",
      "Iteration 29, loss = 0.87882666\n",
      "Iteration 30, loss = 0.86722838\n",
      "Iteration 31, loss = 0.85775846\n",
      "Iteration 32, loss = 0.85013455\n",
      "Iteration 33, loss = 0.84391943\n",
      "Iteration 34, loss = 0.83849144\n",
      "Iteration 35, loss = 0.83342957\n",
      "Iteration 36, loss = 0.82847833\n",
      "Iteration 37, loss = 0.82346968\n",
      "Iteration 38, loss = 0.81840401\n",
      "Iteration 39, loss = 0.81335894\n",
      "Iteration 40, loss = 0.80843796\n",
      "Iteration 41, loss = 0.80372653\n",
      "Iteration 42, loss = 0.79922496\n",
      "Iteration 43, loss = 0.79486287\n",
      "Iteration 44, loss = 0.79050098\n",
      "Iteration 45, loss = 0.78597712\n",
      "Iteration 46, loss = 0.78116397\n",
      "Iteration 47, loss = 0.77590435\n",
      "Iteration 48, loss = 0.77014507\n",
      "Iteration 49, loss = 0.76401568\n",
      "Iteration 50, loss = 0.75758082\n",
      "Iteration 51, loss = 0.75097823\n",
      "Iteration 52, loss = 0.74449167\n",
      "Iteration 53, loss = 0.73813103\n",
      "Iteration 54, loss = 0.73198427\n",
      "Iteration 55, loss = 0.72625289\n",
      "Iteration 56, loss = 0.72101964\n",
      "Iteration 57, loss = 0.71618287\n",
      "Iteration 58, loss = 0.71158081\n",
      "Iteration 59, loss = 0.70717906\n",
      "Iteration 60, loss = 0.70277885\n",
      "Iteration 61, loss = 0.69839630\n",
      "Iteration 62, loss = 0.69394523\n",
      "Iteration 63, loss = 0.68954933\n",
      "Iteration 64, loss = 0.68522081\n",
      "Iteration 65, loss = 0.68098505\n",
      "Iteration 66, loss = 0.67678979\n",
      "Iteration 67, loss = 0.67259911\n",
      "Iteration 68, loss = 0.66840803\n",
      "Iteration 69, loss = 0.66420954\n",
      "Iteration 70, loss = 0.66000101\n",
      "Iteration 71, loss = 0.65578851\n",
      "Iteration 72, loss = 0.65159286\n",
      "Iteration 73, loss = 0.64744686\n",
      "Iteration 74, loss = 0.64340518\n",
      "Iteration 75, loss = 0.63950828\n",
      "Iteration 76, loss = 0.63574351\n",
      "Iteration 77, loss = 0.63208201\n",
      "Iteration 78, loss = 0.62828662\n",
      "Iteration 79, loss = 0.62256091\n",
      "Iteration 80, loss = 0.61431052\n",
      "Iteration 81, loss = 0.60425969\n",
      "Iteration 82, loss = 0.59362694\n",
      "Iteration 83, loss = 0.58931083\n",
      "Iteration 84, loss = 0.58209661\n",
      "Iteration 85, loss = 0.57422871\n",
      "Iteration 86, loss = 0.56628228\n",
      "Iteration 87, loss = 0.55815742\n",
      "Iteration 88, loss = 0.54976057\n",
      "Iteration 89, loss = 0.54113411\n",
      "Iteration 90, loss = 0.53244692\n",
      "Iteration 91, loss = 0.52388205\n",
      "Iteration 92, loss = 0.51555365\n",
      "Iteration 93, loss = 0.50748462\n",
      "Iteration 94, loss = 0.49950608\n",
      "Iteration 95, loss = 0.49139273\n",
      "Iteration 96, loss = 0.48301109\n",
      "Iteration 97, loss = 0.47434712\n",
      "Iteration 98, loss = 0.46550023\n",
      "Iteration 99, loss = 0.45666575\n",
      "Iteration 100, loss = 0.44807765\n",
      "Iteration 101, loss = 0.43990697\n",
      "Iteration 102, loss = 0.43220474\n",
      "Iteration 103, loss = 0.42487503\n",
      "Iteration 104, loss = 0.41773465\n",
      "Iteration 105, loss = 0.41056369\n",
      "Iteration 106, loss = 0.40317691\n",
      "Iteration 107, loss = 0.39548770\n",
      "Iteration 108, loss = 0.38777255\n",
      "Iteration 109, loss = 0.38036338\n",
      "Iteration 110, loss = 0.37340190\n",
      "Iteration 111, loss = 0.36674500\n",
      "Iteration 112, loss = 0.36009203\n",
      "Iteration 113, loss = 0.35321621\n",
      "Iteration 114, loss = 0.34621144\n",
      "Iteration 115, loss = 0.33930176\n",
      "Iteration 116, loss = 0.33274414\n",
      "Iteration 117, loss = 0.32631774\n",
      "Iteration 118, loss = 0.31967497\n",
      "Iteration 119, loss = 0.31299731\n",
      "Iteration 120, loss = 0.30661717\n",
      "Iteration 121, loss = 0.30043375\n",
      "Iteration 122, loss = 0.29431443\n",
      "Iteration 123, loss = 0.28819444\n",
      "Iteration 124, loss = 0.28213461\n",
      "Iteration 125, loss = 0.27623797\n",
      "Iteration 126, loss = 0.27052862\n",
      "Iteration 127, loss = 0.26494283\n",
      "Iteration 128, loss = 0.25938956\n",
      "Iteration 129, loss = 0.25395095\n",
      "Iteration 130, loss = 0.24870400\n",
      "Iteration 131, loss = 0.24361881\n",
      "Iteration 132, loss = 0.23861955\n",
      "Iteration 133, loss = 0.23370580\n",
      "Iteration 134, loss = 0.22895542\n",
      "Iteration 135, loss = 0.22438261\n",
      "Iteration 136, loss = 0.21992161\n",
      "Iteration 137, loss = 0.21553834\n",
      "Iteration 138, loss = 0.21131674\n",
      "Iteration 139, loss = 0.20725478\n",
      "Iteration 140, loss = 0.20328201\n",
      "Iteration 141, loss = 0.19940932\n",
      "Iteration 142, loss = 0.19568417\n",
      "Iteration 143, loss = 0.19209039\n",
      "Iteration 144, loss = 0.18858449\n",
      "Iteration 145, loss = 0.18519067\n",
      "Iteration 146, loss = 0.18193573\n",
      "Iteration 147, loss = 0.17877585\n",
      "Iteration 148, loss = 0.17571415\n",
      "Iteration 149, loss = 0.17276496\n",
      "Iteration 150, loss = 0.16992042\n",
      "Iteration 151, loss = 0.16716724\n",
      "Iteration 152, loss = 0.16450414\n",
      "Iteration 153, loss = 0.16193996\n",
      "Iteration 154, loss = 0.15946263\n",
      "Iteration 155, loss = 0.15705938\n",
      "Iteration 156, loss = 0.15473559\n",
      "Iteration 157, loss = 0.15249524\n",
      "Iteration 158, loss = 0.15032797\n",
      "Iteration 159, loss = 0.14822693\n",
      "Iteration 160, loss = 0.14619779\n",
      "Iteration 161, loss = 0.14424134\n",
      "Iteration 162, loss = 0.14234500\n",
      "Iteration 163, loss = 0.14050495\n",
      "Iteration 164, loss = 0.13872769\n",
      "Iteration 165, loss = 0.13701114\n",
      "Iteration 166, loss = 0.13534543\n",
      "Iteration 167, loss = 0.13373034\n",
      "Iteration 168, loss = 0.13216936\n",
      "Iteration 169, loss = 0.13065827\n",
      "Iteration 170, loss = 0.12919091\n",
      "Iteration 171, loss = 0.12776846\n",
      "Iteration 172, loss = 0.12639169\n",
      "Iteration 173, loss = 0.12505587\n",
      "Iteration 174, loss = 0.12375813\n",
      "Iteration 175, loss = 0.12249985\n",
      "Iteration 176, loss = 0.12127991\n",
      "Iteration 177, loss = 0.12009532\n",
      "Iteration 178, loss = 0.11894744\n",
      "Iteration 179, loss = 0.11783280\n",
      "Iteration 180, loss = 0.11675010\n",
      "Iteration 181, loss = 0.11569828\n",
      "Iteration 182, loss = 0.11467612\n",
      "Iteration 183, loss = 0.11368263\n",
      "Iteration 184, loss = 0.11271661\n",
      "Iteration 185, loss = 0.11177706\n",
      "Iteration 186, loss = 0.11086308\n",
      "Iteration 187, loss = 0.10997369\n",
      "Iteration 188, loss = 0.10910796\n",
      "Iteration 189, loss = 0.10826507\n",
      "Iteration 190, loss = 0.10744426\n",
      "Iteration 191, loss = 0.10664468\n",
      "Iteration 192, loss = 0.10586557\n",
      "Iteration 193, loss = 0.10510625\n",
      "Iteration 194, loss = 0.10436602\n",
      "Iteration 195, loss = 0.10364419\n",
      "Iteration 196, loss = 0.10294017\n",
      "Iteration 197, loss = 0.10225336\n",
      "Iteration 198, loss = 0.10158326\n",
      "Iteration 199, loss = 0.10092944\n",
      "Iteration 200, loss = 0.10029117\n",
      "Iteration 201, loss = 0.09966794\n",
      "Iteration 202, loss = 0.09905925\n",
      "Iteration 203, loss = 0.09846462\n",
      "Iteration 204, loss = 0.09788366\n",
      "Iteration 205, loss = 0.09731661\n",
      "Iteration 206, loss = 0.09676280\n",
      "Iteration 207, loss = 0.09622154\n",
      "Iteration 208, loss = 0.09569227\n",
      "Iteration 209, loss = 0.09517463\n",
      "Iteration 210, loss = 0.09466930\n",
      "Iteration 211, loss = 0.09417486\n",
      "Iteration 212, loss = 0.09369107\n",
      "Iteration 213, loss = 0.09321731\n",
      "Iteration 214, loss = 0.09275338\n",
      "Iteration 215, loss = 0.09229918\n",
      "Iteration 216, loss = 0.09185427\n",
      "Iteration 217, loss = 0.09141827\n",
      "Iteration 218, loss = 0.09099110\n",
      "Iteration 219, loss = 0.09057251\n",
      "Iteration 220, loss = 0.09016210\n",
      "Iteration 221, loss = 0.08975972\n",
      "Iteration 222, loss = 0.08936520\n",
      "Iteration 223, loss = 0.08897824\n",
      "Iteration 224, loss = 0.08859860\n",
      "Iteration 225, loss = 0.08822616\n",
      "Iteration 226, loss = 0.08786070\n",
      "Iteration 227, loss = 0.08750196\n",
      "Iteration 228, loss = 0.08714982\n",
      "Iteration 229, loss = 0.08680412\n",
      "Iteration 230, loss = 0.08646463\n",
      "Iteration 231, loss = 0.08613121\n",
      "Iteration 232, loss = 0.08580372\n",
      "Iteration 233, loss = 0.08548199\n",
      "Iteration 234, loss = 0.08516585\n",
      "Iteration 235, loss = 0.08485519\n",
      "Iteration 236, loss = 0.08454987\n",
      "Iteration 237, loss = 0.08424973\n",
      "Iteration 238, loss = 0.08395465\n",
      "Iteration 239, loss = 0.08366452\n",
      "Iteration 240, loss = 0.08337919\n",
      "Iteration 241, loss = 0.08309856\n",
      "Iteration 242, loss = 0.08282252\n",
      "Iteration 243, loss = 0.08255106\n",
      "Iteration 244, loss = 0.08228409\n",
      "Iteration 245, loss = 0.08202176\n",
      "Iteration 246, loss = 0.08176360\n",
      "Iteration 247, loss = 0.08150919\n",
      "Iteration 248, loss = 0.08125924\n",
      "Iteration 249, loss = 0.08101314\n",
      "Iteration 250, loss = 0.08077060\n",
      "Iteration 251, loss = 0.08053203\n",
      "Iteration 252, loss = 0.08029711\n",
      "Iteration 253, loss = 0.08006550\n",
      "Iteration 254, loss = 0.07983748\n",
      "Iteration 255, loss = 0.07961296\n",
      "Iteration 256, loss = 0.07939158\n",
      "Iteration 257, loss = 0.07917347\n",
      "Iteration 258, loss = 0.07895865\n",
      "Iteration 259, loss = 0.07874684\n",
      "Iteration 260, loss = 0.07853802\n",
      "Iteration 261, loss = 0.07833228\n",
      "Iteration 262, loss = 0.07812940\n",
      "Iteration 263, loss = 0.07792931\n",
      "Iteration 264, loss = 0.07773207\n",
      "Iteration 265, loss = 0.07753756\n",
      "Iteration 266, loss = 0.07734565\n",
      "Iteration 267, loss = 0.07715640\n",
      "Iteration 268, loss = 0.07696973\n",
      "Iteration 269, loss = 0.07678552\n",
      "Iteration 270, loss = 0.07660377\n",
      "Iteration 271, loss = 0.07642447\n",
      "Iteration 272, loss = 0.07624749\n",
      "Iteration 273, loss = 0.07607282\n",
      "Iteration 274, loss = 0.07590044\n",
      "Iteration 275, loss = 0.07573025\n",
      "Iteration 276, loss = 0.07556224\n",
      "Iteration 277, loss = 0.07539637\n",
      "Iteration 278, loss = 0.07523259\n",
      "Iteration 279, loss = 0.07507085\n",
      "Iteration 280, loss = 0.07491113\n",
      "Iteration 281, loss = 0.07475338\n",
      "Iteration 282, loss = 0.07459756\n",
      "Iteration 283, loss = 0.07444364\n",
      "Iteration 284, loss = 0.07429159\n",
      "Iteration 285, loss = 0.07414137\n",
      "Iteration 286, loss = 0.07399294\n",
      "Iteration 287, loss = 0.07384627\n",
      "Iteration 288, loss = 0.07370134\n",
      "Iteration 289, loss = 0.07355810\n",
      "Iteration 290, loss = 0.07341663\n",
      "Iteration 291, loss = 0.07327690\n",
      "Iteration 292, loss = 0.07313882\n",
      "Iteration 293, loss = 0.07300231\n",
      "Iteration 294, loss = 0.07286735\n",
      "Iteration 295, loss = 0.07273395\n",
      "Iteration 296, loss = 0.07260204\n",
      "Iteration 297, loss = 0.07247160\n",
      "Iteration 298, loss = 0.07234264\n",
      "Iteration 299, loss = 0.07221511\n",
      "Iteration 300, loss = 0.07208896\n",
      "Iteration 301, loss = 0.07196421\n",
      "Iteration 302, loss = 0.07184081\n",
      "Iteration 303, loss = 0.07171874\n",
      "Iteration 304, loss = 0.07159799\n",
      "Iteration 305, loss = 0.07147853\n",
      "Iteration 306, loss = 0.07136033\n",
      "Iteration 307, loss = 0.07124338\n",
      "Iteration 308, loss = 0.07112766\n",
      "Iteration 309, loss = 0.07101314\n",
      "Iteration 310, loss = 0.07089981\n",
      "Iteration 311, loss = 0.07078765\n",
      "Iteration 312, loss = 0.07067663\n",
      "Iteration 313, loss = 0.07056674\n",
      "Iteration 314, loss = 0.07045797\n",
      "Iteration 315, loss = 0.07035029\n",
      "Iteration 316, loss = 0.07024369\n",
      "Iteration 317, loss = 0.07013815\n",
      "Iteration 318, loss = 0.07003365\n",
      "Iteration 319, loss = 0.06993018\n",
      "Iteration 320, loss = 0.06982773\n",
      "Iteration 321, loss = 0.06972627\n",
      "Iteration 322, loss = 0.06962579\n",
      "Iteration 323, loss = 0.06952628\n",
      "Iteration 324, loss = 0.06942772\n",
      "Iteration 325, loss = 0.06933010\n",
      "Iteration 326, loss = 0.06923340\n",
      "Iteration 327, loss = 0.06913762\n",
      "Iteration 328, loss = 0.06904273\n",
      "Iteration 329, loss = 0.06894872\n",
      "Iteration 330, loss = 0.06885559\n",
      "Iteration 331, loss = 0.06876331\n",
      "Iteration 332, loss = 0.06867188\n",
      "Iteration 333, loss = 0.06858128\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14020736\n",
      "Iteration 2, loss = 1.98579069\n",
      "Iteration 3, loss = 1.83207573\n",
      "Iteration 4, loss = 1.68609859\n",
      "Iteration 5, loss = 1.55370810\n",
      "Iteration 6, loss = 1.43765047\n",
      "Iteration 7, loss = 1.34013474\n",
      "Iteration 8, loss = 1.26192280\n",
      "Iteration 9, loss = 1.20281852\n",
      "Iteration 10, loss = 1.16160324\n",
      "Iteration 11, loss = 1.13454646\n",
      "Iteration 12, loss = 1.11867576\n",
      "Iteration 13, loss = 1.11074520\n",
      "Iteration 14, loss = 1.10714368\n",
      "Iteration 15, loss = 1.10515197\n",
      "Iteration 16, loss = 1.10224479\n",
      "Iteration 17, loss = 1.09657530\n",
      "Iteration 18, loss = 1.08740909\n",
      "Iteration 19, loss = 1.07465727\n",
      "Iteration 20, loss = 1.05858790\n",
      "Iteration 21, loss = 1.03971649\n",
      "Iteration 22, loss = 1.01873611\n",
      "Iteration 23, loss = 0.99639353\n",
      "Iteration 24, loss = 0.97356487\n",
      "Iteration 25, loss = 0.95129090\n",
      "Iteration 26, loss = 0.93059937\n",
      "Iteration 27, loss = 0.91208772\n",
      "Iteration 28, loss = 0.89590158\n",
      "Iteration 29, loss = 0.88216010\n",
      "Iteration 30, loss = 0.87089435\n",
      "Iteration 31, loss = 0.86194234\n",
      "Iteration 32, loss = 0.85479461\n",
      "Iteration 33, loss = 0.84886955\n",
      "Iteration 34, loss = 0.84352158\n",
      "Iteration 35, loss = 0.83838985\n",
      "Iteration 36, loss = 0.83325132\n",
      "Iteration 37, loss = 0.82802986\n",
      "Iteration 38, loss = 0.82275554\n",
      "Iteration 39, loss = 0.81753142\n",
      "Iteration 40, loss = 0.81247611\n",
      "Iteration 41, loss = 0.80767192\n",
      "Iteration 42, loss = 0.80312787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.79876668\n",
      "Iteration 44, loss = 0.79443826\n",
      "Iteration 45, loss = 0.78995453\n",
      "Iteration 46, loss = 0.78513394\n",
      "Iteration 47, loss = 0.77984188\n",
      "Iteration 48, loss = 0.77401617\n",
      "Iteration 49, loss = 0.76769958\n",
      "Iteration 50, loss = 0.76109923\n",
      "Iteration 51, loss = 0.75436147\n",
      "Iteration 52, loss = 0.74763807\n",
      "Iteration 53, loss = 0.74113745\n",
      "Iteration 54, loss = 0.73496571\n",
      "Iteration 55, loss = 0.72937687\n",
      "Iteration 56, loss = 0.72429757\n",
      "Iteration 57, loss = 0.71962383\n",
      "Iteration 58, loss = 0.71514973\n",
      "Iteration 59, loss = 0.71076918\n",
      "Iteration 60, loss = 0.70630838\n",
      "Iteration 61, loss = 0.70179122\n",
      "Iteration 62, loss = 0.69727601\n",
      "Iteration 63, loss = 0.69289913\n",
      "Iteration 64, loss = 0.68856901\n",
      "Iteration 65, loss = 0.68431235\n",
      "Iteration 66, loss = 0.68007641\n",
      "Iteration 67, loss = 0.67585675\n",
      "Iteration 68, loss = 0.67164621\n",
      "Iteration 69, loss = 0.66742518\n",
      "Iteration 70, loss = 0.66319939\n",
      "Iteration 71, loss = 0.65898676\n",
      "Iteration 72, loss = 0.65479795\n",
      "Iteration 73, loss = 0.65067090\n",
      "Iteration 74, loss = 0.64666319\n",
      "Iteration 75, loss = 0.64275955\n",
      "Iteration 76, loss = 0.63899844\n",
      "Iteration 77, loss = 0.63544214\n",
      "Iteration 78, loss = 0.63194214\n",
      "Iteration 79, loss = 0.62811263\n",
      "Iteration 80, loss = 0.62174726\n",
      "Iteration 81, loss = 0.61318254\n",
      "Iteration 82, loss = 0.60300574\n",
      "Iteration 83, loss = 0.59375822\n",
      "Iteration 84, loss = 0.58929003\n",
      "Iteration 85, loss = 0.58232255\n",
      "Iteration 86, loss = 0.57456168\n",
      "Iteration 87, loss = 0.56660280\n",
      "Iteration 88, loss = 0.55848846\n",
      "Iteration 89, loss = 0.55021717\n",
      "Iteration 90, loss = 0.54182765\n",
      "Iteration 91, loss = 0.53340223\n",
      "Iteration 92, loss = 0.52506047\n",
      "Iteration 93, loss = 0.51685497\n",
      "Iteration 94, loss = 0.50877189\n",
      "Iteration 95, loss = 0.50071530\n",
      "Iteration 96, loss = 0.49255034\n",
      "Iteration 97, loss = 0.48419167\n",
      "Iteration 98, loss = 0.47559695\n",
      "Iteration 99, loss = 0.46681131\n",
      "Iteration 100, loss = 0.45798376\n",
      "Iteration 101, loss = 0.44931346\n",
      "Iteration 102, loss = 0.44100571\n",
      "Iteration 103, loss = 0.43319204\n",
      "Iteration 104, loss = 0.42582539\n",
      "Iteration 105, loss = 0.41859925\n",
      "Iteration 106, loss = 0.41115244\n",
      "Iteration 107, loss = 0.40333072\n",
      "Iteration 108, loss = 0.39533841\n",
      "Iteration 109, loss = 0.38765811\n",
      "Iteration 110, loss = 0.38046452\n",
      "Iteration 111, loss = 0.37355337\n",
      "Iteration 112, loss = 0.36667590\n",
      "Iteration 113, loss = 0.35955623\n",
      "Iteration 114, loss = 0.35221754\n",
      "Iteration 115, loss = 0.34486349\n",
      "Iteration 116, loss = 0.33794409\n",
      "Iteration 117, loss = 0.33132195\n",
      "Iteration 118, loss = 0.32468151\n",
      "Iteration 119, loss = 0.31784192\n",
      "Iteration 120, loss = 0.31096629\n",
      "Iteration 121, loss = 0.30429256\n",
      "Iteration 122, loss = 0.29798939\n",
      "Iteration 123, loss = 0.29175468\n",
      "Iteration 124, loss = 0.28537693\n",
      "Iteration 125, loss = 0.27903768\n",
      "Iteration 126, loss = 0.27299414\n",
      "Iteration 127, loss = 0.26721005\n",
      "Iteration 128, loss = 0.26139137\n",
      "Iteration 129, loss = 0.25558939\n",
      "Iteration 130, loss = 0.24999950\n",
      "Iteration 131, loss = 0.24465306\n",
      "Iteration 132, loss = 0.23940145\n",
      "Iteration 133, loss = 0.23418830\n",
      "Iteration 134, loss = 0.22911078\n",
      "Iteration 135, loss = 0.22425247\n",
      "Iteration 136, loss = 0.21953835\n",
      "Iteration 137, loss = 0.21490201\n",
      "Iteration 138, loss = 0.21037554\n",
      "Iteration 139, loss = 0.20602226\n",
      "Iteration 140, loss = 0.20182752\n",
      "Iteration 141, loss = 0.19773137\n",
      "Iteration 142, loss = 0.19373145\n",
      "Iteration 143, loss = 0.18989078\n",
      "Iteration 144, loss = 0.18619461\n",
      "Iteration 145, loss = 0.18258938\n",
      "Iteration 146, loss = 0.17909257\n",
      "Iteration 147, loss = 0.17574204\n",
      "Iteration 148, loss = 0.17249996\n",
      "Iteration 149, loss = 0.16934832\n",
      "Iteration 150, loss = 0.16631564\n",
      "Iteration 151, loss = 0.16339389\n",
      "Iteration 152, loss = 0.16057755\n",
      "Iteration 153, loss = 0.15784260\n",
      "Iteration 154, loss = 0.15521174\n",
      "Iteration 155, loss = 0.15268605\n",
      "Iteration 156, loss = 0.15023814\n",
      "Iteration 157, loss = 0.14786173\n",
      "Iteration 158, loss = 0.14558408\n",
      "Iteration 159, loss = 0.14338910\n",
      "Iteration 160, loss = 0.14125936\n",
      "Iteration 161, loss = 0.13919955\n",
      "Iteration 162, loss = 0.13721861\n",
      "Iteration 163, loss = 0.13530767\n",
      "Iteration 164, loss = 0.13345375\n",
      "Iteration 165, loss = 0.13166085\n",
      "Iteration 166, loss = 0.12993708\n",
      "Iteration 167, loss = 0.12826897\n",
      "Iteration 168, loss = 0.12665012\n",
      "Iteration 169, loss = 0.12508709\n",
      "Iteration 170, loss = 0.12357940\n",
      "Iteration 171, loss = 0.12211788\n",
      "Iteration 172, loss = 0.12069989\n",
      "Iteration 173, loss = 0.11932958\n",
      "Iteration 174, loss = 0.11800495\n",
      "Iteration 175, loss = 0.11671944\n",
      "Iteration 176, loss = 0.11547217\n",
      "Iteration 177, loss = 0.11426533\n",
      "Iteration 178, loss = 0.11309629\n",
      "Iteration 179, loss = 0.11196099\n",
      "Iteration 180, loss = 0.11085946\n",
      "Iteration 181, loss = 0.10979278\n",
      "Iteration 182, loss = 0.10875884\n",
      "Iteration 183, loss = 0.10775389\n",
      "Iteration 184, loss = 0.10678029\n",
      "Iteration 185, loss = 0.10583575\n",
      "Iteration 186, loss = 0.10491663\n",
      "Iteration 187, loss = 0.10402340\n",
      "Iteration 188, loss = 0.10315776\n",
      "Iteration 189, loss = 0.10231559\n",
      "Iteration 190, loss = 0.10149592\n",
      "Iteration 191, loss = 0.10069925\n",
      "Iteration 192, loss = 0.09992456\n",
      "Iteration 193, loss = 0.09916999\n",
      "Iteration 194, loss = 0.09843517\n",
      "Iteration 195, loss = 0.09772017\n",
      "Iteration 196, loss = 0.09702382\n",
      "Iteration 197, loss = 0.09634501\n",
      "Iteration 198, loss = 0.09568352\n",
      "Iteration 199, loss = 0.09503909\n",
      "Iteration 200, loss = 0.09441068\n",
      "Iteration 201, loss = 0.09379755\n",
      "Iteration 202, loss = 0.09319958\n",
      "Iteration 203, loss = 0.09261632\n",
      "Iteration 204, loss = 0.09204691\n",
      "Iteration 205, loss = 0.09149094\n",
      "Iteration 206, loss = 0.09094825\n",
      "Iteration 207, loss = 0.09041829\n",
      "Iteration 208, loss = 0.08990046\n",
      "Iteration 209, loss = 0.08939450\n",
      "Iteration 210, loss = 0.08890016\n",
      "Iteration 211, loss = 0.08841692\n",
      "Iteration 212, loss = 0.08794438\n",
      "Iteration 213, loss = 0.08748233\n",
      "Iteration 214, loss = 0.08703046\n",
      "Iteration 215, loss = 0.08658836\n",
      "Iteration 216, loss = 0.08615576\n",
      "Iteration 217, loss = 0.08573252\n",
      "Iteration 218, loss = 0.08531825\n",
      "Iteration 219, loss = 0.08491262\n",
      "Iteration 220, loss = 0.08451543\n",
      "Iteration 221, loss = 0.08412698\n",
      "Iteration 222, loss = 0.08374671\n",
      "Iteration 223, loss = 0.08337454\n",
      "Iteration 224, loss = 0.08301040\n",
      "Iteration 225, loss = 0.08265362\n",
      "Iteration 226, loss = 0.08230379\n",
      "Iteration 227, loss = 0.08196278\n",
      "Iteration 228, loss = 0.08162685\n",
      "Iteration 229, loss = 0.08129791\n",
      "Iteration 230, loss = 0.08097610\n",
      "Iteration 231, loss = 0.08065948\n",
      "Iteration 232, loss = 0.08034821\n",
      "Iteration 233, loss = 0.08004355\n",
      "Iteration 234, loss = 0.07974470\n",
      "Iteration 235, loss = 0.07945063\n",
      "Iteration 236, loss = 0.07916200\n",
      "Iteration 237, loss = 0.07887919\n",
      "Iteration 238, loss = 0.07860124\n",
      "Iteration 239, loss = 0.07832787\n",
      "Iteration 240, loss = 0.07805968\n",
      "Iteration 241, loss = 0.07779644\n",
      "Iteration 242, loss = 0.07753746\n",
      "Iteration 243, loss = 0.07728293\n",
      "Iteration 244, loss = 0.07703308\n",
      "Iteration 245, loss = 0.07678744\n",
      "Iteration 246, loss = 0.07654575\n",
      "Iteration 247, loss = 0.07630824\n",
      "Iteration 248, loss = 0.07607484\n",
      "Iteration 249, loss = 0.07584516\n",
      "Iteration 250, loss = 0.07561920\n",
      "Iteration 251, loss = 0.07539714\n",
      "Iteration 252, loss = 0.07517849\n",
      "Iteration 253, loss = 0.07496361\n",
      "Iteration 254, loss = 0.07475181\n",
      "Iteration 255, loss = 0.07454334\n",
      "Iteration 256, loss = 0.07433842\n",
      "Iteration 257, loss = 0.07413661\n",
      "Iteration 258, loss = 0.07393770\n",
      "Iteration 259, loss = 0.07374200\n",
      "Iteration 260, loss = 0.07354938\n",
      "Iteration 261, loss = 0.07335950\n",
      "Iteration 262, loss = 0.07317244\n",
      "Iteration 263, loss = 0.07298830\n",
      "Iteration 264, loss = 0.07280683\n",
      "Iteration 265, loss = 0.07262792\n",
      "Iteration 266, loss = 0.07245169\n",
      "Iteration 267, loss = 0.07227804\n",
      "Iteration 268, loss = 0.07210679\n",
      "Iteration 269, loss = 0.07193798\n",
      "Iteration 270, loss = 0.07177162\n",
      "Iteration 271, loss = 0.07160756\n",
      "Iteration 272, loss = 0.07144574\n",
      "Iteration 273, loss = 0.07128620\n",
      "Iteration 274, loss = 0.07112886\n",
      "Iteration 275, loss = 0.07097363\n",
      "Iteration 276, loss = 0.07082051\n",
      "Iteration 277, loss = 0.07066948\n",
      "Iteration 278, loss = 0.07052045\n",
      "Iteration 279, loss = 0.07037338\n",
      "Iteration 280, loss = 0.07022827\n",
      "Iteration 281, loss = 0.07008508\n",
      "Iteration 282, loss = 0.06994372\n",
      "Iteration 283, loss = 0.06980420\n",
      "Iteration 284, loss = 0.06966649\n",
      "Iteration 285, loss = 0.06953052\n",
      "Iteration 286, loss = 0.06939628\n",
      "Iteration 287, loss = 0.06926374\n",
      "Iteration 288, loss = 0.06913286\n",
      "Iteration 289, loss = 0.06900360\n",
      "Iteration 290, loss = 0.06887595\n",
      "Iteration 291, loss = 0.06874987\n",
      "Iteration 292, loss = 0.06862533\n",
      "Iteration 293, loss = 0.06850231\n",
      "Iteration 294, loss = 0.06838077\n",
      "Iteration 295, loss = 0.06826071\n",
      "Iteration 296, loss = 0.06814207\n",
      "Iteration 297, loss = 0.06802484\n",
      "Iteration 298, loss = 0.06790900\n",
      "Iteration 299, loss = 0.06779452\n",
      "Iteration 300, loss = 0.06768137\n",
      "Iteration 301, loss = 0.06756953\n",
      "Iteration 302, loss = 0.06745899\n",
      "Iteration 303, loss = 0.06734971\n",
      "Iteration 304, loss = 0.06724168\n",
      "Iteration 305, loss = 0.06713487\n",
      "Iteration 306, loss = 0.06702926\n",
      "Iteration 307, loss = 0.06692484\n",
      "Iteration 308, loss = 0.06682158\n",
      "Iteration 309, loss = 0.06671946\n",
      "Iteration 310, loss = 0.06661847\n",
      "Iteration 311, loss = 0.06651859\n",
      "Iteration 312, loss = 0.06641979\n",
      "Iteration 313, loss = 0.06632206\n",
      "Iteration 314, loss = 0.06622539\n",
      "Iteration 315, loss = 0.06612975\n",
      "Iteration 316, loss = 0.06603512\n",
      "Iteration 317, loss = 0.06594150\n",
      "Iteration 318, loss = 0.06584887\n",
      "Iteration 319, loss = 0.06575720\n",
      "Iteration 320, loss = 0.06566649\n",
      "Iteration 321, loss = 0.06557672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13352491\n",
      "Iteration 2, loss = 1.97895502\n",
      "Iteration 3, loss = 1.82725676\n",
      "Iteration 4, loss = 1.68179794\n",
      "Iteration 5, loss = 1.54976855\n",
      "Iteration 6, loss = 1.43403445\n",
      "Iteration 7, loss = 1.33659401\n",
      "Iteration 8, loss = 1.25825685\n",
      "Iteration 9, loss = 1.19907758\n",
      "Iteration 10, loss = 1.15768254\n",
      "Iteration 11, loss = 1.13051381\n",
      "Iteration 12, loss = 1.11475805\n",
      "Iteration 13, loss = 1.10669938\n",
      "Iteration 14, loss = 1.10314837\n",
      "Iteration 15, loss = 1.10085469\n",
      "Iteration 16, loss = 1.09749786\n",
      "Iteration 17, loss = 1.09146516\n",
      "Iteration 18, loss = 1.08206513\n",
      "Iteration 19, loss = 1.06926293\n",
      "Iteration 20, loss = 1.05330721\n",
      "Iteration 21, loss = 1.03466398\n",
      "Iteration 22, loss = 1.01393773\n",
      "Iteration 23, loss = 0.99182314\n",
      "Iteration 24, loss = 0.96914433\n",
      "Iteration 25, loss = 0.94678474\n",
      "Iteration 26, loss = 0.92572604\n",
      "Iteration 27, loss = 0.90650360\n",
      "Iteration 28, loss = 0.88949463\n",
      "Iteration 29, loss = 0.87514890\n",
      "Iteration 30, loss = 0.86337794\n",
      "Iteration 31, loss = 0.85392405\n",
      "Iteration 32, loss = 0.84640835\n",
      "Iteration 33, loss = 0.84025520\n",
      "Iteration 34, loss = 0.83482940\n",
      "Iteration 35, loss = 0.82976360\n",
      "Iteration 36, loss = 0.82472459\n",
      "Iteration 37, loss = 0.81958049\n",
      "Iteration 38, loss = 0.81434122\n",
      "Iteration 39, loss = 0.80909078\n",
      "Iteration 40, loss = 0.80394312\n",
      "Iteration 41, loss = 0.79898651\n",
      "Iteration 42, loss = 0.79424376\n",
      "Iteration 43, loss = 0.78965522\n",
      "Iteration 44, loss = 0.78508803\n",
      "Iteration 45, loss = 0.78036743\n",
      "Iteration 46, loss = 0.77538389\n",
      "Iteration 47, loss = 0.76995516\n",
      "Iteration 48, loss = 0.76400909\n",
      "Iteration 49, loss = 0.75768221\n",
      "Iteration 50, loss = 0.75102345\n",
      "Iteration 51, loss = 0.74420378\n",
      "Iteration 52, loss = 0.73750547\n",
      "Iteration 53, loss = 0.73089891\n",
      "Iteration 54, loss = 0.72466489\n",
      "Iteration 55, loss = 0.71910042\n",
      "Iteration 56, loss = 0.71401231\n",
      "Iteration 57, loss = 0.70943867\n",
      "Iteration 58, loss = 0.70504635\n",
      "Iteration 59, loss = 0.70074743\n",
      "Iteration 60, loss = 0.69646396\n",
      "Iteration 61, loss = 0.69214425\n",
      "Iteration 62, loss = 0.68776415\n",
      "Iteration 63, loss = 0.68333612\n",
      "Iteration 64, loss = 0.67893017\n",
      "Iteration 65, loss = 0.67455646\n",
      "Iteration 66, loss = 0.67023992\n",
      "Iteration 67, loss = 0.66600634\n",
      "Iteration 68, loss = 0.66182016\n",
      "Iteration 69, loss = 0.65764713\n",
      "Iteration 70, loss = 0.65347841\n",
      "Iteration 71, loss = 0.64933903\n",
      "Iteration 72, loss = 0.64523938\n",
      "Iteration 73, loss = 0.64119529\n",
      "Iteration 74, loss = 0.63721551\n",
      "Iteration 75, loss = 0.63332813\n",
      "Iteration 76, loss = 0.62954338\n",
      "Iteration 77, loss = 0.62584676\n",
      "Iteration 78, loss = 0.62205739\n",
      "Iteration 79, loss = 0.61596164\n",
      "Iteration 80, loss = 0.60764607\n",
      "Iteration 81, loss = 0.59755399\n",
      "Iteration 82, loss = 0.58706172\n",
      "Iteration 83, loss = 0.58298343\n",
      "Iteration 84, loss = 0.57586162\n",
      "Iteration 85, loss = 0.56785692\n",
      "Iteration 86, loss = 0.55978298\n",
      "Iteration 87, loss = 0.55158633\n",
      "Iteration 88, loss = 0.54316718\n",
      "Iteration 89, loss = 0.53452206\n",
      "Iteration 90, loss = 0.52579207\n",
      "Iteration 91, loss = 0.51719071\n",
      "Iteration 92, loss = 0.50879387\n",
      "Iteration 93, loss = 0.50055661\n",
      "Iteration 94, loss = 0.49236481\n",
      "Iteration 95, loss = 0.48403705\n",
      "Iteration 96, loss = 0.47543482\n",
      "Iteration 97, loss = 0.46651425\n",
      "Iteration 98, loss = 0.45737457\n",
      "Iteration 99, loss = 0.44821547\n",
      "Iteration 100, loss = 0.43925928\n",
      "Iteration 101, loss = 0.43067827\n",
      "Iteration 102, loss = 0.42253450\n",
      "Iteration 103, loss = 0.41475326\n",
      "Iteration 104, loss = 0.40712680\n",
      "Iteration 105, loss = 0.39944844\n",
      "Iteration 106, loss = 0.39151062\n",
      "Iteration 107, loss = 0.38328048\n",
      "Iteration 108, loss = 0.37499400\n",
      "Iteration 109, loss = 0.36699875\n",
      "Iteration 110, loss = 0.35946824\n",
      "Iteration 111, loss = 0.35223051\n",
      "Iteration 112, loss = 0.34501140\n",
      "Iteration 113, loss = 0.33757772\n",
      "Iteration 114, loss = 0.32992682\n",
      "Iteration 115, loss = 0.32247926\n",
      "Iteration 116, loss = 0.31529492\n",
      "Iteration 117, loss = 0.30831998\n",
      "Iteration 118, loss = 0.30127319\n",
      "Iteration 119, loss = 0.29408239\n",
      "Iteration 120, loss = 0.28700531\n",
      "Iteration 121, loss = 0.28015705\n",
      "Iteration 122, loss = 0.27358177\n",
      "Iteration 123, loss = 0.26707205\n",
      "Iteration 124, loss = 0.26050648\n",
      "Iteration 125, loss = 0.25405311\n",
      "Iteration 126, loss = 0.24787505\n",
      "Iteration 127, loss = 0.24190527\n",
      "Iteration 128, loss = 0.23600017\n",
      "Iteration 129, loss = 0.23014534\n",
      "Iteration 130, loss = 0.22445478\n",
      "Iteration 131, loss = 0.21902511\n",
      "Iteration 132, loss = 0.21376044\n",
      "Iteration 133, loss = 0.20856279\n",
      "Iteration 134, loss = 0.20347724\n",
      "Iteration 135, loss = 0.19860348\n",
      "Iteration 136, loss = 0.19391577\n",
      "Iteration 137, loss = 0.18934392\n",
      "Iteration 138, loss = 0.18487278\n",
      "Iteration 139, loss = 0.18055069\n",
      "Iteration 140, loss = 0.17640246\n",
      "Iteration 141, loss = 0.17239002\n",
      "Iteration 142, loss = 0.16847792\n",
      "Iteration 143, loss = 0.16468506\n",
      "Iteration 144, loss = 0.16105298\n",
      "Iteration 145, loss = 0.15755343\n",
      "Iteration 146, loss = 0.15415180\n",
      "Iteration 147, loss = 0.15086422\n",
      "Iteration 148, loss = 0.14771124\n",
      "Iteration 149, loss = 0.14467744\n",
      "Iteration 150, loss = 0.14173625\n",
      "Iteration 151, loss = 0.13890618\n",
      "Iteration 152, loss = 0.13619065\n",
      "Iteration 153, loss = 0.13356920\n",
      "Iteration 154, loss = 0.13103832\n",
      "Iteration 155, loss = 0.12860009\n",
      "Iteration 156, loss = 0.12625332\n",
      "Iteration 157, loss = 0.12399389\n",
      "Iteration 158, loss = 0.12181510\n",
      "Iteration 159, loss = 0.11971409\n",
      "Iteration 160, loss = 0.11768747\n",
      "Iteration 161, loss = 0.11573202\n",
      "Iteration 162, loss = 0.11384437\n",
      "Iteration 163, loss = 0.11202354\n",
      "Iteration 164, loss = 0.11026658\n",
      "Iteration 165, loss = 0.10857128\n",
      "Iteration 166, loss = 0.10693324\n",
      "Iteration 167, loss = 0.10535023\n",
      "Iteration 168, loss = 0.10382149\n",
      "Iteration 169, loss = 0.10234467\n",
      "Iteration 170, loss = 0.10091666\n",
      "Iteration 171, loss = 0.09953512\n",
      "Iteration 172, loss = 0.09819934\n",
      "Iteration 173, loss = 0.09690810\n",
      "Iteration 174, loss = 0.09565843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 175, loss = 0.09444850\n",
      "Iteration 176, loss = 0.09327687\n",
      "Iteration 177, loss = 0.09214260\n",
      "Iteration 178, loss = 0.09104386\n",
      "Iteration 179, loss = 0.08997869\n",
      "Iteration 180, loss = 0.08894599\n",
      "Iteration 181, loss = 0.08794476\n",
      "Iteration 182, loss = 0.08697361\n",
      "Iteration 183, loss = 0.08603114\n",
      "Iteration 184, loss = 0.08511634\n",
      "Iteration 185, loss = 0.08422836\n",
      "Iteration 186, loss = 0.08336667\n",
      "Iteration 187, loss = 0.08252999\n",
      "Iteration 188, loss = 0.08171725\n",
      "Iteration 189, loss = 0.08092777\n",
      "Iteration 190, loss = 0.08016114\n",
      "Iteration 191, loss = 0.07941645\n",
      "Iteration 192, loss = 0.07869222\n",
      "Iteration 193, loss = 0.07798760\n",
      "Iteration 194, loss = 0.07730189\n",
      "Iteration 195, loss = 0.07663447\n",
      "Iteration 196, loss = 0.07598467\n",
      "Iteration 197, loss = 0.07535183\n",
      "Iteration 198, loss = 0.07473530\n",
      "Iteration 199, loss = 0.07413455\n",
      "Iteration 200, loss = 0.07354904\n",
      "Iteration 201, loss = 0.07297819\n",
      "Iteration 202, loss = 0.07242147\n",
      "Iteration 203, loss = 0.07187845\n",
      "Iteration 204, loss = 0.07134869\n",
      "Iteration 205, loss = 0.07083197\n",
      "Iteration 206, loss = 0.07032759\n",
      "Iteration 207, loss = 0.06983515\n",
      "Iteration 208, loss = 0.06935427\n",
      "Iteration 209, loss = 0.06888456\n",
      "Iteration 210, loss = 0.06842572\n",
      "Iteration 211, loss = 0.06797742\n",
      "Iteration 212, loss = 0.06753926\n",
      "Iteration 213, loss = 0.06711091\n",
      "Iteration 214, loss = 0.06669205\n",
      "Iteration 215, loss = 0.06628239\n",
      "Iteration 216, loss = 0.06588166\n",
      "Iteration 217, loss = 0.06548958\n",
      "Iteration 218, loss = 0.06510588\n",
      "Iteration 219, loss = 0.06473032\n",
      "Iteration 220, loss = 0.06436266\n",
      "Iteration 221, loss = 0.06400266\n",
      "Iteration 222, loss = 0.06365008\n",
      "Iteration 223, loss = 0.06330472\n",
      "Iteration 224, loss = 0.06296637\n",
      "Iteration 225, loss = 0.06263482\n",
      "Iteration 226, loss = 0.06230988\n",
      "Iteration 227, loss = 0.06199137\n",
      "Iteration 228, loss = 0.06167911\n",
      "Iteration 229, loss = 0.06137292\n",
      "Iteration 230, loss = 0.06107377\n",
      "Iteration 231, loss = 0.06077965\n",
      "Iteration 232, loss = 0.06049193\n",
      "Iteration 233, loss = 0.06021062\n",
      "Iteration 234, loss = 0.05993307\n",
      "Iteration 235, loss = 0.05966124\n",
      "Iteration 236, loss = 0.05939494\n",
      "Iteration 237, loss = 0.05913311\n",
      "Iteration 238, loss = 0.05887553\n",
      "Iteration 239, loss = 0.05862283\n",
      "Iteration 240, loss = 0.05837505\n",
      "Iteration 241, loss = 0.05813151\n",
      "Iteration 242, loss = 0.05789189\n",
      "Iteration 243, loss = 0.05765654\n",
      "Iteration 244, loss = 0.05742556\n",
      "Iteration 245, loss = 0.05719853\n",
      "Iteration 246, loss = 0.05697511\n",
      "Iteration 247, loss = 0.05675549\n",
      "Iteration 248, loss = 0.05653976\n",
      "Iteration 249, loss = 0.05632765\n",
      "Iteration 250, loss = 0.05611889\n",
      "Iteration 251, loss = 0.05591353\n",
      "Iteration 252, loss = 0.05571176\n",
      "Iteration 253, loss = 0.05551316\n",
      "Iteration 254, loss = 0.05531760\n",
      "Iteration 255, loss = 0.05512547\n",
      "Iteration 256, loss = 0.05493630\n",
      "Iteration 257, loss = 0.05474990\n",
      "Iteration 258, loss = 0.05456663\n",
      "Iteration 259, loss = 0.05438621\n",
      "Iteration 260, loss = 0.05420853\n",
      "Iteration 261, loss = 0.05403355\n",
      "Iteration 262, loss = 0.05386130\n",
      "Iteration 263, loss = 0.05369167\n",
      "Iteration 264, loss = 0.05352455\n",
      "Iteration 265, loss = 0.05335992\n",
      "Iteration 266, loss = 0.05319775\n",
      "Iteration 267, loss = 0.05303799\n",
      "Iteration 268, loss = 0.05288055\n",
      "Iteration 269, loss = 0.05272539\n",
      "Iteration 270, loss = 0.05257248\n",
      "Iteration 271, loss = 0.05242179\n",
      "Iteration 272, loss = 0.05227336\n",
      "Iteration 273, loss = 0.05212706\n",
      "Iteration 274, loss = 0.05198283\n",
      "Iteration 275, loss = 0.05184063\n",
      "Iteration 276, loss = 0.05170041\n",
      "Iteration 277, loss = 0.05156214\n",
      "Iteration 278, loss = 0.05142577\n",
      "Iteration 279, loss = 0.05129127\n",
      "Iteration 280, loss = 0.05115860\n",
      "Iteration 281, loss = 0.05102772\n",
      "Iteration 282, loss = 0.05089860\n",
      "Iteration 283, loss = 0.05077120\n",
      "Iteration 284, loss = 0.05064549\n",
      "Iteration 285, loss = 0.05052144\n",
      "Iteration 286, loss = 0.05039901\n",
      "Iteration 287, loss = 0.05027817\n",
      "Iteration 288, loss = 0.05015889\n",
      "Iteration 289, loss = 0.05004115\n",
      "Iteration 290, loss = 0.04992491\n",
      "Iteration 291, loss = 0.04981015\n",
      "Iteration 292, loss = 0.04969683\n",
      "Iteration 293, loss = 0.04958494\n",
      "Iteration 294, loss = 0.04947444\n",
      "Iteration 295, loss = 0.04936530\n",
      "Iteration 296, loss = 0.04925752\n",
      "Iteration 297, loss = 0.04915105\n",
      "Iteration 298, loss = 0.04904588\n",
      "Iteration 299, loss = 0.04894199\n",
      "Iteration 300, loss = 0.04883934\n",
      "Iteration 301, loss = 0.04873793\n",
      "Iteration 302, loss = 0.04863772\n",
      "Iteration 303, loss = 0.04853870\n",
      "Iteration 304, loss = 0.04844084\n",
      "Iteration 305, loss = 0.04834413\n",
      "Iteration 306, loss = 0.04824855\n",
      "Iteration 307, loss = 0.04815407\n",
      "Iteration 308, loss = 0.04806069\n",
      "Iteration 309, loss = 0.04796837\n",
      "Iteration 310, loss = 0.04787711\n",
      "Iteration 311, loss = 0.04778688\n",
      "Iteration 312, loss = 0.04769767\n",
      "Iteration 313, loss = 0.04760947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13108299\n",
      "Iteration 2, loss = 1.97680372\n",
      "Iteration 3, loss = 1.82465970\n",
      "Iteration 4, loss = 1.67890696\n",
      "Iteration 5, loss = 1.54674025\n",
      "Iteration 6, loss = 1.43088330\n",
      "Iteration 7, loss = 1.33337393\n",
      "Iteration 8, loss = 1.25498217\n",
      "Iteration 9, loss = 1.19569649\n",
      "Iteration 10, loss = 1.15415827\n",
      "Iteration 11, loss = 1.12707226\n",
      "Iteration 12, loss = 1.11165280\n",
      "Iteration 13, loss = 1.10387726\n",
      "Iteration 14, loss = 1.10090300\n",
      "Iteration 15, loss = 1.09929375\n",
      "Iteration 16, loss = 1.09628312\n",
      "Iteration 17, loss = 1.09037219\n",
      "Iteration 18, loss = 1.08093986\n",
      "Iteration 19, loss = 1.06796847\n",
      "Iteration 20, loss = 1.05171307\n",
      "Iteration 21, loss = 1.03269688\n",
      "Iteration 22, loss = 1.01160328\n",
      "Iteration 23, loss = 0.98916775\n",
      "Iteration 24, loss = 0.96622002\n",
      "Iteration 25, loss = 0.94368215\n",
      "Iteration 26, loss = 0.92257894\n",
      "Iteration 27, loss = 0.90347951\n",
      "Iteration 28, loss = 0.88689650\n",
      "Iteration 29, loss = 0.87296602\n",
      "Iteration 30, loss = 0.86171861\n",
      "Iteration 31, loss = 0.85270384\n",
      "Iteration 32, loss = 0.84568773\n",
      "Iteration 33, loss = 0.83985878\n",
      "Iteration 34, loss = 0.83460897\n",
      "Iteration 35, loss = 0.82958830\n",
      "Iteration 36, loss = 0.82449854\n",
      "Iteration 37, loss = 0.81926699\n",
      "Iteration 38, loss = 0.81394991\n",
      "Iteration 39, loss = 0.80865870\n",
      "Iteration 40, loss = 0.80352096\n",
      "Iteration 41, loss = 0.79862054\n",
      "Iteration 42, loss = 0.79396061\n",
      "Iteration 43, loss = 0.78945338\n",
      "Iteration 44, loss = 0.78493796\n",
      "Iteration 45, loss = 0.78024073\n",
      "Iteration 46, loss = 0.77521694\n",
      "Iteration 47, loss = 0.76969507\n",
      "Iteration 48, loss = 0.76364808\n",
      "Iteration 49, loss = 0.75727048\n",
      "Iteration 50, loss = 0.75055514\n",
      "Iteration 51, loss = 0.74382115\n",
      "Iteration 52, loss = 0.73708976\n",
      "Iteration 53, loss = 0.73048735\n",
      "Iteration 54, loss = 0.72445021\n",
      "Iteration 55, loss = 0.71905158\n",
      "Iteration 56, loss = 0.71420315\n",
      "Iteration 57, loss = 0.70970816\n",
      "Iteration 58, loss = 0.70541030\n",
      "Iteration 59, loss = 0.70118451\n",
      "Iteration 60, loss = 0.69690226\n",
      "Iteration 61, loss = 0.69257846\n",
      "Iteration 62, loss = 0.68814661\n",
      "Iteration 63, loss = 0.68368361\n",
      "Iteration 64, loss = 0.67925451\n",
      "Iteration 65, loss = 0.67488309\n",
      "Iteration 66, loss = 0.67056790\n",
      "Iteration 67, loss = 0.66631104\n",
      "Iteration 68, loss = 0.66208322\n",
      "Iteration 69, loss = 0.65786351\n",
      "Iteration 70, loss = 0.65365514\n",
      "Iteration 71, loss = 0.64950297\n",
      "Iteration 72, loss = 0.64538665\n",
      "Iteration 73, loss = 0.64134230\n",
      "Iteration 74, loss = 0.63739665\n",
      "Iteration 75, loss = 0.63354259\n",
      "Iteration 76, loss = 0.62979465\n",
      "Iteration 77, loss = 0.62613363\n",
      "Iteration 78, loss = 0.62234849\n",
      "Iteration 79, loss = 0.61648745\n",
      "Iteration 80, loss = 0.60820067\n",
      "Iteration 81, loss = 0.59811283\n",
      "Iteration 82, loss = 0.58751724\n",
      "Iteration 83, loss = 0.58305349\n",
      "Iteration 84, loss = 0.57581076\n",
      "Iteration 85, loss = 0.56797709\n",
      "Iteration 86, loss = 0.56014767\n",
      "Iteration 87, loss = 0.55213810\n",
      "Iteration 88, loss = 0.54382348\n",
      "Iteration 89, loss = 0.53525923\n",
      "Iteration 90, loss = 0.52665637\n",
      "Iteration 91, loss = 0.51824411\n",
      "Iteration 92, loss = 0.51006451\n",
      "Iteration 93, loss = 0.50202424\n",
      "Iteration 94, loss = 0.49393287\n",
      "Iteration 95, loss = 0.48560052\n",
      "Iteration 96, loss = 0.47689655\n",
      "Iteration 97, loss = 0.46787226\n",
      "Iteration 98, loss = 0.45869526\n",
      "Iteration 99, loss = 0.44959848\n",
      "Iteration 100, loss = 0.44081126\n",
      "Iteration 101, loss = 0.43246894\n",
      "Iteration 102, loss = 0.42456292\n",
      "Iteration 103, loss = 0.41696588\n",
      "Iteration 104, loss = 0.40945264\n",
      "Iteration 105, loss = 0.40178115\n",
      "Iteration 106, loss = 0.39384033\n",
      "Iteration 107, loss = 0.38568635\n",
      "Iteration 108, loss = 0.37754993\n",
      "Iteration 109, loss = 0.36972653\n",
      "Iteration 110, loss = 0.36231820\n",
      "Iteration 111, loss = 0.35519581\n",
      "Iteration 112, loss = 0.34810131\n",
      "Iteration 113, loss = 0.34080780\n",
      "Iteration 114, loss = 0.33329876\n",
      "Iteration 115, loss = 0.32587051\n",
      "Iteration 116, loss = 0.31876955\n",
      "Iteration 117, loss = 0.31184321\n",
      "Iteration 118, loss = 0.30484478\n",
      "Iteration 119, loss = 0.29775716\n",
      "Iteration 120, loss = 0.29073032\n",
      "Iteration 121, loss = 0.28402206\n",
      "Iteration 122, loss = 0.27751486\n",
      "Iteration 123, loss = 0.27101723\n",
      "Iteration 124, loss = 0.26450415\n",
      "Iteration 125, loss = 0.25813548\n",
      "Iteration 126, loss = 0.25198948\n",
      "Iteration 127, loss = 0.24603657\n",
      "Iteration 128, loss = 0.24014335\n",
      "Iteration 129, loss = 0.23431063\n",
      "Iteration 130, loss = 0.22866134\n",
      "Iteration 131, loss = 0.22323390\n",
      "Iteration 132, loss = 0.21794493\n",
      "Iteration 133, loss = 0.21273274\n",
      "Iteration 134, loss = 0.20764529\n",
      "Iteration 135, loss = 0.20275982\n",
      "Iteration 136, loss = 0.19803253\n",
      "Iteration 137, loss = 0.19341210\n",
      "Iteration 138, loss = 0.18890297\n",
      "Iteration 139, loss = 0.18454554\n",
      "Iteration 140, loss = 0.18034559\n",
      "Iteration 141, loss = 0.17626591\n",
      "Iteration 142, loss = 0.17228979\n",
      "Iteration 143, loss = 0.16845222\n",
      "Iteration 144, loss = 0.16476482\n",
      "Iteration 145, loss = 0.16118441\n",
      "Iteration 146, loss = 0.15770790\n",
      "Iteration 147, loss = 0.15436219\n",
      "Iteration 148, loss = 0.15114273\n",
      "Iteration 149, loss = 0.14802096\n",
      "Iteration 150, loss = 0.14499112\n",
      "Iteration 151, loss = 0.14207016\n",
      "Iteration 152, loss = 0.13927448\n",
      "Iteration 153, loss = 0.13655097\n",
      "Iteration 154, loss = 0.13391040\n",
      "Iteration 155, loss = 0.13138827\n",
      "Iteration 156, loss = 0.12894333\n",
      "Iteration 157, loss = 0.12656653\n",
      "Iteration 158, loss = 0.12428156\n",
      "Iteration 159, loss = 0.12208047\n",
      "Iteration 160, loss = 0.11994095\n",
      "Iteration 161, loss = 0.11786849\n",
      "Iteration 162, loss = 0.11587240\n",
      "Iteration 163, loss = 0.11394431\n",
      "Iteration 164, loss = 0.11207366\n",
      "Iteration 165, loss = 0.11026122\n",
      "Iteration 166, loss = 0.10851214\n",
      "Iteration 167, loss = 0.10682098\n",
      "Iteration 168, loss = 0.10517883\n",
      "Iteration 169, loss = 0.10358591\n",
      "Iteration 170, loss = 0.10204556\n",
      "Iteration 171, loss = 0.10055400\n",
      "Iteration 172, loss = 0.09910545\n",
      "Iteration 173, loss = 0.09769940\n",
      "Iteration 174, loss = 0.09633752\n",
      "Iteration 175, loss = 0.09501695\n",
      "Iteration 176, loss = 0.09373327\n",
      "Iteration 177, loss = 0.09248611\n",
      "Iteration 178, loss = 0.09127618\n",
      "Iteration 179, loss = 0.09010125\n",
      "Iteration 180, loss = 0.08895814\n",
      "Iteration 181, loss = 0.08784647\n",
      "Iteration 182, loss = 0.08676684\n",
      "Iteration 183, loss = 0.08571729\n",
      "Iteration 184, loss = 0.08469529\n",
      "Iteration 185, loss = 0.08370041\n",
      "Iteration 186, loss = 0.08273269\n",
      "Iteration 187, loss = 0.08179062\n",
      "Iteration 188, loss = 0.08087269\n",
      "Iteration 189, loss = 0.07997847\n",
      "Iteration 190, loss = 0.07910868\n",
      "Iteration 191, loss = 0.07826080\n",
      "Iteration 192, loss = 0.07743369\n",
      "Iteration 193, loss = 0.07662741\n",
      "Iteration 194, loss = 0.07584125\n",
      "Iteration 195, loss = 0.07507389\n",
      "Iteration 196, loss = 0.07432469\n",
      "Iteration 197, loss = 0.07359355\n",
      "Iteration 198, loss = 0.07287979\n",
      "Iteration 199, loss = 0.07218241\n",
      "Iteration 200, loss = 0.07150094\n",
      "Iteration 201, loss = 0.07083518\n",
      "Iteration 202, loss = 0.07018454\n",
      "Iteration 203, loss = 0.06954833\n",
      "Iteration 204, loss = 0.06892614\n",
      "Iteration 205, loss = 0.06831770\n",
      "Iteration 206, loss = 0.06772249\n",
      "Iteration 207, loss = 0.06713993\n",
      "Iteration 208, loss = 0.06656970\n",
      "Iteration 209, loss = 0.06601156\n",
      "Iteration 210, loss = 0.06546505\n",
      "Iteration 211, loss = 0.06492974\n",
      "Iteration 212, loss = 0.06440551\n",
      "Iteration 213, loss = 0.06389409\n",
      "Iteration 214, loss = 0.06339355\n",
      "Iteration 215, loss = 0.06290397\n",
      "Iteration 216, loss = 0.06242344\n",
      "Iteration 217, loss = 0.06195152\n",
      "Iteration 218, loss = 0.06148926\n",
      "Iteration 219, loss = 0.06103642\n",
      "Iteration 220, loss = 0.06059172\n",
      "Iteration 221, loss = 0.06015494\n",
      "Iteration 222, loss = 0.05972726\n",
      "Iteration 223, loss = 0.05930813\n",
      "Iteration 224, loss = 0.05889578\n",
      "Iteration 225, loss = 0.05849043\n",
      "Iteration 226, loss = 0.05809309\n",
      "Iteration 227, loss = 0.05770311\n",
      "Iteration 228, loss = 0.05731938\n",
      "Iteration 229, loss = 0.05694221\n",
      "Iteration 230, loss = 0.05657211\n",
      "Iteration 231, loss = 0.05620842\n",
      "Iteration 232, loss = 0.05585044\n",
      "Iteration 233, loss = 0.05549903\n",
      "Iteration 234, loss = 0.05515360\n",
      "Iteration 235, loss = 0.05481360\n",
      "Iteration 236, loss = 0.05447915\n",
      "Iteration 237, loss = 0.05415038\n",
      "Iteration 238, loss = 0.05382691\n",
      "Iteration 239, loss = 0.05350867\n",
      "Iteration 240, loss = 0.05319571\n",
      "Iteration 241, loss = 0.05288764\n",
      "Iteration 242, loss = 0.05258438\n",
      "Iteration 243, loss = 0.05228585\n",
      "Iteration 244, loss = 0.05199189\n",
      "Iteration 245, loss = 0.05170238\n",
      "Iteration 246, loss = 0.05141726\n",
      "Iteration 247, loss = 0.05113643\n",
      "Iteration 248, loss = 0.05085975\n",
      "Iteration 249, loss = 0.05058715\n",
      "Iteration 250, loss = 0.05031854\n",
      "Iteration 251, loss = 0.05005383\n",
      "Iteration 252, loss = 0.04979291\n",
      "Iteration 253, loss = 0.04953571\n",
      "Iteration 254, loss = 0.04928215\n",
      "Iteration 255, loss = 0.04903214\n",
      "Iteration 256, loss = 0.04878560\n",
      "Iteration 257, loss = 0.04854245\n",
      "Iteration 258, loss = 0.04830263\n",
      "Iteration 259, loss = 0.04806606\n",
      "Iteration 260, loss = 0.04783266\n",
      "Iteration 261, loss = 0.04760237\n",
      "Iteration 262, loss = 0.04737512\n",
      "Iteration 263, loss = 0.04715085\n",
      "Iteration 264, loss = 0.04692949\n",
      "Iteration 265, loss = 0.04671099\n",
      "Iteration 266, loss = 0.04649529\n",
      "Iteration 267, loss = 0.04628232\n",
      "Iteration 268, loss = 0.04607209\n",
      "Iteration 269, loss = 0.04586457\n",
      "Iteration 270, loss = 0.04565966\n",
      "Iteration 271, loss = 0.04545729\n",
      "Iteration 272, loss = 0.04525740\n",
      "Iteration 273, loss = 0.04505992\n",
      "Iteration 274, loss = 0.04486485\n",
      "Iteration 275, loss = 0.04467212\n",
      "Iteration 276, loss = 0.04448167\n",
      "Iteration 277, loss = 0.04429347\n",
      "Iteration 278, loss = 0.04410748\n",
      "Iteration 279, loss = 0.04392366\n",
      "Iteration 280, loss = 0.04374195\n",
      "Iteration 281, loss = 0.04356233\n",
      "Iteration 282, loss = 0.04338476\n",
      "Iteration 283, loss = 0.04320919\n",
      "Iteration 284, loss = 0.04303558\n",
      "Iteration 285, loss = 0.04286391\n",
      "Iteration 286, loss = 0.04269414\n",
      "Iteration 287, loss = 0.04252624\n",
      "Iteration 288, loss = 0.04236016\n",
      "Iteration 289, loss = 0.04219588\n",
      "Iteration 290, loss = 0.04203337\n",
      "Iteration 291, loss = 0.04187260\n",
      "Iteration 292, loss = 0.04171353\n",
      "Iteration 293, loss = 0.04155613\n",
      "Iteration 294, loss = 0.04140039\n",
      "Iteration 295, loss = 0.04124626\n",
      "Iteration 296, loss = 0.04109373\n",
      "Iteration 297, loss = 0.04094276\n",
      "Iteration 298, loss = 0.04079333\n",
      "Iteration 299, loss = 0.04064541\n",
      "Iteration 300, loss = 0.04049899\n",
      "Iteration 301, loss = 0.04035402\n",
      "Iteration 302, loss = 0.04021050\n",
      "Iteration 303, loss = 0.04006839\n",
      "Iteration 304, loss = 0.03992768\n",
      "Iteration 305, loss = 0.03978834\n",
      "Iteration 306, loss = 0.03965035\n",
      "Iteration 307, loss = 0.03951369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 308, loss = 0.03937834\n",
      "Iteration 309, loss = 0.03924427\n",
      "Iteration 310, loss = 0.03911157\n",
      "Iteration 311, loss = 0.03898011\n",
      "Iteration 312, loss = 0.03884990\n",
      "Iteration 313, loss = 0.03872091\n",
      "Iteration 314, loss = 0.03859311\n",
      "Iteration 315, loss = 0.03846656\n",
      "Iteration 316, loss = 0.03834125\n",
      "Iteration 317, loss = 0.03821708\n",
      "Iteration 318, loss = 0.03809406\n",
      "Iteration 319, loss = 0.03797215\n",
      "Iteration 320, loss = 0.03785132\n",
      "Iteration 321, loss = 0.03773157\n",
      "Iteration 322, loss = 0.03761289\n",
      "Iteration 323, loss = 0.03749525\n",
      "Iteration 324, loss = 0.03737863\n",
      "Iteration 325, loss = 0.03726303\n",
      "Iteration 326, loss = 0.03714843\n",
      "Iteration 327, loss = 0.03703481\n",
      "Iteration 328, loss = 0.03692215\n",
      "Iteration 329, loss = 0.03681046\n",
      "Iteration 330, loss = 0.03669971\n",
      "Iteration 331, loss = 0.03658988\n",
      "Iteration 332, loss = 0.03648096\n",
      "Iteration 333, loss = 0.03637295\n",
      "Iteration 334, loss = 0.03626582\n",
      "Iteration 335, loss = 0.03615957\n",
      "Iteration 336, loss = 0.03605418\n",
      "Iteration 337, loss = 0.03594964\n",
      "Iteration 338, loss = 0.03584594\n",
      "Iteration 339, loss = 0.03574307\n",
      "Iteration 340, loss = 0.03564101\n",
      "Iteration 341, loss = 0.03553975\n",
      "Iteration 342, loss = 0.03543929\n",
      "Iteration 343, loss = 0.03533961\n",
      "Iteration 344, loss = 0.03524070\n",
      "Iteration 345, loss = 0.03514256\n",
      "Iteration 346, loss = 0.03504516\n",
      "Iteration 347, loss = 0.03494851\n",
      "Iteration 348, loss = 0.03485259\n",
      "Iteration 349, loss = 0.03475739\n",
      "Iteration 350, loss = 0.03466290\n",
      "Iteration 351, loss = 0.03456912\n",
      "Iteration 352, loss = 0.03447603\n",
      "Iteration 353, loss = 0.03438363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15205734\n",
      "Iteration 2, loss = 1.99345951\n",
      "Iteration 3, loss = 1.83668249\n",
      "Iteration 4, loss = 1.68718977\n",
      "Iteration 5, loss = 1.55265756\n",
      "Iteration 6, loss = 1.43505422\n",
      "Iteration 7, loss = 1.33596451\n",
      "Iteration 8, loss = 1.25622092\n",
      "Iteration 9, loss = 1.19581565\n",
      "Iteration 10, loss = 1.15358927\n",
      "Iteration 11, loss = 1.12602739\n",
      "Iteration 12, loss = 1.11027558\n",
      "Iteration 13, loss = 1.10198258\n",
      "Iteration 14, loss = 1.09849213\n",
      "Iteration 15, loss = 1.09658398\n",
      "Iteration 16, loss = 1.09365127\n",
      "Iteration 17, loss = 1.08792351\n",
      "Iteration 18, loss = 1.07865674\n",
      "Iteration 19, loss = 1.06579970\n",
      "Iteration 20, loss = 1.04960865\n",
      "Iteration 21, loss = 1.03058992\n",
      "Iteration 22, loss = 1.00943808\n",
      "Iteration 23, loss = 0.98689357\n",
      "Iteration 24, loss = 0.96385416\n",
      "Iteration 25, loss = 0.94132177\n",
      "Iteration 26, loss = 0.92038139\n",
      "Iteration 27, loss = 0.90154405\n",
      "Iteration 28, loss = 0.88519802\n",
      "Iteration 29, loss = 0.87153997\n",
      "Iteration 30, loss = 0.86022861\n",
      "Iteration 31, loss = 0.85111361\n",
      "Iteration 32, loss = 0.84375916\n",
      "Iteration 33, loss = 0.83775020\n",
      "Iteration 34, loss = 0.83242483\n",
      "Iteration 35, loss = 0.82737269\n",
      "Iteration 36, loss = 0.82232611\n",
      "Iteration 37, loss = 0.81714960\n",
      "Iteration 38, loss = 0.81187658\n",
      "Iteration 39, loss = 0.80662745\n",
      "Iteration 40, loss = 0.80153072\n",
      "Iteration 41, loss = 0.79668951\n",
      "Iteration 42, loss = 0.79213358\n",
      "Iteration 43, loss = 0.78780177\n",
      "Iteration 44, loss = 0.78355224\n",
      "Iteration 45, loss = 0.77922237\n",
      "Iteration 46, loss = 0.77463866\n",
      "Iteration 47, loss = 0.76961985\n",
      "Iteration 48, loss = 0.76409846\n",
      "Iteration 49, loss = 0.75821665\n",
      "Iteration 50, loss = 0.75199059\n",
      "Iteration 51, loss = 0.74557725\n",
      "Iteration 52, loss = 0.73926474\n",
      "Iteration 53, loss = 0.73299258\n",
      "Iteration 54, loss = 0.72693838\n",
      "Iteration 55, loss = 0.72152000\n",
      "Iteration 56, loss = 0.71645445\n",
      "Iteration 57, loss = 0.71166289\n",
      "Iteration 58, loss = 0.70700873\n",
      "Iteration 59, loss = 0.70245590\n",
      "Iteration 60, loss = 0.69800232\n",
      "Iteration 61, loss = 0.69354667\n",
      "Iteration 62, loss = 0.68912732\n",
      "Iteration 63, loss = 0.68476618\n",
      "Iteration 64, loss = 0.68050488\n",
      "Iteration 65, loss = 0.67644024\n",
      "Iteration 66, loss = 0.67241629\n",
      "Iteration 67, loss = 0.66840460\n",
      "Iteration 68, loss = 0.66440106\n",
      "Iteration 69, loss = 0.66034781\n",
      "Iteration 70, loss = 0.65624344\n",
      "Iteration 71, loss = 0.65210468\n",
      "Iteration 72, loss = 0.64798609\n",
      "Iteration 73, loss = 0.64391196\n",
      "Iteration 74, loss = 0.63990133\n",
      "Iteration 75, loss = 0.63602412\n",
      "Iteration 76, loss = 0.63233245\n",
      "Iteration 77, loss = 0.62874570\n",
      "Iteration 78, loss = 0.62504830\n",
      "Iteration 79, loss = 0.61934551\n",
      "Iteration 80, loss = 0.61122042\n",
      "Iteration 81, loss = 0.60129118\n",
      "Iteration 82, loss = 0.59058785\n",
      "Iteration 83, loss = 0.58616817\n",
      "Iteration 84, loss = 0.57943939\n",
      "Iteration 85, loss = 0.57173946\n",
      "Iteration 86, loss = 0.56391262\n",
      "Iteration 87, loss = 0.55593588\n",
      "Iteration 88, loss = 0.54772989\n",
      "Iteration 89, loss = 0.53930509\n",
      "Iteration 90, loss = 0.53078405\n",
      "Iteration 91, loss = 0.52233429\n",
      "Iteration 92, loss = 0.51404044\n",
      "Iteration 93, loss = 0.50589724\n",
      "Iteration 94, loss = 0.49778489\n",
      "Iteration 95, loss = 0.48952925\n",
      "Iteration 96, loss = 0.48101922\n",
      "Iteration 97, loss = 0.47223893\n",
      "Iteration 98, loss = 0.46329461\n",
      "Iteration 99, loss = 0.45438977\n",
      "Iteration 100, loss = 0.44573412\n",
      "Iteration 101, loss = 0.43751030\n",
      "Iteration 102, loss = 0.42978019\n",
      "Iteration 103, loss = 0.42252378\n",
      "Iteration 104, loss = 0.41540471\n",
      "Iteration 105, loss = 0.40805127\n",
      "Iteration 106, loss = 0.40031001\n",
      "Iteration 107, loss = 0.39236053\n",
      "Iteration 108, loss = 0.38470432\n",
      "Iteration 109, loss = 0.37763094\n",
      "Iteration 110, loss = 0.37091741\n",
      "Iteration 111, loss = 0.36426970\n",
      "Iteration 112, loss = 0.35735808\n",
      "Iteration 113, loss = 0.35016574\n",
      "Iteration 114, loss = 0.34294701\n",
      "Iteration 115, loss = 0.33619971\n",
      "Iteration 116, loss = 0.32984210\n",
      "Iteration 117, loss = 0.32344428\n",
      "Iteration 118, loss = 0.31683226\n",
      "Iteration 119, loss = 0.31009032\n",
      "Iteration 120, loss = 0.30361315\n",
      "Iteration 121, loss = 0.29759278\n",
      "Iteration 122, loss = 0.29162002\n",
      "Iteration 123, loss = 0.28550371\n",
      "Iteration 124, loss = 0.27936707\n",
      "Iteration 125, loss = 0.27350738\n",
      "Iteration 126, loss = 0.26795254\n",
      "Iteration 127, loss = 0.26243715\n",
      "Iteration 128, loss = 0.25688433\n",
      "Iteration 129, loss = 0.25147537\n",
      "Iteration 130, loss = 0.24633770\n",
      "Iteration 131, loss = 0.24136685\n",
      "Iteration 132, loss = 0.23637623\n",
      "Iteration 133, loss = 0.23150012\n",
      "Iteration 134, loss = 0.22684165\n",
      "Iteration 135, loss = 0.22234578\n",
      "Iteration 136, loss = 0.21791720\n",
      "Iteration 137, loss = 0.21356951\n",
      "Iteration 138, loss = 0.20938233\n",
      "Iteration 139, loss = 0.20536491\n",
      "Iteration 140, loss = 0.20144829\n",
      "Iteration 141, loss = 0.19760467\n",
      "Iteration 142, loss = 0.19388841\n",
      "Iteration 143, loss = 0.19033080\n",
      "Iteration 144, loss = 0.18687618\n",
      "Iteration 145, loss = 0.18350396\n",
      "Iteration 146, loss = 0.18024419\n",
      "Iteration 147, loss = 0.17711298\n",
      "Iteration 148, loss = 0.17408348\n",
      "Iteration 149, loss = 0.17113301\n",
      "Iteration 150, loss = 0.16827719\n",
      "Iteration 151, loss = 0.16554330\n",
      "Iteration 152, loss = 0.16289485\n",
      "Iteration 153, loss = 0.16031624\n",
      "Iteration 154, loss = 0.15783537\n",
      "Iteration 155, loss = 0.15546005\n",
      "Iteration 156, loss = 0.15315390\n",
      "Iteration 157, loss = 0.15090488\n",
      "Iteration 158, loss = 0.14874934\n",
      "Iteration 159, loss = 0.14667692\n",
      "Iteration 160, loss = 0.14465954\n",
      "Iteration 161, loss = 0.14270192\n",
      "Iteration 162, loss = 0.14081949\n",
      "Iteration 163, loss = 0.13900335\n",
      "Iteration 164, loss = 0.13723681\n",
      "Iteration 165, loss = 0.13552383\n",
      "Iteration 166, loss = 0.13387384\n",
      "Iteration 167, loss = 0.13227907\n",
      "Iteration 168, loss = 0.13072931\n",
      "Iteration 169, loss = 0.12922808\n",
      "Iteration 170, loss = 0.12778032\n",
      "Iteration 171, loss = 0.12637844\n",
      "Iteration 172, loss = 0.12501571\n",
      "Iteration 173, loss = 0.12369539\n",
      "Iteration 174, loss = 0.12241935\n",
      "Iteration 175, loss = 0.12118136\n",
      "Iteration 176, loss = 0.11997826\n",
      "Iteration 177, loss = 0.11881230\n",
      "Iteration 178, loss = 0.11768509\n",
      "Iteration 179, loss = 0.11659017\n",
      "Iteration 180, loss = 0.11552395\n",
      "Iteration 181, loss = 0.11448980\n",
      "Iteration 182, loss = 0.11348765\n",
      "Iteration 183, loss = 0.11251243\n",
      "Iteration 184, loss = 0.11156289\n",
      "Iteration 185, loss = 0.11064125\n",
      "Iteration 186, loss = 0.10974605\n",
      "Iteration 187, loss = 0.10887380\n",
      "Iteration 188, loss = 0.10802453\n",
      "Iteration 189, loss = 0.10719928\n",
      "Iteration 190, loss = 0.10639608\n",
      "Iteration 191, loss = 0.10561316\n",
      "Iteration 192, loss = 0.10485066\n",
      "Iteration 193, loss = 0.10410869\n",
      "Iteration 194, loss = 0.10338540\n",
      "Iteration 195, loss = 0.10267978\n",
      "Iteration 196, loss = 0.10199224\n",
      "Iteration 197, loss = 0.10132229\n",
      "Iteration 198, loss = 0.10066950\n",
      "Iteration 199, loss = 0.10003225\n",
      "Iteration 200, loss = 0.09941215\n",
      "Iteration 201, loss = 0.09880821\n",
      "Iteration 202, loss = 0.09821763\n",
      "Iteration 203, loss = 0.09764023\n",
      "Iteration 204, loss = 0.09707719\n",
      "Iteration 205, loss = 0.09652739\n",
      "Iteration 206, loss = 0.09598922\n",
      "Iteration 207, loss = 0.09546317\n",
      "Iteration 208, loss = 0.09494955\n",
      "Iteration 209, loss = 0.09444706\n",
      "Iteration 210, loss = 0.09395503\n",
      "Iteration 211, loss = 0.09347396\n",
      "Iteration 212, loss = 0.09300351\n",
      "Iteration 213, loss = 0.09254268\n",
      "Iteration 214, loss = 0.09209137\n",
      "Iteration 215, loss = 0.09164980\n",
      "Iteration 216, loss = 0.09121734\n",
      "Iteration 217, loss = 0.09079341\n",
      "Iteration 218, loss = 0.09037813\n",
      "Iteration 219, loss = 0.08997136\n",
      "Iteration 220, loss = 0.08957251\n",
      "Iteration 221, loss = 0.08918138\n",
      "Iteration 222, loss = 0.08879800\n",
      "Iteration 223, loss = 0.08842206\n",
      "Iteration 224, loss = 0.08805316\n",
      "Iteration 225, loss = 0.08769125\n",
      "Iteration 226, loss = 0.08733624\n",
      "Iteration 227, loss = 0.08698780\n",
      "Iteration 228, loss = 0.08664570\n",
      "Iteration 229, loss = 0.08630992\n",
      "Iteration 230, loss = 0.08598025\n",
      "Iteration 231, loss = 0.08565643\n",
      "Iteration 232, loss = 0.08533837\n",
      "Iteration 233, loss = 0.08502597\n",
      "Iteration 234, loss = 0.08471901\n",
      "Iteration 235, loss = 0.08441740\n",
      "Iteration 236, loss = 0.08412101\n",
      "Iteration 237, loss = 0.08382975\n",
      "Iteration 238, loss = 0.08354339\n",
      "Iteration 239, loss = 0.08326184\n",
      "Iteration 240, loss = 0.08298501\n",
      "Iteration 241, loss = 0.08271273\n",
      "Iteration 242, loss = 0.08244490\n",
      "Iteration 243, loss = 0.08218143\n",
      "Iteration 244, loss = 0.08192221\n",
      "Iteration 245, loss = 0.08166804\n",
      "Iteration 246, loss = 0.08141791\n",
      "Iteration 247, loss = 0.08117206\n",
      "Iteration 248, loss = 0.08093017\n",
      "Iteration 249, loss = 0.08069201\n",
      "Iteration 250, loss = 0.08045794\n",
      "Iteration 251, loss = 0.08022739\n",
      "Iteration 252, loss = 0.08000034\n",
      "Iteration 253, loss = 0.07977672\n",
      "Iteration 254, loss = 0.07955641\n",
      "Iteration 255, loss = 0.07933936\n",
      "Iteration 256, loss = 0.07912554\n",
      "Iteration 257, loss = 0.07891485\n",
      "Iteration 258, loss = 0.07870720\n",
      "Iteration 259, loss = 0.07850255\n",
      "Iteration 260, loss = 0.07830085\n",
      "Iteration 261, loss = 0.07810200\n",
      "Iteration 262, loss = 0.07790595\n",
      "Iteration 263, loss = 0.07771266\n",
      "Iteration 264, loss = 0.07752204\n",
      "Iteration 265, loss = 0.07733404\n",
      "Iteration 266, loss = 0.07714862\n",
      "Iteration 267, loss = 0.07696572\n",
      "Iteration 268, loss = 0.07678527\n",
      "Iteration 269, loss = 0.07660723\n",
      "Iteration 270, loss = 0.07643156\n",
      "Iteration 271, loss = 0.07625820\n",
      "Iteration 272, loss = 0.07608710\n",
      "Iteration 273, loss = 0.07591822\n",
      "Iteration 274, loss = 0.07575152\n",
      "Iteration 275, loss = 0.07558695\n",
      "Iteration 276, loss = 0.07542446\n",
      "Iteration 277, loss = 0.07526403\n",
      "Iteration 278, loss = 0.07510560\n",
      "Iteration 279, loss = 0.07494913\n",
      "Iteration 280, loss = 0.07479460\n",
      "Iteration 281, loss = 0.07464196\n",
      "Iteration 282, loss = 0.07449117\n",
      "Iteration 283, loss = 0.07434220\n",
      "Iteration 284, loss = 0.07419502\n",
      "Iteration 285, loss = 0.07404959\n",
      "Iteration 286, loss = 0.07390588\n",
      "Iteration 287, loss = 0.07376386\n",
      "Iteration 288, loss = 0.07362349\n",
      "Iteration 289, loss = 0.07348475\n",
      "Iteration 290, loss = 0.07334760\n",
      "Iteration 291, loss = 0.07321202\n",
      "Iteration 292, loss = 0.07307798\n",
      "Iteration 293, loss = 0.07294546\n",
      "Iteration 294, loss = 0.07281441\n",
      "Iteration 295, loss = 0.07268483\n",
      "Iteration 296, loss = 0.07255668\n",
      "Iteration 297, loss = 0.07242994\n",
      "Iteration 298, loss = 0.07230458\n",
      "Iteration 299, loss = 0.07218059\n",
      "Iteration 300, loss = 0.07205793\n",
      "Iteration 301, loss = 0.07193659\n",
      "Iteration 302, loss = 0.07181654\n",
      "Iteration 303, loss = 0.07169776\n",
      "Iteration 304, loss = 0.07158024\n",
      "Iteration 305, loss = 0.07146394\n",
      "Iteration 306, loss = 0.07134885\n",
      "Iteration 307, loss = 0.07123496\n",
      "Iteration 308, loss = 0.07112224\n",
      "Iteration 309, loss = 0.07101066\n",
      "Iteration 310, loss = 0.07090023\n",
      "Iteration 311, loss = 0.07079090\n",
      "Iteration 312, loss = 0.07068268\n",
      "Iteration 313, loss = 0.07057554\n",
      "Iteration 314, loss = 0.07046946\n",
      "Iteration 315, loss = 0.07036443\n",
      "Iteration 316, loss = 0.07026046\n",
      "Iteration 317, loss = 0.07015764\n",
      "Iteration 318, loss = 0.07005594\n",
      "Iteration 319, loss = 0.06995521\n",
      "Iteration 320, loss = 0.06985538\n",
      "Iteration 321, loss = 0.06975657\n",
      "Iteration 322, loss = 0.06965873\n",
      "Iteration 323, loss = 0.06956174\n",
      "Iteration 324, loss = 0.06946571\n",
      "Iteration 325, loss = 0.06937061\n",
      "Iteration 326, loss = 0.06927634\n",
      "Iteration 327, loss = 0.06918296\n",
      "Iteration 328, loss = 0.06909047\n",
      "Iteration 329, loss = 0.06899880\n",
      "Iteration 330, loss = 0.06890796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13218766\n",
      "Iteration 2, loss = 1.58217076\n",
      "Iteration 3, loss = 1.21750851\n",
      "Iteration 4, loss = 1.20735831\n",
      "Iteration 5, loss = 1.22154231\n",
      "Iteration 6, loss = 1.21687113\n",
      "Iteration 7, loss = 1.20975368\n",
      "Iteration 8, loss = 1.20280296\n",
      "Iteration 9, loss = 1.19681112\n",
      "Iteration 10, loss = 1.19274862\n",
      "Iteration 11, loss = 1.18927851\n",
      "Iteration 12, loss = 1.18558382\n",
      "Iteration 13, loss = 1.18156453\n",
      "Iteration 14, loss = 1.17722204\n",
      "Iteration 15, loss = 1.17260361\n",
      "Iteration 16, loss = 1.16772509\n",
      "Iteration 17, loss = 1.16290985\n",
      "Iteration 18, loss = 1.15855355\n",
      "Iteration 19, loss = 1.15431226\n",
      "Iteration 20, loss = 1.15021570\n",
      "Iteration 21, loss = 1.14614237\n",
      "Iteration 22, loss = 1.14207635\n",
      "Iteration 23, loss = 1.13804851\n",
      "Iteration 24, loss = 1.13402485\n",
      "Iteration 25, loss = 1.13007337\n",
      "Iteration 26, loss = 1.12617121\n",
      "Iteration 27, loss = 1.12237044\n",
      "Iteration 28, loss = 1.11864355\n",
      "Iteration 29, loss = 1.11499966\n",
      "Iteration 30, loss = 1.11142933\n",
      "Iteration 31, loss = 1.10793178\n",
      "Iteration 32, loss = 1.10451507\n",
      "Iteration 33, loss = 1.10111237\n",
      "Iteration 34, loss = 1.09772586\n",
      "Iteration 35, loss = 1.09435485\n",
      "Iteration 36, loss = 1.09099193\n",
      "Iteration 37, loss = 1.08760710\n",
      "Iteration 38, loss = 1.08422649\n",
      "Iteration 39, loss = 1.08079309\n",
      "Iteration 40, loss = 1.07727444\n",
      "Iteration 41, loss = 1.07368053\n",
      "Iteration 42, loss = 1.07002538\n",
      "Iteration 43, loss = 1.06626429\n",
      "Iteration 44, loss = 1.06237104\n",
      "Iteration 45, loss = 1.05833271\n",
      "Iteration 46, loss = 1.05418175\n",
      "Iteration 47, loss = 1.04991750\n",
      "Iteration 48, loss = 1.04549728\n",
      "Iteration 49, loss = 1.04091878\n",
      "Iteration 50, loss = 1.03618213\n",
      "Iteration 51, loss = 1.03129021\n",
      "Iteration 52, loss = 1.02624861\n",
      "Iteration 53, loss = 1.02110734\n",
      "Iteration 54, loss = 1.01584808\n",
      "Iteration 55, loss = 1.01046149\n",
      "Iteration 56, loss = 1.00495620\n",
      "Iteration 57, loss = 0.99934105\n",
      "Iteration 58, loss = 0.99362485\n",
      "Iteration 59, loss = 0.98781615\n",
      "Iteration 60, loss = 0.98192306\n",
      "Iteration 61, loss = 0.97595311\n",
      "Iteration 62, loss = 0.96991324\n",
      "Iteration 63, loss = 0.96380968\n",
      "Iteration 64, loss = 0.95764802\n",
      "Iteration 65, loss = 0.95143323\n",
      "Iteration 66, loss = 0.94516966\n",
      "Iteration 67, loss = 0.93886117\n",
      "Iteration 68, loss = 0.93251120\n",
      "Iteration 69, loss = 0.92612280\n",
      "Iteration 70, loss = 0.91969873\n",
      "Iteration 71, loss = 0.91324153\n",
      "Iteration 72, loss = 0.90675353"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 73, loss = 0.90023694\n",
      "Iteration 74, loss = 0.89369385\n",
      "Iteration 75, loss = 0.88712627\n",
      "Iteration 76, loss = 0.88053618\n",
      "Iteration 77, loss = 0.87392555\n",
      "Iteration 78, loss = 0.86729633\n",
      "Iteration 79, loss = 0.86065052\n",
      "Iteration 80, loss = 0.85399015\n",
      "Iteration 81, loss = 0.84731734\n",
      "Iteration 82, loss = 0.84063429\n",
      "Iteration 83, loss = 0.83394330\n",
      "Iteration 84, loss = 0.82724685\n",
      "Iteration 85, loss = 0.82054753\n",
      "Iteration 86, loss = 0.81384816\n",
      "Iteration 87, loss = 0.80715178\n",
      "Iteration 88, loss = 0.80046163\n",
      "Iteration 89, loss = 0.79378125\n",
      "Iteration 90, loss = 0.78711445\n",
      "Iteration 91, loss = 0.78046535\n",
      "Iteration 92, loss = 0.77383838\n",
      "Iteration 93, loss = 0.76723832\n",
      "Iteration 94, loss = 0.76067026\n",
      "Iteration 95, loss = 0.75413965\n",
      "Iteration 96, loss = 0.74765223\n",
      "Iteration 97, loss = 0.74121407\n",
      "Iteration 98, loss = 0.73483150\n",
      "Iteration 99, loss = 0.72851108\n",
      "Iteration 100, loss = 0.72225954\n",
      "Iteration 101, loss = 0.71608372\n",
      "Iteration 102, loss = 0.70999044\n",
      "Iteration 103, loss = 0.70398650\n",
      "Iteration 104, loss = 0.69807847\n",
      "Iteration 105, loss = 0.69227263\n",
      "Iteration 106, loss = 0.68657481\n",
      "Iteration 107, loss = 0.68099033\n",
      "Iteration 108, loss = 0.67552380\n",
      "Iteration 109, loss = 0.67017904\n",
      "Iteration 110, loss = 0.66495902\n",
      "Iteration 111, loss = 0.65986574\n",
      "Iteration 112, loss = 0.65490019\n",
      "Iteration 113, loss = 0.65006239\n",
      "Iteration 114, loss = 0.64535136\n",
      "Iteration 115, loss = 0.64076520\n",
      "Iteration 116, loss = 0.63630123\n",
      "Iteration 117, loss = 0.63195603\n",
      "Iteration 118, loss = 0.62772568\n",
      "Iteration 119, loss = 0.62360585\n",
      "Iteration 120, loss = 0.61959198\n",
      "Iteration 121, loss = 0.61567945\n",
      "Iteration 122, loss = 0.61186366\n",
      "Iteration 123, loss = 0.60814019\n",
      "Iteration 124, loss = 0.60450481\n",
      "Iteration 125, loss = 0.60095358\n",
      "Iteration 126, loss = 0.59748287\n",
      "Iteration 127, loss = 0.59408931\n",
      "Iteration 128, loss = 0.59076982\n",
      "Iteration 129, loss = 0.58752158\n",
      "Iteration 130, loss = 0.58434203\n",
      "Iteration 131, loss = 0.58122878\n",
      "Iteration 132, loss = 0.57820149\n",
      "Iteration 133, loss = 0.57524646\n",
      "Iteration 134, loss = 0.57233865\n",
      "Iteration 135, loss = 0.56947827\n",
      "Iteration 136, loss = 0.56666549\n",
      "Iteration 137, loss = 0.56390025\n",
      "Iteration 138, loss = 0.56118225\n",
      "Iteration 139, loss = 0.55851100\n",
      "Iteration 140, loss = 0.55588600\n",
      "Iteration 141, loss = 0.55331331\n",
      "Iteration 142, loss = 0.55080176\n",
      "Iteration 143, loss = 0.54833700\n",
      "Iteration 144, loss = 0.54592022\n",
      "Iteration 145, loss = 0.54355204\n",
      "Iteration 146, loss = 0.54123220\n",
      "Iteration 147, loss = 0.53895962\n",
      "Iteration 148, loss = 0.53673260\n",
      "Iteration 149, loss = 0.53454915\n",
      "Iteration 150, loss = 0.53240731\n",
      "Iteration 151, loss = 0.53030539\n",
      "Iteration 152, loss = 0.52824201\n",
      "Iteration 153, loss = 0.52621614\n",
      "Iteration 154, loss = 0.52422696\n",
      "Iteration 155, loss = 0.52227376\n",
      "Iteration 156, loss = 0.52035581\n",
      "Iteration 157, loss = 0.51847233\n",
      "Iteration 158, loss = 0.51662244\n",
      "Iteration 159, loss = 0.51480516\n",
      "Iteration 160, loss = 0.51301951\n",
      "Iteration 161, loss = 0.51126448\n",
      "Iteration 162, loss = 0.50953911\n",
      "Iteration 163, loss = 0.50784250\n",
      "Iteration 164, loss = 0.50617383\n",
      "Iteration 165, loss = 0.50453231\n",
      "Iteration 166, loss = 0.50291721\n",
      "Iteration 167, loss = 0.50132781\n",
      "Iteration 168, loss = 0.49976343\n",
      "Iteration 169, loss = 0.49822335\n",
      "Iteration 170, loss = 0.49670691\n",
      "Iteration 171, loss = 0.49521341\n",
      "Iteration 172, loss = 0.49374219\n",
      "Iteration 173, loss = 0.49229261\n",
      "Iteration 174, loss = 0.49086405\n",
      "Iteration 175, loss = 0.48945591\n",
      "Iteration 176, loss = 0.48806761\n",
      "Iteration 177, loss = 0.48669860\n",
      "Iteration 178, loss = 0.48534834\n",
      "Iteration 179, loss = 0.48401632\n",
      "Iteration 180, loss = 0.48270201\n",
      "Iteration 181, loss = 0.48140494\n",
      "Iteration 182, loss = 0.48012461\n",
      "Iteration 183, loss = 0.47886057\n",
      "Iteration 184, loss = 0.47761235\n",
      "Iteration 185, loss = 0.47637952\n",
      "Iteration 186, loss = 0.47516165\n",
      "Iteration 187, loss = 0.47395834\n",
      "Iteration 188, loss = 0.47276917\n",
      "Iteration 189, loss = 0.47159378\n",
      "Iteration 190, loss = 0.47043178\n",
      "Iteration 191, loss = 0.46928281\n",
      "Iteration 192, loss = 0.46814651\n",
      "Iteration 193, loss = 0.46702255\n",
      "Iteration 194, loss = 0.46591059\n",
      "Iteration 195, loss = 0.46481032\n",
      "Iteration 196, loss = 0.46372141\n",
      "Iteration 197, loss = 0.46264357\n",
      "Iteration 198, loss = 0.46157651\n",
      "Iteration 199, loss = 0.46051993\n",
      "Iteration 200, loss = 0.45947358\n",
      "Iteration 201, loss = 0.45843717\n",
      "Iteration 202, loss = 0.45741045\n",
      "Iteration 203, loss = 0.45639317\n",
      "Iteration 204, loss = 0.45538508\n",
      "Iteration 205, loss = 0.45438596\n",
      "Iteration 206, loss = 0.45339556\n",
      "Iteration 207, loss = 0.45241368\n",
      "Iteration 208, loss = 0.45144008\n",
      "Iteration 209, loss = 0.45047457\n",
      "Iteration 210, loss = 0.44951694\n",
      "Iteration 211, loss = 0.44856699\n",
      "Iteration 212, loss = 0.44762454\n",
      "Iteration 213, loss = 0.44668939\n",
      "Iteration 214, loss = 0.44576137\n",
      "Iteration 215, loss = 0.44484030\n",
      "Iteration 216, loss = 0.44392602\n",
      "Iteration 217, loss = 0.44301836\n",
      "Iteration 218, loss = 0.44211716\n",
      "Iteration 219, loss = 0.44122226\n",
      "Iteration 220, loss = 0.44033352\n",
      "Iteration 221, loss = 0.43945079\n",
      "Iteration 222, loss = 0.43857392\n",
      "Iteration 223, loss = 0.43770279\n",
      "Iteration 224, loss = 0.43683725\n",
      "Iteration 225, loss = 0.43597717\n",
      "Iteration 226, loss = 0.43512244\n",
      "Iteration 227, loss = 0.43427293\n",
      "Iteration 228, loss = 0.43342852\n",
      "Iteration 229, loss = 0.43258910\n",
      "Iteration 230, loss = 0.43175454\n",
      "Iteration 231, loss = 0.43092475\n",
      "Iteration 232, loss = 0.43009963\n",
      "Iteration 233, loss = 0.42927905\n",
      "Iteration 234, loss = 0.42846294\n",
      "Iteration 235, loss = 0.42765118\n",
      "Iteration 236, loss = 0.42684370\n",
      "Iteration 237, loss = 0.42604039\n",
      "Iteration 238, loss = 0.42524116\n",
      "Iteration 239, loss = 0.42444594\n",
      "Iteration 240, loss = 0.42365464\n",
      "Iteration 241, loss = 0.42286717\n",
      "Iteration 242, loss = 0.42208346\n",
      "Iteration 243, loss = 0.42130344\n",
      "Iteration 244, loss = 0.42052702\n",
      "Iteration 245, loss = 0.41975413\n",
      "Iteration 246, loss = 0.41898472\n",
      "Iteration 247, loss = 0.41821870\n",
      "Iteration 248, loss = 0.41745601\n",
      "Iteration 249, loss = 0.41669660\n",
      "Iteration 250, loss = 0.41594039\n",
      "Iteration 251, loss = 0.41518732\n",
      "Iteration 252, loss = 0.41443734\n",
      "Iteration 253, loss = 0.41369039\n",
      "Iteration 254, loss = 0.41294642\n",
      "Iteration 255, loss = 0.41220537\n",
      "Iteration 256, loss = 0.41146719\n",
      "Iteration 257, loss = 0.41073182\n",
      "Iteration 258, loss = 0.40999923\n",
      "Iteration 259, loss = 0.40926936\n",
      "Iteration 260, loss = 0.40854217\n",
      "Iteration 261, loss = 0.40781761\n",
      "Iteration 262, loss = 0.40709564\n",
      "Iteration 263, loss = 0.40637621\n",
      "Iteration 264, loss = 0.40565929\n",
      "Iteration 265, loss = 0.40494484\n",
      "Iteration 266, loss = 0.40423281\n",
      "Iteration 267, loss = 0.40352318\n",
      "Iteration 268, loss = 0.40281590\n",
      "Iteration 269, loss = 0.40211094\n",
      "Iteration 270, loss = 0.40140826\n",
      "Iteration 271, loss = 0.40070784\n",
      "Iteration 272, loss = 0.40000964\n",
      "Iteration 273, loss = 0.39931363\n",
      "Iteration 274, loss = 0.39861978\n",
      "Iteration 275, loss = 0.39792806\n",
      "Iteration 276, loss = 0.39723844\n",
      "Iteration 277, loss = 0.39655090\n",
      "Iteration 278, loss = 0.39586541\n",
      "Iteration 279, loss = 0.39518194\n",
      "Iteration 280, loss = 0.39450047\n",
      "Iteration 281, loss = 0.39382098\n",
      "Iteration 282, loss = 0.39314344\n",
      "Iteration 283, loss = 0.39246783\n",
      "Iteration 284, loss = 0.39179413\n",
      "Iteration 285, loss = 0.39112232\n",
      "Iteration 286, loss = 0.39045238\n",
      "Iteration 287, loss = 0.38978428\n",
      "Iteration 288, loss = 0.38911801\n",
      "Iteration 289, loss = 0.38845356\n",
      "Iteration 290, loss = 0.38779089\n",
      "Iteration 291, loss = 0.38713001\n",
      "Iteration 292, loss = 0.38647088\n",
      "Iteration 293, loss = 0.38581350\n",
      "Iteration 294, loss = 0.38515785\n",
      "Iteration 295, loss = 0.38450391\n",
      "Iteration 296, loss = 0.38385167\n",
      "Iteration 297, loss = 0.38320112\n",
      "Iteration 298, loss = 0.38255224\n",
      "Iteration 299, loss = 0.38190501\n",
      "Iteration 300, loss = 0.38125944\n",
      "Iteration 301, loss = 0.38061551\n",
      "Iteration 302, loss = 0.37997319\n",
      "Iteration 303, loss = 0.37933250\n",
      "Iteration 304, loss = 0.37869340\n",
      "Iteration 305, loss = 0.37805590\n",
      "Iteration 306, loss = 0.37741998\n",
      "Iteration 307, loss = 0.37678563\n",
      "Iteration 308, loss = 0.37615285\n",
      "Iteration 309, loss = 0.37552162\n",
      "Iteration 310, loss = 0.37489194\n",
      "Iteration 311, loss = 0.37426381\n",
      "Iteration 312, loss = 0.37363720\n",
      "Iteration 313, loss = 0.37301212\n",
      "Iteration 314, loss = 0.37238855\n",
      "Iteration 315, loss = 0.37176650\n",
      "Iteration 316, loss = 0.37114594\n",
      "Iteration 317, loss = 0.37052689\n",
      "Iteration 318, loss = 0.36990933\n",
      "Iteration 319, loss = 0.36929326\n",
      "Iteration 320, loss = 0.36867866\n",
      "Iteration 321, loss = 0.36806554\n",
      "Iteration 322, loss = 0.36745390\n",
      "Iteration 323, loss = 0.36684371\n",
      "Iteration 324, loss = 0.36623499\n",
      "Iteration 325, loss = 0.36562773\n",
      "Iteration 326, loss = 0.36502192\n",
      "Iteration 327, loss = 0.36441756\n",
      "Iteration 328, loss = 0.36381464\n",
      "Iteration 329, loss = 0.36321317\n",
      "Iteration 330, loss = 0.36261313\n",
      "Iteration 331, loss = 0.36201453\n",
      "Iteration 332, loss = 0.36141736\n",
      "Iteration 333, loss = 0.36082162\n",
      "Iteration 334, loss = 0.36022731\n",
      "Iteration 335, loss = 0.35963442\n",
      "Iteration 336, loss = 0.35904295\n",
      "Iteration 337, loss = 0.35845290\n",
      "Iteration 338, loss = 0.35786427\n",
      "Iteration 339, loss = 0.35727705\n",
      "Iteration 340, loss = 0.35669124\n",
      "Iteration 341, loss = 0.35610685\n",
      "Iteration 342, loss = 0.35552386\n",
      "Iteration 343, loss = 0.35494228\n",
      "Iteration 344, loss = 0.35436211\n",
      "Iteration 345, loss = 0.35378334\n",
      "Iteration 346, loss = 0.35320598\n",
      "Iteration 347, loss = 0.35263001\n",
      "Iteration 348, loss = 0.35205545\n",
      "Iteration 349, loss = 0.35148228\n",
      "Iteration 350, loss = 0.35091052\n",
      "Iteration 351, loss = 0.35034015\n",
      "Iteration 352, loss = 0.34977118\n",
      "Iteration 353, loss = 0.34920360\n",
      "Iteration 354, loss = 0.34863741\n",
      "Iteration 355, loss = 0.34807262\n",
      "Iteration 356, loss = 0.34750923\n",
      "Iteration 357, loss = 0.34694722\n",
      "Iteration 358, loss = 0.34638661\n",
      "Iteration 359, loss = 0.34582739\n",
      "Iteration 360, loss = 0.34526955\n",
      "Iteration 361, loss = 0.34471311\n",
      "Iteration 362, loss = 0.34415806\n",
      "Iteration 363, loss = 0.34360439\n",
      "Iteration 364, loss = 0.34305212\n",
      "Iteration 365, loss = 0.34250123\n",
      "Iteration 366, loss = 0.34195173\n",
      "Iteration 367, loss = 0.34140361\n",
      "Iteration 368, loss = 0.34085688\n",
      "Iteration 369, loss = 0.34031154\n",
      "Iteration 370, loss = 0.33976758\n",
      "Iteration 371, loss = 0.33922501\n",
      "Iteration 372, loss = 0.33868382\n",
      "Iteration 373, loss = 0.33814402\n",
      "Iteration 374, loss = 0.33760560\n",
      "Iteration 375, loss = 0.33706857\n",
      "Iteration 376, loss = 0.33653292\n",
      "Iteration 377, loss = 0.33599865\n",
      "Iteration 378, loss = 0.33546576\n",
      "Iteration 379, loss = 0.33493426\n",
      "Iteration 380, loss = 0.33440414\n",
      "Iteration 381, loss = 0.33387540\n",
      "Iteration 382, loss = 0.33334804\n",
      "Iteration 383, loss = 0.33282206\n",
      "Iteration 384, loss = 0.33229746\n",
      "Iteration 385, loss = 0.33177424\n",
      "Iteration 386, loss = 0.33125240\n",
      "Iteration 387, loss = 0.33073194\n",
      "Iteration 388, loss = 0.33021285\n",
      "Iteration 389, loss = 0.32969515\n",
      "Iteration 390, loss = 0.32917882\n",
      "Iteration 391, loss = 0.32866386\n",
      "Iteration 392, loss = 0.32815029\n",
      "Iteration 393, loss = 0.32763808\n",
      "Iteration 394, loss = 0.32712725\n",
      "Iteration 395, loss = 0.32661780\n",
      "Iteration 396, loss = 0.32610972\n",
      "Iteration 397, loss = 0.32560301\n",
      "Iteration 398, loss = 0.32509767\n",
      "Iteration 399, loss = 0.32459370\n",
      "Iteration 400, loss = 0.32409110\n",
      "Iteration 401, loss = 0.32358987\n",
      "Iteration 402, loss = 0.32309001\n",
      "Iteration 403, loss = 0.32259152\n",
      "Iteration 404, loss = 0.32209439\n",
      "Iteration 405, loss = 0.32159863\n",
      "Iteration 406, loss = 0.32110423\n",
      "Iteration 407, loss = 0.32061119\n",
      "Iteration 408, loss = 0.32011952\n",
      "Iteration 409, loss = 0.31962920\n",
      "Iteration 410, loss = 0.31914025\n",
      "Iteration 411, loss = 0.31865265\n",
      "Iteration 412, loss = 0.31816641\n",
      "Iteration 413, loss = 0.31768153\n",
      "Iteration 414, loss = 0.31719800\n",
      "Iteration 415, loss = 0.31671583\n",
      "Iteration 416, loss = 0.31623501\n",
      "Iteration 417, loss = 0.31575553\n",
      "Iteration 418, loss = 0.31527741\n",
      "Iteration 419, loss = 0.31480063\n",
      "Iteration 420, loss = 0.31432521\n",
      "Iteration 421, loss = 0.31385112\n",
      "Iteration 422, loss = 0.31337838\n",
      "Iteration 423, loss = 0.31290698\n",
      "Iteration 424, loss = 0.31243692\n",
      "Iteration 425, loss = 0.31196819\n",
      "Iteration 426, loss = 0.31150081\n",
      "Iteration 427, loss = 0.31103476\n",
      "Iteration 428, loss = 0.31057004\n",
      "Iteration 429, loss = 0.31010665\n",
      "Iteration 430, loss = 0.30964459\n",
      "Iteration 431, loss = 0.30918386\n",
      "Iteration 432, loss = 0.30872445\n",
      "Iteration 433, loss = 0.30826637\n",
      "Iteration 434, loss = 0.30780961\n",
      "Iteration 435, loss = 0.30735417\n",
      "Iteration 436, loss = 0.30690004\n",
      "Iteration 437, loss = 0.30644723\n",
      "Iteration 438, loss = 0.30599573\n",
      "Iteration 439, loss = 0.30554555\n",
      "Iteration 440, loss = 0.30509667\n",
      "Iteration 441, loss = 0.30464910\n",
      "Iteration 442, loss = 0.30420284\n",
      "Iteration 443, loss = 0.30375787\n",
      "Iteration 444, loss = 0.30331421\n",
      "Iteration 445, loss = 0.30287184\n",
      "Iteration 446, loss = 0.30243077\n",
      "Iteration 447, loss = 0.30199099\n",
      "Iteration 448, loss = 0.30155250\n",
      "Iteration 449, loss = 0.30111530\n",
      "Iteration 450, loss = 0.30067938\n",
      "Iteration 451, loss = 0.30024475\n",
      "Iteration 452, loss = 0.29981140\n",
      "Iteration 453, loss = 0.29937932\n",
      "Iteration 454, loss = 0.29894852\n",
      "Iteration 455, loss = 0.29851899\n",
      "Iteration 456, loss = 0.29809074\n",
      "Iteration 457, loss = 0.29766375\n",
      "Iteration 458, loss = 0.29723802\n",
      "Iteration 459, loss = 0.29681355\n",
      "Iteration 460, loss = 0.29639035\n",
      "Iteration 461, loss = 0.29596840\n",
      "Iteration 462, loss = 0.29554770\n",
      "Iteration 463, loss = 0.29512826\n",
      "Iteration 464, loss = 0.29471006\n",
      "Iteration 465, loss = 0.29429311\n",
      "Iteration 466, loss = 0.29387739\n",
      "Iteration 467, loss = 0.29346292\n",
      "Iteration 468, loss = 0.29304968\n",
      "Iteration 469, loss = 0.29263768\n",
      "Iteration 470, loss = 0.29222691\n",
      "Iteration 471, loss = 0.29181736\n",
      "Iteration 472, loss = 0.29140904\n",
      "Iteration 473, loss = 0.29100194\n",
      "Iteration 474, loss = 0.29059606\n",
      "Iteration 475, loss = 0.29019139\n",
      "Iteration 476, loss = 0.28978794\n",
      "Iteration 477, loss = 0.28938569\n",
      "Iteration 478, loss = 0.28898465\n",
      "Iteration 479, loss = 0.28858481\n",
      "Iteration 480, loss = 0.28818617\n",
      "Iteration 481, loss = 0.28778873\n",
      "Iteration 482, loss = 0.28739248\n",
      "Iteration 483, loss = 0.28699742\n",
      "Iteration 484, loss = 0.28660354\n",
      "Iteration 485, loss = 0.28621085\n",
      "Iteration 486, loss = 0.28581934\n",
      "Iteration 487, loss = 0.28542901\n",
      "Iteration 488, loss = 0.28503985\n",
      "Iteration 489, loss = 0.28465186\n",
      "Iteration 490, loss = 0.28426504\n",
      "Iteration 491, loss = 0.28387938\n",
      "Iteration 492, loss = 0.28349488\n",
      "Iteration 493, loss = 0.28311154\n",
      "Iteration 494, loss = 0.28272935\n",
      "Iteration 495, loss = 0.28234831\n",
      "Iteration 496, loss = 0.28196842\n",
      "Iteration 497, loss = 0.28158967\n",
      "Iteration 498, loss = 0.28121207\n",
      "Iteration 499, loss = 0.28083560\n",
      "Iteration 500, loss = 0.28046026\n",
      "Iteration 501, loss = 0.28008605\n",
      "Iteration 502, loss = 0.27971297\n",
      "Iteration 503, loss = 0.27934102\n",
      "Iteration 504, loss = 0.27897018\n",
      "Iteration 505, loss = 0.27860046\n",
      "Iteration 506, loss = 0.27823185\n",
      "Iteration 507, loss = 0.27786435\n",
      "Iteration 508, loss = 0.27749795\n",
      "Iteration 509, loss = 0.27713266\n",
      "Iteration 510, loss = 0.27676847\n",
      "Iteration 511, loss = 0.27640537\n",
      "Iteration 512, loss = 0.27604337\n",
      "Iteration 513, loss = 0.27568245\n",
      "Iteration 514, loss = 0.27532262\n",
      "Iteration 515, loss = 0.27496387\n",
      "Iteration 516, loss = 0.27460620\n",
      "Iteration 517, loss = 0.27424960\n",
      "Iteration 518, loss = 0.27389407\n",
      "Iteration 519, loss = 0.27353961\n",
      "Iteration 520, loss = 0.27318622\n",
      "Iteration 521, loss = 0.27283388\n",
      "Iteration 522, loss = 0.27248260\n",
      "Iteration 523, loss = 0.27213238\n",
      "Iteration 524, loss = 0.27178320\n",
      "Iteration 525, loss = 0.27143507\n",
      "Iteration 526, loss = 0.27108799\n",
      "Iteration 527, loss = 0.27074194\n",
      "Iteration 528, loss = 0.27039693\n",
      "Iteration 529, loss = 0.27005295\n",
      "Iteration 530, loss = 0.26971000\n",
      "Iteration 531, loss = 0.26936808\n",
      "Iteration 532, loss = 0.26902717\n",
      "Iteration 533, loss = 0.26868729\n",
      "Iteration 534, loss = 0.26834842\n",
      "Iteration 535, loss = 0.26801056\n",
      "Iteration 536, loss = 0.26767371\n",
      "Iteration 537, loss = 0.26733786\n",
      "Iteration 538, loss = 0.26700302\n",
      "Iteration 539, loss = 0.26666917\n",
      "Iteration 540, loss = 0.26633631\n",
      "Iteration 541, loss = 0.26600445\n",
      "Iteration 542, loss = 0.26567357\n",
      "Iteration 543, loss = 0.26534367\n",
      "Iteration 544, loss = 0.26501476\n",
      "Iteration 545, loss = 0.26468682\n",
      "Iteration 546, loss = 0.26435985\n",
      "Iteration 547, loss = 0.26403385\n",
      "Iteration 548, loss = 0.26370882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 549, loss = 0.26338475\n",
      "Iteration 550, loss = 0.26306164\n",
      "Iteration 551, loss = 0.26273949\n",
      "Iteration 552, loss = 0.26241828\n",
      "Iteration 553, loss = 0.26209803\n",
      "Iteration 554, loss = 0.26177872\n",
      "Iteration 555, loss = 0.26146035\n",
      "Iteration 556, loss = 0.26114292\n",
      "Iteration 557, loss = 0.26082643\n",
      "Iteration 558, loss = 0.26051086\n",
      "Iteration 559, loss = 0.26019623\n",
      "Iteration 560, loss = 0.25988252\n",
      "Iteration 561, loss = 0.25956973\n",
      "Iteration 562, loss = 0.25925785\n",
      "Iteration 563, loss = 0.25894690\n",
      "Iteration 564, loss = 0.25863685\n",
      "Iteration 565, loss = 0.25832771\n",
      "Iteration 566, loss = 0.25801948\n",
      "Iteration 567, loss = 0.25771214\n",
      "Iteration 568, loss = 0.25740571\n",
      "Iteration 569, loss = 0.25710016\n",
      "Iteration 570, loss = 0.25679551\n",
      "Iteration 571, loss = 0.25649175\n",
      "Iteration 572, loss = 0.25618887\n",
      "Iteration 573, loss = 0.25588687\n",
      "Iteration 574, loss = 0.25558575\n",
      "Iteration 575, loss = 0.25528550\n",
      "Iteration 576, loss = 0.25498613\n",
      "Iteration 577, loss = 0.25468762\n",
      "Iteration 578, loss = 0.25438997\n",
      "Iteration 579, loss = 0.25409319\n",
      "Iteration 580, loss = 0.25379727\n",
      "Iteration 581, loss = 0.25350220\n",
      "Iteration 582, loss = 0.25320798\n",
      "Iteration 583, loss = 0.25291461\n",
      "Iteration 584, loss = 0.25262209\n",
      "Iteration 585, loss = 0.25233040\n",
      "Iteration 586, loss = 0.25203956\n",
      "Iteration 587, loss = 0.25174955\n",
      "Iteration 588, loss = 0.25146037\n",
      "Iteration 589, loss = 0.25117203\n",
      "Iteration 590, loss = 0.25088451\n",
      "Iteration 591, loss = 0.25059781\n",
      "Iteration 592, loss = 0.25031193\n",
      "Iteration 593, loss = 0.25002687\n",
      "Iteration 594, loss = 0.24974262\n",
      "Iteration 595, loss = 0.24945918\n",
      "Iteration 596, loss = 0.24917655\n",
      "Iteration 597, loss = 0.24889472\n",
      "Iteration 598, loss = 0.24861369\n",
      "Iteration 599, loss = 0.24833346\n",
      "Iteration 600, loss = 0.24805403\n",
      "Iteration 601, loss = 0.24777539\n",
      "Iteration 602, loss = 0.24749754\n",
      "Iteration 603, loss = 0.24722047\n",
      "Iteration 604, loss = 0.24694418\n",
      "Iteration 605, loss = 0.24666868\n",
      "Iteration 606, loss = 0.24639395\n",
      "Iteration 607, loss = 0.24612000\n",
      "Iteration 608, loss = 0.24584681\n",
      "Iteration 609, loss = 0.24557440\n",
      "Iteration 610, loss = 0.24530274\n",
      "Iteration 611, loss = 0.24503185\n",
      "Iteration 612, loss = 0.24476172\n",
      "Iteration 613, loss = 0.24449235\n",
      "Iteration 614, loss = 0.24422373\n",
      "Iteration 615, loss = 0.24395586\n",
      "Iteration 616, loss = 0.24368873\n",
      "Iteration 617, loss = 0.24342235\n",
      "Iteration 618, loss = 0.24315671\n",
      "Iteration 619, loss = 0.24289181\n",
      "Iteration 620, loss = 0.24262765\n",
      "Iteration 621, loss = 0.24236422\n",
      "Iteration 622, loss = 0.24210152\n",
      "Iteration 623, loss = 0.24183954\n",
      "Iteration 624, loss = 0.24157829\n",
      "Iteration 625, loss = 0.24131777\n",
      "Iteration 626, loss = 0.24105796\n",
      "Iteration 627, loss = 0.24079886\n",
      "Iteration 628, loss = 0.24054048\n",
      "Iteration 629, loss = 0.24028281\n",
      "Iteration 630, loss = 0.24002585\n",
      "Iteration 631, loss = 0.23976960\n",
      "Iteration 632, loss = 0.23951404\n",
      "Iteration 633, loss = 0.23925919\n",
      "Iteration 634, loss = 0.23900503\n",
      "Iteration 635, loss = 0.23875157\n",
      "Iteration 636, loss = 0.23849879\n",
      "Iteration 637, loss = 0.23824671\n",
      "Iteration 638, loss = 0.23799531\n",
      "Iteration 639, loss = 0.23774460\n",
      "Iteration 640, loss = 0.23749456\n",
      "Iteration 641, loss = 0.23724521\n",
      "Iteration 642, loss = 0.23699653\n",
      "Iteration 643, loss = 0.23674852\n",
      "Iteration 644, loss = 0.23650119\n",
      "Iteration 645, loss = 0.23625452\n",
      "Iteration 646, loss = 0.23600852\n",
      "Iteration 647, loss = 0.23576318\n",
      "Iteration 648, loss = 0.23551849\n",
      "Iteration 649, loss = 0.23527447\n",
      "Iteration 650, loss = 0.23503110\n",
      "Iteration 651, loss = 0.23478839\n",
      "Iteration 652, loss = 0.23454632\n",
      "Iteration 653, loss = 0.23430490\n",
      "Iteration 654, loss = 0.23406413\n",
      "Iteration 655, loss = 0.23382400\n",
      "Iteration 656, loss = 0.23358451\n",
      "Iteration 657, loss = 0.23334565\n",
      "Iteration 658, loss = 0.23310743\n",
      "Iteration 659, loss = 0.23286985\n",
      "Iteration 660, loss = 0.23263289\n",
      "Iteration 661, loss = 0.23239656\n",
      "Iteration 662, loss = 0.23216086\n",
      "Iteration 663, loss = 0.23192578\n",
      "Iteration 664, loss = 0.23169132\n",
      "Iteration 665, loss = 0.23145748\n",
      "Iteration 666, loss = 0.23122425\n",
      "Iteration 667, loss = 0.23099164\n",
      "Iteration 668, loss = 0.23075964\n",
      "Iteration 669, loss = 0.23052825\n",
      "Iteration 670, loss = 0.23029746\n",
      "Iteration 671, loss = 0.23006728\n",
      "Iteration 672, loss = 0.22983770\n",
      "Iteration 673, loss = 0.22960872\n",
      "Iteration 674, loss = 0.22938033\n",
      "Iteration 675, loss = 0.22915254\n",
      "Iteration 676, loss = 0.22892535\n",
      "Iteration 677, loss = 0.22869874\n",
      "Iteration 678, loss = 0.22847272\n",
      "Iteration 679, loss = 0.22824729\n",
      "Iteration 680, loss = 0.22802244\n",
      "Iteration 681, loss = 0.22779817\n",
      "Iteration 682, loss = 0.22757448\n",
      "Iteration 683, loss = 0.22735137\n",
      "Iteration 684, loss = 0.22712883\n",
      "Iteration 685, loss = 0.22690687\n",
      "Iteration 686, loss = 0.22668548\n",
      "Iteration 687, loss = 0.22646465\n",
      "Iteration 688, loss = 0.22624439\n",
      "Iteration 689, loss = 0.22602469\n",
      "Iteration 690, loss = 0.22580556\n",
      "Iteration 691, loss = 0.22558699\n",
      "Iteration 692, loss = 0.22536897\n",
      "Iteration 693, loss = 0.22515151\n",
      "Iteration 694, loss = 0.22493460\n",
      "Iteration 695, loss = 0.22471824\n",
      "Iteration 696, loss = 0.22450243\n",
      "Iteration 697, loss = 0.22428717\n",
      "Iteration 698, loss = 0.22407246\n",
      "Iteration 699, loss = 0.22385829\n",
      "Iteration 700, loss = 0.22364465\n",
      "Iteration 701, loss = 0.22343156\n",
      "Iteration 702, loss = 0.22321900\n",
      "Iteration 703, loss = 0.22300698\n",
      "Iteration 704, loss = 0.22279549\n",
      "Iteration 705, loss = 0.22258454\n",
      "Iteration 706, loss = 0.22237411\n",
      "Iteration 707, loss = 0.22216421\n",
      "Iteration 708, loss = 0.22195483\n",
      "Iteration 709, loss = 0.22174598\n",
      "Iteration 710, loss = 0.22153764\n",
      "Iteration 711, loss = 0.22132983\n",
      "Iteration 712, loss = 0.22112253\n",
      "Iteration 713, loss = 0.22091575\n",
      "Iteration 714, loss = 0.22070948\n",
      "Iteration 715, loss = 0.22050373\n",
      "Iteration 716, loss = 0.22029848\n",
      "Iteration 717, loss = 0.22009374\n",
      "Iteration 718, loss = 0.21988950\n",
      "Iteration 719, loss = 0.21968577\n",
      "Iteration 720, loss = 0.21948255\n",
      "Iteration 721, loss = 0.21927982\n",
      "Iteration 722, loss = 0.21907759\n",
      "Iteration 723, loss = 0.21887585\n",
      "Iteration 724, loss = 0.21867461\n",
      "Iteration 725, loss = 0.21847387\n",
      "Iteration 726, loss = 0.21827361\n",
      "Iteration 727, loss = 0.21807385\n",
      "Iteration 728, loss = 0.21787457\n",
      "Iteration 729, loss = 0.21767577\n",
      "Iteration 730, loss = 0.21747746\n",
      "Iteration 731, loss = 0.21727964\n",
      "Iteration 732, loss = 0.21708229\n",
      "Iteration 733, loss = 0.21688542\n",
      "Iteration 734, loss = 0.21668903\n",
      "Iteration 735, loss = 0.21649311\n",
      "Iteration 736, loss = 0.21629766\n",
      "Iteration 737, loss = 0.21610269\n",
      "Iteration 738, loss = 0.21590819\n",
      "Iteration 739, loss = 0.21571415\n",
      "Iteration 740, loss = 0.21552059\n",
      "Iteration 741, loss = 0.21532748\n",
      "Iteration 742, loss = 0.21513484\n",
      "Iteration 743, loss = 0.21494266\n",
      "Iteration 744, loss = 0.21475094\n",
      "Iteration 745, loss = 0.21455968\n",
      "Iteration 746, loss = 0.21436887\n",
      "Iteration 747, loss = 0.21417852\n",
      "Iteration 748, loss = 0.21398862\n",
      "Iteration 749, loss = 0.21379918\n",
      "Iteration 750, loss = 0.21361018\n",
      "Iteration 751, loss = 0.21342163\n",
      "Iteration 752, loss = 0.21323353\n",
      "Iteration 753, loss = 0.21304587\n",
      "Iteration 754, loss = 0.21285866\n",
      "Iteration 755, loss = 0.21267188\n",
      "Iteration 756, loss = 0.21248555\n",
      "Iteration 757, loss = 0.21229966\n",
      "Iteration 758, loss = 0.21211420\n",
      "Iteration 759, loss = 0.21192918\n",
      "Iteration 760, loss = 0.21174459\n",
      "Iteration 761, loss = 0.21156044\n",
      "Iteration 762, loss = 0.21137671\n",
      "Iteration 763, loss = 0.21119342\n",
      "Iteration 764, loss = 0.21101055\n",
      "Iteration 765, loss = 0.21082811\n",
      "Iteration 766, loss = 0.21064609\n",
      "Iteration 767, loss = 0.21046450\n",
      "Iteration 768, loss = 0.21028333\n",
      "Iteration 769, loss = 0.21010258\n",
      "Iteration 770, loss = 0.20992224\n",
      "Iteration 771, loss = 0.20974233\n",
      "Iteration 772, loss = 0.20956283\n",
      "Iteration 773, loss = 0.20938374\n",
      "Iteration 774, loss = 0.20920507\n",
      "Iteration 775, loss = 0.20902681\n",
      "Iteration 776, loss = 0.20884896\n",
      "Iteration 777, loss = 0.20867152\n",
      "Iteration 778, loss = 0.20849448\n",
      "Iteration 779, loss = 0.20831785\n",
      "Iteration 780, loss = 0.20814163\n",
      "Iteration 781, loss = 0.20796580\n",
      "Iteration 782, loss = 0.20779038\n",
      "Iteration 783, loss = 0.20761536\n",
      "Iteration 784, loss = 0.20744074\n",
      "Iteration 785, loss = 0.20726651\n",
      "Iteration 786, loss = 0.20709268\n",
      "Iteration 787, loss = 0.20691925\n",
      "Iteration 788, loss = 0.20674620\n",
      "Iteration 789, loss = 0.20657355\n",
      "Iteration 790, loss = 0.20640129\n",
      "Iteration 791, loss = 0.20622942\n",
      "Iteration 792, loss = 0.20605794\n",
      "Iteration 793, loss = 0.20588684\n",
      "Iteration 794, loss = 0.20571613\n",
      "Iteration 795, loss = 0.20554580\n",
      "Iteration 796, loss = 0.20537586\n",
      "Iteration 797, loss = 0.20520629\n",
      "Iteration 798, loss = 0.20503711\n",
      "Iteration 799, loss = 0.20486830\n",
      "Iteration 800, loss = 0.20469987\n",
      "Iteration 801, loss = 0.20453182\n",
      "Iteration 802, loss = 0.20436414\n",
      "Iteration 803, loss = 0.20419683\n",
      "Iteration 804, loss = 0.20402990\n",
      "Iteration 805, loss = 0.20386333\n",
      "Iteration 806, loss = 0.20369714\n",
      "Iteration 807, loss = 0.20353131\n",
      "Iteration 808, loss = 0.20336586\n",
      "Iteration 809, loss = 0.20320076\n",
      "Iteration 810, loss = 0.20303604\n",
      "Iteration 811, loss = 0.20287167\n",
      "Iteration 812, loss = 0.20270767\n",
      "Iteration 813, loss = 0.20254403\n",
      "Iteration 814, loss = 0.20238074\n",
      "Iteration 815, loss = 0.20221782\n",
      "Iteration 816, loss = 0.20205525\n",
      "Iteration 817, loss = 0.20189304\n",
      "Iteration 818, loss = 0.20173119\n",
      "Iteration 819, loss = 0.20156968\n",
      "Iteration 820, loss = 0.20140853\n",
      "Iteration 821, loss = 0.20124774\n",
      "Iteration 822, loss = 0.20108729\n",
      "Iteration 823, loss = 0.20092719\n",
      "Iteration 824, loss = 0.20076743\n",
      "Iteration 825, loss = 0.20060803\n",
      "Iteration 826, loss = 0.20044897\n",
      "Iteration 827, loss = 0.20029025\n",
      "Iteration 828, loss = 0.20013188\n",
      "Iteration 829, loss = 0.19997385\n",
      "Iteration 830, loss = 0.19981616\n",
      "Iteration 831, loss = 0.19965881\n",
      "Iteration 832, loss = 0.19950180\n",
      "Iteration 833, loss = 0.19934512\n",
      "Iteration 834, loss = 0.19918879\n",
      "Iteration 835, loss = 0.19903278\n",
      "Iteration 836, loss = 0.19887711\n",
      "Iteration 837, loss = 0.19872178\n",
      "Iteration 838, loss = 0.19856677\n",
      "Iteration 839, loss = 0.19841210\n",
      "Iteration 840, loss = 0.19825776\n",
      "Iteration 841, loss = 0.19810374\n",
      "Iteration 842, loss = 0.19795006\n",
      "Iteration 843, loss = 0.19779670\n",
      "Iteration 844, loss = 0.19764366\n",
      "Iteration 845, loss = 0.19749095\n",
      "Iteration 846, loss = 0.19733857\n",
      "Iteration 847, loss = 0.19718650\n",
      "Iteration 848, loss = 0.19703476\n",
      "Iteration 849, loss = 0.19688333\n",
      "Iteration 850, loss = 0.19673223\n",
      "Iteration 851, loss = 0.19658144\n",
      "Iteration 852, loss = 0.19643098\n",
      "Iteration 853, loss = 0.19628082\n",
      "Iteration 854, loss = 0.19613099\n",
      "Iteration 855, loss = 0.19598146\n",
      "Iteration 856, loss = 0.19583225\n",
      "Iteration 857, loss = 0.19568335\n",
      "Iteration 858, loss = 0.19553477\n",
      "Iteration 859, loss = 0.19538649\n",
      "Iteration 860, loss = 0.19523852\n",
      "Iteration 861, loss = 0.19509086\n",
      "Iteration 862, loss = 0.19494351\n",
      "Iteration 863, loss = 0.19479646\n",
      "Iteration 864, loss = 0.19464972\n",
      "Iteration 865, loss = 0.19450328\n",
      "Iteration 866, loss = 0.19435714\n",
      "Iteration 867, loss = 0.19421131\n",
      "Iteration 868, loss = 0.19406578\n",
      "Iteration 869, loss = 0.19392055\n",
      "Iteration 870, loss = 0.19377561\n",
      "Iteration 871, loss = 0.19363098\n",
      "Iteration 872, loss = 0.19348664\n",
      "Iteration 873, loss = 0.19334260\n",
      "Iteration 874, loss = 0.19319886\n",
      "Iteration 875, loss = 0.19305541\n",
      "Iteration 876, loss = 0.19291225\n",
      "Iteration 877, loss = 0.19276939\n",
      "Iteration 878, loss = 0.19262682\n",
      "Iteration 879, loss = 0.19248454\n",
      "Iteration 880, loss = 0.19234255\n",
      "Iteration 881, loss = 0.19220085\n",
      "Iteration 882, loss = 0.19205943\n",
      "Iteration 883, loss = 0.19191831\n",
      "Iteration 884, loss = 0.19177747\n",
      "Iteration 885, loss = 0.19163691\n",
      "Iteration 886, loss = 0.19149664\n",
      "Iteration 887, loss = 0.19135666\n",
      "Iteration 888, loss = 0.19121695\n",
      "Iteration 889, loss = 0.19107753\n",
      "Iteration 890, loss = 0.19093839\n",
      "Iteration 891, loss = 0.19079953\n",
      "Iteration 892, loss = 0.19066095\n",
      "Iteration 893, loss = 0.19052265\n",
      "Iteration 894, loss = 0.19038463\n",
      "Iteration 895, loss = 0.19024688\n",
      "Iteration 896, loss = 0.19010941\n",
      "Iteration 897, loss = 0.18997221\n",
      "Iteration 898, loss = 0.18983529\n",
      "Iteration 899, loss = 0.18969864\n",
      "Iteration 900, loss = 0.18956226\n",
      "Iteration 901, loss = 0.18942616\n",
      "Iteration 902, loss = 0.18929032\n",
      "Iteration 903, loss = 0.18915476\n",
      "Iteration 904, loss = 0.18901946\n",
      "Iteration 905, loss = 0.18888444\n",
      "Iteration 906, loss = 0.18874968\n",
      "Iteration 907, loss = 0.18861519\n",
      "Iteration 908, loss = 0.18848096\n",
      "Iteration 909, loss = 0.18834700\n",
      "Iteration 910, loss = 0.18821330\n",
      "Iteration 911, loss = 0.18807987\n",
      "Iteration 912, loss = 0.18794670\n",
      "Iteration 913, loss = 0.18781379\n",
      "Iteration 914, loss = 0.18768114\n",
      "Iteration 915, loss = 0.18754875\n",
      "Iteration 916, loss = 0.18741663\n",
      "Iteration 917, loss = 0.18728476\n",
      "Iteration 918, loss = 0.18715315\n",
      "Iteration 919, loss = 0.18702179\n",
      "Iteration 920, loss = 0.18689070\n",
      "Iteration 921, loss = 0.18675985\n",
      "Iteration 922, loss = 0.18662927\n",
      "Iteration 923, loss = 0.18649893\n",
      "Iteration 924, loss = 0.18636886\n",
      "Iteration 925, loss = 0.18623903\n",
      "Iteration 926, loss = 0.18610945\n",
      "Iteration 927, loss = 0.18598013\n",
      "Iteration 928, loss = 0.18585106\n",
      "Iteration 929, loss = 0.18572223\n",
      "Iteration 930, loss = 0.18559366\n",
      "Iteration 931, loss = 0.18546533\n",
      "Iteration 932, loss = 0.18533725\n",
      "Iteration 933, loss = 0.18520942\n",
      "Iteration 934, loss = 0.18508183\n",
      "Iteration 935, loss = 0.18495449\n",
      "Iteration 936, loss = 0.18482740\n",
      "Iteration 937, loss = 0.18470054\n",
      "Iteration 938, loss = 0.18457393\n",
      "Iteration 939, loss = 0.18444756\n",
      "Iteration 940, loss = 0.18432144\n",
      "Iteration 941, loss = 0.18419555\n",
      "Iteration 942, loss = 0.18406991\n",
      "Iteration 943, loss = 0.18394450\n",
      "Iteration 944, loss = 0.18381934\n",
      "Iteration 945, loss = 0.18369441\n",
      "Iteration 946, loss = 0.18356972\n",
      "Iteration 947, loss = 0.18344526\n",
      "Iteration 948, loss = 0.18332104\n",
      "Iteration 949, loss = 0.18319706\n",
      "Iteration 950, loss = 0.18307331\n",
      "Iteration 951, loss = 0.18294980\n",
      "Iteration 952, loss = 0.18282652\n",
      "Iteration 953, loss = 0.18270347\n",
      "Iteration 954, loss = 0.18258065\n",
      "Iteration 955, loss = 0.18245807\n",
      "Iteration 956, loss = 0.18233571\n",
      "Iteration 957, loss = 0.18221359\n",
      "Iteration 958, loss = 0.18209169\n",
      "Iteration 959, loss = 0.18197002\n",
      "Iteration 960, loss = 0.18184858\n",
      "Iteration 961, loss = 0.18172737\n",
      "Iteration 962, loss = 0.18160639\n",
      "Iteration 963, loss = 0.18148563\n",
      "Iteration 964, loss = 0.18136510\n",
      "Iteration 965, loss = 0.18124479\n",
      "Iteration 966, loss = 0.18112470\n",
      "Iteration 967, loss = 0.18100484\n",
      "Iteration 968, loss = 0.18088520\n",
      "Iteration 969, loss = 0.18076578\n",
      "Iteration 970, loss = 0.18064659\n",
      "Iteration 971, loss = 0.18052762\n",
      "Iteration 972, loss = 0.18040886\n",
      "Iteration 973, loss = 0.18029033\n",
      "Iteration 974, loss = 0.18017201\n",
      "Iteration 975, loss = 0.18005392\n",
      "Iteration 976, loss = 0.17993604\n",
      "Iteration 977, loss = 0.17981837\n",
      "Iteration 978, loss = 0.17970093\n",
      "Iteration 979, loss = 0.17958370\n",
      "Iteration 980, loss = 0.17946669\n",
      "Iteration 981, loss = 0.17934989\n",
      "Iteration 982, loss = 0.17923330\n",
      "Iteration 983, loss = 0.17911693\n",
      "Iteration 984, loss = 0.17900077\n",
      "Iteration 985, loss = 0.17888482\n",
      "Iteration 986, loss = 0.17876909\n",
      "Iteration 987, loss = 0.17865357\n",
      "Iteration 988, loss = 0.17853825\n",
      "Iteration 989, loss = 0.17842315\n",
      "Iteration 990, loss = 0.17830825\n",
      "Iteration 991, loss = 0.17819357\n",
      "Iteration 992, loss = 0.17807909\n",
      "Iteration 993, loss = 0.17796482\n",
      "Iteration 994, loss = 0.17785076\n",
      "Iteration 995, loss = 0.17773691\n",
      "Iteration 996, loss = 0.17762326\n",
      "Iteration 997, loss = 0.17750981\n",
      "Iteration 998, loss = 0.17739657\n",
      "Iteration 999, loss = 0.17728354\n",
      "Iteration 1000, loss = 0.17717070\n",
      "Iteration 1, loss = 2.14020736\n",
      "Iteration 2, loss = 1.58628739\n",
      "Iteration 3, loss = 1.21341305\n",
      "Iteration 4, loss = 1.21475011\n",
      "Iteration 5, loss = 1.22369678\n",
      "Iteration 6, loss = 1.21684566\n",
      "Iteration 7, loss = 1.20645287\n",
      "Iteration 8, loss = 1.19759552\n",
      "Iteration 9, loss = 1.19312155\n",
      "Iteration 10, loss = 1.18981255\n",
      "Iteration 11, loss = 1.18614504\n",
      "Iteration 12, loss = 1.18160822\n",
      "Iteration 13, loss = 1.17634630\n",
      "Iteration 14, loss = 1.17082427\n",
      "Iteration 15, loss = 1.16530582\n",
      "Iteration 16, loss = 1.16032721\n",
      "Iteration 17, loss = 1.15553718"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 18, loss = 1.15060204\n",
      "Iteration 19, loss = 1.14551470\n",
      "Iteration 20, loss = 1.14042822\n",
      "Iteration 21, loss = 1.13544095\n",
      "Iteration 22, loss = 1.13058790\n",
      "Iteration 23, loss = 1.12578290\n",
      "Iteration 24, loss = 1.12106744\n",
      "Iteration 25, loss = 1.11638653\n",
      "Iteration 26, loss = 1.11174505\n",
      "Iteration 27, loss = 1.10719819\n",
      "Iteration 28, loss = 1.10261836\n",
      "Iteration 29, loss = 1.09813988\n",
      "Iteration 30, loss = 1.09366364\n",
      "Iteration 31, loss = 1.08916377\n",
      "Iteration 32, loss = 1.08467716\n",
      "Iteration 33, loss = 1.08015368\n",
      "Iteration 34, loss = 1.07556682\n",
      "Iteration 35, loss = 1.07080562\n",
      "Iteration 36, loss = 1.06594703\n",
      "Iteration 37, loss = 1.06100571\n",
      "Iteration 38, loss = 1.05592499\n",
      "Iteration 39, loss = 1.05070119\n",
      "Iteration 40, loss = 1.04533628\n",
      "Iteration 41, loss = 1.03983700\n",
      "Iteration 42, loss = 1.03421335\n",
      "Iteration 43, loss = 1.02847679\n",
      "Iteration 44, loss = 1.02265819\n",
      "Iteration 45, loss = 1.01679279\n",
      "Iteration 46, loss = 1.01082887\n",
      "Iteration 47, loss = 1.00477500\n",
      "Iteration 48, loss = 0.99864227\n",
      "Iteration 49, loss = 0.99244371\n",
      "Iteration 50, loss = 0.98619329\n",
      "Iteration 51, loss = 0.97990459\n",
      "Iteration 52, loss = 0.97358967\n",
      "Iteration 53, loss = 0.96725815\n",
      "Iteration 54, loss = 0.96091700\n",
      "Iteration 55, loss = 0.95457076\n",
      "Iteration 56, loss = 0.94822225\n",
      "Iteration 57, loss = 0.94187320\n",
      "Iteration 58, loss = 0.93552479\n",
      "Iteration 59, loss = 0.92917796\n",
      "Iteration 60, loss = 0.92283342\n",
      "Iteration 61, loss = 0.91649163\n",
      "Iteration 62, loss = 0.91015276\n",
      "Iteration 63, loss = 0.90381675\n",
      "Iteration 64, loss = 0.89748351\n",
      "Iteration 65, loss = 0.89115296\n",
      "Iteration 66, loss = 0.88482519\n",
      "Iteration 67, loss = 0.87850044\n",
      "Iteration 68, loss = 0.87217906\n",
      "Iteration 69, loss = 0.86586148\n",
      "Iteration 70, loss = 0.85954813\n",
      "Iteration 71, loss = 0.85323952\n",
      "Iteration 72, loss = 0.84693617\n",
      "Iteration 73, loss = 0.84063869\n",
      "Iteration 74, loss = 0.83434777\n",
      "Iteration 75, loss = 0.82806425\n",
      "Iteration 76, loss = 0.82178904\n",
      "Iteration 77, loss = 0.81552321\n",
      "Iteration 78, loss = 0.80926795\n",
      "Iteration 79, loss = 0.80302457\n",
      "Iteration 80, loss = 0.79679457\n",
      "Iteration 81, loss = 0.79057956\n",
      "Iteration 82, loss = 0.78438138\n",
      "Iteration 83, loss = 0.77820199\n",
      "Iteration 84, loss = 0.77204358\n",
      "Iteration 85, loss = 0.76590851\n",
      "Iteration 86, loss = 0.75979936\n",
      "Iteration 87, loss = 0.75371890\n",
      "Iteration 88, loss = 0.74767012\n",
      "Iteration 89, loss = 0.74165621\n",
      "Iteration 90, loss = 0.73568059\n",
      "Iteration 91, loss = 0.72974684\n",
      "Iteration 92, loss = 0.72385878\n",
      "Iteration 93, loss = 0.71802037\n",
      "Iteration 94, loss = 0.71223572\n",
      "Iteration 95, loss = 0.70650907\n",
      "Iteration 96, loss = 0.70084473\n",
      "Iteration 97, loss = 0.69524707\n",
      "Iteration 98, loss = 0.68972044\n",
      "Iteration 99, loss = 0.68426914\n",
      "Iteration 100, loss = 0.67889736\n",
      "Iteration 101, loss = 0.67360911\n",
      "Iteration 102, loss = 0.66840814\n",
      "Iteration 103, loss = 0.66329794\n",
      "Iteration 104, loss = 0.65828160\n",
      "Iteration 105, loss = 0.65336182\n",
      "Iteration 106, loss = 0.64854080\n",
      "Iteration 107, loss = 0.64382026\n",
      "Iteration 108, loss = 0.63920137\n",
      "Iteration 109, loss = 0.63468473\n",
      "Iteration 110, loss = 0.63027041\n",
      "Iteration 111, loss = 0.62595789\n",
      "Iteration 112, loss = 0.62174618\n",
      "Iteration 113, loss = 0.61763377\n",
      "Iteration 114, loss = 0.61361875\n",
      "Iteration 115, loss = 0.60969885\n",
      "Iteration 116, loss = 0.60587150\n",
      "Iteration 117, loss = 0.60213390\n",
      "Iteration 118, loss = 0.59848311\n",
      "Iteration 119, loss = 0.59491613\n",
      "Iteration 120, loss = 0.59142990\n",
      "Iteration 121, loss = 0.58802142\n",
      "Iteration 122, loss = 0.58468777\n",
      "Iteration 123, loss = 0.58142612\n",
      "Iteration 124, loss = 0.57823378\n",
      "Iteration 125, loss = 0.57510821\n",
      "Iteration 126, loss = 0.57204703\n",
      "Iteration 127, loss = 0.56904800\n",
      "Iteration 128, loss = 0.56610904\n",
      "Iteration 129, loss = 0.56322821\n",
      "Iteration 130, loss = 0.56040371\n",
      "Iteration 131, loss = 0.55763388\n",
      "Iteration 132, loss = 0.55491714\n",
      "Iteration 133, loss = 0.55225207\n",
      "Iteration 134, loss = 0.54963729\n",
      "Iteration 135, loss = 0.54707153\n",
      "Iteration 136, loss = 0.54455359\n",
      "Iteration 137, loss = 0.54208232\n",
      "Iteration 138, loss = 0.53965664\n",
      "Iteration 139, loss = 0.53727550\n",
      "Iteration 140, loss = 0.53493788\n",
      "Iteration 141, loss = 0.53264281\n",
      "Iteration 142, loss = 0.53038931\n",
      "Iteration 143, loss = 0.52817645\n",
      "Iteration 144, loss = 0.52600330\n",
      "Iteration 145, loss = 0.52386895\n",
      "Iteration 146, loss = 0.52177248\n",
      "Iteration 147, loss = 0.51971301\n",
      "Iteration 148, loss = 0.51768965\n",
      "Iteration 149, loss = 0.51570152\n",
      "Iteration 150, loss = 0.51374774\n",
      "Iteration 151, loss = 0.51182747\n",
      "Iteration 152, loss = 0.50993984\n",
      "Iteration 153, loss = 0.50808403\n",
      "Iteration 154, loss = 0.50625918\n",
      "Iteration 155, loss = 0.50446449\n",
      "Iteration 156, loss = 0.50269915\n",
      "Iteration 157, loss = 0.50096237\n",
      "Iteration 158, loss = 0.49925336\n",
      "Iteration 159, loss = 0.49757137\n",
      "Iteration 160, loss = 0.49591564\n",
      "Iteration 161, loss = 0.49428544\n",
      "Iteration 162, loss = 0.49268005\n",
      "Iteration 163, loss = 0.49109878\n",
      "Iteration 164, loss = 0.48954095\n",
      "Iteration 165, loss = 0.48800590\n",
      "Iteration 166, loss = 0.48649297\n",
      "Iteration 167, loss = 0.48500154\n",
      "Iteration 168, loss = 0.48353101\n",
      "Iteration 169, loss = 0.48208078\n",
      "Iteration 170, loss = 0.48065027\n",
      "Iteration 171, loss = 0.47923894\n",
      "Iteration 172, loss = 0.47784624\n",
      "Iteration 173, loss = 0.47647165\n",
      "Iteration 174, loss = 0.47511467\n",
      "Iteration 175, loss = 0.47377480\n",
      "Iteration 176, loss = 0.47245157\n",
      "Iteration 177, loss = 0.47114452\n",
      "Iteration 178, loss = 0.46985321\n",
      "Iteration 179, loss = 0.46857720\n",
      "Iteration 180, loss = 0.46731609\n",
      "Iteration 181, loss = 0.46606945\n",
      "Iteration 182, loss = 0.46483691\n",
      "Iteration 183, loss = 0.46361809\n",
      "Iteration 184, loss = 0.46241261\n",
      "Iteration 185, loss = 0.46122013\n",
      "Iteration 186, loss = 0.46004030\n",
      "Iteration 187, loss = 0.45887278\n",
      "Iteration 188, loss = 0.45771725\n",
      "Iteration 189, loss = 0.45657341\n",
      "Iteration 190, loss = 0.45544094\n",
      "Iteration 191, loss = 0.45431955\n",
      "Iteration 192, loss = 0.45320896\n",
      "Iteration 193, loss = 0.45210889\n",
      "Iteration 194, loss = 0.45101907\n",
      "Iteration 195, loss = 0.44993924\n",
      "Iteration 196, loss = 0.44886916\n",
      "Iteration 197, loss = 0.44780857\n",
      "Iteration 198, loss = 0.44675724\n",
      "Iteration 199, loss = 0.44571494\n",
      "Iteration 200, loss = 0.44468145\n",
      "Iteration 201, loss = 0.44365654\n",
      "Iteration 202, loss = 0.44264002\n",
      "Iteration 203, loss = 0.44163168\n",
      "Iteration 204, loss = 0.44063132\n",
      "Iteration 205, loss = 0.43963876\n",
      "Iteration 206, loss = 0.43865379\n",
      "Iteration 207, loss = 0.43767626\n",
      "Iteration 208, loss = 0.43670597\n",
      "Iteration 209, loss = 0.43574277\n",
      "Iteration 210, loss = 0.43478649\n",
      "Iteration 211, loss = 0.43383696\n",
      "Iteration 212, loss = 0.43289405\n",
      "Iteration 213, loss = 0.43195759\n",
      "Iteration 214, loss = 0.43102744\n",
      "Iteration 215, loss = 0.43010346\n",
      "Iteration 216, loss = 0.42918551\n",
      "Iteration 217, loss = 0.42827347\n",
      "Iteration 218, loss = 0.42736720\n",
      "Iteration 219, loss = 0.42646659\n",
      "Iteration 220, loss = 0.42557150\n",
      "Iteration 221, loss = 0.42468182\n",
      "Iteration 222, loss = 0.42379744\n",
      "Iteration 223, loss = 0.42291824\n",
      "Iteration 224, loss = 0.42204413\n",
      "Iteration 225, loss = 0.42117499\n",
      "Iteration 226, loss = 0.42031073\n",
      "Iteration 227, loss = 0.41945125\n",
      "Iteration 228, loss = 0.41859645\n",
      "Iteration 229, loss = 0.41774625\n",
      "Iteration 230, loss = 0.41690054\n",
      "Iteration 231, loss = 0.41605925\n",
      "Iteration 232, loss = 0.41522230\n",
      "Iteration 233, loss = 0.41438959\n",
      "Iteration 234, loss = 0.41356105\n",
      "Iteration 235, loss = 0.41273661\n",
      "Iteration 236, loss = 0.41191619\n",
      "Iteration 237, loss = 0.41109972\n",
      "Iteration 238, loss = 0.41028712\n",
      "Iteration 239, loss = 0.40947833\n",
      "Iteration 240, loss = 0.40867328\n",
      "Iteration 241, loss = 0.40787192\n",
      "Iteration 242, loss = 0.40707417\n",
      "Iteration 243, loss = 0.40627998\n",
      "Iteration 244, loss = 0.40548928\n",
      "Iteration 245, loss = 0.40470203\n",
      "Iteration 246, loss = 0.40391817\n",
      "Iteration 247, loss = 0.40313763\n",
      "Iteration 248, loss = 0.40236038\n",
      "Iteration 249, loss = 0.40158636\n",
      "Iteration 250, loss = 0.40081553\n",
      "Iteration 251, loss = 0.40004782\n",
      "Iteration 252, loss = 0.39928321\n",
      "Iteration 253, loss = 0.39852164\n",
      "Iteration 254, loss = 0.39776307\n",
      "Iteration 255, loss = 0.39700747\n",
      "Iteration 256, loss = 0.39625478\n",
      "Iteration 257, loss = 0.39550497\n",
      "Iteration 258, loss = 0.39475800\n",
      "Iteration 259, loss = 0.39401384\n",
      "Iteration 260, loss = 0.39327245\n",
      "Iteration 261, loss = 0.39253379\n",
      "Iteration 262, loss = 0.39179783\n",
      "Iteration 263, loss = 0.39106454\n",
      "Iteration 264, loss = 0.39033388\n",
      "Iteration 265, loss = 0.38960583\n",
      "Iteration 266, loss = 0.38888036\n",
      "Iteration 267, loss = 0.38815743\n",
      "Iteration 268, loss = 0.38743703\n",
      "Iteration 269, loss = 0.38671911\n",
      "Iteration 270, loss = 0.38600366\n",
      "Iteration 271, loss = 0.38529064\n",
      "Iteration 272, loss = 0.38458005\n",
      "Iteration 273, loss = 0.38387184\n",
      "Iteration 274, loss = 0.38316600\n",
      "Iteration 275, loss = 0.38246251\n",
      "Iteration 276, loss = 0.38176134\n",
      "Iteration 277, loss = 0.38106248\n",
      "Iteration 278, loss = 0.38036590\n",
      "Iteration 279, loss = 0.37967158\n",
      "Iteration 280, loss = 0.37897950\n",
      "Iteration 281, loss = 0.37828965\n",
      "Iteration 282, loss = 0.37760200\n",
      "Iteration 283, loss = 0.37691654\n",
      "Iteration 284, loss = 0.37623326\n",
      "Iteration 285, loss = 0.37555213\n",
      "Iteration 286, loss = 0.37487314\n",
      "Iteration 287, loss = 0.37419627\n",
      "Iteration 288, loss = 0.37352152\n",
      "Iteration 289, loss = 0.37284886\n",
      "Iteration 290, loss = 0.37217828\n",
      "Iteration 291, loss = 0.37150977\n",
      "Iteration 292, loss = 0.37084332\n",
      "Iteration 293, loss = 0.37017890\n",
      "Iteration 294, loss = 0.36951652\n",
      "Iteration 295, loss = 0.36885615\n",
      "Iteration 296, loss = 0.36819779\n",
      "Iteration 297, loss = 0.36754142\n",
      "Iteration 298, loss = 0.36688704\n",
      "Iteration 299, loss = 0.36623463\n",
      "Iteration 300, loss = 0.36558418\n",
      "Iteration 301, loss = 0.36493569\n",
      "Iteration 302, loss = 0.36428914\n",
      "Iteration 303, loss = 0.36364453\n",
      "Iteration 304, loss = 0.36300184\n",
      "Iteration 305, loss = 0.36236106\n",
      "Iteration 306, loss = 0.36172219\n",
      "Iteration 307, loss = 0.36108523\n",
      "Iteration 308, loss = 0.36045015\n",
      "Iteration 309, loss = 0.35981696\n",
      "Iteration 310, loss = 0.35918564\n",
      "Iteration 311, loss = 0.35855620\n",
      "Iteration 312, loss = 0.35792861\n",
      "Iteration 313, loss = 0.35730288\n",
      "Iteration 314, loss = 0.35667900\n",
      "Iteration 315, loss = 0.35605696\n",
      "Iteration 316, loss = 0.35543675\n",
      "Iteration 317, loss = 0.35481837\n",
      "Iteration 318, loss = 0.35420181\n",
      "Iteration 319, loss = 0.35358708\n",
      "Iteration 320, loss = 0.35297415\n",
      "Iteration 321, loss = 0.35236303\n",
      "Iteration 322, loss = 0.35175370\n",
      "Iteration 323, loss = 0.35114618\n",
      "Iteration 324, loss = 0.35054044\n",
      "Iteration 325, loss = 0.34993648\n",
      "Iteration 326, loss = 0.34933431\n",
      "Iteration 327, loss = 0.34873391\n",
      "Iteration 328, loss = 0.34813528\n",
      "Iteration 329, loss = 0.34753841\n",
      "Iteration 330, loss = 0.34694331\n",
      "Iteration 331, loss = 0.34634996\n",
      "Iteration 332, loss = 0.34575836\n",
      "Iteration 333, loss = 0.34516851\n",
      "Iteration 334, loss = 0.34458041\n",
      "Iteration 335, loss = 0.34399404\n",
      "Iteration 336, loss = 0.34340941\n",
      "Iteration 337, loss = 0.34282651\n",
      "Iteration 338, loss = 0.34224533\n",
      "Iteration 339, loss = 0.34166588\n",
      "Iteration 340, loss = 0.34108815\n",
      "Iteration 341, loss = 0.34051214\n",
      "Iteration 342, loss = 0.33993784\n",
      "Iteration 343, loss = 0.33936524\n",
      "Iteration 344, loss = 0.33879435\n",
      "Iteration 345, loss = 0.33822516\n",
      "Iteration 346, loss = 0.33765767\n",
      "Iteration 347, loss = 0.33709188\n",
      "Iteration 348, loss = 0.33652777\n",
      "Iteration 349, loss = 0.33596535\n",
      "Iteration 350, loss = 0.33540462\n",
      "Iteration 351, loss = 0.33484556\n",
      "Iteration 352, loss = 0.33428819\n",
      "Iteration 353, loss = 0.33373248\n",
      "Iteration 354, loss = 0.33317845\n",
      "Iteration 355, loss = 0.33262609\n",
      "Iteration 356, loss = 0.33207538\n",
      "Iteration 357, loss = 0.33152634\n",
      "Iteration 358, loss = 0.33097896\n",
      "Iteration 359, loss = 0.33043323\n",
      "Iteration 360, loss = 0.32988915\n",
      "Iteration 361, loss = 0.32934672\n",
      "Iteration 362, loss = 0.32880593\n",
      "Iteration 363, loss = 0.32826679\n",
      "Iteration 364, loss = 0.32772928\n",
      "Iteration 365, loss = 0.32719340\n",
      "Iteration 366, loss = 0.32665916\n",
      "Iteration 367, loss = 0.32612655\n",
      "Iteration 368, loss = 0.32559556\n",
      "Iteration 369, loss = 0.32506619\n",
      "Iteration 370, loss = 0.32453844\n",
      "Iteration 371, loss = 0.32401231\n",
      "Iteration 372, loss = 0.32348779\n",
      "Iteration 373, loss = 0.32296488\n",
      "Iteration 374, loss = 0.32244357\n",
      "Iteration 375, loss = 0.32192386\n",
      "Iteration 376, loss = 0.32140576\n",
      "Iteration 377, loss = 0.32088924\n",
      "Iteration 378, loss = 0.32037432\n",
      "Iteration 379, loss = 0.31986099\n",
      "Iteration 380, loss = 0.31934925\n",
      "Iteration 381, loss = 0.31883908\n",
      "Iteration 382, loss = 0.31833050\n",
      "Iteration 383, loss = 0.31782349\n",
      "Iteration 384, loss = 0.31731805\n",
      "Iteration 385, loss = 0.31681418\n",
      "Iteration 386, loss = 0.31631188\n",
      "Iteration 387, loss = 0.31581114\n",
      "Iteration 388, loss = 0.31531195\n",
      "Iteration 389, loss = 0.31481432\n",
      "Iteration 390, loss = 0.31431824\n",
      "Iteration 391, loss = 0.31382371\n",
      "Iteration 392, loss = 0.31333073\n",
      "Iteration 393, loss = 0.31283928\n",
      "Iteration 394, loss = 0.31234937\n",
      "Iteration 395, loss = 0.31186099\n",
      "Iteration 396, loss = 0.31137414\n",
      "Iteration 397, loss = 0.31088882\n",
      "Iteration 398, loss = 0.31040502\n",
      "Iteration 399, loss = 0.30992274\n",
      "Iteration 400, loss = 0.30944198\n",
      "Iteration 401, loss = 0.30896272\n",
      "Iteration 402, loss = 0.30848498\n",
      "Iteration 403, loss = 0.30800873\n",
      "Iteration 404, loss = 0.30753399\n",
      "Iteration 405, loss = 0.30706074\n",
      "Iteration 406, loss = 0.30658899\n",
      "Iteration 407, loss = 0.30611872\n",
      "Iteration 408, loss = 0.30564994\n",
      "Iteration 409, loss = 0.30518264\n",
      "Iteration 410, loss = 0.30471682\n",
      "Iteration 411, loss = 0.30425247\n",
      "Iteration 412, loss = 0.30378959\n",
      "Iteration 413, loss = 0.30332817\n",
      "Iteration 414, loss = 0.30286822\n",
      "Iteration 415, loss = 0.30240972\n",
      "Iteration 416, loss = 0.30195268\n",
      "Iteration 417, loss = 0.30149708\n",
      "Iteration 418, loss = 0.30104293\n",
      "Iteration 419, loss = 0.30059022\n",
      "Iteration 420, loss = 0.30013895\n",
      "Iteration 421, loss = 0.29968912\n",
      "Iteration 422, loss = 0.29924071\n",
      "Iteration 423, loss = 0.29879373\n",
      "Iteration 424, loss = 0.29834817\n",
      "Iteration 425, loss = 0.29790402\n",
      "Iteration 426, loss = 0.29746129\n",
      "Iteration 427, loss = 0.29701997\n",
      "Iteration 428, loss = 0.29658005\n",
      "Iteration 429, loss = 0.29614153\n",
      "Iteration 430, loss = 0.29570441\n",
      "Iteration 431, loss = 0.29526868\n",
      "Iteration 432, loss = 0.29483434\n",
      "Iteration 433, loss = 0.29440138\n",
      "Iteration 434, loss = 0.29396980\n",
      "Iteration 435, loss = 0.29353959\n",
      "Iteration 436, loss = 0.29311076\n",
      "Iteration 437, loss = 0.29268329\n",
      "Iteration 438, loss = 0.29225719\n",
      "Iteration 439, loss = 0.29183244\n",
      "Iteration 440, loss = 0.29140905\n",
      "Iteration 441, loss = 0.29098700\n",
      "Iteration 442, loss = 0.29056631\n",
      "Iteration 443, loss = 0.29014695\n",
      "Iteration 444, loss = 0.28972893\n",
      "Iteration 445, loss = 0.28931224\n",
      "Iteration 446, loss = 0.28889688\n",
      "Iteration 447, loss = 0.28848284\n",
      "Iteration 448, loss = 0.28807012\n",
      "Iteration 449, loss = 0.28765872\n",
      "Iteration 450, loss = 0.28724863\n",
      "Iteration 451, loss = 0.28683984\n",
      "Iteration 452, loss = 0.28643235\n",
      "Iteration 453, loss = 0.28602617\n",
      "Iteration 454, loss = 0.28562127\n",
      "Iteration 455, loss = 0.28521767\n",
      "Iteration 456, loss = 0.28481534\n",
      "Iteration 457, loss = 0.28441430\n",
      "Iteration 458, loss = 0.28401453\n",
      "Iteration 459, loss = 0.28361604\n",
      "Iteration 460, loss = 0.28321881\n",
      "Iteration 461, loss = 0.28282284\n",
      "Iteration 462, loss = 0.28242813\n",
      "Iteration 463, loss = 0.28203468\n",
      "Iteration 464, loss = 0.28164247\n",
      "Iteration 465, loss = 0.28125151\n",
      "Iteration 466, loss = 0.28086179\n",
      "Iteration 467, loss = 0.28047330\n",
      "Iteration 468, loss = 0.28008605\n",
      "Iteration 469, loss = 0.27970002\n",
      "Iteration 470, loss = 0.27931521\n",
      "Iteration 471, loss = 0.27893163\n",
      "Iteration 472, loss = 0.27854925\n",
      "Iteration 473, loss = 0.27816809\n",
      "Iteration 474, loss = 0.27778813\n",
      "Iteration 475, loss = 0.27740937\n",
      "Iteration 476, loss = 0.27703181\n",
      "Iteration 477, loss = 0.27665544\n",
      "Iteration 478, loss = 0.27628026\n",
      "Iteration 479, loss = 0.27590626\n",
      "Iteration 480, loss = 0.27553343\n",
      "Iteration 481, loss = 0.27516179\n",
      "Iteration 482, loss = 0.27479131\n",
      "Iteration 483, loss = 0.27442200\n",
      "Iteration 484, loss = 0.27405385\n",
      "Iteration 485, loss = 0.27368685\n",
      "Iteration 486, loss = 0.27332101\n",
      "Iteration 487, loss = 0.27295632\n",
      "Iteration 488, loss = 0.27259277\n",
      "Iteration 489, loss = 0.27223036\n",
      "Iteration 490, loss = 0.27186908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 491, loss = 0.27150894\n",
      "Iteration 492, loss = 0.27114992\n",
      "Iteration 493, loss = 0.27079202\n",
      "Iteration 494, loss = 0.27043524\n",
      "Iteration 495, loss = 0.27007958\n",
      "Iteration 496, loss = 0.26972502\n",
      "Iteration 497, loss = 0.26937157\n",
      "Iteration 498, loss = 0.26901922\n",
      "Iteration 499, loss = 0.26866797\n",
      "Iteration 500, loss = 0.26831780\n",
      "Iteration 501, loss = 0.26796873\n",
      "Iteration 502, loss = 0.26762074\n",
      "Iteration 503, loss = 0.26727383\n",
      "Iteration 504, loss = 0.26692799\n",
      "Iteration 505, loss = 0.26658323\n",
      "Iteration 506, loss = 0.26623953\n",
      "Iteration 507, loss = 0.26589689\n",
      "Iteration 508, loss = 0.26555531\n",
      "Iteration 509, loss = 0.26521479\n",
      "Iteration 510, loss = 0.26487531\n",
      "Iteration 511, loss = 0.26453689\n",
      "Iteration 512, loss = 0.26419950\n",
      "Iteration 513, loss = 0.26386315\n",
      "Iteration 514, loss = 0.26352783\n",
      "Iteration 515, loss = 0.26319355\n",
      "Iteration 516, loss = 0.26286028\n",
      "Iteration 517, loss = 0.26252804\n",
      "Iteration 518, loss = 0.26219682\n",
      "Iteration 519, loss = 0.26186661\n",
      "Iteration 520, loss = 0.26153741\n",
      "Iteration 521, loss = 0.26120921\n",
      "Iteration 522, loss = 0.26088201\n",
      "Iteration 523, loss = 0.26055581\n",
      "Iteration 524, loss = 0.26023060\n",
      "Iteration 525, loss = 0.25990638\n",
      "Iteration 526, loss = 0.25958315\n",
      "Iteration 527, loss = 0.25926089\n",
      "Iteration 528, loss = 0.25893962\n",
      "Iteration 529, loss = 0.25861931\n",
      "Iteration 530, loss = 0.25829997\n",
      "Iteration 531, loss = 0.25798160\n",
      "Iteration 532, loss = 0.25766419\n",
      "Iteration 533, loss = 0.25734774\n",
      "Iteration 534, loss = 0.25703224\n",
      "Iteration 535, loss = 0.25671769\n",
      "Iteration 536, loss = 0.25640408\n",
      "Iteration 537, loss = 0.25609141\n",
      "Iteration 538, loss = 0.25577969\n",
      "Iteration 539, loss = 0.25546889\n",
      "Iteration 540, loss = 0.25515903\n",
      "Iteration 541, loss = 0.25485009\n",
      "Iteration 542, loss = 0.25454208\n",
      "Iteration 543, loss = 0.25423498\n",
      "Iteration 544, loss = 0.25392880\n",
      "Iteration 545, loss = 0.25362353\n",
      "Iteration 546, loss = 0.25331916\n",
      "Iteration 547, loss = 0.25301570\n",
      "Iteration 548, loss = 0.25271314\n",
      "Iteration 549, loss = 0.25241148\n",
      "Iteration 550, loss = 0.25211070\n",
      "Iteration 551, loss = 0.25181082\n",
      "Iteration 552, loss = 0.25151182\n",
      "Iteration 553, loss = 0.25121370\n",
      "Iteration 554, loss = 0.25091646\n",
      "Iteration 555, loss = 0.25062010\n",
      "Iteration 556, loss = 0.25032460\n",
      "Iteration 557, loss = 0.25002998\n",
      "Iteration 558, loss = 0.24973621\n",
      "Iteration 559, loss = 0.24944331\n",
      "Iteration 560, loss = 0.24915126\n",
      "Iteration 561, loss = 0.24886006\n",
      "Iteration 562, loss = 0.24856972\n",
      "Iteration 563, loss = 0.24828021\n",
      "Iteration 564, loss = 0.24799156\n",
      "Iteration 565, loss = 0.24770373\n",
      "Iteration 566, loss = 0.24741675\n",
      "Iteration 567, loss = 0.24713060\n",
      "Iteration 568, loss = 0.24684527\n",
      "Iteration 569, loss = 0.24656077\n",
      "Iteration 570, loss = 0.24627709\n",
      "Iteration 571, loss = 0.24599423\n",
      "Iteration 572, loss = 0.24571219\n",
      "Iteration 573, loss = 0.24543095\n",
      "Iteration 574, loss = 0.24515052\n",
      "Iteration 575, loss = 0.24487090\n",
      "Iteration 576, loss = 0.24459208\n",
      "Iteration 577, loss = 0.24431406\n",
      "Iteration 578, loss = 0.24403683\n",
      "Iteration 579, loss = 0.24376039\n",
      "Iteration 580, loss = 0.24348475\n",
      "Iteration 581, loss = 0.24320988\n",
      "Iteration 582, loss = 0.24293580\n",
      "Iteration 583, loss = 0.24266250\n",
      "Iteration 584, loss = 0.24238997\n",
      "Iteration 585, loss = 0.24211821\n",
      "Iteration 586, loss = 0.24184722\n",
      "Iteration 587, loss = 0.24157700\n",
      "Iteration 588, loss = 0.24130754\n",
      "Iteration 589, loss = 0.24103884\n",
      "Iteration 590, loss = 0.24077089\n",
      "Iteration 591, loss = 0.24050370\n",
      "Iteration 592, loss = 0.24023726\n",
      "Iteration 593, loss = 0.23997157\n",
      "Iteration 594, loss = 0.23970661\n",
      "Iteration 595, loss = 0.23944240\n",
      "Iteration 596, loss = 0.23917893\n",
      "Iteration 597, loss = 0.23891619\n",
      "Iteration 598, loss = 0.23865419\n",
      "Iteration 599, loss = 0.23839291\n",
      "Iteration 600, loss = 0.23813235\n",
      "Iteration 601, loss = 0.23787252\n",
      "Iteration 602, loss = 0.23761341\n",
      "Iteration 603, loss = 0.23735502\n",
      "Iteration 604, loss = 0.23709734\n",
      "Iteration 605, loss = 0.23684037\n",
      "Iteration 606, loss = 0.23658411\n",
      "Iteration 607, loss = 0.23632855\n",
      "Iteration 608, loss = 0.23607370\n",
      "Iteration 609, loss = 0.23581954\n",
      "Iteration 610, loss = 0.23556608\n",
      "Iteration 611, loss = 0.23531332\n",
      "Iteration 612, loss = 0.23506124\n",
      "Iteration 613, loss = 0.23480986\n",
      "Iteration 614, loss = 0.23455915\n",
      "Iteration 615, loss = 0.23430913\n",
      "Iteration 616, loss = 0.23405979\n",
      "Iteration 617, loss = 0.23381113\n",
      "Iteration 618, loss = 0.23356314\n",
      "Iteration 619, loss = 0.23331582\n",
      "Iteration 620, loss = 0.23306917\n",
      "Iteration 621, loss = 0.23282318\n",
      "Iteration 622, loss = 0.23257786\n",
      "Iteration 623, loss = 0.23233320\n",
      "Iteration 624, loss = 0.23208920\n",
      "Iteration 625, loss = 0.23184585\n",
      "Iteration 626, loss = 0.23160315\n",
      "Iteration 627, loss = 0.23136110\n",
      "Iteration 628, loss = 0.23111971\n",
      "Iteration 629, loss = 0.23087895\n",
      "Iteration 630, loss = 0.23063884\n",
      "Iteration 631, loss = 0.23039936\n",
      "Iteration 632, loss = 0.23016052\n",
      "Iteration 633, loss = 0.22992232\n",
      "Iteration 634, loss = 0.22968475\n",
      "Iteration 635, loss = 0.22944780\n",
      "Iteration 636, loss = 0.22921149\n",
      "Iteration 637, loss = 0.22897579\n",
      "Iteration 638, loss = 0.22874072\n",
      "Iteration 639, loss = 0.22850627\n",
      "Iteration 640, loss = 0.22827243\n",
      "Iteration 641, loss = 0.22803921\n",
      "Iteration 642, loss = 0.22780660\n",
      "Iteration 643, loss = 0.22757460\n",
      "Iteration 644, loss = 0.22734320\n",
      "Iteration 645, loss = 0.22711241\n",
      "Iteration 646, loss = 0.22688222\n",
      "Iteration 647, loss = 0.22665263\n",
      "Iteration 648, loss = 0.22642364\n",
      "Iteration 649, loss = 0.22619524\n",
      "Iteration 650, loss = 0.22596744\n",
      "Iteration 651, loss = 0.22574022\n",
      "Iteration 652, loss = 0.22551359\n",
      "Iteration 653, loss = 0.22528755\n",
      "Iteration 654, loss = 0.22506209\n",
      "Iteration 655, loss = 0.22483721\n",
      "Iteration 656, loss = 0.22461291\n",
      "Iteration 657, loss = 0.22438918\n",
      "Iteration 658, loss = 0.22416603\n",
      "Iteration 659, loss = 0.22394345\n",
      "Iteration 660, loss = 0.22372144\n",
      "Iteration 661, loss = 0.22350000\n",
      "Iteration 662, loss = 0.22327912\n",
      "Iteration 663, loss = 0.22305880\n",
      "Iteration 664, loss = 0.22283904\n",
      "Iteration 665, loss = 0.22261985\n",
      "Iteration 666, loss = 0.22240120\n",
      "Iteration 667, loss = 0.22218312\n",
      "Iteration 668, loss = 0.22196558\n",
      "Iteration 669, loss = 0.22174859\n",
      "Iteration 670, loss = 0.22153215\n",
      "Iteration 671, loss = 0.22131626\n",
      "Iteration 672, loss = 0.22110090\n",
      "Iteration 673, loss = 0.22088609\n",
      "Iteration 674, loss = 0.22067182\n",
      "Iteration 675, loss = 0.22045809\n",
      "Iteration 676, loss = 0.22024489\n",
      "Iteration 677, loss = 0.22003222\n",
      "Iteration 678, loss = 0.21982008\n",
      "Iteration 679, loss = 0.21960847\n",
      "Iteration 680, loss = 0.21939739\n",
      "Iteration 681, loss = 0.21918683\n",
      "Iteration 682, loss = 0.21897680\n",
      "Iteration 683, loss = 0.21876728\n",
      "Iteration 684, loss = 0.21855828\n",
      "Iteration 685, loss = 0.21834980\n",
      "Iteration 686, loss = 0.21814184\n",
      "Iteration 687, loss = 0.21793438\n",
      "Iteration 688, loss = 0.21772744\n",
      "Iteration 689, loss = 0.21752100\n",
      "Iteration 690, loss = 0.21731508\n",
      "Iteration 691, loss = 0.21710965\n",
      "Iteration 692, loss = 0.21690473\n",
      "Iteration 693, loss = 0.21670031\n",
      "Iteration 694, loss = 0.21649639\n",
      "Iteration 695, loss = 0.21629297\n",
      "Iteration 696, loss = 0.21609004\n",
      "Iteration 697, loss = 0.21588760\n",
      "Iteration 698, loss = 0.21568566\n",
      "Iteration 699, loss = 0.21548420\n",
      "Iteration 700, loss = 0.21528323\n",
      "Iteration 701, loss = 0.21508275\n",
      "Iteration 702, loss = 0.21488275\n",
      "Iteration 703, loss = 0.21468324\n",
      "Iteration 704, loss = 0.21448420\n",
      "Iteration 705, loss = 0.21428565\n",
      "Iteration 706, loss = 0.21408757\n",
      "Iteration 707, loss = 0.21388996\n",
      "Iteration 708, loss = 0.21369283\n",
      "Iteration 709, loss = 0.21349617\n",
      "Iteration 710, loss = 0.21329998\n",
      "Iteration 711, loss = 0.21310426\n",
      "Iteration 712, loss = 0.21290900\n",
      "Iteration 713, loss = 0.21271421\n",
      "Iteration 714, loss = 0.21251988\n",
      "Iteration 715, loss = 0.21232601\n",
      "Iteration 716, loss = 0.21213260\n",
      "Iteration 717, loss = 0.21193965\n",
      "Iteration 718, loss = 0.21174716\n",
      "Iteration 719, loss = 0.21155511\n",
      "Iteration 720, loss = 0.21136353\n",
      "Iteration 721, loss = 0.21117239\n",
      "Iteration 722, loss = 0.21098170\n",
      "Iteration 723, loss = 0.21079146\n",
      "Iteration 724, loss = 0.21060166\n",
      "Iteration 725, loss = 0.21041231\n",
      "Iteration 726, loss = 0.21022340\n",
      "Iteration 727, loss = 0.21003493\n",
      "Iteration 728, loss = 0.20984691\n",
      "Iteration 729, loss = 0.20965932\n",
      "Iteration 730, loss = 0.20947216\n",
      "Iteration 731, loss = 0.20928544\n",
      "Iteration 732, loss = 0.20909915\n",
      "Iteration 733, loss = 0.20891330\n",
      "Iteration 734, loss = 0.20872787\n",
      "Iteration 735, loss = 0.20854288\n",
      "Iteration 736, loss = 0.20835831\n",
      "Iteration 737, loss = 0.20817416\n",
      "Iteration 738, loss = 0.20799044\n",
      "Iteration 739, loss = 0.20780714\n",
      "Iteration 740, loss = 0.20762426\n",
      "Iteration 741, loss = 0.20744180\n",
      "Iteration 742, loss = 0.20725976\n",
      "Iteration 743, loss = 0.20707814\n",
      "Iteration 744, loss = 0.20689692\n",
      "Iteration 745, loss = 0.20671613\n",
      "Iteration 746, loss = 0.20653574\n",
      "Iteration 747, loss = 0.20635576\n",
      "Iteration 748, loss = 0.20617620\n",
      "Iteration 749, loss = 0.20599704\n",
      "Iteration 750, loss = 0.20581828\n",
      "Iteration 751, loss = 0.20563993\n",
      "Iteration 752, loss = 0.20546198\n",
      "Iteration 753, loss = 0.20528444\n",
      "Iteration 754, loss = 0.20510729\n",
      "Iteration 755, loss = 0.20493055\n",
      "Iteration 756, loss = 0.20475420\n",
      "Iteration 757, loss = 0.20457824\n",
      "Iteration 758, loss = 0.20440268\n",
      "Iteration 759, loss = 0.20422752\n",
      "Iteration 760, loss = 0.20405274\n",
      "Iteration 761, loss = 0.20387836\n",
      "Iteration 762, loss = 0.20370437\n",
      "Iteration 763, loss = 0.20353076\n",
      "Iteration 764, loss = 0.20335754\n",
      "Iteration 765, loss = 0.20318470\n",
      "Iteration 766, loss = 0.20301225\n",
      "Iteration 767, loss = 0.20284018\n",
      "Iteration 768, loss = 0.20266849\n",
      "Iteration 769, loss = 0.20249718\n",
      "Iteration 770, loss = 0.20232624\n",
      "Iteration 771, loss = 0.20215569\n",
      "Iteration 772, loss = 0.20198551\n",
      "Iteration 773, loss = 0.20181571\n",
      "Iteration 774, loss = 0.20164627\n",
      "Iteration 775, loss = 0.20147721\n",
      "Iteration 776, loss = 0.20130852\n",
      "Iteration 777, loss = 0.20114020\n",
      "Iteration 778, loss = 0.20097225\n",
      "Iteration 779, loss = 0.20080466\n",
      "Iteration 780, loss = 0.20063744\n",
      "Iteration 781, loss = 0.20047059\n",
      "Iteration 782, loss = 0.20030409\n",
      "Iteration 783, loss = 0.20013796\n",
      "Iteration 784, loss = 0.19997219\n",
      "Iteration 785, loss = 0.19980678\n",
      "Iteration 786, loss = 0.19964172\n",
      "Iteration 787, loss = 0.19947702\n",
      "Iteration 788, loss = 0.19931268\n",
      "Iteration 789, loss = 0.19914869\n",
      "Iteration 790, loss = 0.19898506\n",
      "Iteration 791, loss = 0.19882178\n",
      "Iteration 792, loss = 0.19865884\n",
      "Iteration 793, loss = 0.19849626\n",
      "Iteration 794, loss = 0.19833403\n",
      "Iteration 795, loss = 0.19817214\n",
      "Iteration 796, loss = 0.19801060\n",
      "Iteration 797, loss = 0.19784940\n",
      "Iteration 798, loss = 0.19768855\n",
      "Iteration 799, loss = 0.19752804\n",
      "Iteration 800, loss = 0.19736787\n",
      "Iteration 801, loss = 0.19720805\n",
      "Iteration 802, loss = 0.19704856\n",
      "Iteration 803, loss = 0.19688941\n",
      "Iteration 804, loss = 0.19673059\n",
      "Iteration 805, loss = 0.19657212\n",
      "Iteration 806, loss = 0.19641397\n",
      "Iteration 807, loss = 0.19625616\n",
      "Iteration 808, loss = 0.19609869\n",
      "Iteration 809, loss = 0.19594154\n",
      "Iteration 810, loss = 0.19578473\n",
      "Iteration 811, loss = 0.19562824\n",
      "Iteration 812, loss = 0.19547208\n",
      "Iteration 813, loss = 0.19531625\n",
      "Iteration 814, loss = 0.19516075\n",
      "Iteration 815, loss = 0.19500556\n",
      "Iteration 816, loss = 0.19485071\n",
      "Iteration 817, loss = 0.19469617\n",
      "Iteration 818, loss = 0.19454196\n",
      "Iteration 819, loss = 0.19438807\n",
      "Iteration 820, loss = 0.19423450\n",
      "Iteration 821, loss = 0.19408125\n",
      "Iteration 822, loss = 0.19392831\n",
      "Iteration 823, loss = 0.19377569\n",
      "Iteration 824, loss = 0.19362339\n",
      "Iteration 825, loss = 0.19347140\n",
      "Iteration 826, loss = 0.19331972\n",
      "Iteration 827, loss = 0.19316836\n",
      "Iteration 828, loss = 0.19301731\n",
      "Iteration 829, loss = 0.19286657\n",
      "Iteration 830, loss = 0.19271614\n",
      "Iteration 831, loss = 0.19256601\n",
      "Iteration 832, loss = 0.19241620\n",
      "Iteration 833, loss = 0.19226669\n",
      "Iteration 834, loss = 0.19211748\n",
      "Iteration 835, loss = 0.19196858\n",
      "Iteration 836, loss = 0.19181999\n",
      "Iteration 837, loss = 0.19167169\n",
      "Iteration 838, loss = 0.19152370\n",
      "Iteration 839, loss = 0.19137601\n",
      "Iteration 840, loss = 0.19122862\n",
      "Iteration 841, loss = 0.19108152\n",
      "Iteration 842, loss = 0.19093473\n",
      "Iteration 843, loss = 0.19078823\n",
      "Iteration 844, loss = 0.19064203\n",
      "Iteration 845, loss = 0.19049612\n",
      "Iteration 846, loss = 0.19035050\n",
      "Iteration 847, loss = 0.19020518\n",
      "Iteration 848, loss = 0.19006016\n",
      "Iteration 849, loss = 0.18991542\n",
      "Iteration 850, loss = 0.18977097\n",
      "Iteration 851, loss = 0.18962681\n",
      "Iteration 852, loss = 0.18948295\n",
      "Iteration 853, loss = 0.18933936\n",
      "Iteration 854, loss = 0.18919607\n",
      "Iteration 855, loss = 0.18905306\n",
      "Iteration 856, loss = 0.18891034\n",
      "Iteration 857, loss = 0.18876790\n",
      "Iteration 858, loss = 0.18862574\n",
      "Iteration 859, loss = 0.18848387\n",
      "Iteration 860, loss = 0.18834227\n",
      "Iteration 861, loss = 0.18820096\n",
      "Iteration 862, loss = 0.18805993\n",
      "Iteration 863, loss = 0.18791918\n",
      "Iteration 864, loss = 0.18777870\n",
      "Iteration 865, loss = 0.18763850\n",
      "Iteration 866, loss = 0.18749858\n",
      "Iteration 867, loss = 0.18735893\n",
      "Iteration 868, loss = 0.18721956\n",
      "Iteration 869, loss = 0.18708046\n",
      "Iteration 870, loss = 0.18694163\n",
      "Iteration 871, loss = 0.18680308\n",
      "Iteration 872, loss = 0.18666480\n",
      "Iteration 873, loss = 0.18652678\n",
      "Iteration 874, loss = 0.18638904\n",
      "Iteration 875, loss = 0.18625157\n",
      "Iteration 876, loss = 0.18611436\n",
      "Iteration 877, loss = 0.18597742\n",
      "Iteration 878, loss = 0.18584075\n",
      "Iteration 879, loss = 0.18570434\n",
      "Iteration 880, loss = 0.18556820\n",
      "Iteration 881, loss = 0.18543232\n",
      "Iteration 882, loss = 0.18529670\n",
      "Iteration 883, loss = 0.18516135\n",
      "Iteration 884, loss = 0.18502626\n",
      "Iteration 885, loss = 0.18489143\n",
      "Iteration 886, loss = 0.18475686\n",
      "Iteration 887, loss = 0.18462254\n",
      "Iteration 888, loss = 0.18448849\n",
      "Iteration 889, loss = 0.18435470\n",
      "Iteration 890, loss = 0.18422116\n",
      "Iteration 891, loss = 0.18408787\n",
      "Iteration 892, loss = 0.18395484\n",
      "Iteration 893, loss = 0.18382207\n",
      "Iteration 894, loss = 0.18368955\n",
      "Iteration 895, loss = 0.18355729\n",
      "Iteration 896, loss = 0.18342527\n",
      "Iteration 897, loss = 0.18329351\n",
      "Iteration 898, loss = 0.18316200\n",
      "Iteration 899, loss = 0.18303073\n",
      "Iteration 900, loss = 0.18289972\n",
      "Iteration 901, loss = 0.18276896\n",
      "Iteration 902, loss = 0.18263844\n",
      "Iteration 903, loss = 0.18250817\n",
      "Iteration 904, loss = 0.18237815\n",
      "Iteration 905, loss = 0.18224837\n",
      "Iteration 906, loss = 0.18211884\n",
      "Iteration 907, loss = 0.18198955\n",
      "Iteration 908, loss = 0.18186051\n",
      "Iteration 909, loss = 0.18173171\n",
      "Iteration 910, loss = 0.18160315\n",
      "Iteration 911, loss = 0.18147483\n",
      "Iteration 912, loss = 0.18134675\n",
      "Iteration 913, loss = 0.18121892\n",
      "Iteration 914, loss = 0.18109132\n",
      "Iteration 915, loss = 0.18096396\n",
      "Iteration 916, loss = 0.18083684\n",
      "Iteration 917, loss = 0.18070995\n",
      "Iteration 918, loss = 0.18058330\n",
      "Iteration 919, loss = 0.18045689\n",
      "Iteration 920, loss = 0.18033071\n",
      "Iteration 921, loss = 0.18020477\n",
      "Iteration 922, loss = 0.18007906\n",
      "Iteration 923, loss = 0.17995359\n",
      "Iteration 924, loss = 0.17982835\n",
      "Iteration 925, loss = 0.17970333\n",
      "Iteration 926, loss = 0.17957855\n",
      "Iteration 927, loss = 0.17945401\n",
      "Iteration 928, loss = 0.17932969\n",
      "Iteration 929, loss = 0.17920560\n",
      "Iteration 930, loss = 0.17908173\n",
      "Iteration 931, loss = 0.17895810\n",
      "Iteration 932, loss = 0.17883469\n",
      "Iteration 933, loss = 0.17871151\n",
      "Iteration 934, loss = 0.17858856\n",
      "Iteration 935, loss = 0.17846583\n",
      "Iteration 936, loss = 0.17834332\n",
      "Iteration 937, loss = 0.17822104\n",
      "Iteration 938, loss = 0.17809899\n",
      "Iteration 939, loss = 0.17797715\n",
      "Iteration 940, loss = 0.17785554\n",
      "Iteration 941, loss = 0.17773415\n",
      "Iteration 942, loss = 0.17761298\n",
      "Iteration 943, loss = 0.17749203\n",
      "Iteration 944, loss = 0.17737130\n",
      "Iteration 945, loss = 0.17725079\n",
      "Iteration 946, loss = 0.17713050\n",
      "Iteration 947, loss = 0.17701042\n",
      "Iteration 948, loss = 0.17689056\n",
      "Iteration 949, loss = 0.17677092\n",
      "Iteration 950, loss = 0.17665150\n",
      "Iteration 951, loss = 0.17653229\n",
      "Iteration 952, loss = 0.17641329\n",
      "Iteration 953, loss = 0.17629451\n",
      "Iteration 954, loss = 0.17617595\n",
      "Iteration 955, loss = 0.17605759\n",
      "Iteration 956, loss = 0.17593945\n",
      "Iteration 957, loss = 0.17582152\n",
      "Iteration 958, loss = 0.17570380\n",
      "Iteration 959, loss = 0.17558629\n",
      "Iteration 960, loss = 0.17546900\n",
      "Iteration 961, loss = 0.17535191\n",
      "Iteration 962, loss = 0.17523503\n",
      "Iteration 963, loss = 0.17511836\n",
      "Iteration 964, loss = 0.17500190\n",
      "Iteration 965, loss = 0.17488564\n",
      "Iteration 966, loss = 0.17476959\n",
      "Iteration 967, loss = 0.17465375\n",
      "Iteration 968, loss = 0.17453811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 969, loss = 0.17442268\n",
      "Iteration 970, loss = 0.17430745\n",
      "Iteration 971, loss = 0.17419242\n",
      "Iteration 972, loss = 0.17407760\n",
      "Iteration 973, loss = 0.17396298\n",
      "Iteration 974, loss = 0.17384857\n",
      "Iteration 975, loss = 0.17373435\n",
      "Iteration 976, loss = 0.17362034\n",
      "Iteration 977, loss = 0.17350653\n",
      "Iteration 978, loss = 0.17339291\n",
      "Iteration 979, loss = 0.17327950\n",
      "Iteration 980, loss = 0.17316629\n",
      "Iteration 981, loss = 0.17305327\n",
      "Iteration 982, loss = 0.17294045\n",
      "Iteration 983, loss = 0.17282783\n",
      "Iteration 984, loss = 0.17271541\n",
      "Iteration 985, loss = 0.17260318\n",
      "Iteration 986, loss = 0.17249115\n",
      "Iteration 987, loss = 0.17237931\n",
      "Iteration 988, loss = 0.17226767\n",
      "Iteration 989, loss = 0.17215622\n",
      "Iteration 990, loss = 0.17204496\n",
      "Iteration 991, loss = 0.17193390\n",
      "Iteration 992, loss = 0.17182303\n",
      "Iteration 993, loss = 0.17171236\n",
      "Iteration 994, loss = 0.17160187\n",
      "Iteration 995, loss = 0.17149158\n",
      "Iteration 996, loss = 0.17138148\n",
      "Iteration 997, loss = 0.17127156\n",
      "Iteration 998, loss = 0.17116184\n",
      "Iteration 999, loss = 0.17105231\n",
      "Iteration 1000, loss = 0.17094296\n",
      "Iteration 1, loss = 2.13352491\n",
      "Iteration 2, loss = 1.58234843\n",
      "Iteration 3, loss = 1.21533605\n",
      "Iteration 4, loss = 1.21118740\n",
      "Iteration 5, loss = 1.22422152\n",
      "Iteration 6, loss = 1.21832012\n",
      "Iteration 7, loss = 1.20883208\n",
      "Iteration 8, loss = 1.19901121\n",
      "Iteration 9, loss = 1.19278317\n",
      "Iteration 10, loss = 1.18913016\n",
      "Iteration 11, loss = 1.18599312\n",
      "Iteration 12, loss = 1.18195314\n",
      "Iteration 13, loss = 1.17694233\n",
      "Iteration 14, loss = 1.17140674\n",
      "Iteration 15, loss = 1.16598159\n",
      "Iteration 16, loss = 1.16092461\n",
      "Iteration 17, loss = 1.15576822\n",
      "Iteration 18, loss = 1.15084020\n",
      "Iteration 19, loss = 1.14606226\n",
      "Iteration 20, loss = 1.14124904\n",
      "Iteration 21, loss = 1.13632298\n",
      "Iteration 22, loss = 1.13134516\n",
      "Iteration 23, loss = 1.12643481\n",
      "Iteration 24, loss = 1.12163915\n",
      "Iteration 25, loss = 1.11692545\n",
      "Iteration 26, loss = 1.11228050\n",
      "Iteration 27, loss = 1.10768299\n",
      "Iteration 28, loss = 1.10308002\n",
      "Iteration 29, loss = 1.09841152\n",
      "Iteration 30, loss = 1.09387508\n",
      "Iteration 31, loss = 1.08932940\n",
      "Iteration 32, loss = 1.08474079\n",
      "Iteration 33, loss = 1.08008454\n",
      "Iteration 34, loss = 1.07526001\n",
      "Iteration 35, loss = 1.07032621\n",
      "Iteration 36, loss = 1.06523497\n",
      "Iteration 37, loss = 1.05997317\n",
      "Iteration 38, loss = 1.05453622\n",
      "Iteration 39, loss = 1.04899557\n",
      "Iteration 40, loss = 1.04336667\n",
      "Iteration 41, loss = 1.03759065\n",
      "Iteration 42, loss = 1.03167544\n",
      "Iteration 43, loss = 1.02563131\n",
      "Iteration 44, loss = 1.01953176\n",
      "Iteration 45, loss = 1.01332214\n",
      "Iteration 46, loss = 1.00700653\n",
      "Iteration 47, loss = 1.00060057\n",
      "Iteration 48, loss = 0.99412156\n",
      "Iteration 49, loss = 0.98758716\n",
      "Iteration 50, loss = 0.98101409\n",
      "Iteration 51, loss = 0.97441714\n",
      "Iteration 52, loss = 0.96780839\n",
      "Iteration 53, loss = 0.96119707\n",
      "Iteration 54, loss = 0.95458973\n",
      "Iteration 55, loss = 0.94799082\n",
      "Iteration 56, loss = 0.94140328\n",
      "Iteration 57, loss = 0.93482911\n",
      "Iteration 58, loss = 0.92826968\n",
      "Iteration 59, loss = 0.92172595\n",
      "Iteration 60, loss = 0.91519847\n",
      "Iteration 61, loss = 0.90868748\n",
      "Iteration 62, loss = 0.90219293\n",
      "Iteration 63, loss = 0.89571466\n",
      "Iteration 64, loss = 0.88925249\n",
      "Iteration 65, loss = 0.88280631\n",
      "Iteration 66, loss = 0.87637613\n",
      "Iteration 67, loss = 0.86996205\n",
      "Iteration 68, loss = 0.86356425\n",
      "Iteration 69, loss = 0.85718294\n",
      "Iteration 70, loss = 0.85081833\n",
      "Iteration 71, loss = 0.84447070\n",
      "Iteration 72, loss = 0.83814036\n",
      "Iteration 73, loss = 0.83182768\n",
      "Iteration 74, loss = 0.82553315\n",
      "Iteration 75, loss = 0.81925730\n",
      "Iteration 76, loss = 0.81300078\n",
      "Iteration 77, loss = 0.80676430\n",
      "Iteration 78, loss = 0.80054869\n",
      "Iteration 79, loss = 0.79435488\n",
      "Iteration 80, loss = 0.78818389\n",
      "Iteration 81, loss = 0.78203686\n",
      "Iteration 82, loss = 0.77591506\n",
      "Iteration 83, loss = 0.76981989\n",
      "Iteration 84, loss = 0.76375286\n",
      "Iteration 85, loss = 0.75771564\n",
      "Iteration 86, loss = 0.75171004\n",
      "Iteration 87, loss = 0.74573801\n",
      "Iteration 88, loss = 0.73980165\n",
      "Iteration 89, loss = 0.73390323\n",
      "Iteration 90, loss = 0.72804517\n",
      "Iteration 91, loss = 0.72223003\n",
      "Iteration 92, loss = 0.71646053\n",
      "Iteration 93, loss = 0.71073952\n",
      "Iteration 94, loss = 0.70506996\n",
      "Iteration 95, loss = 0.69945494\n",
      "Iteration 96, loss = 0.69389761\n",
      "Iteration 97, loss = 0.68840119\n",
      "Iteration 98, loss = 0.68296894\n",
      "Iteration 99, loss = 0.67760409\n",
      "Iteration 100, loss = 0.67230985\n",
      "Iteration 101, loss = 0.66708934\n",
      "Iteration 102, loss = 0.66194557\n",
      "Iteration 103, loss = 0.65688137\n",
      "Iteration 104, loss = 0.65189938\n",
      "Iteration 105, loss = 0.64700197\n",
      "Iteration 106, loss = 0.64219126\n",
      "Iteration 107, loss = 0.63746900\n",
      "Iteration 108, loss = 0.63283662\n",
      "Iteration 109, loss = 0.62829516\n",
      "Iteration 110, loss = 0.62384528\n",
      "Iteration 111, loss = 0.61948722\n",
      "Iteration 112, loss = 0.61522084\n",
      "Iteration 113, loss = 0.61104561\n",
      "Iteration 114, loss = 0.60696061\n",
      "Iteration 115, loss = 0.60296463\n",
      "Iteration 116, loss = 0.59905610\n",
      "Iteration 117, loss = 0.59523323\n",
      "Iteration 118, loss = 0.59149399\n",
      "Iteration 119, loss = 0.58783619\n",
      "Iteration 120, loss = 0.58425750\n",
      "Iteration 121, loss = 0.58075553\n",
      "Iteration 122, loss = 0.57732783\n",
      "Iteration 123, loss = 0.57397196\n",
      "Iteration 124, loss = 0.57068551\n",
      "Iteration 125, loss = 0.56746612\n",
      "Iteration 126, loss = 0.56431152\n",
      "Iteration 127, loss = 0.56121951\n",
      "Iteration 128, loss = 0.55818801\n",
      "Iteration 129, loss = 0.55521505\n",
      "Iteration 130, loss = 0.55229875\n",
      "Iteration 131, loss = 0.54943734\n",
      "Iteration 132, loss = 0.54662919\n",
      "Iteration 133, loss = 0.54387273\n",
      "Iteration 134, loss = 0.54116650\n",
      "Iteration 135, loss = 0.53850913\n",
      "Iteration 136, loss = 0.53589931\n",
      "Iteration 137, loss = 0.53333584\n",
      "Iteration 138, loss = 0.53081755\n",
      "Iteration 139, loss = 0.52834333\n",
      "Iteration 140, loss = 0.52591212\n",
      "Iteration 141, loss = 0.52352292\n",
      "Iteration 142, loss = 0.52117474\n",
      "Iteration 143, loss = 0.51886662\n",
      "Iteration 144, loss = 0.51659766\n",
      "Iteration 145, loss = 0.51436693\n",
      "Iteration 146, loss = 0.51217356\n",
      "Iteration 147, loss = 0.51001667\n",
      "Iteration 148, loss = 0.50789541\n",
      "Iteration 149, loss = 0.50580893\n",
      "Iteration 150, loss = 0.50375639\n",
      "Iteration 151, loss = 0.50173698\n",
      "Iteration 152, loss = 0.49974988\n",
      "Iteration 153, loss = 0.49779427\n",
      "Iteration 154, loss = 0.49586938\n",
      "Iteration 155, loss = 0.49397441\n",
      "Iteration 156, loss = 0.49210859\n",
      "Iteration 157, loss = 0.49027116\n",
      "Iteration 158, loss = 0.48846137\n",
      "Iteration 159, loss = 0.48667848\n",
      "Iteration 160, loss = 0.48492177\n",
      "Iteration 161, loss = 0.48319053\n",
      "Iteration 162, loss = 0.48148407\n",
      "Iteration 163, loss = 0.47980171\n",
      "Iteration 164, loss = 0.47814278\n",
      "Iteration 165, loss = 0.47650663\n",
      "Iteration 166, loss = 0.47489264\n",
      "Iteration 167, loss = 0.47330018\n",
      "Iteration 168, loss = 0.47172867\n",
      "Iteration 169, loss = 0.47017751\n",
      "Iteration 170, loss = 0.46864614\n",
      "Iteration 171, loss = 0.46713402\n",
      "Iteration 172, loss = 0.46564061\n",
      "Iteration 173, loss = 0.46416539\n",
      "Iteration 174, loss = 0.46270786\n",
      "Iteration 175, loss = 0.46126754\n",
      "Iteration 176, loss = 0.45984396\n",
      "Iteration 177, loss = 0.45843666\n",
      "Iteration 178, loss = 0.45704520\n",
      "Iteration 179, loss = 0.45566915\n",
      "Iteration 180, loss = 0.45430810\n",
      "Iteration 181, loss = 0.45296165\n",
      "Iteration 182, loss = 0.45162941\n",
      "Iteration 183, loss = 0.45031101\n",
      "Iteration 184, loss = 0.44900608\n",
      "Iteration 185, loss = 0.44771427\n",
      "Iteration 186, loss = 0.44643524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 187, loss = 0.44516866\n",
      "Iteration 188, loss = 0.44391421\n",
      "Iteration 189, loss = 0.44267158\n",
      "Iteration 190, loss = 0.44144048\n",
      "Iteration 191, loss = 0.44022060\n",
      "Iteration 192, loss = 0.43901167\n",
      "Iteration 193, loss = 0.43781342\n",
      "Iteration 194, loss = 0.43662558\n",
      "Iteration 195, loss = 0.43544790\n",
      "Iteration 196, loss = 0.43428012\n",
      "Iteration 197, loss = 0.43312201\n",
      "Iteration 198, loss = 0.43197334\n",
      "Iteration 199, loss = 0.43083387\n",
      "Iteration 200, loss = 0.42970339\n",
      "Iteration 201, loss = 0.42858169\n",
      "Iteration 202, loss = 0.42746856\n",
      "Iteration 203, loss = 0.42636380\n",
      "Iteration 204, loss = 0.42526722\n",
      "Iteration 205, loss = 0.42417864\n",
      "Iteration 206, loss = 0.42309787\n",
      "Iteration 207, loss = 0.42202473\n",
      "Iteration 208, loss = 0.42095905\n",
      "Iteration 209, loss = 0.41990068\n",
      "Iteration 210, loss = 0.41884945\n",
      "Iteration 211, loss = 0.41780521\n",
      "Iteration 212, loss = 0.41676780\n",
      "Iteration 213, loss = 0.41573708\n",
      "Iteration 214, loss = 0.41471291\n",
      "Iteration 215, loss = 0.41369515\n",
      "Iteration 216, loss = 0.41268367\n",
      "Iteration 217, loss = 0.41167835\n",
      "Iteration 218, loss = 0.41067906\n",
      "Iteration 219, loss = 0.40968567\n",
      "Iteration 220, loss = 0.40869808\n",
      "Iteration 221, loss = 0.40771616\n",
      "Iteration 222, loss = 0.40673981\n",
      "Iteration 223, loss = 0.40576893\n",
      "Iteration 224, loss = 0.40480340\n",
      "Iteration 225, loss = 0.40384314\n",
      "Iteration 226, loss = 0.40288804\n",
      "Iteration 227, loss = 0.40193801\n",
      "Iteration 228, loss = 0.40099295\n",
      "Iteration 229, loss = 0.40005279\n",
      "Iteration 230, loss = 0.39911744\n",
      "Iteration 231, loss = 0.39818681\n",
      "Iteration 232, loss = 0.39726082\n",
      "Iteration 233, loss = 0.39633939\n",
      "Iteration 234, loss = 0.39542246\n",
      "Iteration 235, loss = 0.39450995\n",
      "Iteration 236, loss = 0.39360178\n",
      "Iteration 237, loss = 0.39269789\n",
      "Iteration 238, loss = 0.39179822\n",
      "Iteration 239, loss = 0.39090269\n",
      "Iteration 240, loss = 0.39001125\n",
      "Iteration 241, loss = 0.38912383\n",
      "Iteration 242, loss = 0.38824038\n",
      "Iteration 243, loss = 0.38736084\n",
      "Iteration 244, loss = 0.38648515\n",
      "Iteration 245, loss = 0.38561327\n",
      "Iteration 246, loss = 0.38474514\n",
      "Iteration 247, loss = 0.38388070\n",
      "Iteration 248, loss = 0.38301992\n",
      "Iteration 249, loss = 0.38216275\n",
      "Iteration 250, loss = 0.38130913\n",
      "Iteration 251, loss = 0.38045903\n",
      "Iteration 252, loss = 0.37961240\n",
      "Iteration 253, loss = 0.37876921\n",
      "Iteration 254, loss = 0.37792940\n",
      "Iteration 255, loss = 0.37709295\n",
      "Iteration 256, loss = 0.37625981\n",
      "Iteration 257, loss = 0.37542995\n",
      "Iteration 258, loss = 0.37460333\n",
      "Iteration 259, loss = 0.37377992\n",
      "Iteration 260, loss = 0.37295969\n",
      "Iteration 261, loss = 0.37214260\n",
      "Iteration 262, loss = 0.37132862\n",
      "Iteration 263, loss = 0.37051773\n",
      "Iteration 264, loss = 0.36970988\n",
      "Iteration 265, loss = 0.36890506\n",
      "Iteration 266, loss = 0.36810324\n",
      "Iteration 267, loss = 0.36730439\n",
      "Iteration 268, loss = 0.36650848\n",
      "Iteration 269, loss = 0.36571549\n",
      "Iteration 270, loss = 0.36492540\n",
      "Iteration 271, loss = 0.36413817\n",
      "Iteration 272, loss = 0.36335380\n",
      "Iteration 273, loss = 0.36257225\n",
      "Iteration 274, loss = 0.36179350\n",
      "Iteration 275, loss = 0.36101754\n",
      "Iteration 276, loss = 0.36024434\n",
      "Iteration 277, loss = 0.35947389\n",
      "Iteration 278, loss = 0.35870616\n",
      "Iteration 279, loss = 0.35794114\n",
      "Iteration 280, loss = 0.35717881\n",
      "Iteration 281, loss = 0.35641915\n",
      "Iteration 282, loss = 0.35566214\n",
      "Iteration 283, loss = 0.35490777\n",
      "Iteration 284, loss = 0.35415602\n",
      "Iteration 285, loss = 0.35340688\n",
      "Iteration 286, loss = 0.35266033\n",
      "Iteration 287, loss = 0.35191636\n",
      "Iteration 288, loss = 0.35117494\n",
      "Iteration 289, loss = 0.35043608\n",
      "Iteration 290, loss = 0.34969976\n",
      "Iteration 291, loss = 0.34896595\n",
      "Iteration 292, loss = 0.34823466\n",
      "Iteration 293, loss = 0.34750586\n",
      "Iteration 294, loss = 0.34677955\n",
      "Iteration 295, loss = 0.34605571\n",
      "Iteration 296, loss = 0.34533434\n",
      "Iteration 297, loss = 0.34461541\n",
      "Iteration 298, loss = 0.34389893\n",
      "Iteration 299, loss = 0.34318487\n",
      "Iteration 300, loss = 0.34247324\n",
      "Iteration 301, loss = 0.34176401\n",
      "Iteration 302, loss = 0.34105719\n",
      "Iteration 303, loss = 0.34035275\n",
      "Iteration 304, loss = 0.33965069\n",
      "Iteration 305, loss = 0.33895101\n",
      "Iteration 306, loss = 0.33825368\n",
      "Iteration 307, loss = 0.33755871\n",
      "Iteration 308, loss = 0.33686609\n",
      "Iteration 309, loss = 0.33617580\n",
      "Iteration 310, loss = 0.33548784\n",
      "Iteration 311, loss = 0.33480219\n",
      "Iteration 312, loss = 0.33411887\n",
      "Iteration 313, loss = 0.33343784\n",
      "Iteration 314, loss = 0.33275911\n",
      "Iteration 315, loss = 0.33208267\n",
      "Iteration 316, loss = 0.33140852\n",
      "Iteration 317, loss = 0.33073663\n",
      "Iteration 318, loss = 0.33006702\n",
      "Iteration 319, loss = 0.32939966\n",
      "Iteration 320, loss = 0.32873456\n",
      "Iteration 321, loss = 0.32807171\n",
      "Iteration 322, loss = 0.32741109\n",
      "Iteration 323, loss = 0.32675271\n",
      "Iteration 324, loss = 0.32609655\n",
      "Iteration 325, loss = 0.32544261\n",
      "Iteration 326, loss = 0.32479089\n",
      "Iteration 327, loss = 0.32414138\n",
      "Iteration 328, loss = 0.32349406\n",
      "Iteration 329, loss = 0.32284894\n",
      "Iteration 330, loss = 0.32220601\n",
      "Iteration 331, loss = 0.32156526\n",
      "Iteration 332, loss = 0.32092669\n",
      "Iteration 333, loss = 0.32029028\n",
      "Iteration 334, loss = 0.31965605\n",
      "Iteration 335, loss = 0.31902397\n",
      "Iteration 336, loss = 0.31839404\n",
      "Iteration 337, loss = 0.31776626\n",
      "Iteration 338, loss = 0.31714062\n",
      "Iteration 339, loss = 0.31651712\n",
      "Iteration 340, loss = 0.31589574\n",
      "Iteration 341, loss = 0.31527649\n",
      "Iteration 342, loss = 0.31465936\n",
      "Iteration 343, loss = 0.31404433\n",
      "Iteration 344, loss = 0.31343142\n",
      "Iteration 345, loss = 0.31282061\n",
      "Iteration 346, loss = 0.31221189\n",
      "Iteration 347, loss = 0.31160526\n",
      "Iteration 348, loss = 0.31100071\n",
      "Iteration 349, loss = 0.31039825\n",
      "Iteration 350, loss = 0.30979785\n",
      "Iteration 351, loss = 0.30919953\n",
      "Iteration 352, loss = 0.30860326\n",
      "Iteration 353, loss = 0.30800905\n",
      "Iteration 354, loss = 0.30741689\n",
      "Iteration 355, loss = 0.30682678\n",
      "Iteration 356, loss = 0.30623871\n",
      "Iteration 357, loss = 0.30565267\n",
      "Iteration 358, loss = 0.30506865\n",
      "Iteration 359, loss = 0.30448666\n",
      "Iteration 360, loss = 0.30390669\n",
      "Iteration 361, loss = 0.30332873\n",
      "Iteration 362, loss = 0.30275277\n",
      "Iteration 363, loss = 0.30217881\n",
      "Iteration 364, loss = 0.30160685\n",
      "Iteration 365, loss = 0.30103688\n",
      "Iteration 366, loss = 0.30046889\n",
      "Iteration 367, loss = 0.29990287\n",
      "Iteration 368, loss = 0.29933883\n",
      "Iteration 369, loss = 0.29877676\n",
      "Iteration 370, loss = 0.29821664\n",
      "Iteration 371, loss = 0.29765848\n",
      "Iteration 372, loss = 0.29710227\n",
      "Iteration 373, loss = 0.29654800\n",
      "Iteration 374, loss = 0.29599567\n",
      "Iteration 375, loss = 0.29544527\n",
      "Iteration 376, loss = 0.29489680\n",
      "Iteration 377, loss = 0.29435025\n",
      "Iteration 378, loss = 0.29380561\n",
      "Iteration 379, loss = 0.29326288\n",
      "Iteration 380, loss = 0.29272205\n",
      "Iteration 381, loss = 0.29218313\n",
      "Iteration 382, loss = 0.29164609\n",
      "Iteration 383, loss = 0.29111093\n",
      "Iteration 384, loss = 0.29057766\n",
      "Iteration 385, loss = 0.29004626\n",
      "Iteration 386, loss = 0.28951673\n",
      "Iteration 387, loss = 0.28898906\n",
      "Iteration 388, loss = 0.28846325\n",
      "Iteration 389, loss = 0.28793928\n",
      "Iteration 390, loss = 0.28741716\n",
      "Iteration 391, loss = 0.28689688\n",
      "Iteration 392, loss = 0.28637843\n",
      "Iteration 393, loss = 0.28586180\n",
      "Iteration 394, loss = 0.28534700\n",
      "Iteration 395, loss = 0.28483401\n",
      "Iteration 396, loss = 0.28432283\n",
      "Iteration 397, loss = 0.28381345\n",
      "Iteration 398, loss = 0.28330587\n",
      "Iteration 399, loss = 0.28280007\n",
      "Iteration 400, loss = 0.28229606\n",
      "Iteration 401, loss = 0.28179383\n",
      "Iteration 402, loss = 0.28129337\n",
      "Iteration 403, loss = 0.28079468\n",
      "Iteration 404, loss = 0.28029775\n",
      "Iteration 405, loss = 0.27980257\n",
      "Iteration 406, loss = 0.27930913\n",
      "Iteration 407, loss = 0.27881744\n",
      "Iteration 408, loss = 0.27832749\n",
      "Iteration 409, loss = 0.27783926\n",
      "Iteration 410, loss = 0.27735276\n",
      "Iteration 411, loss = 0.27686797\n",
      "Iteration 412, loss = 0.27638490\n",
      "Iteration 413, loss = 0.27590352\n",
      "Iteration 414, loss = 0.27542385\n",
      "Iteration 415, loss = 0.27494587\n",
      "Iteration 416, loss = 0.27446958\n",
      "Iteration 417, loss = 0.27399496\n",
      "Iteration 418, loss = 0.27352202\n",
      "Iteration 419, loss = 0.27305075\n",
      "Iteration 420, loss = 0.27258114\n",
      "Iteration 421, loss = 0.27211319\n",
      "Iteration 422, loss = 0.27164688\n",
      "Iteration 423, loss = 0.27118222\n",
      "Iteration 424, loss = 0.27071919\n",
      "Iteration 425, loss = 0.27025780\n",
      "Iteration 426, loss = 0.26979803\n",
      "Iteration 427, loss = 0.26933988\n",
      "Iteration 428, loss = 0.26888334\n",
      "Iteration 429, loss = 0.26842840\n",
      "Iteration 430, loss = 0.26797507\n",
      "Iteration 431, loss = 0.26752333\n",
      "Iteration 432, loss = 0.26707317\n",
      "Iteration 433, loss = 0.26662460\n",
      "Iteration 434, loss = 0.26617761\n",
      "Iteration 435, loss = 0.26573218\n",
      "Iteration 436, loss = 0.26528832\n",
      "Iteration 437, loss = 0.26484601\n",
      "Iteration 438, loss = 0.26440525\n",
      "Iteration 439, loss = 0.26396604\n",
      "Iteration 440, loss = 0.26352837\n",
      "Iteration 441, loss = 0.26309223\n",
      "Iteration 442, loss = 0.26265761\n",
      "Iteration 443, loss = 0.26222452\n",
      "Iteration 444, loss = 0.26179294\n",
      "Iteration 445, loss = 0.26136287\n",
      "Iteration 446, loss = 0.26093430\n",
      "Iteration 447, loss = 0.26050722\n",
      "Iteration 448, loss = 0.26008164\n",
      "Iteration 449, loss = 0.25965754\n",
      "Iteration 450, loss = 0.25923491\n",
      "Iteration 451, loss = 0.25881376\n",
      "Iteration 452, loss = 0.25839407\n",
      "Iteration 453, loss = 0.25797584\n",
      "Iteration 454, loss = 0.25755907\n",
      "Iteration 455, loss = 0.25714375\n",
      "Iteration 456, loss = 0.25672986\n",
      "Iteration 457, loss = 0.25631741\n",
      "Iteration 458, loss = 0.25590639\n",
      "Iteration 459, loss = 0.25549680\n",
      "Iteration 460, loss = 0.25508862\n",
      "Iteration 461, loss = 0.25468185\n",
      "Iteration 462, loss = 0.25427649\n",
      "Iteration 463, loss = 0.25387253\n",
      "Iteration 464, loss = 0.25346996\n",
      "Iteration 465, loss = 0.25306879\n",
      "Iteration 466, loss = 0.25266899\n",
      "Iteration 467, loss = 0.25227057\n",
      "Iteration 468, loss = 0.25187352\n",
      "Iteration 469, loss = 0.25147784\n",
      "Iteration 470, loss = 0.25108351\n",
      "Iteration 471, loss = 0.25069054\n",
      "Iteration 472, loss = 0.25029892\n",
      "Iteration 473, loss = 0.24990863\n",
      "Iteration 474, loss = 0.24951969\n",
      "Iteration 475, loss = 0.24913207\n",
      "Iteration 476, loss = 0.24874578\n",
      "Iteration 477, loss = 0.24836081\n",
      "Iteration 478, loss = 0.24797715\n",
      "Iteration 479, loss = 0.24759480\n",
      "Iteration 480, loss = 0.24721375\n",
      "Iteration 481, loss = 0.24683399\n",
      "Iteration 482, loss = 0.24645553\n",
      "Iteration 483, loss = 0.24607835\n",
      "Iteration 484, loss = 0.24570245\n",
      "Iteration 485, loss = 0.24532783\n",
      "Iteration 486, loss = 0.24495447\n",
      "Iteration 487, loss = 0.24458237\n",
      "Iteration 488, loss = 0.24421154\n",
      "Iteration 489, loss = 0.24384195\n",
      "Iteration 490, loss = 0.24347361\n",
      "Iteration 491, loss = 0.24310652\n",
      "Iteration 492, loss = 0.24274065\n",
      "Iteration 493, loss = 0.24237602\n",
      "Iteration 494, loss = 0.24201261\n",
      "Iteration 495, loss = 0.24165043\n",
      "Iteration 496, loss = 0.24128945\n",
      "Iteration 497, loss = 0.24092969\n",
      "Iteration 498, loss = 0.24057112\n",
      "Iteration 499, loss = 0.24021376\n",
      "Iteration 500, loss = 0.23985759\n",
      "Iteration 501, loss = 0.23950260\n",
      "Iteration 502, loss = 0.23914880\n",
      "Iteration 503, loss = 0.23879617\n",
      "Iteration 504, loss = 0.23844472\n",
      "Iteration 505, loss = 0.23809443\n",
      "Iteration 506, loss = 0.23774531\n",
      "Iteration 507, loss = 0.23739734\n",
      "Iteration 508, loss = 0.23705052\n",
      "Iteration 509, loss = 0.23670485\n",
      "Iteration 510, loss = 0.23636032\n",
      "Iteration 511, loss = 0.23601692\n",
      "Iteration 512, loss = 0.23567466\n",
      "Iteration 513, loss = 0.23533352\n",
      "Iteration 514, loss = 0.23499350\n",
      "Iteration 515, loss = 0.23465459\n",
      "Iteration 516, loss = 0.23431680\n",
      "Iteration 517, loss = 0.23398012\n",
      "Iteration 518, loss = 0.23364453\n",
      "Iteration 519, loss = 0.23331004\n",
      "Iteration 520, loss = 0.23297665\n",
      "Iteration 521, loss = 0.23264433\n",
      "Iteration 522, loss = 0.23231310\n",
      "Iteration 523, loss = 0.23198295\n",
      "Iteration 524, loss = 0.23165387\n",
      "Iteration 525, loss = 0.23132585\n",
      "Iteration 526, loss = 0.23099890\n",
      "Iteration 527, loss = 0.23067301\n",
      "Iteration 528, loss = 0.23034817\n",
      "Iteration 529, loss = 0.23002437\n",
      "Iteration 530, loss = 0.22970162\n",
      "Iteration 531, loss = 0.22937991\n",
      "Iteration 532, loss = 0.22905923\n",
      "Iteration 533, loss = 0.22873959\n",
      "Iteration 534, loss = 0.22842096\n",
      "Iteration 535, loss = 0.22810336\n",
      "Iteration 536, loss = 0.22778677\n",
      "Iteration 537, loss = 0.22747120\n",
      "Iteration 538, loss = 0.22715663\n",
      "Iteration 539, loss = 0.22684306\n",
      "Iteration 540, loss = 0.22653050\n",
      "Iteration 541, loss = 0.22621892\n",
      "Iteration 542, loss = 0.22590833\n",
      "Iteration 543, loss = 0.22559873\n",
      "Iteration 544, loss = 0.22529011\n",
      "Iteration 545, loss = 0.22498247\n",
      "Iteration 546, loss = 0.22467579\n",
      "Iteration 547, loss = 0.22437008\n",
      "Iteration 548, loss = 0.22406534\n",
      "Iteration 549, loss = 0.22376155\n",
      "Iteration 550, loss = 0.22345872\n",
      "Iteration 551, loss = 0.22315684\n",
      "Iteration 552, loss = 0.22285590\n",
      "Iteration 553, loss = 0.22255591\n",
      "Iteration 554, loss = 0.22225685\n",
      "Iteration 555, loss = 0.22195873\n",
      "Iteration 556, loss = 0.22166153\n",
      "Iteration 557, loss = 0.22136526\n",
      "Iteration 558, loss = 0.22106991\n",
      "Iteration 559, loss = 0.22077548\n",
      "Iteration 560, loss = 0.22048196\n",
      "Iteration 561, loss = 0.22018935\n",
      "Iteration 562, loss = 0.21989764\n",
      "Iteration 563, loss = 0.21960683\n",
      "Iteration 564, loss = 0.21931692\n",
      "Iteration 565, loss = 0.21902790\n",
      "Iteration 566, loss = 0.21873977\n",
      "Iteration 567, loss = 0.21845253\n",
      "Iteration 568, loss = 0.21816617\n",
      "Iteration 569, loss = 0.21788068\n",
      "Iteration 570, loss = 0.21759606\n",
      "Iteration 571, loss = 0.21731232\n",
      "Iteration 572, loss = 0.21702944\n",
      "Iteration 573, loss = 0.21674742\n",
      "Iteration 574, loss = 0.21646626\n",
      "Iteration 575, loss = 0.21618595\n",
      "Iteration 576, loss = 0.21590650\n",
      "Iteration 577, loss = 0.21562789\n",
      "Iteration 578, loss = 0.21535012\n",
      "Iteration 579, loss = 0.21507319\n",
      "Iteration 580, loss = 0.21479710\n",
      "Iteration 581, loss = 0.21452183\n",
      "Iteration 582, loss = 0.21424740\n",
      "Iteration 583, loss = 0.21397379\n",
      "Iteration 584, loss = 0.21370100\n",
      "Iteration 585, loss = 0.21342903\n",
      "Iteration 586, loss = 0.21315787\n",
      "Iteration 587, loss = 0.21288752\n",
      "Iteration 588, loss = 0.21261798\n",
      "Iteration 589, loss = 0.21234924\n",
      "Iteration 590, loss = 0.21208130\n",
      "Iteration 591, loss = 0.21181415\n",
      "Iteration 592, loss = 0.21154780\n",
      "Iteration 593, loss = 0.21128224\n",
      "Iteration 594, loss = 0.21101746\n",
      "Iteration 595, loss = 0.21075347\n",
      "Iteration 596, loss = 0.21049025\n",
      "Iteration 597, loss = 0.21022781\n",
      "Iteration 598, loss = 0.20996614\n",
      "Iteration 599, loss = 0.20970524\n",
      "Iteration 600, loss = 0.20944510\n",
      "Iteration 601, loss = 0.20918573\n",
      "Iteration 602, loss = 0.20892711\n",
      "Iteration 603, loss = 0.20866925\n",
      "Iteration 604, loss = 0.20841215\n",
      "Iteration 605, loss = 0.20815579\n",
      "Iteration 606, loss = 0.20790018\n",
      "Iteration 607, loss = 0.20764530\n",
      "Iteration 608, loss = 0.20739117\n",
      "Iteration 609, loss = 0.20713778\n",
      "Iteration 610, loss = 0.20688512\n",
      "Iteration 611, loss = 0.20663318\n",
      "Iteration 612, loss = 0.20638198\n",
      "Iteration 613, loss = 0.20613149\n",
      "Iteration 614, loss = 0.20588173\n",
      "Iteration 615, loss = 0.20563269\n",
      "Iteration 616, loss = 0.20538435\n",
      "Iteration 617, loss = 0.20513674\n",
      "Iteration 618, loss = 0.20488982\n",
      "Iteration 619, loss = 0.20464362\n",
      "Iteration 620, loss = 0.20439811\n",
      "Iteration 621, loss = 0.20415331\n",
      "Iteration 622, loss = 0.20390920\n",
      "Iteration 623, loss = 0.20366578\n",
      "Iteration 624, loss = 0.20342306\n",
      "Iteration 625, loss = 0.20318102\n",
      "Iteration 626, loss = 0.20293966\n",
      "Iteration 627, loss = 0.20269899\n",
      "Iteration 628, loss = 0.20245900\n",
      "Iteration 629, loss = 0.20221968\n",
      "Iteration 630, loss = 0.20198103\n",
      "Iteration 631, loss = 0.20174306\n",
      "Iteration 632, loss = 0.20150575\n",
      "Iteration 633, loss = 0.20126911\n",
      "Iteration 634, loss = 0.20103312\n",
      "Iteration 635, loss = 0.20079780\n",
      "Iteration 636, loss = 0.20056313\n",
      "Iteration 637, loss = 0.20032912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 638, loss = 0.20009576\n",
      "Iteration 639, loss = 0.19986304\n",
      "Iteration 640, loss = 0.19963097\n",
      "Iteration 641, loss = 0.19939954\n",
      "Iteration 642, loss = 0.19916876\n",
      "Iteration 643, loss = 0.19893861\n",
      "Iteration 644, loss = 0.19870909\n",
      "Iteration 645, loss = 0.19848021\n",
      "Iteration 646, loss = 0.19825195\n",
      "Iteration 647, loss = 0.19802432\n",
      "Iteration 648, loss = 0.19779732\n",
      "Iteration 649, loss = 0.19757093\n",
      "Iteration 650, loss = 0.19734517\n",
      "Iteration 651, loss = 0.19712002\n",
      "Iteration 652, loss = 0.19689549\n",
      "Iteration 653, loss = 0.19667156\n",
      "Iteration 654, loss = 0.19644825\n",
      "Iteration 655, loss = 0.19622554\n",
      "Iteration 656, loss = 0.19600344\n",
      "Iteration 657, loss = 0.19578193\n",
      "Iteration 658, loss = 0.19556103\n",
      "Iteration 659, loss = 0.19534072\n",
      "Iteration 660, loss = 0.19512101\n",
      "Iteration 661, loss = 0.19490188\n",
      "Iteration 662, loss = 0.19468335\n",
      "Iteration 663, loss = 0.19446540\n",
      "Iteration 664, loss = 0.19424804\n",
      "Iteration 665, loss = 0.19403126\n",
      "Iteration 666, loss = 0.19381506\n",
      "Iteration 667, loss = 0.19359943\n",
      "Iteration 668, loss = 0.19338438\n",
      "Iteration 669, loss = 0.19316991\n",
      "Iteration 670, loss = 0.19295600\n",
      "Iteration 671, loss = 0.19274266\n",
      "Iteration 672, loss = 0.19252989\n",
      "Iteration 673, loss = 0.19231767\n",
      "Iteration 674, loss = 0.19210603\n",
      "Iteration 675, loss = 0.19189493\n",
      "Iteration 676, loss = 0.19168440\n",
      "Iteration 677, loss = 0.19147442\n",
      "Iteration 678, loss = 0.19126499\n",
      "Iteration 679, loss = 0.19105611\n",
      "Iteration 680, loss = 0.19084778\n",
      "Iteration 681, loss = 0.19064000\n",
      "Iteration 682, loss = 0.19043275\n",
      "Iteration 683, loss = 0.19022605\n",
      "Iteration 684, loss = 0.19001989\n",
      "Iteration 685, loss = 0.18981427\n",
      "Iteration 686, loss = 0.18960918\n",
      "Iteration 687, loss = 0.18940462\n",
      "Iteration 688, loss = 0.18920059\n",
      "Iteration 689, loss = 0.18899709\n",
      "Iteration 690, loss = 0.18879412\n",
      "Iteration 691, loss = 0.18859167\n",
      "Iteration 692, loss = 0.18838974\n",
      "Iteration 693, loss = 0.18818833\n",
      "Iteration 694, loss = 0.18798744\n",
      "Iteration 695, loss = 0.18778707\n",
      "Iteration 696, loss = 0.18758721\n",
      "Iteration 697, loss = 0.18738786\n",
      "Iteration 698, loss = 0.18718903\n",
      "Iteration 699, loss = 0.18699070\n",
      "Iteration 700, loss = 0.18679287\n",
      "Iteration 701, loss = 0.18659555\n",
      "Iteration 702, loss = 0.18639873\n",
      "Iteration 703, loss = 0.18620241\n",
      "Iteration 704, loss = 0.18600659\n",
      "Iteration 705, loss = 0.18581127\n",
      "Iteration 706, loss = 0.18561644\n",
      "Iteration 707, loss = 0.18542210\n",
      "Iteration 708, loss = 0.18522825\n",
      "Iteration 709, loss = 0.18503490\n",
      "Iteration 710, loss = 0.18484202\n",
      "Iteration 711, loss = 0.18464964\n",
      "Iteration 712, loss = 0.18445773\n",
      "Iteration 713, loss = 0.18426631\n",
      "Iteration 714, loss = 0.18407536\n",
      "Iteration 715, loss = 0.18388489\n",
      "Iteration 716, loss = 0.18369490\n",
      "Iteration 717, loss = 0.18350538\n",
      "Iteration 718, loss = 0.18331634\n",
      "Iteration 719, loss = 0.18312776\n",
      "Iteration 720, loss = 0.18293965\n",
      "Iteration 721, loss = 0.18275201\n",
      "Iteration 722, loss = 0.18256483\n",
      "Iteration 723, loss = 0.18237811\n",
      "Iteration 724, loss = 0.18219186\n",
      "Iteration 725, loss = 0.18200607\n",
      "Iteration 726, loss = 0.18182073\n",
      "Iteration 727, loss = 0.18163585\n",
      "Iteration 728, loss = 0.18145142\n",
      "Iteration 729, loss = 0.18126745\n",
      "Iteration 730, loss = 0.18108392\n",
      "Iteration 731, loss = 0.18090085\n",
      "Iteration 732, loss = 0.18071822\n",
      "Iteration 733, loss = 0.18053604\n",
      "Iteration 734, loss = 0.18035430\n",
      "Iteration 735, loss = 0.18017300\n",
      "Iteration 736, loss = 0.17999215\n",
      "Iteration 737, loss = 0.17981173\n",
      "Iteration 738, loss = 0.17963175\n",
      "Iteration 739, loss = 0.17945221\n",
      "Iteration 740, loss = 0.17927310\n",
      "Iteration 741, loss = 0.17909442\n",
      "Iteration 742, loss = 0.17891618\n",
      "Iteration 743, loss = 0.17873836\n",
      "Iteration 744, loss = 0.17856097\n",
      "Iteration 745, loss = 0.17838401\n",
      "Iteration 746, loss = 0.17820747\n",
      "Iteration 747, loss = 0.17803135\n",
      "Iteration 748, loss = 0.17785566\n",
      "Iteration 749, loss = 0.17768038\n",
      "Iteration 750, loss = 0.17750553\n",
      "Iteration 751, loss = 0.17733109\n",
      "Iteration 752, loss = 0.17715706\n",
      "Iteration 753, loss = 0.17698345\n",
      "Iteration 754, loss = 0.17681025\n",
      "Iteration 755, loss = 0.17663747\n",
      "Iteration 756, loss = 0.17646509\n",
      "Iteration 757, loss = 0.17629312\n",
      "Iteration 758, loss = 0.17612155\n",
      "Iteration 759, loss = 0.17595039\n",
      "Iteration 760, loss = 0.17577963\n",
      "Iteration 761, loss = 0.17560928\n",
      "Iteration 762, loss = 0.17543932\n",
      "Iteration 763, loss = 0.17526977\n",
      "Iteration 764, loss = 0.17510061\n",
      "Iteration 765, loss = 0.17493184\n",
      "Iteration 766, loss = 0.17476347\n",
      "Iteration 767, loss = 0.17459550\n",
      "Iteration 768, loss = 0.17442792\n",
      "Iteration 769, loss = 0.17426072\n",
      "Iteration 770, loss = 0.17409392\n",
      "Iteration 771, loss = 0.17392750\n",
      "Iteration 772, loss = 0.17376147\n",
      "Iteration 773, loss = 0.17359582\n",
      "Iteration 774, loss = 0.17343056\n",
      "Iteration 775, loss = 0.17326567\n",
      "Iteration 776, loss = 0.17310117\n",
      "Iteration 777, loss = 0.17293705\n",
      "Iteration 778, loss = 0.17277331\n",
      "Iteration 779, loss = 0.17260994\n",
      "Iteration 780, loss = 0.17244695\n",
      "Iteration 781, loss = 0.17228433\n",
      "Iteration 782, loss = 0.17212208\n",
      "Iteration 783, loss = 0.17196020\n",
      "Iteration 784, loss = 0.17179870\n",
      "Iteration 785, loss = 0.17163756\n",
      "Iteration 786, loss = 0.17147679\n",
      "Iteration 787, loss = 0.17131639\n",
      "Iteration 788, loss = 0.17115635\n",
      "Iteration 789, loss = 0.17099667\n",
      "Iteration 790, loss = 0.17083736\n",
      "Iteration 791, loss = 0.17067841\n",
      "Iteration 792, loss = 0.17051981\n",
      "Iteration 793, loss = 0.17036158\n",
      "Iteration 794, loss = 0.17020370\n",
      "Iteration 795, loss = 0.17004618\n",
      "Iteration 796, loss = 0.16988901\n",
      "Iteration 797, loss = 0.16973220\n",
      "Iteration 798, loss = 0.16957574\n",
      "Iteration 799, loss = 0.16941962\n",
      "Iteration 800, loss = 0.16926386\n",
      "Iteration 801, loss = 0.16910845\n",
      "Iteration 802, loss = 0.16895338\n",
      "Iteration 803, loss = 0.16879866\n",
      "Iteration 804, loss = 0.16864429\n",
      "Iteration 805, loss = 0.16849026\n",
      "Iteration 806, loss = 0.16833657\n",
      "Iteration 807, loss = 0.16818322\n",
      "Iteration 808, loss = 0.16803021\n",
      "Iteration 809, loss = 0.16787754\n",
      "Iteration 810, loss = 0.16772521\n",
      "Iteration 811, loss = 0.16757322\n",
      "Iteration 812, loss = 0.16742156\n",
      "Iteration 813, loss = 0.16727023\n",
      "Iteration 814, loss = 0.16711924\n",
      "Iteration 815, loss = 0.16696858\n",
      "Iteration 816, loss = 0.16681825\n",
      "Iteration 817, loss = 0.16666826\n",
      "Iteration 818, loss = 0.16651859\n",
      "Iteration 819, loss = 0.16636924\n",
      "Iteration 820, loss = 0.16622023\n",
      "Iteration 821, loss = 0.16607154\n",
      "Iteration 822, loss = 0.16592317\n",
      "Iteration 823, loss = 0.16577513\n",
      "Iteration 824, loss = 0.16562740\n",
      "Iteration 825, loss = 0.16548000\n",
      "Iteration 826, loss = 0.16533292\n",
      "Iteration 827, loss = 0.16518616\n",
      "Iteration 828, loss = 0.16503972\n",
      "Iteration 829, loss = 0.16489359\n",
      "Iteration 830, loss = 0.16474778\n",
      "Iteration 831, loss = 0.16460228\n",
      "Iteration 832, loss = 0.16445710\n",
      "Iteration 833, loss = 0.16431223\n",
      "Iteration 834, loss = 0.16416767\n",
      "Iteration 835, loss = 0.16402342\n",
      "Iteration 836, loss = 0.16387948\n",
      "Iteration 837, loss = 0.16373584\n",
      "Iteration 838, loss = 0.16359252\n",
      "Iteration 839, loss = 0.16344950\n",
      "Iteration 840, loss = 0.16330679\n",
      "Iteration 841, loss = 0.16316438\n",
      "Iteration 842, loss = 0.16302227\n",
      "Iteration 843, loss = 0.16288047\n",
      "Iteration 844, loss = 0.16273897\n",
      "Iteration 845, loss = 0.16259777\n",
      "Iteration 846, loss = 0.16245686\n",
      "Iteration 847, loss = 0.16231626\n",
      "Iteration 848, loss = 0.16217595\n",
      "Iteration 849, loss = 0.16203594\n",
      "Iteration 850, loss = 0.16189623\n",
      "Iteration 851, loss = 0.16175681\n",
      "Iteration 852, loss = 0.16161768\n",
      "Iteration 853, loss = 0.16147884\n",
      "Iteration 854, loss = 0.16134030\n",
      "Iteration 855, loss = 0.16120205\n",
      "Iteration 856, loss = 0.16106408\n",
      "Iteration 857, loss = 0.16092641\n",
      "Iteration 858, loss = 0.16078902\n",
      "Iteration 859, loss = 0.16065192\n",
      "Iteration 860, loss = 0.16051511\n",
      "Iteration 861, loss = 0.16037858\n",
      "Iteration 862, loss = 0.16024233\n",
      "Iteration 863, loss = 0.16010637\n",
      "Iteration 864, loss = 0.15997069\n",
      "Iteration 865, loss = 0.15983529\n",
      "Iteration 866, loss = 0.15970017\n",
      "Iteration 867, loss = 0.15956533\n",
      "Iteration 868, loss = 0.15943077\n",
      "Iteration 869, loss = 0.15929649\n",
      "Iteration 870, loss = 0.15916249\n",
      "Iteration 871, loss = 0.15902876\n",
      "Iteration 872, loss = 0.15889530\n",
      "Iteration 873, loss = 0.15876212\n",
      "Iteration 874, loss = 0.15862921\n",
      "Iteration 875, loss = 0.15849658\n",
      "Iteration 876, loss = 0.15836422\n",
      "Iteration 877, loss = 0.15823212\n",
      "Iteration 878, loss = 0.15810030\n",
      "Iteration 879, loss = 0.15796875\n",
      "Iteration 880, loss = 0.15783746\n",
      "Iteration 881, loss = 0.15770644\n",
      "Iteration 882, loss = 0.15757569\n",
      "Iteration 883, loss = 0.15744521\n",
      "Iteration 884, loss = 0.15731499\n",
      "Iteration 885, loss = 0.15718503\n",
      "Iteration 886, loss = 0.15705533\n",
      "Iteration 887, loss = 0.15692590\n",
      "Iteration 888, loss = 0.15679673\n",
      "Iteration 889, loss = 0.15666782\n",
      "Iteration 890, loss = 0.15653917\n",
      "Iteration 891, loss = 0.15641078\n",
      "Iteration 892, loss = 0.15628265\n",
      "Iteration 893, loss = 0.15615478\n",
      "Iteration 894, loss = 0.15602716\n",
      "Iteration 895, loss = 0.15589979\n",
      "Iteration 896, loss = 0.15577269\n",
      "Iteration 897, loss = 0.15564583\n",
      "Iteration 898, loss = 0.15551923\n",
      "Iteration 899, loss = 0.15539289\n",
      "Iteration 900, loss = 0.15526679\n",
      "Iteration 901, loss = 0.15514095\n",
      "Iteration 902, loss = 0.15501535\n",
      "Iteration 903, loss = 0.15489001\n",
      "Iteration 904, loss = 0.15476491\n",
      "Iteration 905, loss = 0.15464007\n",
      "Iteration 906, loss = 0.15451547\n",
      "Iteration 907, loss = 0.15439111\n",
      "Iteration 908, loss = 0.15426700\n",
      "Iteration 909, loss = 0.15414314\n",
      "Iteration 910, loss = 0.15401952\n",
      "Iteration 911, loss = 0.15389615\n",
      "Iteration 912, loss = 0.15377302\n",
      "Iteration 913, loss = 0.15365013\n",
      "Iteration 914, loss = 0.15352748\n",
      "Iteration 915, loss = 0.15340507\n",
      "Iteration 916, loss = 0.15328290\n",
      "Iteration 917, loss = 0.15316097\n",
      "Iteration 918, loss = 0.15303928\n",
      "Iteration 919, loss = 0.15291783\n",
      "Iteration 920, loss = 0.15279661\n",
      "Iteration 921, loss = 0.15267563\n",
      "Iteration 922, loss = 0.15255488\n",
      "Iteration 923, loss = 0.15243437\n",
      "Iteration 924, loss = 0.15231410\n",
      "Iteration 925, loss = 0.15219406\n",
      "Iteration 926, loss = 0.15207425\n",
      "Iteration 927, loss = 0.15195467\n",
      "Iteration 928, loss = 0.15183532\n",
      "Iteration 929, loss = 0.15171621\n",
      "Iteration 930, loss = 0.15159732\n",
      "Iteration 931, loss = 0.15147866\n",
      "Iteration 932, loss = 0.15136024\n",
      "Iteration 933, loss = 0.15124204\n",
      "Iteration 934, loss = 0.15112406\n",
      "Iteration 935, loss = 0.15100632\n",
      "Iteration 936, loss = 0.15088880\n",
      "Iteration 937, loss = 0.15077150\n",
      "Iteration 938, loss = 0.15065443\n",
      "Iteration 939, loss = 0.15053758\n",
      "Iteration 940, loss = 0.15042096\n",
      "Iteration 941, loss = 0.15030456\n",
      "Iteration 942, loss = 0.15018838\n",
      "Iteration 943, loss = 0.15007242\n",
      "Iteration 944, loss = 0.14995668\n",
      "Iteration 945, loss = 0.14984116\n",
      "Iteration 946, loss = 0.14972586\n",
      "Iteration 947, loss = 0.14961079\n",
      "Iteration 948, loss = 0.14949592\n",
      "Iteration 949, loss = 0.14938128\n",
      "Iteration 950, loss = 0.14926685\n",
      "Iteration 951, loss = 0.14915264\n",
      "Iteration 952, loss = 0.14903864\n",
      "Iteration 953, loss = 0.14892486\n",
      "Iteration 954, loss = 0.14881129\n",
      "Iteration 955, loss = 0.14869794\n",
      "Iteration 956, loss = 0.14858480\n",
      "Iteration 957, loss = 0.14847187\n",
      "Iteration 958, loss = 0.14835915\n",
      "Iteration 959, loss = 0.14824665\n",
      "Iteration 960, loss = 0.14813435\n",
      "Iteration 961, loss = 0.14802227\n",
      "Iteration 962, loss = 0.14791039\n",
      "Iteration 963, loss = 0.14779873\n",
      "Iteration 964, loss = 0.14768727\n",
      "Iteration 965, loss = 0.14757602\n",
      "Iteration 966, loss = 0.14746497\n",
      "Iteration 967, loss = 0.14735413\n",
      "Iteration 968, loss = 0.14724350\n",
      "Iteration 969, loss = 0.14713307\n",
      "Iteration 970, loss = 0.14702285\n",
      "Iteration 971, loss = 0.14691283\n",
      "Iteration 972, loss = 0.14680301\n",
      "Iteration 973, loss = 0.14669340\n",
      "Iteration 974, loss = 0.14658399\n",
      "Iteration 975, loss = 0.14647478\n",
      "Iteration 976, loss = 0.14636577\n",
      "Iteration 977, loss = 0.14625697\n",
      "Iteration 978, loss = 0.14614836\n",
      "Iteration 979, loss = 0.14603995\n",
      "Iteration 980, loss = 0.14593174\n",
      "Iteration 981, loss = 0.14582373\n",
      "Iteration 982, loss = 0.14571591\n",
      "Iteration 983, loss = 0.14560830\n",
      "Iteration 984, loss = 0.14550088\n",
      "Iteration 985, loss = 0.14539365\n",
      "Iteration 986, loss = 0.14528663\n",
      "Iteration 987, loss = 0.14517979\n",
      "Iteration 988, loss = 0.14507315\n",
      "Iteration 989, loss = 0.14496671\n",
      "Iteration 990, loss = 0.14486046\n",
      "Iteration 991, loss = 0.14475440\n",
      "Iteration 992, loss = 0.14464853\n",
      "Iteration 993, loss = 0.14454285\n",
      "Iteration 994, loss = 0.14443737\n",
      "Iteration 995, loss = 0.14433207\n",
      "Iteration 996, loss = 0.14422697\n",
      "Iteration 997, loss = 0.14412206\n",
      "Iteration 998, loss = 0.14401733\n",
      "Iteration 999, loss = 0.14391279\n",
      "Iteration 1000, loss = 0.14380844\n",
      "Iteration 1, loss = 2.13108299\n",
      "Iteration 2, loss = 1.58078950\n",
      "Iteration 3, loss = 1.21326186\n",
      "Iteration 4, loss = 1.21182892\n",
      "Iteration 5, loss = 1.22326506\n",
      "Iteration 6, loss = 1.21701100\n",
      "Iteration 7, loss = 1.20748166\n",
      "Iteration 8, loss = 1.19894701\n",
      "Iteration 9, loss = 1.19309698\n",
      "Iteration 10, loss = 1.18945329\n",
      "Iteration 11, loss = 1.18607232\n",
      "Iteration 12, loss = 1.18198191\n",
      "Iteration 13, loss = 1.17723330\n",
      "Iteration 14, loss = 1.17182911\n",
      "Iteration 15, loss = 1.16652491\n",
      "Iteration 16, loss = 1.16139210\n",
      "Iteration 17, loss = 1.15619492\n",
      "Iteration 18, loss = 1.15140012\n",
      "Iteration 19, loss = 1.14664838\n",
      "Iteration 20, loss = 1.14179476\n",
      "Iteration 21, loss = 1.13688320\n",
      "Iteration 22, loss = 1.13196733\n",
      "Iteration 23, loss = 1.12708775\n",
      "Iteration 24, loss = 1.12225341\n",
      "Iteration 25, loss = 1.11748119\n",
      "Iteration 26, loss = 1.11272790\n",
      "Iteration 27, loss = 1.10796194\n",
      "Iteration 28, loss = 1.10330655\n",
      "Iteration 29, loss = 1.09865872\n",
      "Iteration 30, loss = 1.09398504\n",
      "Iteration 31, loss = 1.08930316\n",
      "Iteration 32, loss = 1.08457382\n",
      "Iteration 33, loss = 1.07979800\n",
      "Iteration 34, loss = 1.07492653\n",
      "Iteration 35, loss = 1.06994504\n",
      "Iteration 36, loss = 1.06488984\n",
      "Iteration 37, loss = 1.05968940\n",
      "Iteration 38, loss = 1.05433239\n",
      "Iteration 39, loss = 1.04888381\n",
      "Iteration 40, loss = 1.04332747\n",
      "Iteration 41, loss = 1.03763087\n",
      "Iteration 42, loss = 1.03180260\n",
      "Iteration 43, loss = 1.02585459\n",
      "Iteration 44, loss = 1.01980759\n",
      "Iteration 45, loss = 1.01372183\n",
      "Iteration 46, loss = 1.00754980\n",
      "Iteration 47, loss = 1.00130465\n",
      "Iteration 48, loss = 0.99499960\n",
      "Iteration 49, loss = 0.98864749\n",
      "Iteration 50, loss = 0.98226050\n",
      "Iteration 51, loss = 0.97584976\n",
      "Iteration 52, loss = 0.96942508\n",
      "Iteration 53, loss = 0.96299471\n",
      "Iteration 54, loss = 0.95656524\n",
      "Iteration 55, loss = 0.95014163\n",
      "Iteration 56, loss = 0.94372744\n",
      "Iteration 57, loss = 0.93732505\n",
      "Iteration 58, loss = 0.93093604\n",
      "Iteration 59, loss = 0.92456143\n",
      "Iteration 60, loss = 0.91820184\n",
      "Iteration 61, loss = 0.91185767\n",
      "Iteration 62, loss = 0.90552913\n",
      "Iteration 63, loss = 0.89921628\n",
      "Iteration 64, loss = 0.89291916\n",
      "Iteration 65, loss = 0.88663773\n",
      "Iteration 66, loss = 0.88037203\n",
      "Iteration 67, loss = 0.87412211\n",
      "Iteration 68, loss = 0.86788810\n",
      "Iteration 69, loss = 0.86167020\n",
      "Iteration 70, loss = 0.85546863\n",
      "Iteration 71, loss = 0.84928366\n",
      "Iteration 72, loss = 0.84311559\n",
      "Iteration 73, loss = 0.83696475\n",
      "Iteration 74, loss = 0.83083153\n",
      "Iteration 75, loss = 0.82471634\n",
      "Iteration 76, loss = 0.81861967\n",
      "Iteration 77, loss = 0.81254206\n",
      "Iteration 78, loss = 0.80648411\n",
      "Iteration 79, loss = 0.80044649\n",
      "Iteration 80, loss = 0.79442994\n",
      "Iteration 81, loss = 0.78843529\n",
      "Iteration 82, loss = 0.78246344\n",
      "Iteration 83, loss = 0.77651540\n",
      "Iteration 84, loss = 0.77059228\n",
      "Iteration 85, loss = 0.76469530\n",
      "Iteration 86, loss = 0.75882579\n",
      "Iteration 87, loss = 0.75298521\n",
      "Iteration 88, loss = 0.74717516\n",
      "Iteration 89, loss = 0.74139737\n",
      "Iteration 90, loss = 0.73565370\n",
      "Iteration 91, loss = 0.72994617\n",
      "Iteration 92, loss = 0.72427694\n",
      "Iteration 93, loss = 0.71864832\n",
      "Iteration 94, loss = 0.71306275\n",
      "Iteration 95, loss = 0.70752280\n",
      "Iteration 96, loss = 0.70203118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 97, loss = 0.69659069\n",
      "Iteration 98, loss = 0.69120422\n",
      "Iteration 99, loss = 0.68587473\n",
      "Iteration 100, loss = 0.68060522\n",
      "Iteration 101, loss = 0.67539872\n",
      "Iteration 102, loss = 0.67025820\n",
      "Iteration 103, loss = 0.66518663\n",
      "Iteration 104, loss = 0.66018684\n",
      "Iteration 105, loss = 0.65526157\n",
      "Iteration 106, loss = 0.65041336\n",
      "Iteration 107, loss = 0.64564455\n",
      "Iteration 108, loss = 0.64095725\n",
      "Iteration 109, loss = 0.63635328\n",
      "Iteration 110, loss = 0.63183414\n",
      "Iteration 111, loss = 0.62740102\n",
      "Iteration 112, loss = 0.62305472\n",
      "Iteration 113, loss = 0.61879572\n",
      "Iteration 114, loss = 0.61462409\n",
      "Iteration 115, loss = 0.61053957\n",
      "Iteration 116, loss = 0.60654152\n",
      "Iteration 117, loss = 0.60262900\n",
      "Iteration 118, loss = 0.59880074\n",
      "Iteration 119, loss = 0.59505523\n",
      "Iteration 120, loss = 0.59139070\n",
      "Iteration 121, loss = 0.58780522\n",
      "Iteration 122, loss = 0.58429669\n",
      "Iteration 123, loss = 0.58086293\n",
      "Iteration 124, loss = 0.57750167\n",
      "Iteration 125, loss = 0.57421062\n",
      "Iteration 126, loss = 0.57098750\n",
      "Iteration 127, loss = 0.56783007\n",
      "Iteration 128, loss = 0.56473612\n",
      "Iteration 129, loss = 0.56170355\n",
      "Iteration 130, loss = 0.55873031\n",
      "Iteration 131, loss = 0.55581446\n",
      "Iteration 132, loss = 0.55295418\n",
      "Iteration 133, loss = 0.55014772\n",
      "Iteration 134, loss = 0.54739345\n",
      "Iteration 135, loss = 0.54468983\n",
      "Iteration 136, loss = 0.54203542\n",
      "Iteration 137, loss = 0.53942886\n",
      "Iteration 138, loss = 0.53686887\n",
      "Iteration 139, loss = 0.53435424\n",
      "Iteration 140, loss = 0.53188383\n",
      "Iteration 141, loss = 0.52945657\n",
      "Iteration 142, loss = 0.52707141\n",
      "Iteration 143, loss = 0.52472736\n",
      "Iteration 144, loss = 0.52242347\n",
      "Iteration 145, loss = 0.52015883\n",
      "Iteration 146, loss = 0.51793252\n",
      "Iteration 147, loss = 0.51574369\n",
      "Iteration 148, loss = 0.51359147\n",
      "Iteration 149, loss = 0.51147503\n",
      "Iteration 150, loss = 0.50939354\n",
      "Iteration 151, loss = 0.50734618\n",
      "Iteration 152, loss = 0.50533215\n",
      "Iteration 153, loss = 0.50335066\n",
      "Iteration 154, loss = 0.50140093\n",
      "Iteration 155, loss = 0.49948216\n",
      "Iteration 156, loss = 0.49759360\n",
      "Iteration 157, loss = 0.49573449\n",
      "Iteration 158, loss = 0.49390408\n",
      "Iteration 159, loss = 0.49210164\n",
      "Iteration 160, loss = 0.49032642\n",
      "Iteration 161, loss = 0.48857773\n",
      "Iteration 162, loss = 0.48685485\n",
      "Iteration 163, loss = 0.48515710\n",
      "Iteration 164, loss = 0.48348380\n",
      "Iteration 165, loss = 0.48183429\n",
      "Iteration 166, loss = 0.48020791\n",
      "Iteration 167, loss = 0.47860405\n",
      "Iteration 168, loss = 0.47702208\n",
      "Iteration 169, loss = 0.47546139\n",
      "Iteration 170, loss = 0.47392142\n",
      "Iteration 171, loss = 0.47240157\n",
      "Iteration 172, loss = 0.47090132\n",
      "Iteration 173, loss = 0.46942011\n",
      "Iteration 174, loss = 0.46795742\n",
      "Iteration 175, loss = 0.46651275\n",
      "Iteration 176, loss = 0.46508562\n",
      "Iteration 177, loss = 0.46367553\n",
      "Iteration 178, loss = 0.46228204\n",
      "Iteration 179, loss = 0.46090470\n",
      "Iteration 180, loss = 0.45954308\n",
      "Iteration 181, loss = 0.45819675\n",
      "Iteration 182, loss = 0.45686531\n",
      "Iteration 183, loss = 0.45554838\n",
      "Iteration 184, loss = 0.45424557\n",
      "Iteration 185, loss = 0.45295650\n",
      "Iteration 186, loss = 0.45168084\n",
      "Iteration 187, loss = 0.45041822\n",
      "Iteration 188, loss = 0.44916833\n",
      "Iteration 189, loss = 0.44793082\n",
      "Iteration 190, loss = 0.44670540\n",
      "Iteration 191, loss = 0.44549175\n",
      "Iteration 192, loss = 0.44428958\n",
      "Iteration 193, loss = 0.44309861\n",
      "Iteration 194, loss = 0.44191856\n",
      "Iteration 195, loss = 0.44074915\n",
      "Iteration 196, loss = 0.43959014\n",
      "Iteration 197, loss = 0.43844127\n",
      "Iteration 198, loss = 0.43730229\n",
      "Iteration 199, loss = 0.43617296\n",
      "Iteration 200, loss = 0.43505307\n",
      "Iteration 201, loss = 0.43394238\n",
      "Iteration 202, loss = 0.43284068\n",
      "Iteration 203, loss = 0.43174776\n",
      "Iteration 204, loss = 0.43066342\n",
      "Iteration 205, loss = 0.42958746\n",
      "Iteration 206, loss = 0.42851968\n",
      "Iteration 207, loss = 0.42745992\n",
      "Iteration 208, loss = 0.42640798\n",
      "Iteration 209, loss = 0.42536369\n",
      "Iteration 210, loss = 0.42432689\n",
      "Iteration 211, loss = 0.42329741\n",
      "Iteration 212, loss = 0.42227510\n",
      "Iteration 213, loss = 0.42125979\n",
      "Iteration 214, loss = 0.42025134\n",
      "Iteration 215, loss = 0.41924961\n",
      "Iteration 216, loss = 0.41825446\n",
      "Iteration 217, loss = 0.41726575\n",
      "Iteration 218, loss = 0.41628334\n",
      "Iteration 219, loss = 0.41530713\n",
      "Iteration 220, loss = 0.41433697\n",
      "Iteration 221, loss = 0.41337274\n",
      "Iteration 222, loss = 0.41241435\n",
      "Iteration 223, loss = 0.41146166\n",
      "Iteration 224, loss = 0.41051457\n",
      "Iteration 225, loss = 0.40957298\n",
      "Iteration 226, loss = 0.40863678\n",
      "Iteration 227, loss = 0.40770588\n",
      "Iteration 228, loss = 0.40678017\n",
      "Iteration 229, loss = 0.40585956\n",
      "Iteration 230, loss = 0.40494396\n",
      "Iteration 231, loss = 0.40403329\n",
      "Iteration 232, loss = 0.40312745\n",
      "Iteration 233, loss = 0.40222637\n",
      "Iteration 234, loss = 0.40132996\n",
      "Iteration 235, loss = 0.40043814\n",
      "Iteration 236, loss = 0.39955085\n",
      "Iteration 237, loss = 0.39866800\n",
      "Iteration 238, loss = 0.39778953\n",
      "Iteration 239, loss = 0.39691536\n",
      "Iteration 240, loss = 0.39604543\n",
      "Iteration 241, loss = 0.39517968\n",
      "Iteration 242, loss = 0.39431803\n",
      "Iteration 243, loss = 0.39346043\n",
      "Iteration 244, loss = 0.39260683\n",
      "Iteration 245, loss = 0.39175715\n",
      "Iteration 246, loss = 0.39091135\n",
      "Iteration 247, loss = 0.39006936\n",
      "Iteration 248, loss = 0.38923115\n",
      "Iteration 249, loss = 0.38839665\n",
      "Iteration 250, loss = 0.38756582\n",
      "Iteration 251, loss = 0.38673861\n",
      "Iteration 252, loss = 0.38591497\n",
      "Iteration 253, loss = 0.38509485\n",
      "Iteration 254, loss = 0.38427822\n",
      "Iteration 255, loss = 0.38346502\n",
      "Iteration 256, loss = 0.38265523\n",
      "Iteration 257, loss = 0.38184879\n",
      "Iteration 258, loss = 0.38104566\n",
      "Iteration 259, loss = 0.38024582\n",
      "Iteration 260, loss = 0.37944922\n",
      "Iteration 261, loss = 0.37865583\n",
      "Iteration 262, loss = 0.37786561\n",
      "Iteration 263, loss = 0.37707852\n",
      "Iteration 264, loss = 0.37629454\n",
      "Iteration 265, loss = 0.37551364\n",
      "Iteration 266, loss = 0.37473578\n",
      "Iteration 267, loss = 0.37396093\n",
      "Iteration 268, loss = 0.37318906\n",
      "Iteration 269, loss = 0.37242015\n",
      "Iteration 270, loss = 0.37165416\n",
      "Iteration 271, loss = 0.37089107\n",
      "Iteration 272, loss = 0.37013086\n",
      "Iteration 273, loss = 0.36937350\n",
      "Iteration 274, loss = 0.36861895\n",
      "Iteration 275, loss = 0.36786721\n",
      "Iteration 276, loss = 0.36711825\n",
      "Iteration 277, loss = 0.36637204\n",
      "Iteration 278, loss = 0.36562856\n",
      "Iteration 279, loss = 0.36488779\n",
      "Iteration 280, loss = 0.36414971\n",
      "Iteration 281, loss = 0.36341430\n",
      "Iteration 282, loss = 0.36268153\n",
      "Iteration 283, loss = 0.36195140\n",
      "Iteration 284, loss = 0.36122388\n",
      "Iteration 285, loss = 0.36049895\n",
      "Iteration 286, loss = 0.35977660\n",
      "Iteration 287, loss = 0.35905681\n",
      "Iteration 288, loss = 0.35833955\n",
      "Iteration 289, loss = 0.35762482\n",
      "Iteration 290, loss = 0.35691260\n",
      "Iteration 291, loss = 0.35620288\n",
      "Iteration 292, loss = 0.35549563\n",
      "Iteration 293, loss = 0.35479084\n",
      "Iteration 294, loss = 0.35408850\n",
      "Iteration 295, loss = 0.35338860\n",
      "Iteration 296, loss = 0.35269111\n",
      "Iteration 297, loss = 0.35199604\n",
      "Iteration 298, loss = 0.35130335\n",
      "Iteration 299, loss = 0.35061305\n",
      "Iteration 300, loss = 0.34992512\n",
      "Iteration 301, loss = 0.34923954\n",
      "Iteration 302, loss = 0.34855631\n",
      "Iteration 303, loss = 0.34787540\n",
      "Iteration 304, loss = 0.34719682\n",
      "Iteration 305, loss = 0.34652055\n",
      "Iteration 306, loss = 0.34584658\n",
      "Iteration 307, loss = 0.34517490\n",
      "Iteration 308, loss = 0.34450549\n",
      "Iteration 309, loss = 0.34383835\n",
      "Iteration 310, loss = 0.34317347\n",
      "Iteration 311, loss = 0.34251083\n",
      "Iteration 312, loss = 0.34185043\n",
      "Iteration 313, loss = 0.34119226\n",
      "Iteration 314, loss = 0.34053630\n",
      "Iteration 315, loss = 0.33988256\n",
      "Iteration 316, loss = 0.33923101\n",
      "Iteration 317, loss = 0.33858165\n",
      "Iteration 318, loss = 0.33793448\n",
      "Iteration 319, loss = 0.33728948\n",
      "Iteration 320, loss = 0.33664664\n",
      "Iteration 321, loss = 0.33600596\n",
      "Iteration 322, loss = 0.33536743\n",
      "Iteration 323, loss = 0.33473104\n",
      "Iteration 324, loss = 0.33409678\n",
      "Iteration 325, loss = 0.33346464\n",
      "Iteration 326, loss = 0.33283462\n",
      "Iteration 327, loss = 0.33220671\n",
      "Iteration 328, loss = 0.33158090\n",
      "Iteration 329, loss = 0.33095719\n",
      "Iteration 330, loss = 0.33033555\n",
      "Iteration 331, loss = 0.32971600\n",
      "Iteration 332, loss = 0.32909852\n",
      "Iteration 333, loss = 0.32848310\n",
      "Iteration 334, loss = 0.32786974\n",
      "Iteration 335, loss = 0.32725844\n",
      "Iteration 336, loss = 0.32664917\n",
      "Iteration 337, loss = 0.32604194\n",
      "Iteration 338, loss = 0.32543674\n",
      "Iteration 339, loss = 0.32483356\n",
      "Iteration 340, loss = 0.32423240\n",
      "Iteration 341, loss = 0.32363325\n",
      "Iteration 342, loss = 0.32303609\n",
      "Iteration 343, loss = 0.32244094\n",
      "Iteration 344, loss = 0.32184778\n",
      "Iteration 345, loss = 0.32125659\n",
      "Iteration 346, loss = 0.32066739\n",
      "Iteration 347, loss = 0.32008016\n",
      "Iteration 348, loss = 0.31949488\n",
      "Iteration 349, loss = 0.31891157\n",
      "Iteration 350, loss = 0.31833021\n",
      "Iteration 351, loss = 0.31775079\n",
      "Iteration 352, loss = 0.31717332\n",
      "Iteration 353, loss = 0.31659777\n",
      "Iteration 354, loss = 0.31602415\n",
      "Iteration 355, loss = 0.31545245\n",
      "Iteration 356, loss = 0.31488267\n",
      "Iteration 357, loss = 0.31431479\n",
      "Iteration 358, loss = 0.31374882\n",
      "Iteration 359, loss = 0.31318474\n",
      "Iteration 360, loss = 0.31262255\n",
      "Iteration 361, loss = 0.31206225\n",
      "Iteration 362, loss = 0.31150382\n",
      "Iteration 363, loss = 0.31094727\n",
      "Iteration 364, loss = 0.31039258\n",
      "Iteration 365, loss = 0.30983975\n",
      "Iteration 366, loss = 0.30928878\n",
      "Iteration 367, loss = 0.30873965\n",
      "Iteration 368, loss = 0.30819237\n",
      "Iteration 369, loss = 0.30764692\n",
      "Iteration 370, loss = 0.30710330\n",
      "Iteration 371, loss = 0.30656151\n",
      "Iteration 372, loss = 0.30602154\n",
      "Iteration 373, loss = 0.30548338\n",
      "Iteration 374, loss = 0.30494702\n",
      "Iteration 375, loss = 0.30441247\n",
      "Iteration 376, loss = 0.30387971\n",
      "Iteration 377, loss = 0.30334874\n",
      "Iteration 378, loss = 0.30281956\n",
      "Iteration 379, loss = 0.30229215\n",
      "Iteration 380, loss = 0.30176651\n",
      "Iteration 381, loss = 0.30124264\n",
      "Iteration 382, loss = 0.30072053\n",
      "Iteration 383, loss = 0.30020018\n",
      "Iteration 384, loss = 0.29968157\n",
      "Iteration 385, loss = 0.29916470\n",
      "Iteration 386, loss = 0.29864958\n",
      "Iteration 387, loss = 0.29813618\n",
      "Iteration 388, loss = 0.29762451\n",
      "Iteration 389, loss = 0.29711456\n",
      "Iteration 390, loss = 0.29660632\n",
      "Iteration 391, loss = 0.29609979\n",
      "Iteration 392, loss = 0.29559496\n",
      "Iteration 393, loss = 0.29509183\n",
      "Iteration 394, loss = 0.29459038\n",
      "Iteration 395, loss = 0.29409063\n",
      "Iteration 396, loss = 0.29359255\n",
      "Iteration 397, loss = 0.29309614\n",
      "Iteration 398, loss = 0.29260140\n",
      "Iteration 399, loss = 0.29210833\n",
      "Iteration 400, loss = 0.29161690\n",
      "Iteration 401, loss = 0.29112713\n",
      "Iteration 402, loss = 0.29063901\n",
      "Iteration 403, loss = 0.29015252\n",
      "Iteration 404, loss = 0.28966766\n",
      "Iteration 405, loss = 0.28918443\n",
      "Iteration 406, loss = 0.28870282\n",
      "Iteration 407, loss = 0.28822283\n",
      "Iteration 408, loss = 0.28774445\n",
      "Iteration 409, loss = 0.28726767\n",
      "Iteration 410, loss = 0.28679248\n",
      "Iteration 411, loss = 0.28631889\n",
      "Iteration 412, loss = 0.28584689\n",
      "Iteration 413, loss = 0.28537647\n",
      "Iteration 414, loss = 0.28490762\n",
      "Iteration 415, loss = 0.28444034\n",
      "Iteration 416, loss = 0.28397463\n",
      "Iteration 417, loss = 0.28351047\n",
      "Iteration 418, loss = 0.28304787\n",
      "Iteration 419, loss = 0.28258681\n",
      "Iteration 420, loss = 0.28212729\n",
      "Iteration 421, loss = 0.28166931\n",
      "Iteration 422, loss = 0.28121286\n",
      "Iteration 423, loss = 0.28075793\n",
      "Iteration 424, loss = 0.28030452\n",
      "Iteration 425, loss = 0.27985262\n",
      "Iteration 426, loss = 0.27940222\n",
      "Iteration 427, loss = 0.27895333\n",
      "Iteration 428, loss = 0.27850593\n",
      "Iteration 429, loss = 0.27806003\n",
      "Iteration 430, loss = 0.27761560\n",
      "Iteration 431, loss = 0.27717266\n",
      "Iteration 432, loss = 0.27673118\n",
      "Iteration 433, loss = 0.27629118\n",
      "Iteration 434, loss = 0.27585263\n",
      "Iteration 435, loss = 0.27541555\n",
      "Iteration 436, loss = 0.27497991\n",
      "Iteration 437, loss = 0.27454571\n",
      "Iteration 438, loss = 0.27411296\n",
      "Iteration 439, loss = 0.27368164\n",
      "Iteration 440, loss = 0.27325174\n",
      "Iteration 441, loss = 0.27282327\n",
      "Iteration 442, loss = 0.27239622\n",
      "Iteration 443, loss = 0.27197057\n",
      "Iteration 444, loss = 0.27154633\n",
      "Iteration 445, loss = 0.27112350\n",
      "Iteration 446, loss = 0.27070205\n",
      "Iteration 447, loss = 0.27028200\n",
      "Iteration 448, loss = 0.26986333\n",
      "Iteration 449, loss = 0.26944603\n",
      "Iteration 450, loss = 0.26903011\n",
      "Iteration 451, loss = 0.26861556\n",
      "Iteration 452, loss = 0.26820237\n",
      "Iteration 453, loss = 0.26779054\n",
      "Iteration 454, loss = 0.26738005\n",
      "Iteration 455, loss = 0.26697091\n",
      "Iteration 456, loss = 0.26656312\n",
      "Iteration 457, loss = 0.26615665\n",
      "Iteration 458, loss = 0.26575152\n",
      "Iteration 459, loss = 0.26534771\n",
      "Iteration 460, loss = 0.26494521\n",
      "Iteration 461, loss = 0.26454404\n",
      "Iteration 462, loss = 0.26414417\n",
      "Iteration 463, loss = 0.26374560\n",
      "Iteration 464, loss = 0.26334833\n",
      "Iteration 465, loss = 0.26295235\n",
      "Iteration 466, loss = 0.26255766\n",
      "Iteration 467, loss = 0.26216425\n",
      "Iteration 468, loss = 0.26177211\n",
      "Iteration 469, loss = 0.26138125\n",
      "Iteration 470, loss = 0.26099165\n",
      "Iteration 471, loss = 0.26060332\n",
      "Iteration 472, loss = 0.26021624\n",
      "Iteration 473, loss = 0.25983041\n",
      "Iteration 474, loss = 0.25944582\n",
      "Iteration 475, loss = 0.25906248\n",
      "Iteration 476, loss = 0.25868037\n",
      "Iteration 477, loss = 0.25829949\n",
      "Iteration 478, loss = 0.25791983\n",
      "Iteration 479, loss = 0.25754139\n",
      "Iteration 480, loss = 0.25716417\n",
      "Iteration 481, loss = 0.25678816\n",
      "Iteration 482, loss = 0.25641335\n",
      "Iteration 483, loss = 0.25603975\n",
      "Iteration 484, loss = 0.25566733\n",
      "Iteration 485, loss = 0.25529611\n",
      "Iteration 486, loss = 0.25492607\n",
      "Iteration 487, loss = 0.25455721\n",
      "Iteration 488, loss = 0.25418953\n",
      "Iteration 489, loss = 0.25382301\n",
      "Iteration 490, loss = 0.25345766\n",
      "Iteration 491, loss = 0.25309347\n",
      "Iteration 492, loss = 0.25273044\n",
      "Iteration 493, loss = 0.25236855\n",
      "Iteration 494, loss = 0.25200781\n",
      "Iteration 495, loss = 0.25164821\n",
      "Iteration 496, loss = 0.25128975\n",
      "Iteration 497, loss = 0.25093242\n",
      "Iteration 498, loss = 0.25057621\n",
      "Iteration 499, loss = 0.25022113\n",
      "Iteration 500, loss = 0.24986716\n",
      "Iteration 501, loss = 0.24951431\n",
      "Iteration 502, loss = 0.24916256\n",
      "Iteration 503, loss = 0.24881192\n",
      "Iteration 504, loss = 0.24846237\n",
      "Iteration 505, loss = 0.24811392\n",
      "Iteration 506, loss = 0.24776655\n",
      "Iteration 507, loss = 0.24742027\n",
      "Iteration 508, loss = 0.24707507\n",
      "Iteration 509, loss = 0.24673095\n",
      "Iteration 510, loss = 0.24638789\n",
      "Iteration 511, loss = 0.24604590\n",
      "Iteration 512, loss = 0.24570497\n",
      "Iteration 513, loss = 0.24536510\n",
      "Iteration 514, loss = 0.24502629\n",
      "Iteration 515, loss = 0.24468852\n",
      "Iteration 516, loss = 0.24435179\n",
      "Iteration 517, loss = 0.24401610\n",
      "Iteration 518, loss = 0.24368145\n",
      "Iteration 519, loss = 0.24334783\n",
      "Iteration 520, loss = 0.24301523\n",
      "Iteration 521, loss = 0.24268366\n",
      "Iteration 522, loss = 0.24235310\n",
      "Iteration 523, loss = 0.24202355\n",
      "Iteration 524, loss = 0.24169502\n",
      "Iteration 525, loss = 0.24136749\n",
      "Iteration 526, loss = 0.24104096\n",
      "Iteration 527, loss = 0.24071542\n",
      "Iteration 528, loss = 0.24039088\n",
      "Iteration 529, loss = 0.24006732\n",
      "Iteration 530, loss = 0.23974475\n",
      "Iteration 531, loss = 0.23942315\n",
      "Iteration 532, loss = 0.23910253\n",
      "Iteration 533, loss = 0.23878288\n",
      "Iteration 534, loss = 0.23846420\n",
      "Iteration 535, loss = 0.23814648\n",
      "Iteration 536, loss = 0.23782972\n",
      "Iteration 537, loss = 0.23751391\n",
      "Iteration 538, loss = 0.23719905\n",
      "Iteration 539, loss = 0.23688514\n",
      "Iteration 540, loss = 0.23657217\n",
      "Iteration 541, loss = 0.23626014\n",
      "Iteration 542, loss = 0.23594905\n",
      "Iteration 543, loss = 0.23563888\n",
      "Iteration 544, loss = 0.23532964\n",
      "Iteration 545, loss = 0.23502132\n",
      "Iteration 546, loss = 0.23471393\n",
      "Iteration 547, loss = 0.23440744\n",
      "Iteration 548, loss = 0.23410187\n",
      "Iteration 549, loss = 0.23379720\n",
      "Iteration 550, loss = 0.23349344\n",
      "Iteration 551, loss = 0.23319057\n",
      "Iteration 552, loss = 0.23288860\n",
      "Iteration 553, loss = 0.23258752\n",
      "Iteration 554, loss = 0.23228733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 555, loss = 0.23198803\n",
      "Iteration 556, loss = 0.23168960\n",
      "Iteration 557, loss = 0.23139205\n",
      "Iteration 558, loss = 0.23109538\n",
      "Iteration 559, loss = 0.23079957\n",
      "Iteration 560, loss = 0.23050463\n",
      "Iteration 561, loss = 0.23021055\n",
      "Iteration 562, loss = 0.22991733\n",
      "Iteration 563, loss = 0.22962496\n",
      "Iteration 564, loss = 0.22933344\n",
      "Iteration 565, loss = 0.22904278\n",
      "Iteration 566, loss = 0.22875295\n",
      "Iteration 567, loss = 0.22846397\n",
      "Iteration 568, loss = 0.22817582\n",
      "Iteration 569, loss = 0.22788850\n",
      "Iteration 570, loss = 0.22760202\n",
      "Iteration 571, loss = 0.22731636\n",
      "Iteration 572, loss = 0.22703153\n",
      "Iteration 573, loss = 0.22674751\n",
      "Iteration 574, loss = 0.22646431\n",
      "Iteration 575, loss = 0.22618192\n",
      "Iteration 576, loss = 0.22590034\n",
      "Iteration 577, loss = 0.22561957\n",
      "Iteration 578, loss = 0.22533960\n",
      "Iteration 579, loss = 0.22506042\n",
      "Iteration 580, loss = 0.22478205\n",
      "Iteration 581, loss = 0.22450446\n",
      "Iteration 582, loss = 0.22422767\n",
      "Iteration 583, loss = 0.22395166\n",
      "Iteration 584, loss = 0.22367643\n",
      "Iteration 585, loss = 0.22340198\n",
      "Iteration 586, loss = 0.22312831\n",
      "Iteration 587, loss = 0.22285541\n",
      "Iteration 588, loss = 0.22258328\n",
      "Iteration 589, loss = 0.22231192\n",
      "Iteration 590, loss = 0.22204132\n",
      "Iteration 591, loss = 0.22177148\n",
      "Iteration 592, loss = 0.22150240\n",
      "Iteration 593, loss = 0.22123407\n",
      "Iteration 594, loss = 0.22096649\n",
      "Iteration 595, loss = 0.22069965\n",
      "Iteration 596, loss = 0.22043357\n",
      "Iteration 597, loss = 0.22016822\n",
      "Iteration 598, loss = 0.21990361\n",
      "Iteration 599, loss = 0.21963974\n",
      "Iteration 600, loss = 0.21937660\n",
      "Iteration 601, loss = 0.21911418\n",
      "Iteration 602, loss = 0.21885250\n",
      "Iteration 603, loss = 0.21859154\n",
      "Iteration 604, loss = 0.21833129\n",
      "Iteration 605, loss = 0.21807177\n",
      "Iteration 606, loss = 0.21781296\n",
      "Iteration 607, loss = 0.21755486\n",
      "Iteration 608, loss = 0.21729747\n",
      "Iteration 609, loss = 0.21704078\n",
      "Iteration 610, loss = 0.21678480\n",
      "Iteration 611, loss = 0.21652951\n",
      "Iteration 612, loss = 0.21627493\n",
      "Iteration 613, loss = 0.21602104\n",
      "Iteration 614, loss = 0.21576783\n",
      "Iteration 615, loss = 0.21551532\n",
      "Iteration 616, loss = 0.21526349\n",
      "Iteration 617, loss = 0.21501235\n",
      "Iteration 618, loss = 0.21476189\n",
      "Iteration 619, loss = 0.21451210\n",
      "Iteration 620, loss = 0.21426299\n",
      "Iteration 621, loss = 0.21401455\n",
      "Iteration 622, loss = 0.21376678\n",
      "Iteration 623, loss = 0.21351967\n",
      "Iteration 624, loss = 0.21327323\n",
      "Iteration 625, loss = 0.21302745\n",
      "Iteration 626, loss = 0.21278233\n",
      "Iteration 627, loss = 0.21253786\n",
      "Iteration 628, loss = 0.21229405\n",
      "Iteration 629, loss = 0.21205089\n",
      "Iteration 630, loss = 0.21180837\n",
      "Iteration 631, loss = 0.21156650\n",
      "Iteration 632, loss = 0.21132528\n",
      "Iteration 633, loss = 0.21108469\n",
      "Iteration 634, loss = 0.21084474\n",
      "Iteration 635, loss = 0.21060543\n",
      "Iteration 636, loss = 0.21036675\n",
      "Iteration 637, loss = 0.21012869\n",
      "Iteration 638, loss = 0.20989127\n",
      "Iteration 639, loss = 0.20965447\n",
      "Iteration 640, loss = 0.20941830\n",
      "Iteration 641, loss = 0.20918274\n",
      "Iteration 642, loss = 0.20894780\n",
      "Iteration 643, loss = 0.20871348\n",
      "Iteration 644, loss = 0.20847976\n",
      "Iteration 645, loss = 0.20824666\n",
      "Iteration 646, loss = 0.20801417\n",
      "Iteration 647, loss = 0.20778228\n",
      "Iteration 648, loss = 0.20755100\n",
      "Iteration 649, loss = 0.20732031\n",
      "Iteration 650, loss = 0.20709023\n",
      "Iteration 651, loss = 0.20686074\n",
      "Iteration 652, loss = 0.20663184\n",
      "Iteration 653, loss = 0.20640354\n",
      "Iteration 654, loss = 0.20617582\n",
      "Iteration 655, loss = 0.20594869\n",
      "Iteration 656, loss = 0.20572214\n",
      "Iteration 657, loss = 0.20549618\n",
      "Iteration 658, loss = 0.20527080\n",
      "Iteration 659, loss = 0.20504599\n",
      "Iteration 660, loss = 0.20482176\n",
      "Iteration 661, loss = 0.20459810\n",
      "Iteration 662, loss = 0.20437502\n",
      "Iteration 663, loss = 0.20415250\n",
      "Iteration 664, loss = 0.20393055\n",
      "Iteration 665, loss = 0.20370916\n",
      "Iteration 666, loss = 0.20348833\n",
      "Iteration 667, loss = 0.20326807\n",
      "Iteration 668, loss = 0.20304836\n",
      "Iteration 669, loss = 0.20282921\n",
      "Iteration 670, loss = 0.20261061\n",
      "Iteration 671, loss = 0.20239256\n",
      "Iteration 672, loss = 0.20217506\n",
      "Iteration 673, loss = 0.20195811\n",
      "Iteration 674, loss = 0.20174170\n",
      "Iteration 675, loss = 0.20152584\n",
      "Iteration 676, loss = 0.20131051\n",
      "Iteration 677, loss = 0.20109573\n",
      "Iteration 678, loss = 0.20088148\n",
      "Iteration 679, loss = 0.20066777\n",
      "Iteration 680, loss = 0.20045459\n",
      "Iteration 681, loss = 0.20024194\n",
      "Iteration 682, loss = 0.20002982\n",
      "Iteration 683, loss = 0.19981822\n",
      "Iteration 684, loss = 0.19960715\n",
      "Iteration 685, loss = 0.19939660\n",
      "Iteration 686, loss = 0.19918657\n",
      "Iteration 687, loss = 0.19897706\n",
      "Iteration 688, loss = 0.19876807\n",
      "Iteration 689, loss = 0.19855959\n",
      "Iteration 690, loss = 0.19835163\n",
      "Iteration 691, loss = 0.19814417\n",
      "Iteration 692, loss = 0.19793723\n",
      "Iteration 693, loss = 0.19773079\n",
      "Iteration 694, loss = 0.19752486\n",
      "Iteration 695, loss = 0.19731943\n",
      "Iteration 696, loss = 0.19711450\n",
      "Iteration 697, loss = 0.19691007\n",
      "Iteration 698, loss = 0.19670613\n",
      "Iteration 699, loss = 0.19650270\n",
      "Iteration 700, loss = 0.19629975\n",
      "Iteration 701, loss = 0.19609730\n",
      "Iteration 702, loss = 0.19589534\n",
      "Iteration 703, loss = 0.19569387\n",
      "Iteration 704, loss = 0.19549289\n",
      "Iteration 705, loss = 0.19529238\n",
      "Iteration 706, loss = 0.19509237\n",
      "Iteration 707, loss = 0.19489283\n",
      "Iteration 708, loss = 0.19469377\n",
      "Iteration 709, loss = 0.19449519\n",
      "Iteration 710, loss = 0.19429709\n",
      "Iteration 711, loss = 0.19409946\n",
      "Iteration 712, loss = 0.19390230\n",
      "Iteration 713, loss = 0.19370561\n",
      "Iteration 714, loss = 0.19350940\n",
      "Iteration 715, loss = 0.19331364\n",
      "Iteration 716, loss = 0.19311836\n",
      "Iteration 717, loss = 0.19292354\n",
      "Iteration 718, loss = 0.19272918\n",
      "Iteration 719, loss = 0.19253528\n",
      "Iteration 720, loss = 0.19234184\n",
      "Iteration 721, loss = 0.19214885\n",
      "Iteration 722, loss = 0.19195633\n",
      "Iteration 723, loss = 0.19176425\n",
      "Iteration 724, loss = 0.19157263\n",
      "Iteration 725, loss = 0.19138146\n",
      "Iteration 726, loss = 0.19119073\n",
      "Iteration 727, loss = 0.19100046\n",
      "Iteration 728, loss = 0.19081063\n",
      "Iteration 729, loss = 0.19062124\n",
      "Iteration 730, loss = 0.19043230\n",
      "Iteration 731, loss = 0.19024380\n",
      "Iteration 732, loss = 0.19005573\n",
      "Iteration 733, loss = 0.18986811\n",
      "Iteration 734, loss = 0.18968092\n",
      "Iteration 735, loss = 0.18949416\n",
      "Iteration 736, loss = 0.18930784\n",
      "Iteration 737, loss = 0.18912195\n",
      "Iteration 738, loss = 0.18893648\n",
      "Iteration 739, loss = 0.18875145\n",
      "Iteration 740, loss = 0.18856684\n",
      "Iteration 741, loss = 0.18838266\n",
      "Iteration 742, loss = 0.18819891\n",
      "Iteration 743, loss = 0.18801557\n",
      "Iteration 744, loss = 0.18783266\n",
      "Iteration 745, loss = 0.18765016\n",
      "Iteration 746, loss = 0.18746808\n",
      "Iteration 747, loss = 0.18728642\n",
      "Iteration 748, loss = 0.18710518\n",
      "Iteration 749, loss = 0.18692435\n",
      "Iteration 750, loss = 0.18674392\n",
      "Iteration 751, loss = 0.18656391\n",
      "Iteration 752, loss = 0.18638431\n",
      "Iteration 753, loss = 0.18620512\n",
      "Iteration 754, loss = 0.18602633\n",
      "Iteration 755, loss = 0.18584795\n",
      "Iteration 756, loss = 0.18566997\n",
      "Iteration 757, loss = 0.18549240\n",
      "Iteration 758, loss = 0.18531522\n",
      "Iteration 759, loss = 0.18513844\n",
      "Iteration 760, loss = 0.18496206\n",
      "Iteration 761, loss = 0.18478608\n",
      "Iteration 762, loss = 0.18461049\n",
      "Iteration 763, loss = 0.18443530\n",
      "Iteration 764, loss = 0.18426050\n",
      "Iteration 765, loss = 0.18408608\n",
      "Iteration 766, loss = 0.18391206\n",
      "Iteration 767, loss = 0.18373843\n",
      "Iteration 768, loss = 0.18356518\n",
      "Iteration 769, loss = 0.18339232\n",
      "Iteration 770, loss = 0.18321985\n",
      "Iteration 771, loss = 0.18304775\n",
      "Iteration 772, loss = 0.18287604\n",
      "Iteration 773, loss = 0.18270471\n",
      "Iteration 774, loss = 0.18253376\n",
      "Iteration 775, loss = 0.18236318\n",
      "Iteration 776, loss = 0.18219299\n",
      "Iteration 777, loss = 0.18202316\n",
      "Iteration 778, loss = 0.18185371\n",
      "Iteration 779, loss = 0.18168464\n",
      "Iteration 780, loss = 0.18151593\n",
      "Iteration 781, loss = 0.18134760\n",
      "Iteration 782, loss = 0.18117963\n",
      "Iteration 783, loss = 0.18101203\n",
      "Iteration 784, loss = 0.18084480\n",
      "Iteration 785, loss = 0.18067793\n",
      "Iteration 786, loss = 0.18051143\n",
      "Iteration 787, loss = 0.18034529\n",
      "Iteration 788, loss = 0.18017951\n",
      "Iteration 789, loss = 0.18001409\n",
      "Iteration 790, loss = 0.17984903\n",
      "Iteration 791, loss = 0.17968433\n",
      "Iteration 792, loss = 0.17951998\n",
      "Iteration 793, loss = 0.17935599\n",
      "Iteration 794, loss = 0.17919236\n",
      "Iteration 795, loss = 0.17902908\n",
      "Iteration 796, loss = 0.17886615\n",
      "Iteration 797, loss = 0.17870357\n",
      "Iteration 798, loss = 0.17854133\n",
      "Iteration 799, loss = 0.17837945\n",
      "Iteration 800, loss = 0.17821792\n",
      "Iteration 801, loss = 0.17805673\n",
      "Iteration 802, loss = 0.17789589\n",
      "Iteration 803, loss = 0.17773539\n",
      "Iteration 804, loss = 0.17757523\n",
      "Iteration 805, loss = 0.17741541\n",
      "Iteration 806, loss = 0.17725594\n",
      "Iteration 807, loss = 0.17709680\n",
      "Iteration 808, loss = 0.17693800\n",
      "Iteration 809, loss = 0.17677954\n",
      "Iteration 810, loss = 0.17662142\n",
      "Iteration 811, loss = 0.17646363\n",
      "Iteration 812, loss = 0.17630617\n",
      "Iteration 813, loss = 0.17614905\n",
      "Iteration 814, loss = 0.17599226\n",
      "Iteration 815, loss = 0.17583579\n",
      "Iteration 816, loss = 0.17567966\n",
      "Iteration 817, loss = 0.17552386\n",
      "Iteration 818, loss = 0.17536838\n",
      "Iteration 819, loss = 0.17521323\n",
      "Iteration 820, loss = 0.17505841\n",
      "Iteration 821, loss = 0.17490391\n",
      "Iteration 822, loss = 0.17474973\n",
      "Iteration 823, loss = 0.17459587\n",
      "Iteration 824, loss = 0.17444234\n",
      "Iteration 825, loss = 0.17428912\n",
      "Iteration 826, loss = 0.17413623\n",
      "Iteration 827, loss = 0.17398365\n",
      "Iteration 828, loss = 0.17383139\n",
      "Iteration 829, loss = 0.17367944\n",
      "Iteration 830, loss = 0.17352781\n",
      "Iteration 831, loss = 0.17337650\n",
      "Iteration 832, loss = 0.17322549\n",
      "Iteration 833, loss = 0.17307480\n",
      "Iteration 834, loss = 0.17292442\n",
      "Iteration 835, loss = 0.17277435\n",
      "Iteration 836, loss = 0.17262459\n",
      "Iteration 837, loss = 0.17247514\n",
      "Iteration 838, loss = 0.17232599\n",
      "Iteration 839, loss = 0.17217715\n",
      "Iteration 840, loss = 0.17202861\n",
      "Iteration 841, loss = 0.17188038\n",
      "Iteration 842, loss = 0.17173245\n",
      "Iteration 843, loss = 0.17158482\n",
      "Iteration 844, loss = 0.17143750\n",
      "Iteration 845, loss = 0.17129047\n",
      "Iteration 846, loss = 0.17114375\n",
      "Iteration 847, loss = 0.17099732\n",
      "Iteration 848, loss = 0.17085119\n",
      "Iteration 849, loss = 0.17070535\n",
      "Iteration 850, loss = 0.17055981\n",
      "Iteration 851, loss = 0.17041457\n",
      "Iteration 852, loss = 0.17026962\n",
      "Iteration 853, loss = 0.17012496\n",
      "Iteration 854, loss = 0.16998060\n",
      "Iteration 855, loss = 0.16983652\n",
      "Iteration 856, loss = 0.16969274\n",
      "Iteration 857, loss = 0.16954924\n",
      "Iteration 858, loss = 0.16940604\n",
      "Iteration 859, loss = 0.16926312\n",
      "Iteration 860, loss = 0.16912048\n",
      "Iteration 861, loss = 0.16897814\n",
      "Iteration 862, loss = 0.16883607\n",
      "Iteration 863, loss = 0.16869429\n",
      "Iteration 864, loss = 0.16855280\n",
      "Iteration 865, loss = 0.16841158\n",
      "Iteration 866, loss = 0.16827065\n",
      "Iteration 867, loss = 0.16813000\n",
      "Iteration 868, loss = 0.16798962\n",
      "Iteration 869, loss = 0.16784953\n",
      "Iteration 870, loss = 0.16770971\n",
      "Iteration 871, loss = 0.16757017\n",
      "Iteration 872, loss = 0.16743091\n",
      "Iteration 873, loss = 0.16729192\n",
      "Iteration 874, loss = 0.16715321\n",
      "Iteration 875, loss = 0.16701477\n",
      "Iteration 876, loss = 0.16687660\n",
      "Iteration 877, loss = 0.16673870\n",
      "Iteration 878, loss = 0.16660108\n",
      "Iteration 879, loss = 0.16646372\n",
      "Iteration 880, loss = 0.16632664\n",
      "Iteration 881, loss = 0.16618982\n",
      "Iteration 882, loss = 0.16605327\n",
      "Iteration 883, loss = 0.16591699\n",
      "Iteration 884, loss = 0.16578098\n",
      "Iteration 885, loss = 0.16564523\n",
      "Iteration 886, loss = 0.16550974\n",
      "Iteration 887, loss = 0.16537452\n",
      "Iteration 888, loss = 0.16523956\n",
      "Iteration 889, loss = 0.16510487\n",
      "Iteration 890, loss = 0.16497043\n",
      "Iteration 891, loss = 0.16483626\n",
      "Iteration 892, loss = 0.16470234\n",
      "Iteration 893, loss = 0.16456869\n",
      "Iteration 894, loss = 0.16443529\n",
      "Iteration 895, loss = 0.16430215\n",
      "Iteration 896, loss = 0.16416927\n",
      "Iteration 897, loss = 0.16403665\n",
      "Iteration 898, loss = 0.16390428\n",
      "Iteration 899, loss = 0.16377216\n",
      "Iteration 900, loss = 0.16364030\n",
      "Iteration 901, loss = 0.16350869\n",
      "Iteration 902, loss = 0.16337733\n",
      "Iteration 903, loss = 0.16324622\n",
      "Iteration 904, loss = 0.16311537\n",
      "Iteration 905, loss = 0.16298477\n",
      "Iteration 906, loss = 0.16285441\n",
      "Iteration 907, loss = 0.16272430\n",
      "Iteration 908, loss = 0.16259444\n",
      "Iteration 909, loss = 0.16246483\n",
      "Iteration 910, loss = 0.16233547\n",
      "Iteration 911, loss = 0.16220635\n",
      "Iteration 912, loss = 0.16207747\n",
      "Iteration 913, loss = 0.16194884\n",
      "Iteration 914, loss = 0.16182046\n",
      "Iteration 915, loss = 0.16169231\n",
      "Iteration 916, loss = 0.16156441\n",
      "Iteration 917, loss = 0.16143675\n",
      "Iteration 918, loss = 0.16130933\n",
      "Iteration 919, loss = 0.16118215\n",
      "Iteration 920, loss = 0.16105521\n",
      "Iteration 921, loss = 0.16092851\n",
      "Iteration 922, loss = 0.16080204\n",
      "Iteration 923, loss = 0.16067582\n",
      "Iteration 924, loss = 0.16054983\n",
      "Iteration 925, loss = 0.16042407\n",
      "Iteration 926, loss = 0.16029856\n",
      "Iteration 927, loss = 0.16017327\n",
      "Iteration 928, loss = 0.16004822\n",
      "Iteration 929, loss = 0.15992340\n",
      "Iteration 930, loss = 0.15979882\n",
      "Iteration 931, loss = 0.15967447\n",
      "Iteration 932, loss = 0.15955035\n",
      "Iteration 933, loss = 0.15942646\n",
      "Iteration 934, loss = 0.15930279\n",
      "Iteration 935, loss = 0.15917936\n",
      "Iteration 936, loss = 0.15905616\n",
      "Iteration 937, loss = 0.15893318\n",
      "Iteration 938, loss = 0.15881044\n",
      "Iteration 939, loss = 0.15868792\n",
      "Iteration 940, loss = 0.15856562\n",
      "Iteration 941, loss = 0.15844355\n",
      "Iteration 942, loss = 0.15832171\n",
      "Iteration 943, loss = 0.15820008\n",
      "Iteration 944, loss = 0.15807869\n",
      "Iteration 945, loss = 0.15795751\n",
      "Iteration 946, loss = 0.15783656\n",
      "Iteration 947, loss = 0.15771583\n",
      "Iteration 948, loss = 0.15759532\n",
      "Iteration 949, loss = 0.15747503\n",
      "Iteration 950, loss = 0.15735496\n",
      "Iteration 951, loss = 0.15723511\n",
      "Iteration 952, loss = 0.15711548\n",
      "Iteration 953, loss = 0.15699606\n",
      "Iteration 954, loss = 0.15687686\n",
      "Iteration 955, loss = 0.15675788\n",
      "Iteration 956, loss = 0.15663912\n",
      "Iteration 957, loss = 0.15652057\n",
      "Iteration 958, loss = 0.15640224\n",
      "Iteration 959, loss = 0.15628412\n",
      "Iteration 960, loss = 0.15616621\n",
      "Iteration 961, loss = 0.15604852\n",
      "Iteration 962, loss = 0.15593104\n",
      "Iteration 963, loss = 0.15581377\n",
      "Iteration 964, loss = 0.15569671\n",
      "Iteration 965, loss = 0.15557986\n",
      "Iteration 966, loss = 0.15546323\n",
      "Iteration 967, loss = 0.15534680\n",
      "Iteration 968, loss = 0.15523058\n",
      "Iteration 969, loss = 0.15511457\n",
      "Iteration 970, loss = 0.15499877\n",
      "Iteration 971, loss = 0.15488318\n",
      "Iteration 972, loss = 0.15476779\n",
      "Iteration 973, loss = 0.15465261\n",
      "Iteration 974, loss = 0.15453763\n",
      "Iteration 975, loss = 0.15442286\n",
      "Iteration 976, loss = 0.15430829\n",
      "Iteration 977, loss = 0.15419393\n",
      "Iteration 978, loss = 0.15407977\n",
      "Iteration 979, loss = 0.15396581\n",
      "Iteration 980, loss = 0.15385206\n",
      "Iteration 981, loss = 0.15373851\n",
      "Iteration 982, loss = 0.15362516\n",
      "Iteration 983, loss = 0.15351201\n",
      "Iteration 984, loss = 0.15339905\n",
      "Iteration 985, loss = 0.15328630\n",
      "Iteration 986, loss = 0.15317375\n",
      "Iteration 987, loss = 0.15306140\n",
      "Iteration 988, loss = 0.15294924\n",
      "Iteration 989, loss = 0.15283728\n",
      "Iteration 990, loss = 0.15272552\n",
      "Iteration 991, loss = 0.15261395\n",
      "Iteration 992, loss = 0.15250258\n",
      "Iteration 993, loss = 0.15239141\n",
      "Iteration 994, loss = 0.15228043\n",
      "Iteration 995, loss = 0.15216964\n",
      "Iteration 996, loss = 0.15205905\n",
      "Iteration 997, loss = 0.15194865\n",
      "Iteration 998, loss = 0.15183845\n",
      "Iteration 999, loss = 0.15172843\n",
      "Iteration 1000, loss = 0.15161861\n",
      "Iteration 1, loss = 2.15205734\n",
      "Iteration 2, loss = 1.58395159\n",
      "Iteration 3, loss = 1.20794470\n",
      "Iteration 4, loss = 1.21375035\n",
      "Iteration 5, loss = 1.22788367\n",
      "Iteration 6, loss = 1.22031904\n",
      "Iteration 7, loss = 1.20858616\n",
      "Iteration 8, loss = 1.19649715\n",
      "Iteration 9, loss = 1.18951171\n",
      "Iteration 10, loss = 1.18546661\n",
      "Iteration 11, loss = 1.18218296\n",
      "Iteration 12, loss = 1.17806664\n",
      "Iteration 13, loss = 1.17296322\n",
      "Iteration 14, loss = 1.16694031\n",
      "Iteration 15, loss = 1.16064695\n",
      "Iteration 16, loss = 1.15431786\n",
      "Iteration 17, loss = 1.14825596\n",
      "Iteration 18, loss = 1.14254936\n",
      "Iteration 19, loss = 1.13723299\n",
      "Iteration 20, loss = 1.13164362\n",
      "Iteration 21, loss = 1.12579539\n",
      "Iteration 22, loss = 1.11984496\n",
      "Iteration 23, loss = 1.11388896\n",
      "Iteration 24, loss = 1.10796487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 1.10205493\n",
      "Iteration 26, loss = 1.09612653\n",
      "Iteration 27, loss = 1.09017261\n",
      "Iteration 28, loss = 1.08416180\n",
      "Iteration 29, loss = 1.07804356\n",
      "Iteration 30, loss = 1.07180705\n",
      "Iteration 31, loss = 1.06537857\n",
      "Iteration 32, loss = 1.05873275\n",
      "Iteration 33, loss = 1.05194560\n",
      "Iteration 34, loss = 1.04503624\n",
      "Iteration 35, loss = 1.03797391\n",
      "Iteration 36, loss = 1.03079265\n",
      "Iteration 37, loss = 1.02353115\n",
      "Iteration 38, loss = 1.01622613\n",
      "Iteration 39, loss = 1.00890639\n",
      "Iteration 40, loss = 1.00158931\n",
      "Iteration 41, loss = 0.99428107\n",
      "Iteration 42, loss = 0.98698073\n",
      "Iteration 43, loss = 0.97968645\n",
      "Iteration 44, loss = 0.97240112\n",
      "Iteration 45, loss = 0.96513514\n",
      "Iteration 46, loss = 0.95790505\n",
      "Iteration 47, loss = 0.95072907\n",
      "Iteration 48, loss = 0.94362185\n",
      "Iteration 49, loss = 0.93659083\n",
      "Iteration 50, loss = 0.92963564\n",
      "Iteration 51, loss = 0.92275022\n",
      "Iteration 52, loss = 0.91592627\n",
      "Iteration 53, loss = 0.90915621\n",
      "Iteration 54, loss = 0.90243453\n",
      "Iteration 55, loss = 0.89575762\n",
      "Iteration 56, loss = 0.88912266\n",
      "Iteration 57, loss = 0.88252663\n",
      "Iteration 58, loss = 0.87596597\n",
      "Iteration 59, loss = 0.86943683\n",
      "Iteration 60, loss = 0.86293566\n",
      "Iteration 61, loss = 0.85645968\n",
      "Iteration 62, loss = 0.85000698\n",
      "Iteration 63, loss = 0.84357634\n",
      "Iteration 64, loss = 0.83716696\n",
      "Iteration 65, loss = 0.83077823\n",
      "Iteration 66, loss = 0.82440964\n",
      "Iteration 67, loss = 0.81806082\n",
      "Iteration 68, loss = 0.81173162\n",
      "Iteration 69, loss = 0.80542221\n",
      "Iteration 70, loss = 0.79913304\n",
      "Iteration 71, loss = 0.79286490\n",
      "Iteration 72, loss = 0.78661883\n",
      "Iteration 73, loss = 0.78039613\n",
      "Iteration 74, loss = 0.77419837\n",
      "Iteration 75, loss = 0.76802736\n",
      "Iteration 76, loss = 0.76188517\n",
      "Iteration 77, loss = 0.75577413\n",
      "Iteration 78, loss = 0.74969683\n",
      "Iteration 79, loss = 0.74365609\n",
      "Iteration 80, loss = 0.73765499\n",
      "Iteration 81, loss = 0.73169679\n",
      "Iteration 82, loss = 0.72578498\n",
      "Iteration 83, loss = 0.71992322\n",
      "Iteration 84, loss = 0.71411533\n",
      "Iteration 85, loss = 0.70836526\n",
      "Iteration 86, loss = 0.70267709\n",
      "Iteration 87, loss = 0.69705491\n",
      "Iteration 88, loss = 0.69150287\n",
      "Iteration 89, loss = 0.68602506\n",
      "Iteration 90, loss = 0.68062548\n",
      "Iteration 91, loss = 0.67530800\n",
      "Iteration 92, loss = 0.67007626\n",
      "Iteration 93, loss = 0.66493363\n",
      "Iteration 94, loss = 0.65988317\n",
      "Iteration 95, loss = 0.65492755\n",
      "Iteration 96, loss = 0.65006898\n",
      "Iteration 97, loss = 0.64530925\n",
      "Iteration 98, loss = 0.64064958\n",
      "Iteration 99, loss = 0.63609072\n",
      "Iteration 100, loss = 0.63163285\n",
      "Iteration 101, loss = 0.62727564\n",
      "Iteration 102, loss = 0.62301827\n",
      "Iteration 103, loss = 0.61885942\n",
      "Iteration 104, loss = 0.61479739\n",
      "Iteration 105, loss = 0.61083007\n",
      "Iteration 106, loss = 0.60695508\n",
      "Iteration 107, loss = 0.60316978\n",
      "Iteration 108, loss = 0.59947139\n",
      "Iteration 109, loss = 0.59585699\n",
      "Iteration 110, loss = 0.59232365\n",
      "Iteration 111, loss = 0.58886843\n",
      "Iteration 112, loss = 0.58548844\n",
      "Iteration 113, loss = 0.58218089\n",
      "Iteration 114, loss = 0.57894310\n",
      "Iteration 115, loss = 0.57577254\n",
      "Iteration 116, loss = 0.57266680\n",
      "Iteration 117, loss = 0.56962364\n",
      "Iteration 118, loss = 0.56664094\n",
      "Iteration 119, loss = 0.56371677\n",
      "Iteration 120, loss = 0.56084929\n",
      "Iteration 121, loss = 0.55803681\n",
      "Iteration 122, loss = 0.55527777\n",
      "Iteration 123, loss = 0.55257069\n",
      "Iteration 124, loss = 0.54991419\n",
      "Iteration 125, loss = 0.54730701\n",
      "Iteration 126, loss = 0.54474791\n",
      "Iteration 127, loss = 0.54223575\n",
      "Iteration 128, loss = 0.53976944\n",
      "Iteration 129, loss = 0.53734793\n",
      "Iteration 130, loss = 0.53497020\n",
      "Iteration 131, loss = 0.53263526\n",
      "Iteration 132, loss = 0.53034215\n",
      "Iteration 133, loss = 0.52808993\n",
      "Iteration 134, loss = 0.52587768\n",
      "Iteration 135, loss = 0.52370448\n",
      "Iteration 136, loss = 0.52156942\n",
      "Iteration 137, loss = 0.51947161\n",
      "Iteration 138, loss = 0.51741016\n",
      "Iteration 139, loss = 0.51538419\n",
      "Iteration 140, loss = 0.51339283\n",
      "Iteration 141, loss = 0.51143521\n",
      "Iteration 142, loss = 0.50951048\n",
      "Iteration 143, loss = 0.50761778\n",
      "Iteration 144, loss = 0.50575629\n",
      "Iteration 145, loss = 0.50392516\n",
      "Iteration 146, loss = 0.50212360\n",
      "Iteration 147, loss = 0.50035078\n",
      "Iteration 148, loss = 0.49860594\n",
      "Iteration 149, loss = 0.49688829\n",
      "Iteration 150, loss = 0.49519707\n",
      "Iteration 151, loss = 0.49353154\n",
      "Iteration 152, loss = 0.49189098\n",
      "Iteration 153, loss = 0.49027468\n",
      "Iteration 154, loss = 0.48868195\n",
      "Iteration 155, loss = 0.48711211\n",
      "Iteration 156, loss = 0.48556451\n",
      "Iteration 157, loss = 0.48403851\n",
      "Iteration 158, loss = 0.48253350\n",
      "Iteration 159, loss = 0.48104888\n",
      "Iteration 160, loss = 0.47958407\n",
      "Iteration 161, loss = 0.47813850\n",
      "Iteration 162, loss = 0.47671162\n",
      "Iteration 163, loss = 0.47530292\n",
      "Iteration 164, loss = 0.47391187\n",
      "Iteration 165, loss = 0.47253798\n",
      "Iteration 166, loss = 0.47118077\n",
      "Iteration 167, loss = 0.46983977\n",
      "Iteration 168, loss = 0.46851455\n",
      "Iteration 169, loss = 0.46720465\n",
      "Iteration 170, loss = 0.46590967\n",
      "Iteration 171, loss = 0.46462918\n",
      "Iteration 172, loss = 0.46336281\n",
      "Iteration 173, loss = 0.46211016\n",
      "Iteration 174, loss = 0.46087087\n",
      "Iteration 175, loss = 0.45964458\n",
      "Iteration 176, loss = 0.45843093\n",
      "Iteration 177, loss = 0.45722961\n",
      "Iteration 178, loss = 0.45604027\n",
      "Iteration 179, loss = 0.45486261\n",
      "Iteration 180, loss = 0.45369632\n",
      "Iteration 181, loss = 0.45254110\n",
      "Iteration 182, loss = 0.45139667\n",
      "Iteration 183, loss = 0.45026274\n",
      "Iteration 184, loss = 0.44913906\n",
      "Iteration 185, loss = 0.44802535\n",
      "Iteration 186, loss = 0.44692137\n",
      "Iteration 187, loss = 0.44582686\n",
      "Iteration 188, loss = 0.44474160\n",
      "Iteration 189, loss = 0.44366535\n",
      "Iteration 190, loss = 0.44259789\n",
      "Iteration 191, loss = 0.44153900\n",
      "Iteration 192, loss = 0.44048847\n",
      "Iteration 193, loss = 0.43944610\n",
      "Iteration 194, loss = 0.43841169\n",
      "Iteration 195, loss = 0.43738505\n",
      "Iteration 196, loss = 0.43636600\n",
      "Iteration 197, loss = 0.43535436\n",
      "Iteration 198, loss = 0.43434995\n",
      "Iteration 199, loss = 0.43335261\n",
      "Iteration 200, loss = 0.43236216\n",
      "Iteration 201, loss = 0.43137847\n",
      "Iteration 202, loss = 0.43040136\n",
      "Iteration 203, loss = 0.42943069\n",
      "Iteration 204, loss = 0.42846632\n",
      "Iteration 205, loss = 0.42750812\n",
      "Iteration 206, loss = 0.42655593\n",
      "Iteration 207, loss = 0.42560964\n",
      "Iteration 208, loss = 0.42466912\n",
      "Iteration 209, loss = 0.42373423\n",
      "Iteration 210, loss = 0.42280488\n",
      "Iteration 211, loss = 0.42188093\n",
      "Iteration 212, loss = 0.42096228\n",
      "Iteration 213, loss = 0.42004881\n",
      "Iteration 214, loss = 0.41914043\n",
      "Iteration 215, loss = 0.41823704\n",
      "Iteration 216, loss = 0.41733852\n",
      "Iteration 217, loss = 0.41644479\n",
      "Iteration 218, loss = 0.41555576\n",
      "Iteration 219, loss = 0.41467133\n",
      "Iteration 220, loss = 0.41379142\n",
      "Iteration 221, loss = 0.41291594\n",
      "Iteration 222, loss = 0.41204481\n",
      "Iteration 223, loss = 0.41117795\n",
      "Iteration 224, loss = 0.41031529\n",
      "Iteration 225, loss = 0.40945675\n",
      "Iteration 226, loss = 0.40860226\n",
      "Iteration 227, loss = 0.40775175\n",
      "Iteration 228, loss = 0.40690515\n",
      "Iteration 229, loss = 0.40606240\n",
      "Iteration 230, loss = 0.40522342\n",
      "Iteration 231, loss = 0.40438817\n",
      "Iteration 232, loss = 0.40355658\n",
      "Iteration 233, loss = 0.40272859\n",
      "Iteration 234, loss = 0.40190415\n",
      "Iteration 235, loss = 0.40108321\n",
      "Iteration 236, loss = 0.40026570\n",
      "Iteration 237, loss = 0.39945159\n",
      "Iteration 238, loss = 0.39864082\n",
      "Iteration 239, loss = 0.39783333\n",
      "Iteration 240, loss = 0.39702910\n",
      "Iteration 241, loss = 0.39622807\n",
      "Iteration 242, loss = 0.39543020\n",
      "Iteration 243, loss = 0.39463545\n",
      "Iteration 244, loss = 0.39384377\n",
      "Iteration 245, loss = 0.39305513\n",
      "Iteration 246, loss = 0.39226950\n",
      "Iteration 247, loss = 0.39148682\n",
      "Iteration 248, loss = 0.39070708\n",
      "Iteration 249, loss = 0.38993023\n",
      "Iteration 250, loss = 0.38915623\n",
      "Iteration 251, loss = 0.38838507\n",
      "Iteration 252, loss = 0.38761670\n",
      "Iteration 253, loss = 0.38685110\n",
      "Iteration 254, loss = 0.38608824\n",
      "Iteration 255, loss = 0.38532809\n",
      "Iteration 256, loss = 0.38457062\n",
      "Iteration 257, loss = 0.38381580\n",
      "Iteration 258, loss = 0.38306362\n",
      "Iteration 259, loss = 0.38231404\n",
      "Iteration 260, loss = 0.38156705\n",
      "Iteration 261, loss = 0.38082261\n",
      "Iteration 262, loss = 0.38008071\n",
      "Iteration 263, loss = 0.37934133\n",
      "Iteration 264, loss = 0.37860444\n",
      "Iteration 265, loss = 0.37787002\n",
      "Iteration 266, loss = 0.37713806\n",
      "Iteration 267, loss = 0.37640853\n",
      "Iteration 268, loss = 0.37568143\n",
      "Iteration 269, loss = 0.37495672\n",
      "Iteration 270, loss = 0.37423439\n",
      "Iteration 271, loss = 0.37351443\n",
      "Iteration 272, loss = 0.37279683\n",
      "Iteration 273, loss = 0.37208155\n",
      "Iteration 274, loss = 0.37136860\n",
      "Iteration 275, loss = 0.37065795\n",
      "Iteration 276, loss = 0.36994960\n",
      "Iteration 277, loss = 0.36924352\n",
      "Iteration 278, loss = 0.36853971\n",
      "Iteration 279, loss = 0.36783815\n",
      "Iteration 280, loss = 0.36713884\n",
      "Iteration 281, loss = 0.36644175\n",
      "Iteration 282, loss = 0.36574688\n",
      "Iteration 283, loss = 0.36505421\n",
      "Iteration 284, loss = 0.36436374\n",
      "Iteration 285, loss = 0.36367546\n",
      "Iteration 286, loss = 0.36298935\n",
      "Iteration 287, loss = 0.36230541\n",
      "Iteration 288, loss = 0.36162363\n",
      "Iteration 289, loss = 0.36094399\n",
      "Iteration 290, loss = 0.36026649\n",
      "Iteration 291, loss = 0.35959112\n",
      "Iteration 292, loss = 0.35891788\n",
      "Iteration 293, loss = 0.35824674\n",
      "Iteration 294, loss = 0.35757772\n",
      "Iteration 295, loss = 0.35691079\n",
      "Iteration 296, loss = 0.35624595\n",
      "Iteration 297, loss = 0.35558320\n",
      "Iteration 298, loss = 0.35492252\n",
      "Iteration 299, loss = 0.35426391\n",
      "Iteration 300, loss = 0.35360737\n",
      "Iteration 301, loss = 0.35295289\n",
      "Iteration 302, loss = 0.35230046\n",
      "Iteration 303, loss = 0.35165007\n",
      "Iteration 304, loss = 0.35100172\n",
      "Iteration 305, loss = 0.35035541\n",
      "Iteration 306, loss = 0.34971112\n",
      "Iteration 307, loss = 0.34906886\n",
      "Iteration 308, loss = 0.34842861\n",
      "Iteration 309, loss = 0.34779038\n",
      "Iteration 310, loss = 0.34715415\n",
      "Iteration 311, loss = 0.34651993\n",
      "Iteration 312, loss = 0.34588770\n",
      "Iteration 313, loss = 0.34525747\n",
      "Iteration 314, loss = 0.34462922\n",
      "Iteration 315, loss = 0.34400296\n",
      "Iteration 316, loss = 0.34337867\n",
      "Iteration 317, loss = 0.34275636\n",
      "Iteration 318, loss = 0.34213602\n",
      "Iteration 319, loss = 0.34151765\n",
      "Iteration 320, loss = 0.34090124\n",
      "Iteration 321, loss = 0.34028678\n",
      "Iteration 322, loss = 0.33967428\n",
      "Iteration 323, loss = 0.33906373\n",
      "Iteration 324, loss = 0.33845512\n",
      "Iteration 325, loss = 0.33784845\n",
      "Iteration 326, loss = 0.33724372\n",
      "Iteration 327, loss = 0.33664093\n",
      "Iteration 328, loss = 0.33604006\n",
      "Iteration 329, loss = 0.33544112\n",
      "Iteration 330, loss = 0.33484410\n",
      "Iteration 331, loss = 0.33424900\n",
      "Iteration 332, loss = 0.33365581\n",
      "Iteration 333, loss = 0.33306453\n",
      "Iteration 334, loss = 0.33247516\n",
      "Iteration 335, loss = 0.33188770\n",
      "Iteration 336, loss = 0.33130213\n",
      "Iteration 337, loss = 0.33071846\n",
      "Iteration 338, loss = 0.33013667\n",
      "Iteration 339, loss = 0.32955678\n",
      "Iteration 340, loss = 0.32897877\n",
      "Iteration 341, loss = 0.32840265\n",
      "Iteration 342, loss = 0.32782840\n",
      "Iteration 343, loss = 0.32725602\n",
      "Iteration 344, loss = 0.32668551\n",
      "Iteration 345, loss = 0.32611687\n",
      "Iteration 346, loss = 0.32555009\n",
      "Iteration 347, loss = 0.32498517\n",
      "Iteration 348, loss = 0.32442211\n",
      "Iteration 349, loss = 0.32386090\n",
      "Iteration 350, loss = 0.32330153\n",
      "Iteration 351, loss = 0.32274401\n",
      "Iteration 352, loss = 0.32218832\n",
      "Iteration 353, loss = 0.32163447\n",
      "Iteration 354, loss = 0.32108246\n",
      "Iteration 355, loss = 0.32053227\n",
      "Iteration 356, loss = 0.31998391\n",
      "Iteration 357, loss = 0.31943737\n",
      "Iteration 358, loss = 0.31889264\n",
      "Iteration 359, loss = 0.31834973\n",
      "Iteration 360, loss = 0.31780863\n",
      "Iteration 361, loss = 0.31726933\n",
      "Iteration 362, loss = 0.31673183\n",
      "Iteration 363, loss = 0.31619613\n",
      "Iteration 364, loss = 0.31566223\n",
      "Iteration 365, loss = 0.31513011\n",
      "Iteration 366, loss = 0.31459977\n",
      "Iteration 367, loss = 0.31407122\n",
      "Iteration 368, loss = 0.31354444\n",
      "Iteration 369, loss = 0.31301944\n",
      "Iteration 370, loss = 0.31249620\n",
      "Iteration 371, loss = 0.31197473\n",
      "Iteration 372, loss = 0.31145502\n",
      "Iteration 373, loss = 0.31093706\n",
      "Iteration 374, loss = 0.31042086\n",
      "Iteration 375, loss = 0.30990640\n",
      "Iteration 376, loss = 0.30939369\n",
      "Iteration 377, loss = 0.30888271\n",
      "Iteration 378, loss = 0.30837347\n",
      "Iteration 379, loss = 0.30786595\n",
      "Iteration 380, loss = 0.30736017\n",
      "Iteration 381, loss = 0.30685610\n",
      "Iteration 382, loss = 0.30635375\n",
      "Iteration 383, loss = 0.30585311\n",
      "Iteration 384, loss = 0.30535418\n",
      "Iteration 385, loss = 0.30485695\n",
      "Iteration 386, loss = 0.30436142\n",
      "Iteration 387, loss = 0.30386758\n",
      "Iteration 388, loss = 0.30337544\n",
      "Iteration 389, loss = 0.30288497\n",
      "Iteration 390, loss = 0.30239619\n",
      "Iteration 391, loss = 0.30190908\n",
      "Iteration 392, loss = 0.30142364\n",
      "Iteration 393, loss = 0.30093987\n",
      "Iteration 394, loss = 0.30045776\n",
      "Iteration 395, loss = 0.29997730\n",
      "Iteration 396, loss = 0.29949850\n",
      "Iteration 397, loss = 0.29902134\n",
      "Iteration 398, loss = 0.29854582\n",
      "Iteration 399, loss = 0.29807194\n",
      "Iteration 400, loss = 0.29759969\n",
      "Iteration 401, loss = 0.29712907\n",
      "Iteration 402, loss = 0.29666008\n",
      "Iteration 403, loss = 0.29619269\n",
      "Iteration 404, loss = 0.29572693\n",
      "Iteration 405, loss = 0.29526276\n",
      "Iteration 406, loss = 0.29480021\n",
      "Iteration 407, loss = 0.29433924\n",
      "Iteration 408, loss = 0.29387988\n",
      "Iteration 409, loss = 0.29342209\n",
      "Iteration 410, loss = 0.29296590\n",
      "Iteration 411, loss = 0.29251127\n",
      "Iteration 412, loss = 0.29205822\n",
      "Iteration 413, loss = 0.29160674\n",
      "Iteration 414, loss = 0.29115682\n",
      "Iteration 415, loss = 0.29070846\n",
      "Iteration 416, loss = 0.29026165\n",
      "Iteration 417, loss = 0.28981638\n",
      "Iteration 418, loss = 0.28937266\n",
      "Iteration 419, loss = 0.28893047\n",
      "Iteration 420, loss = 0.28848982\n",
      "Iteration 421, loss = 0.28805069\n",
      "Iteration 422, loss = 0.28761308\n",
      "Iteration 423, loss = 0.28717699\n",
      "Iteration 424, loss = 0.28674241\n",
      "Iteration 425, loss = 0.28630934\n",
      "Iteration 426, loss = 0.28587776\n",
      "Iteration 427, loss = 0.28544768\n",
      "Iteration 428, loss = 0.28501909\n",
      "Iteration 429, loss = 0.28459198\n",
      "Iteration 430, loss = 0.28416636\n",
      "Iteration 431, loss = 0.28374221\n",
      "Iteration 432, loss = 0.28331952\n",
      "Iteration 433, loss = 0.28289831\n",
      "Iteration 434, loss = 0.28247854\n",
      "Iteration 435, loss = 0.28206024\n",
      "Iteration 436, loss = 0.28164338\n",
      "Iteration 437, loss = 0.28122796\n",
      "Iteration 438, loss = 0.28081398\n",
      "Iteration 439, loss = 0.28040143\n",
      "Iteration 440, loss = 0.27999031\n",
      "Iteration 441, loss = 0.27958062\n",
      "Iteration 442, loss = 0.27917233\n",
      "Iteration 443, loss = 0.27876546\n",
      "Iteration 444, loss = 0.27836000\n",
      "Iteration 445, loss = 0.27795593\n",
      "Iteration 446, loss = 0.27755327\n",
      "Iteration 447, loss = 0.27715199\n",
      "Iteration 448, loss = 0.27675209\n",
      "Iteration 449, loss = 0.27635358\n",
      "Iteration 450, loss = 0.27595644\n",
      "Iteration 451, loss = 0.27556067\n",
      "Iteration 452, loss = 0.27516626\n",
      "Iteration 453, loss = 0.27477321\n",
      "Iteration 454, loss = 0.27438152\n",
      "Iteration 455, loss = 0.27399117\n",
      "Iteration 456, loss = 0.27360217\n",
      "Iteration 457, loss = 0.27321451\n",
      "Iteration 458, loss = 0.27282817\n",
      "Iteration 459, loss = 0.27244317\n",
      "Iteration 460, loss = 0.27205948\n",
      "Iteration 461, loss = 0.27167712\n",
      "Iteration 462, loss = 0.27129606\n",
      "Iteration 463, loss = 0.27091632\n",
      "Iteration 464, loss = 0.27053787\n",
      "Iteration 465, loss = 0.27016072\n",
      "Iteration 466, loss = 0.26978486\n",
      "Iteration 467, loss = 0.26941029\n",
      "Iteration 468, loss = 0.26903700\n",
      "Iteration 469, loss = 0.26866498\n",
      "Iteration 470, loss = 0.26829423\n",
      "Iteration 471, loss = 0.26792475\n",
      "Iteration 472, loss = 0.26755653\n",
      "Iteration 473, loss = 0.26718957\n",
      "Iteration 474, loss = 0.26682385\n",
      "Iteration 475, loss = 0.26645938\n",
      "Iteration 476, loss = 0.26609615\n",
      "Iteration 477, loss = 0.26573416\n",
      "Iteration 478, loss = 0.26537339\n",
      "Iteration 479, loss = 0.26501385\n",
      "Iteration 480, loss = 0.26465553\n",
      "Iteration 481, loss = 0.26429843\n",
      "Iteration 482, loss = 0.26394253\n",
      "Iteration 483, loss = 0.26358784\n",
      "Iteration 484, loss = 0.26323435\n",
      "Iteration 485, loss = 0.26288205\n",
      "Iteration 486, loss = 0.26253095\n",
      "Iteration 487, loss = 0.26218103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 488, loss = 0.26183229\n",
      "Iteration 489, loss = 0.26148472\n",
      "Iteration 490, loss = 0.26113833\n",
      "Iteration 491, loss = 0.26079310\n",
      "Iteration 492, loss = 0.26044903\n",
      "Iteration 493, loss = 0.26010611\n",
      "Iteration 494, loss = 0.25976435\n",
      "Iteration 495, loss = 0.25942374\n",
      "Iteration 496, loss = 0.25908426\n",
      "Iteration 497, loss = 0.25874592\n",
      "Iteration 498, loss = 0.25840871\n",
      "Iteration 499, loss = 0.25807263\n",
      "Iteration 500, loss = 0.25773767\n",
      "Iteration 501, loss = 0.25740383\n",
      "Iteration 502, loss = 0.25707111\n",
      "Iteration 503, loss = 0.25673948\n",
      "Iteration 504, loss = 0.25640897\n",
      "Iteration 505, loss = 0.25607955\n",
      "Iteration 506, loss = 0.25575122\n",
      "Iteration 507, loss = 0.25542399\n",
      "Iteration 508, loss = 0.25509784\n",
      "Iteration 509, loss = 0.25477277\n",
      "Iteration 510, loss = 0.25444877\n",
      "Iteration 511, loss = 0.25412584\n",
      "Iteration 512, loss = 0.25380399\n",
      "Iteration 513, loss = 0.25348319\n",
      "Iteration 514, loss = 0.25316345\n",
      "Iteration 515, loss = 0.25284476\n",
      "Iteration 516, loss = 0.25252712\n",
      "Iteration 517, loss = 0.25221053\n",
      "Iteration 518, loss = 0.25189497\n",
      "Iteration 519, loss = 0.25158045\n",
      "Iteration 520, loss = 0.25126695\n",
      "Iteration 521, loss = 0.25095449\n",
      "Iteration 522, loss = 0.25064304\n",
      "Iteration 523, loss = 0.25033261\n",
      "Iteration 524, loss = 0.25002320\n",
      "Iteration 525, loss = 0.24971479\n",
      "Iteration 526, loss = 0.24940739\n",
      "Iteration 527, loss = 0.24910098\n",
      "Iteration 528, loss = 0.24879558\n",
      "Iteration 529, loss = 0.24849116\n",
      "Iteration 530, loss = 0.24818773\n",
      "Iteration 531, loss = 0.24788528\n",
      "Iteration 532, loss = 0.24758381\n",
      "Iteration 533, loss = 0.24728331\n",
      "Iteration 534, loss = 0.24698378\n",
      "Iteration 535, loss = 0.24668522\n",
      "Iteration 536, loss = 0.24638762\n",
      "Iteration 537, loss = 0.24609098\n",
      "Iteration 538, loss = 0.24579529\n",
      "Iteration 539, loss = 0.24550055\n",
      "Iteration 540, loss = 0.24520675\n",
      "Iteration 541, loss = 0.24491390\n",
      "Iteration 542, loss = 0.24462198\n",
      "Iteration 543, loss = 0.24433100\n",
      "Iteration 544, loss = 0.24404094\n",
      "Iteration 545, loss = 0.24375181\n",
      "Iteration 546, loss = 0.24346359\n",
      "Iteration 547, loss = 0.24317630\n",
      "Iteration 548, loss = 0.24288991\n",
      "Iteration 549, loss = 0.24260444\n",
      "Iteration 550, loss = 0.24231987\n",
      "Iteration 551, loss = 0.24203620\n",
      "Iteration 552, loss = 0.24175343\n",
      "Iteration 553, loss = 0.24147155\n",
      "Iteration 554, loss = 0.24119056\n",
      "Iteration 555, loss = 0.24091046\n",
      "Iteration 556, loss = 0.24063123\n",
      "Iteration 557, loss = 0.24035289\n",
      "Iteration 558, loss = 0.24007542\n",
      "Iteration 559, loss = 0.23979881\n",
      "Iteration 560, loss = 0.23952308\n",
      "Iteration 561, loss = 0.23924820\n",
      "Iteration 562, loss = 0.23897419\n",
      "Iteration 563, loss = 0.23870103\n",
      "Iteration 564, loss = 0.23842872\n",
      "Iteration 565, loss = 0.23815726\n",
      "Iteration 566, loss = 0.23788665\n",
      "Iteration 567, loss = 0.23761687\n",
      "Iteration 568, loss = 0.23734793\n",
      "Iteration 569, loss = 0.23707983\n",
      "Iteration 570, loss = 0.23681255\n",
      "Iteration 571, loss = 0.23654610\n",
      "Iteration 572, loss = 0.23628047\n",
      "Iteration 573, loss = 0.23601566\n",
      "Iteration 574, loss = 0.23575167\n",
      "Iteration 575, loss = 0.23548849\n",
      "Iteration 576, loss = 0.23522612\n",
      "Iteration 577, loss = 0.23496455\n",
      "Iteration 578, loss = 0.23470378\n",
      "Iteration 579, loss = 0.23444381\n",
      "Iteration 580, loss = 0.23418463\n",
      "Iteration 581, loss = 0.23392625\n",
      "Iteration 582, loss = 0.23366865\n",
      "Iteration 583, loss = 0.23341184\n",
      "Iteration 584, loss = 0.23315581\n",
      "Iteration 585, loss = 0.23290056\n",
      "Iteration 586, loss = 0.23264608\n",
      "Iteration 587, loss = 0.23239237\n",
      "Iteration 588, loss = 0.23213943\n",
      "Iteration 589, loss = 0.23188725\n",
      "Iteration 590, loss = 0.23163583\n",
      "Iteration 591, loss = 0.23138517\n",
      "Iteration 592, loss = 0.23113527\n",
      "Iteration 593, loss = 0.23088612\n",
      "Iteration 594, loss = 0.23063771\n",
      "Iteration 595, loss = 0.23039005\n",
      "Iteration 596, loss = 0.23014314\n",
      "Iteration 597, loss = 0.22989696\n",
      "Iteration 598, loss = 0.22965151\n",
      "Iteration 599, loss = 0.22940680\n",
      "Iteration 600, loss = 0.22916282\n",
      "Iteration 601, loss = 0.22891956\n",
      "Iteration 602, loss = 0.22867702\n",
      "Iteration 603, loss = 0.22843521\n",
      "Iteration 604, loss = 0.22819411\n",
      "Iteration 605, loss = 0.22795373\n",
      "Iteration 606, loss = 0.22771405\n",
      "Iteration 607, loss = 0.22747509\n",
      "Iteration 608, loss = 0.22723683\n",
      "Iteration 609, loss = 0.22699927\n",
      "Iteration 610, loss = 0.22676241\n",
      "Iteration 611, loss = 0.22652624\n",
      "Iteration 612, loss = 0.22629077\n",
      "Iteration 613, loss = 0.22605599\n",
      "Iteration 614, loss = 0.22582189\n",
      "Iteration 615, loss = 0.22558848\n",
      "Iteration 616, loss = 0.22535575\n",
      "Iteration 617, loss = 0.22512369\n",
      "Iteration 618, loss = 0.22489232\n",
      "Iteration 619, loss = 0.22466161\n",
      "Iteration 620, loss = 0.22443158\n",
      "Iteration 621, loss = 0.22420221\n",
      "Iteration 622, loss = 0.22397351\n",
      "Iteration 623, loss = 0.22374546\n",
      "Iteration 624, loss = 0.22351808\n",
      "Iteration 625, loss = 0.22329135\n",
      "Iteration 626, loss = 0.22306528\n",
      "Iteration 627, loss = 0.22283985\n",
      "Iteration 628, loss = 0.22261507\n",
      "Iteration 629, loss = 0.22239094\n",
      "Iteration 630, loss = 0.22216745\n",
      "Iteration 631, loss = 0.22194460\n",
      "Iteration 632, loss = 0.22172239\n",
      "Iteration 633, loss = 0.22150081\n",
      "Iteration 634, loss = 0.22127986\n",
      "Iteration 635, loss = 0.22105954\n",
      "Iteration 636, loss = 0.22083985\n",
      "Iteration 637, loss = 0.22062078\n",
      "Iteration 638, loss = 0.22040233\n",
      "Iteration 639, loss = 0.22018450\n",
      "Iteration 640, loss = 0.21996728\n",
      "Iteration 641, loss = 0.21975068\n",
      "Iteration 642, loss = 0.21953469\n",
      "Iteration 643, loss = 0.21931931\n",
      "Iteration 644, loss = 0.21910454\n",
      "Iteration 645, loss = 0.21889036\n",
      "Iteration 646, loss = 0.21867679\n",
      "Iteration 647, loss = 0.21846382\n",
      "Iteration 648, loss = 0.21825144\n",
      "Iteration 649, loss = 0.21803966\n",
      "Iteration 650, loss = 0.21782847\n",
      "Iteration 651, loss = 0.21761786\n",
      "Iteration 652, loss = 0.21740784\n",
      "Iteration 653, loss = 0.21719841\n",
      "Iteration 654, loss = 0.21698956\n",
      "Iteration 655, loss = 0.21678128\n",
      "Iteration 656, loss = 0.21657359\n",
      "Iteration 657, loss = 0.21636646\n",
      "Iteration 658, loss = 0.21615991\n",
      "Iteration 659, loss = 0.21595393\n",
      "Iteration 660, loss = 0.21574852\n",
      "Iteration 661, loss = 0.21554367\n",
      "Iteration 662, loss = 0.21533938\n",
      "Iteration 663, loss = 0.21513565\n",
      "Iteration 664, loss = 0.21493249\n",
      "Iteration 665, loss = 0.21472987\n",
      "Iteration 666, loss = 0.21452781\n",
      "Iteration 667, loss = 0.21432631\n",
      "Iteration 668, loss = 0.21412535\n",
      "Iteration 669, loss = 0.21392494\n",
      "Iteration 670, loss = 0.21372507\n",
      "Iteration 671, loss = 0.21352575\n",
      "Iteration 672, loss = 0.21332696\n",
      "Iteration 673, loss = 0.21312872\n",
      "Iteration 674, loss = 0.21293101\n",
      "Iteration 675, loss = 0.21273383\n",
      "Iteration 676, loss = 0.21253719\n",
      "Iteration 677, loss = 0.21234107\n",
      "Iteration 678, loss = 0.21214548\n",
      "Iteration 679, loss = 0.21195042\n",
      "Iteration 680, loss = 0.21175588\n",
      "Iteration 681, loss = 0.21156187\n",
      "Iteration 682, loss = 0.21136837\n",
      "Iteration 683, loss = 0.21117539\n",
      "Iteration 684, loss = 0.21098292\n",
      "Iteration 685, loss = 0.21079097\n",
      "Iteration 686, loss = 0.21059952\n",
      "Iteration 687, loss = 0.21040859\n",
      "Iteration 688, loss = 0.21021816\n",
      "Iteration 689, loss = 0.21002824\n",
      "Iteration 690, loss = 0.20983882\n",
      "Iteration 691, loss = 0.20964990\n",
      "Iteration 692, loss = 0.20946148\n",
      "Iteration 693, loss = 0.20927355\n",
      "Iteration 694, loss = 0.20908613\n",
      "Iteration 695, loss = 0.20889919\n",
      "Iteration 696, loss = 0.20871275\n",
      "Iteration 697, loss = 0.20852679\n",
      "Iteration 698, loss = 0.20834132\n",
      "Iteration 699, loss = 0.20815634\n",
      "Iteration 700, loss = 0.20797184\n",
      "Iteration 701, loss = 0.20778782\n",
      "Iteration 702, loss = 0.20760428\n",
      "Iteration 703, loss = 0.20742122\n",
      "Iteration 704, loss = 0.20723863\n",
      "Iteration 705, loss = 0.20705652\n",
      "Iteration 706, loss = 0.20687488\n",
      "Iteration 707, loss = 0.20669371\n",
      "Iteration 708, loss = 0.20651301\n",
      "Iteration 709, loss = 0.20633278\n",
      "Iteration 710, loss = 0.20615301\n",
      "Iteration 711, loss = 0.20597370\n",
      "Iteration 712, loss = 0.20579486\n",
      "Iteration 713, loss = 0.20561647\n",
      "Iteration 714, loss = 0.20543854\n",
      "Iteration 715, loss = 0.20526107\n",
      "Iteration 716, loss = 0.20508405\n",
      "Iteration 717, loss = 0.20490749\n",
      "Iteration 718, loss = 0.20473137\n",
      "Iteration 719, loss = 0.20455571\n",
      "Iteration 720, loss = 0.20438049\n",
      "Iteration 721, loss = 0.20420571\n",
      "Iteration 722, loss = 0.20403138\n",
      "Iteration 723, loss = 0.20385750\n",
      "Iteration 724, loss = 0.20368405\n",
      "Iteration 725, loss = 0.20351104\n",
      "Iteration 726, loss = 0.20333847\n",
      "Iteration 727, loss = 0.20316633\n",
      "Iteration 728, loss = 0.20299463\n",
      "Iteration 729, loss = 0.20282336\n",
      "Iteration 730, loss = 0.20265252\n",
      "Iteration 731, loss = 0.20248211\n",
      "Iteration 732, loss = 0.20231212\n",
      "Iteration 733, loss = 0.20214256\n",
      "Iteration 734, loss = 0.20197343\n",
      "Iteration 735, loss = 0.20180471\n",
      "Iteration 736, loss = 0.20163642\n",
      "Iteration 737, loss = 0.20146855\n",
      "Iteration 738, loss = 0.20130109\n",
      "Iteration 739, loss = 0.20113405\n",
      "Iteration 740, loss = 0.20096743\n",
      "Iteration 741, loss = 0.20080122\n",
      "Iteration 742, loss = 0.20063542\n",
      "Iteration 743, loss = 0.20047002\n",
      "Iteration 744, loss = 0.20030504\n",
      "Iteration 745, loss = 0.20014046\n",
      "Iteration 746, loss = 0.19997629\n",
      "Iteration 747, loss = 0.19981253\n",
      "Iteration 748, loss = 0.19964916\n",
      "Iteration 749, loss = 0.19948620\n",
      "Iteration 750, loss = 0.19932363\n",
      "Iteration 751, loss = 0.19916146\n",
      "Iteration 752, loss = 0.19899969\n",
      "Iteration 753, loss = 0.19883831\n",
      "Iteration 754, loss = 0.19867733\n",
      "Iteration 755, loss = 0.19851674\n",
      "Iteration 756, loss = 0.19835653\n",
      "Iteration 757, loss = 0.19819672\n",
      "Iteration 758, loss = 0.19803730\n",
      "Iteration 759, loss = 0.19787826\n",
      "Iteration 760, loss = 0.19771960\n",
      "Iteration 761, loss = 0.19756133\n",
      "Iteration 762, loss = 0.19740344\n",
      "Iteration 763, loss = 0.19724593\n",
      "Iteration 764, loss = 0.19708880\n",
      "Iteration 765, loss = 0.19693205\n",
      "Iteration 766, loss = 0.19677567\n",
      "Iteration 767, loss = 0.19661967\n",
      "Iteration 768, loss = 0.19646404\n",
      "Iteration 769, loss = 0.19630878\n",
      "Iteration 770, loss = 0.19615390\n",
      "Iteration 771, loss = 0.19599938\n",
      "Iteration 772, loss = 0.19584523\n",
      "Iteration 773, loss = 0.19569145\n",
      "Iteration 774, loss = 0.19553804\n",
      "Iteration 775, loss = 0.19538499\n",
      "Iteration 776, loss = 0.19523230\n",
      "Iteration 777, loss = 0.19507997\n",
      "Iteration 778, loss = 0.19492800\n",
      "Iteration 779, loss = 0.19477639\n",
      "Iteration 780, loss = 0.19462514\n",
      "Iteration 781, loss = 0.19447425\n",
      "Iteration 782, loss = 0.19432371\n",
      "Iteration 783, loss = 0.19417352\n",
      "Iteration 784, loss = 0.19402369\n",
      "Iteration 785, loss = 0.19387420\n",
      "Iteration 786, loss = 0.19372507\n",
      "Iteration 787, loss = 0.19357629\n",
      "Iteration 788, loss = 0.19342785\n",
      "Iteration 789, loss = 0.19327976\n",
      "Iteration 790, loss = 0.19313201\n",
      "Iteration 791, loss = 0.19298461\n",
      "Iteration 792, loss = 0.19283755\n",
      "Iteration 793, loss = 0.19269083\n",
      "Iteration 794, loss = 0.19254445\n",
      "Iteration 795, loss = 0.19239842\n",
      "Iteration 796, loss = 0.19225271\n",
      "Iteration 797, loss = 0.19210735\n",
      "Iteration 798, loss = 0.19196232\n",
      "Iteration 799, loss = 0.19181762\n",
      "Iteration 800, loss = 0.19167326\n",
      "Iteration 801, loss = 0.19152923\n",
      "Iteration 802, loss = 0.19138553\n",
      "Iteration 803, loss = 0.19124215\n",
      "Iteration 804, loss = 0.19109911\n",
      "Iteration 805, loss = 0.19095639\n",
      "Iteration 806, loss = 0.19081400\n",
      "Iteration 807, loss = 0.19067194\n",
      "Iteration 808, loss = 0.19053019\n",
      "Iteration 809, loss = 0.19038877\n",
      "Iteration 810, loss = 0.19024768\n",
      "Iteration 811, loss = 0.19010690\n",
      "Iteration 812, loss = 0.18996644\n",
      "Iteration 813, loss = 0.18982630\n",
      "Iteration 814, loss = 0.18968647\n",
      "Iteration 815, loss = 0.18954697\n",
      "Iteration 816, loss = 0.18940777\n",
      "Iteration 817, loss = 0.18926889\n",
      "Iteration 818, loss = 0.18913033\n",
      "Iteration 819, loss = 0.18899207\n",
      "Iteration 820, loss = 0.18885412\n",
      "Iteration 821, loss = 0.18871649\n",
      "Iteration 822, loss = 0.18857916\n",
      "Iteration 823, loss = 0.18844214\n",
      "Iteration 824, loss = 0.18830542\n",
      "Iteration 825, loss = 0.18816901\n",
      "Iteration 826, loss = 0.18803291\n",
      "Iteration 827, loss = 0.18789711\n",
      "Iteration 828, loss = 0.18776160\n",
      "Iteration 829, loss = 0.18762640\n",
      "Iteration 830, loss = 0.18749150\n",
      "Iteration 831, loss = 0.18735690\n",
      "Iteration 832, loss = 0.18722260\n",
      "Iteration 833, loss = 0.18708859\n",
      "Iteration 834, loss = 0.18695488\n",
      "Iteration 835, loss = 0.18682147\n",
      "Iteration 836, loss = 0.18668834\n",
      "Iteration 837, loss = 0.18655551\n",
      "Iteration 838, loss = 0.18642298\n",
      "Iteration 839, loss = 0.18629073\n",
      "Iteration 840, loss = 0.18615877\n",
      "Iteration 841, loss = 0.18602711\n",
      "Iteration 842, loss = 0.18589572\n",
      "Iteration 843, loss = 0.18576463\n",
      "Iteration 844, loss = 0.18563382\n",
      "Iteration 845, loss = 0.18550330\n",
      "Iteration 846, loss = 0.18537306\n",
      "Iteration 847, loss = 0.18524311\n",
      "Iteration 848, loss = 0.18511343\n",
      "Iteration 849, loss = 0.18498404\n",
      "Iteration 850, loss = 0.18485493\n",
      "Iteration 851, loss = 0.18472610\n",
      "Iteration 852, loss = 0.18459754\n",
      "Iteration 853, loss = 0.18446927\n",
      "Iteration 854, loss = 0.18434127\n",
      "Iteration 855, loss = 0.18421354\n",
      "Iteration 856, loss = 0.18408609\n",
      "Iteration 857, loss = 0.18395891\n",
      "Iteration 858, loss = 0.18383201\n",
      "Iteration 859, loss = 0.18370538\n",
      "Iteration 860, loss = 0.18357902\n",
      "Iteration 861, loss = 0.18345293\n",
      "Iteration 862, loss = 0.18332710\n",
      "Iteration 863, loss = 0.18320155\n",
      "Iteration 864, loss = 0.18307627\n",
      "Iteration 865, loss = 0.18295125\n",
      "Iteration 866, loss = 0.18282649\n",
      "Iteration 867, loss = 0.18270200\n",
      "Iteration 868, loss = 0.18257778\n",
      "Iteration 869, loss = 0.18245382\n",
      "Iteration 870, loss = 0.18233012\n",
      "Iteration 871, loss = 0.18220668\n",
      "Iteration 872, loss = 0.18208350\n",
      "Iteration 873, loss = 0.18196059\n",
      "Iteration 874, loss = 0.18183793\n",
      "Iteration 875, loss = 0.18171552\n",
      "Iteration 876, loss = 0.18159338\n",
      "Iteration 877, loss = 0.18147149\n",
      "Iteration 878, loss = 0.18134986\n",
      "Iteration 879, loss = 0.18122848\n",
      "Iteration 880, loss = 0.18110736\n",
      "Iteration 881, loss = 0.18098649\n",
      "Iteration 882, loss = 0.18086587\n",
      "Iteration 883, loss = 0.18074550\n",
      "Iteration 884, loss = 0.18062538\n",
      "Iteration 885, loss = 0.18050552\n",
      "Iteration 886, loss = 0.18038590\n",
      "Iteration 887, loss = 0.18026653\n",
      "Iteration 888, loss = 0.18014741\n",
      "Iteration 889, loss = 0.18002853\n",
      "Iteration 890, loss = 0.17990990\n",
      "Iteration 891, loss = 0.17979151\n",
      "Iteration 892, loss = 0.17967337\n",
      "Iteration 893, loss = 0.17955547\n",
      "Iteration 894, loss = 0.17943782\n",
      "Iteration 895, loss = 0.17932040\n",
      "Iteration 896, loss = 0.17920323\n",
      "Iteration 897, loss = 0.17908630\n",
      "Iteration 898, loss = 0.17896960\n",
      "Iteration 899, loss = 0.17885315\n",
      "Iteration 900, loss = 0.17873693\n",
      "Iteration 901, loss = 0.17862095\n",
      "Iteration 902, loss = 0.17850521\n",
      "Iteration 903, loss = 0.17838970\n",
      "Iteration 904, loss = 0.17827443\n",
      "Iteration 905, loss = 0.17815939\n",
      "Iteration 906, loss = 0.17804459\n",
      "Iteration 907, loss = 0.17793001\n",
      "Iteration 908, loss = 0.17781567\n",
      "Iteration 909, loss = 0.17770157\n",
      "Iteration 910, loss = 0.17758769\n",
      "Iteration 911, loss = 0.17747404\n",
      "Iteration 912, loss = 0.17736062\n",
      "Iteration 913, loss = 0.17724743\n",
      "Iteration 914, loss = 0.17713447\n",
      "Iteration 915, loss = 0.17702173\n",
      "Iteration 916, loss = 0.17690922\n",
      "Iteration 917, loss = 0.17679693\n",
      "Iteration 918, loss = 0.17668487\n",
      "Iteration 919, loss = 0.17657304\n",
      "Iteration 920, loss = 0.17646143\n",
      "Iteration 921, loss = 0.17635004\n",
      "Iteration 922, loss = 0.17623887\n",
      "Iteration 923, loss = 0.17612792\n",
      "Iteration 924, loss = 0.17601720\n",
      "Iteration 925, loss = 0.17590669\n",
      "Iteration 926, loss = 0.17579641\n",
      "Iteration 927, loss = 0.17568634\n",
      "Iteration 928, loss = 0.17557649\n",
      "Iteration 929, loss = 0.17546686\n",
      "Iteration 930, loss = 0.17535744\n",
      "Iteration 931, loss = 0.17524824\n",
      "Iteration 932, loss = 0.17513926\n",
      "Iteration 933, loss = 0.17503049\n",
      "Iteration 934, loss = 0.17492193\n",
      "Iteration 935, loss = 0.17481359\n",
      "Iteration 936, loss = 0.17470546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 937, loss = 0.17459754\n",
      "Iteration 938, loss = 0.17448983\n",
      "Iteration 939, loss = 0.17438234\n",
      "Iteration 940, loss = 0.17427505\n",
      "Iteration 941, loss = 0.17416797\n",
      "Iteration 942, loss = 0.17406111\n",
      "Iteration 943, loss = 0.17395445\n",
      "Iteration 944, loss = 0.17384799\n",
      "Iteration 945, loss = 0.17374175\n",
      "Iteration 946, loss = 0.17363571\n",
      "Iteration 947, loss = 0.17352988\n",
      "Iteration 948, loss = 0.17342425\n",
      "Iteration 949, loss = 0.17331882\n",
      "Iteration 950, loss = 0.17321360\n",
      "Iteration 951, loss = 0.17310858\n",
      "Iteration 952, loss = 0.17300377\n",
      "Iteration 953, loss = 0.17289916\n",
      "Iteration 954, loss = 0.17279474\n",
      "Iteration 955, loss = 0.17269053\n",
      "Iteration 956, loss = 0.17258652\n",
      "Iteration 957, loss = 0.17248271\n",
      "Iteration 958, loss = 0.17237910\n",
      "Iteration 959, loss = 0.17227568\n",
      "Iteration 960, loss = 0.17217247\n",
      "Iteration 961, loss = 0.17206945\n",
      "Iteration 962, loss = 0.17196662\n",
      "Iteration 963, loss = 0.17186400\n",
      "Iteration 964, loss = 0.17176156\n",
      "Iteration 965, loss = 0.17165933\n",
      "Iteration 966, loss = 0.17155728\n",
      "Iteration 967, loss = 0.17145543\n",
      "Iteration 968, loss = 0.17135378\n",
      "Iteration 969, loss = 0.17125231\n",
      "Iteration 970, loss = 0.17115104\n",
      "Iteration 971, loss = 0.17104996\n",
      "Iteration 972, loss = 0.17094907\n",
      "Iteration 973, loss = 0.17084837\n",
      "Iteration 974, loss = 0.17074786\n",
      "Iteration 975, loss = 0.17064754\n",
      "Iteration 976, loss = 0.17054741\n",
      "Iteration 977, loss = 0.17044746\n",
      "Iteration 978, loss = 0.17034770\n",
      "Iteration 979, loss = 0.17024813\n",
      "Iteration 980, loss = 0.17014875\n",
      "Iteration 981, loss = 0.17004955\n",
      "Iteration 982, loss = 0.16995054\n",
      "Iteration 983, loss = 0.16985171\n",
      "Iteration 984, loss = 0.16975306\n",
      "Iteration 985, loss = 0.16965460\n",
      "Iteration 986, loss = 0.16955633\n",
      "Iteration 987, loss = 0.16945823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13218766\n",
      "Iteration 2, loss = 2.11700894\n",
      "Iteration 3, loss = 2.10189252\n",
      "Iteration 4, loss = 2.08678597\n",
      "Iteration 5, loss = 2.07160349\n",
      "Iteration 6, loss = 2.05645952\n",
      "Iteration 7, loss = 2.04133530\n",
      "Iteration 8, loss = 2.02619543\n",
      "Iteration 9, loss = 2.01097543\n",
      "Iteration 10, loss = 1.99585918\n",
      "Iteration 11, loss = 1.98085127\n",
      "Iteration 12, loss = 1.96586942\n",
      "Iteration 13, loss = 1.95092701\n",
      "Iteration 14, loss = 1.93587291\n",
      "Iteration 15, loss = 1.92067335\n",
      "Iteration 16, loss = 1.90538004\n",
      "Iteration 17, loss = 1.88999754\n",
      "Iteration 18, loss = 1.87463291\n",
      "Iteration 19, loss = 1.85917555\n",
      "Iteration 20, loss = 1.84367337\n",
      "Iteration 21, loss = 1.82813349\n",
      "Iteration 22, loss = 1.81266850\n",
      "Iteration 23, loss = 1.79734524\n",
      "Iteration 24, loss = 1.78214808\n",
      "Iteration 25, loss = 1.76709462\n",
      "Iteration 26, loss = 1.75219100\n",
      "Iteration 27, loss = 1.73738688\n",
      "Iteration 28, loss = 1.72269484\n",
      "Iteration 29, loss = 1.70817480\n",
      "Iteration 30, loss = 1.69381473\n",
      "Iteration 31, loss = 1.67961866\n",
      "Iteration 32, loss = 1.66559018\n",
      "Iteration 33, loss = 1.65173243\n",
      "Iteration 34, loss = 1.63793721\n",
      "Iteration 35, loss = 1.62431998\n",
      "Iteration 36, loss = 1.61088593\n",
      "Iteration 37, loss = 1.59762862\n",
      "Iteration 38, loss = 1.58455029\n",
      "Iteration 39, loss = 1.57165277\n",
      "Iteration 40, loss = 1.55893760\n",
      "Iteration 41, loss = 1.54641669\n",
      "Iteration 42, loss = 1.53409071\n",
      "Iteration 43, loss = 1.52195156\n",
      "Iteration 44, loss = 1.50999951\n",
      "Iteration 45, loss = 1.49823459\n",
      "Iteration 46, loss = 1.48665668\n",
      "Iteration 47, loss = 1.47526544\n",
      "Iteration 48, loss = 1.46406041\n",
      "Iteration 49, loss = 1.45304097\n",
      "Iteration 50, loss = 1.44220640\n",
      "Iteration 51, loss = 1.43155585\n",
      "Iteration 52, loss = 1.42108837\n",
      "Iteration 53, loss = 1.41080292\n",
      "Iteration 54, loss = 1.40069839\n",
      "Iteration 55, loss = 1.39077360\n",
      "Iteration 56, loss = 1.38102728\n",
      "Iteration 57, loss = 1.37145811\n",
      "Iteration 58, loss = 1.36206471\n",
      "Iteration 59, loss = 1.35284564\n",
      "Iteration 60, loss = 1.34379939\n",
      "Iteration 61, loss = 1.33492440\n",
      "Iteration 62, loss = 1.32621904\n",
      "Iteration 63, loss = 1.31768068\n",
      "Iteration 64, loss = 1.30928037\n",
      "Iteration 65, loss = 1.30104348\n",
      "Iteration 66, loss = 1.29296845\n",
      "Iteration 67, loss = 1.28505363\n",
      "Iteration 68, loss = 1.27729727\n",
      "Iteration 69, loss = 1.26969752\n",
      "Iteration 70, loss = 1.26225246\n",
      "Iteration 71, loss = 1.25496007\n",
      "Iteration 72, loss = 1.24781828\n",
      "Iteration 73, loss = 1.24082493\n",
      "Iteration 74, loss = 1.23397778\n",
      "Iteration 75, loss = 1.22727455\n",
      "Iteration 76, loss = 1.22071287\n",
      "Iteration 77, loss = 1.21429035\n",
      "Iteration 78, loss = 1.20800453\n",
      "Iteration 79, loss = 1.20185288\n",
      "Iteration 80, loss = 1.19582629\n",
      "Iteration 81, loss = 1.18991470\n",
      "Iteration 82, loss = 1.18411752\n",
      "Iteration 83, loss = 1.17843372\n",
      "Iteration 84, loss = 1.17286474\n",
      "Iteration 85, loss = 1.16738958\n",
      "Iteration 86, loss = 1.16202177\n",
      "Iteration 87, loss = 1.15673556\n",
      "Iteration 88, loss = 1.15152794\n",
      "Iteration 89, loss = 1.14642548\n",
      "Iteration 90, loss = 1.14143278\n",
      "Iteration 91, loss = 1.13655759\n",
      "Iteration 92, loss = 1.13177743\n",
      "Iteration 93, loss = 1.12707320\n",
      "Iteration 94, loss = 1.12245573\n",
      "Iteration 95, loss = 1.11791764\n",
      "Iteration 96, loss = 1.11346113\n",
      "Iteration 97, loss = 1.10907656\n",
      "Iteration 98, loss = 1.10476803\n",
      "Iteration 99, loss = 1.10052387\n",
      "Iteration 100, loss = 1.09635755\n",
      "Iteration 101, loss = 1.09226619\n",
      "Iteration 102, loss = 1.08822448\n",
      "Iteration 103, loss = 1.08424446\n",
      "Iteration 104, loss = 1.08034320\n",
      "Iteration 105, loss = 1.07650408\n",
      "Iteration 106, loss = 1.07271044\n",
      "Iteration 107, loss = 1.06897245\n",
      "Iteration 108, loss = 1.06530007\n",
      "Iteration 109, loss = 1.06170891\n",
      "Iteration 110, loss = 1.05815777\n",
      "Iteration 111, loss = 1.05466076\n",
      "Iteration 112, loss = 1.05121674\n",
      "Iteration 113, loss = 1.04782184\n",
      "Iteration 114, loss = 1.04447210\n",
      "Iteration 115, loss = 1.04116793\n",
      "Iteration 116, loss = 1.03791161\n",
      "Iteration 117, loss = 1.03470428\n",
      "Iteration 118, loss = 1.03153915\n",
      "Iteration 119, loss = 1.02842461\n",
      "Iteration 120, loss = 1.02536064\n",
      "Iteration 121, loss = 1.02234645\n",
      "Iteration 122, loss = 1.01936993\n",
      "Iteration 123, loss = 1.01643901\n",
      "Iteration 124, loss = 1.01355213\n",
      "Iteration 125, loss = 1.01070293\n",
      "Iteration 126, loss = 1.00788875\n",
      "Iteration 127, loss = 1.00511116\n",
      "Iteration 128, loss = 1.00237144\n",
      "Iteration 129, loss = 0.99967815\n",
      "Iteration 130, loss = 0.99702693\n",
      "Iteration 131, loss = 0.99441528\n",
      "Iteration 132, loss = 0.99183559\n",
      "Iteration 133, loss = 0.98928952\n",
      "Iteration 134, loss = 0.98678006\n",
      "Iteration 135, loss = 0.98430649\n",
      "Iteration 136, loss = 0.98186085\n",
      "Iteration 137, loss = 0.97944905\n",
      "Iteration 138, loss = 0.97706912\n",
      "Iteration 139, loss = 0.97471746\n",
      "Iteration 140, loss = 0.97239776\n",
      "Iteration 141, loss = 0.97010945\n",
      "Iteration 142, loss = 0.96785197\n",
      "Iteration 143, loss = 0.96562475\n",
      "Iteration 144, loss = 0.96342693\n",
      "Iteration 145, loss = 0.96125230\n",
      "Iteration 146, loss = 0.95910606\n",
      "Iteration 147, loss = 0.95698774\n",
      "Iteration 148, loss = 0.95489689\n",
      "Iteration 149, loss = 0.95283303\n",
      "Iteration 150, loss = 0.95079572\n",
      "Iteration 151, loss = 0.94878450\n",
      "Iteration 152, loss = 0.94679895\n",
      "Iteration 153, loss = 0.94483863\n",
      "Iteration 154, loss = 0.94290294\n",
      "Iteration 155, loss = 0.94098653\n",
      "Iteration 156, loss = 0.93909403\n",
      "Iteration 157, loss = 0.93722511\n",
      "Iteration 158, loss = 0.93537946\n",
      "Iteration 159, loss = 0.93355677\n",
      "Iteration 160, loss = 0.93175673\n",
      "Iteration 161, loss = 0.92997905\n",
      "Iteration 162, loss = 0.92822343\n",
      "Iteration 163, loss = 0.92648958\n",
      "Iteration 164, loss = 0.92477723\n",
      "Iteration 165, loss = 0.92308609\n",
      "Iteration 166, loss = 0.92141591\n",
      "Iteration 167, loss = 0.91976641\n",
      "Iteration 168, loss = 0.91813733\n",
      "Iteration 169, loss = 0.91652843\n",
      "Iteration 170, loss = 0.91493946\n",
      "Iteration 171, loss = 0.91337017\n",
      "Iteration 172, loss = 0.91182033\n",
      "Iteration 173, loss = 0.91028970\n",
      "Iteration 174, loss = 0.90877805\n",
      "Iteration 175, loss = 0.90728516\n",
      "Iteration 176, loss = 0.90581079\n",
      "Iteration 177, loss = 0.90435474\n",
      "Iteration 178, loss = 0.90291679\n",
      "Iteration 179, loss = 0.90149671\n",
      "Iteration 180, loss = 0.90009429\n",
      "Iteration 181, loss = 0.89870933\n",
      "Iteration 182, loss = 0.89734161\n",
      "Iteration 183, loss = 0.89599092\n",
      "Iteration 184, loss = 0.89465706\n",
      "Iteration 185, loss = 0.89333982\n",
      "Iteration 186, loss = 0.89203899\n",
      "Iteration 187, loss = 0.89075438\n",
      "Iteration 188, loss = 0.88948576\n",
      "Iteration 189, loss = 0.88823295\n",
      "Iteration 190, loss = 0.88699574\n",
      "Iteration 191, loss = 0.88577392\n",
      "Iteration 192, loss = 0.88456728\n",
      "Iteration 193, loss = 0.88337564\n",
      "Iteration 194, loss = 0.88219878\n",
      "Iteration 195, loss = 0.88103649\n",
      "Iteration 196, loss = 0.87988859\n",
      "Iteration 197, loss = 0.87875485\n",
      "Iteration 198, loss = 0.87763509\n",
      "Iteration 199, loss = 0.87652909\n",
      "Iteration 200, loss = 0.87543666\n",
      "Iteration 201, loss = 0.87435758\n",
      "Iteration 202, loss = 0.87329166\n",
      "Iteration 203, loss = 0.87223869\n",
      "Iteration 204, loss = 0.87119847\n",
      "Iteration 205, loss = 0.87017079\n",
      "Iteration 206, loss = 0.86915546\n",
      "Iteration 207, loss = 0.86815227\n",
      "Iteration 208, loss = 0.86716102\n",
      "Iteration 209, loss = 0.86618150\n",
      "Iteration 210, loss = 0.86521351\n",
      "Iteration 211, loss = 0.86425686\n",
      "Iteration 212, loss = 0.86331134\n",
      "Iteration 213, loss = 0.86237676\n",
      "Iteration 214, loss = 0.86145291\n",
      "Iteration 215, loss = 0.86053960\n",
      "Iteration 216, loss = 0.85963662\n",
      "Iteration 217, loss = 0.85874379\n",
      "Iteration 218, loss = 0.85786090\n",
      "Iteration 219, loss = 0.85698777\n",
      "Iteration 220, loss = 0.85612420\n",
      "Iteration 221, loss = 0.85527000\n",
      "Iteration 222, loss = 0.85442497\n",
      "Iteration 223, loss = 0.85358893\n",
      "Iteration 224, loss = 0.85276169\n",
      "Iteration 225, loss = 0.85194307\n",
      "Iteration 226, loss = 0.85113288\n",
      "Iteration 227, loss = 0.85033093\n",
      "Iteration 228, loss = 0.84953705\n",
      "Iteration 229, loss = 0.84875105\n",
      "Iteration 230, loss = 0.84797276\n",
      "Iteration 231, loss = 0.84720199\n",
      "Iteration 232, loss = 0.84643859\n",
      "Iteration 233, loss = 0.84568237\n",
      "Iteration 234, loss = 0.84493316\n",
      "Iteration 235, loss = 0.84419079\n",
      "Iteration 236, loss = 0.84345510\n",
      "Iteration 237, loss = 0.84272593\n",
      "Iteration 238, loss = 0.84200310\n",
      "Iteration 239, loss = 0.84128647\n",
      "Iteration 240, loss = 0.84057586\n",
      "Iteration 241, loss = 0.83987113\n",
      "Iteration 242, loss = 0.83917212\n",
      "Iteration 243, loss = 0.83847868\n",
      "Iteration 244, loss = 0.83779066\n",
      "Iteration 245, loss = 0.83710791\n",
      "Iteration 246, loss = 0.83643029\n",
      "Iteration 247, loss = 0.83575765\n",
      "Iteration 248, loss = 0.83508985\n",
      "Iteration 249, loss = 0.83442676\n",
      "Iteration 250, loss = 0.83376823\n",
      "Iteration 251, loss = 0.83311414\n",
      "Iteration 252, loss = 0.83246434\n",
      "Iteration 253, loss = 0.83181872\n",
      "Iteration 254, loss = 0.83117715\n",
      "Iteration 255, loss = 0.83053949\n",
      "Iteration 256, loss = 0.82990563\n",
      "Iteration 257, loss = 0.82927544\n",
      "Iteration 258, loss = 0.82864882\n",
      "Iteration 259, loss = 0.82802563\n",
      "Iteration 260, loss = 0.82740577\n",
      "Iteration 261, loss = 0.82678913\n",
      "Iteration 262, loss = 0.82617559\n",
      "Iteration 263, loss = 0.82556505\n",
      "Iteration 264, loss = 0.82495740\n",
      "Iteration 265, loss = 0.82435254\n",
      "Iteration 266, loss = 0.82375037\n",
      "Iteration 267, loss = 0.82315078\n",
      "Iteration 268, loss = 0.82255369\n",
      "Iteration 269, loss = 0.82195899\n",
      "Iteration 270, loss = 0.82136660\n",
      "Iteration 271, loss = 0.82077642\n",
      "Iteration 272, loss = 0.82018837\n",
      "Iteration 273, loss = 0.81960235\n",
      "Iteration 274, loss = 0.81901828\n",
      "Iteration 275, loss = 0.81843607\n",
      "Iteration 276, loss = 0.81785566\n",
      "Iteration 277, loss = 0.81727695\n",
      "Iteration 278, loss = 0.81669986\n",
      "Iteration 279, loss = 0.81612433\n",
      "Iteration 280, loss = 0.81555028\n",
      "Iteration 281, loss = 0.81497764\n",
      "Iteration 282, loss = 0.81440633\n",
      "Iteration 283, loss = 0.81383628\n",
      "Iteration 284, loss = 0.81326743\n",
      "Iteration 285, loss = 0.81269972\n",
      "Iteration 286, loss = 0.81213307\n",
      "Iteration 287, loss = 0.81156743\n",
      "Iteration 288, loss = 0.81100273\n",
      "Iteration 289, loss = 0.81043892\n",
      "Iteration 290, loss = 0.80987593\n",
      "Iteration 291, loss = 0.80931372\n",
      "Iteration 292, loss = 0.80875221\n",
      "Iteration 293, loss = 0.80819137\n",
      "Iteration 294, loss = 0.80763114\n",
      "Iteration 295, loss = 0.80707146\n",
      "Iteration 296, loss = 0.80651229\n",
      "Iteration 297, loss = 0.80595358\n",
      "Iteration 298, loss = 0.80539528\n",
      "Iteration 299, loss = 0.80483735\n",
      "Iteration 300, loss = 0.80427974\n",
      "Iteration 301, loss = 0.80372241\n",
      "Iteration 302, loss = 0.80316531\n",
      "Iteration 303, loss = 0.80260840\n",
      "Iteration 304, loss = 0.80205166\n",
      "Iteration 305, loss = 0.80149502\n",
      "Iteration 306, loss = 0.80093847\n",
      "Iteration 307, loss = 0.80038195\n",
      "Iteration 308, loss = 0.79982544\n",
      "Iteration 309, loss = 0.79926890\n",
      "Iteration 310, loss = 0.79871230\n",
      "Iteration 311, loss = 0.79815560\n",
      "Iteration 312, loss = 0.79760036\n",
      "Iteration 313, loss = 0.79704691\n",
      "Iteration 314, loss = 0.79649359\n",
      "Iteration 315, loss = 0.79594036\n",
      "Iteration 316, loss = 0.79538717\n",
      "Iteration 317, loss = 0.79483399\n",
      "Iteration 318, loss = 0.79428076\n",
      "Iteration 319, loss = 0.79372745\n",
      "Iteration 320, loss = 0.79317403\n",
      "Iteration 321, loss = 0.79262057\n",
      "Iteration 322, loss = 0.79207119\n",
      "Iteration 323, loss = 0.79152195\n",
      "Iteration 324, loss = 0.79097279\n",
      "Iteration 325, loss = 0.79042366\n",
      "Iteration 326, loss = 0.78987453\n",
      "Iteration 327, loss = 0.78932536\n",
      "Iteration 328, loss = 0.78877609\n",
      "Iteration 329, loss = 0.78822671\n",
      "Iteration 330, loss = 0.78767716\n",
      "Iteration 331, loss = 0.78712742\n",
      "Iteration 332, loss = 0.78657747\n",
      "Iteration 333, loss = 0.78602725\n",
      "Iteration 334, loss = 0.78547675\n",
      "Iteration 335, loss = 0.78492594\n",
      "Iteration 336, loss = 0.78437479\n",
      "Iteration 337, loss = 0.78382328\n",
      "Iteration 338, loss = 0.78327137\n",
      "Iteration 339, loss = 0.78271905\n",
      "Iteration 340, loss = 0.78216629\n",
      "Iteration 341, loss = 0.78161307\n",
      "Iteration 342, loss = 0.78105937\n",
      "Iteration 343, loss = 0.78050516\n",
      "Iteration 344, loss = 0.77995043\n",
      "Iteration 345, loss = 0.77939516\n",
      "Iteration 346, loss = 0.77883933\n",
      "Iteration 347, loss = 0.77828292\n",
      "Iteration 348, loss = 0.77772592\n",
      "Iteration 349, loss = 0.77716831\n",
      "Iteration 350, loss = 0.77661008\n",
      "Iteration 351, loss = 0.77605120\n",
      "Iteration 352, loss = 0.77549168\n",
      "Iteration 353, loss = 0.77493149\n",
      "Iteration 354, loss = 0.77437061\n",
      "Iteration 355, loss = 0.77380905\n",
      "Iteration 356, loss = 0.77324679\n",
      "Iteration 357, loss = 0.77268381\n",
      "Iteration 358, loss = 0.77212011\n",
      "Iteration 359, loss = 0.77156111\n",
      "Iteration 360, loss = 0.77100435\n",
      "Iteration 361, loss = 0.77044746\n",
      "Iteration 362, loss = 0.76989037\n",
      "Iteration 363, loss = 0.76933306\n",
      "Iteration 364, loss = 0.76877547\n",
      "Iteration 365, loss = 0.76821758\n",
      "Iteration 366, loss = 0.76765935\n",
      "Iteration 367, loss = 0.76710075\n",
      "Iteration 368, loss = 0.76654176\n",
      "Iteration 369, loss = 0.76598234\n",
      "Iteration 370, loss = 0.76542247\n",
      "Iteration 371, loss = 0.76486212\n",
      "Iteration 372, loss = 0.76430129\n",
      "Iteration 373, loss = 0.76373993\n",
      "Iteration 374, loss = 0.76317805\n",
      "Iteration 375, loss = 0.76261561\n",
      "Iteration 376, loss = 0.76205260\n",
      "Iteration 377, loss = 0.76148901\n",
      "Iteration 378, loss = 0.76092481\n",
      "Iteration 379, loss = 0.76036000\n",
      "Iteration 380, loss = 0.75979455\n",
      "Iteration 381, loss = 0.75922846\n",
      "Iteration 382, loss = 0.75866172\n",
      "Iteration 383, loss = 0.75809430\n",
      "Iteration 384, loss = 0.75752621\n",
      "Iteration 385, loss = 0.75695743\n",
      "Iteration 386, loss = 0.75638794\n",
      "Iteration 387, loss = 0.75581775\n",
      "Iteration 388, loss = 0.75524684\n",
      "Iteration 389, loss = 0.75467521\n",
      "Iteration 390, loss = 0.75410285\n",
      "Iteration 391, loss = 0.75352974\n",
      "Iteration 392, loss = 0.75295589\n",
      "Iteration 393, loss = 0.75238129\n",
      "Iteration 394, loss = 0.75180593\n",
      "Iteration 395, loss = 0.75122982\n",
      "Iteration 396, loss = 0.75065293\n",
      "Iteration 397, loss = 0.75007527\n",
      "Iteration 398, loss = 0.74949684\n",
      "Iteration 399, loss = 0.74891764\n",
      "Iteration 400, loss = 0.74833987\n",
      "Iteration 401, loss = 0.74776322\n",
      "Iteration 402, loss = 0.74718611\n",
      "Iteration 403, loss = 0.74660851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 404, loss = 0.74603041\n",
      "Iteration 405, loss = 0.74545178\n",
      "Iteration 406, loss = 0.74487259\n",
      "Iteration 407, loss = 0.74429284\n",
      "Iteration 408, loss = 0.74371250\n",
      "Iteration 409, loss = 0.74313157\n",
      "Iteration 410, loss = 0.74255002\n",
      "Iteration 411, loss = 0.74196784\n",
      "Iteration 412, loss = 0.74138502\n",
      "Iteration 413, loss = 0.74080156\n",
      "Iteration 414, loss = 0.74021744\n",
      "Iteration 415, loss = 0.73963266\n",
      "Iteration 416, loss = 0.73904720\n",
      "Iteration 417, loss = 0.73846106\n",
      "Iteration 418, loss = 0.73787423\n",
      "Iteration 419, loss = 0.73728672\n",
      "Iteration 420, loss = 0.73669850\n",
      "Iteration 421, loss = 0.73610958\n",
      "Iteration 422, loss = 0.73551996\n",
      "Iteration 423, loss = 0.73492962\n",
      "Iteration 424, loss = 0.73433857\n",
      "Iteration 425, loss = 0.73374680\n",
      "Iteration 426, loss = 0.73315582\n",
      "Iteration 427, loss = 0.73256709\n",
      "Iteration 428, loss = 0.73197799\n",
      "Iteration 429, loss = 0.73138847\n",
      "Iteration 430, loss = 0.73079874\n",
      "Iteration 431, loss = 0.73021087\n",
      "Iteration 432, loss = 0.72962274\n",
      "Iteration 433, loss = 0.72903433\n",
      "Iteration 434, loss = 0.72844559\n",
      "Iteration 435, loss = 0.72785649\n",
      "Iteration 436, loss = 0.72726702\n",
      "Iteration 437, loss = 0.72667714\n",
      "Iteration 438, loss = 0.72608683\n",
      "Iteration 439, loss = 0.72549608\n",
      "Iteration 440, loss = 0.72490487\n",
      "Iteration 441, loss = 0.72431318\n",
      "Iteration 442, loss = 0.72372099\n",
      "Iteration 443, loss = 0.72312830\n",
      "Iteration 444, loss = 0.72253510\n",
      "Iteration 445, loss = 0.72194136\n",
      "Iteration 446, loss = 0.72134969\n",
      "Iteration 447, loss = 0.72075927\n",
      "Iteration 448, loss = 0.72016861\n",
      "Iteration 449, loss = 0.71957769\n",
      "Iteration 450, loss = 0.71898648\n",
      "Iteration 451, loss = 0.71839495\n",
      "Iteration 452, loss = 0.71780309\n",
      "Iteration 453, loss = 0.71721097\n",
      "Iteration 454, loss = 0.71662187\n",
      "Iteration 455, loss = 0.71603258\n",
      "Iteration 456, loss = 0.71544307\n",
      "Iteration 457, loss = 0.71485333\n",
      "Iteration 458, loss = 0.71426334\n",
      "Iteration 459, loss = 0.71367390\n",
      "Iteration 460, loss = 0.71308598\n",
      "Iteration 461, loss = 0.71249798\n",
      "Iteration 462, loss = 0.71190987\n",
      "Iteration 463, loss = 0.71132162\n",
      "Iteration 464, loss = 0.71073319\n",
      "Iteration 465, loss = 0.71014458\n",
      "Iteration 466, loss = 0.70955574\n",
      "Iteration 467, loss = 0.70896837\n",
      "Iteration 468, loss = 0.70838178\n",
      "Iteration 469, loss = 0.70779513\n",
      "Iteration 470, loss = 0.70720841\n",
      "Iteration 471, loss = 0.70662157\n",
      "Iteration 472, loss = 0.70603458\n",
      "Iteration 473, loss = 0.70544743\n",
      "Iteration 474, loss = 0.70486009\n",
      "Iteration 475, loss = 0.70427408\n",
      "Iteration 476, loss = 0.70368986\n",
      "Iteration 477, loss = 0.70310565\n",
      "Iteration 478, loss = 0.70252142\n",
      "Iteration 479, loss = 0.70193714\n",
      "Iteration 480, loss = 0.70135278\n",
      "Iteration 481, loss = 0.70076833\n",
      "Iteration 482, loss = 0.70018376\n",
      "Iteration 483, loss = 0.69959905\n",
      "Iteration 484, loss = 0.69901419\n",
      "Iteration 485, loss = 0.69842915\n",
      "Iteration 486, loss = 0.69784394\n",
      "Iteration 487, loss = 0.69726068\n",
      "Iteration 488, loss = 0.69667797\n",
      "Iteration 489, loss = 0.69609523\n",
      "Iteration 490, loss = 0.69551345\n",
      "Iteration 491, loss = 0.69493378\n",
      "Iteration 492, loss = 0.69435428\n",
      "Iteration 493, loss = 0.69377493\n",
      "Iteration 494, loss = 0.69319774\n",
      "Iteration 495, loss = 0.69262140\n",
      "Iteration 496, loss = 0.69204535\n",
      "Iteration 497, loss = 0.69146953\n",
      "Iteration 498, loss = 0.69089428\n",
      "Iteration 499, loss = 0.69032260\n",
      "Iteration 500, loss = 0.68975278\n",
      "Iteration 501, loss = 0.68918343\n",
      "Iteration 502, loss = 0.68861451\n",
      "Iteration 503, loss = 0.68804597\n",
      "Iteration 504, loss = 0.68747777\n",
      "Iteration 505, loss = 0.68690988\n",
      "Iteration 506, loss = 0.68634227\n",
      "Iteration 507, loss = 0.68577491\n",
      "Iteration 508, loss = 0.68520777\n",
      "Iteration 509, loss = 0.68464083\n",
      "Iteration 510, loss = 0.68407406\n",
      "Iteration 511, loss = 0.68350919\n",
      "Iteration 512, loss = 0.68294526\n",
      "Iteration 513, loss = 0.68238166\n",
      "Iteration 514, loss = 0.68181836\n",
      "Iteration 515, loss = 0.68125534\n",
      "Iteration 516, loss = 0.68069256\n",
      "Iteration 517, loss = 0.68013000\n",
      "Iteration 518, loss = 0.67956764\n",
      "Iteration 519, loss = 0.67900547\n",
      "Iteration 520, loss = 0.67844346\n",
      "Iteration 521, loss = 0.67788159\n",
      "Iteration 522, loss = 0.67731986\n",
      "Iteration 523, loss = 0.67675970\n",
      "Iteration 524, loss = 0.67620090\n",
      "Iteration 525, loss = 0.67564242\n",
      "Iteration 526, loss = 0.67508424\n",
      "Iteration 527, loss = 0.67452633\n",
      "Iteration 528, loss = 0.67396866\n",
      "Iteration 529, loss = 0.67341122\n",
      "Iteration 530, loss = 0.67285399\n",
      "Iteration 531, loss = 0.67229694\n",
      "Iteration 532, loss = 0.67174007\n",
      "Iteration 533, loss = 0.67118336\n",
      "Iteration 534, loss = 0.67062678\n",
      "Iteration 535, loss = 0.67007034\n",
      "Iteration 536, loss = 0.66951400\n",
      "Iteration 537, loss = 0.66895778\n",
      "Iteration 538, loss = 0.66840165\n",
      "Iteration 539, loss = 0.66784560\n",
      "Iteration 540, loss = 0.66728962\n",
      "Iteration 541, loss = 0.66673371\n",
      "Iteration 542, loss = 0.66617786\n",
      "Iteration 543, loss = 0.66562217\n",
      "Iteration 544, loss = 0.66506631\n",
      "Iteration 545, loss = 0.66451060\n",
      "Iteration 546, loss = 0.66395492\n",
      "Iteration 547, loss = 0.66339927\n",
      "Iteration 548, loss = 0.66284364\n",
      "Iteration 549, loss = 0.66228804\n",
      "Iteration 550, loss = 0.66173245\n",
      "Iteration 551, loss = 0.66117697\n",
      "Iteration 552, loss = 0.66062456\n",
      "Iteration 553, loss = 0.66007243\n",
      "Iteration 554, loss = 0.65952056\n",
      "Iteration 555, loss = 0.65897102\n",
      "Iteration 556, loss = 0.65842257\n",
      "Iteration 557, loss = 0.65787456\n",
      "Iteration 558, loss = 0.65733013\n",
      "Iteration 559, loss = 0.65679052\n",
      "Iteration 560, loss = 0.65625266\n",
      "Iteration 561, loss = 0.65571736\n",
      "Iteration 562, loss = 0.65518429\n",
      "Iteration 563, loss = 0.65465217\n",
      "Iteration 564, loss = 0.65412094\n",
      "Iteration 565, loss = 0.65359317\n",
      "Iteration 566, loss = 0.65306678\n",
      "Iteration 567, loss = 0.65254134\n",
      "Iteration 568, loss = 0.65201690\n",
      "Iteration 569, loss = 0.65149577\n",
      "Iteration 570, loss = 0.65097729\n",
      "Iteration 571, loss = 0.65046069\n",
      "Iteration 572, loss = 0.64994512\n",
      "Iteration 573, loss = 0.64943265\n",
      "Iteration 574, loss = 0.64892145\n",
      "Iteration 575, loss = 0.64841132\n",
      "Iteration 576, loss = 0.64790224\n",
      "Iteration 577, loss = 0.64739419\n",
      "Iteration 578, loss = 0.64688714\n",
      "Iteration 579, loss = 0.64638106\n",
      "Iteration 580, loss = 0.64587592\n",
      "Iteration 581, loss = 0.64537171\n",
      "Iteration 582, loss = 0.64486839\n",
      "Iteration 583, loss = 0.64436592\n",
      "Iteration 584, loss = 0.64386429\n",
      "Iteration 585, loss = 0.64336345\n",
      "Iteration 586, loss = 0.64286339\n",
      "Iteration 587, loss = 0.64236552\n",
      "Iteration 588, loss = 0.64186906\n",
      "Iteration 589, loss = 0.64137348\n",
      "Iteration 590, loss = 0.64087876\n",
      "Iteration 591, loss = 0.64038486\n",
      "Iteration 592, loss = 0.63989176\n",
      "Iteration 593, loss = 0.63939941\n",
      "Iteration 594, loss = 0.63890780\n",
      "Iteration 595, loss = 0.63841690\n",
      "Iteration 596, loss = 0.63792667\n",
      "Iteration 597, loss = 0.63743711\n",
      "Iteration 598, loss = 0.63694817\n",
      "Iteration 599, loss = 0.63646164\n",
      "Iteration 600, loss = 0.63597629\n",
      "Iteration 601, loss = 0.63549174\n",
      "Iteration 602, loss = 0.63500795\n",
      "Iteration 603, loss = 0.63452489\n",
      "Iteration 604, loss = 0.63404255\n",
      "Iteration 605, loss = 0.63356113\n",
      "Iteration 606, loss = 0.63308255\n",
      "Iteration 607, loss = 0.63260482\n",
      "Iteration 608, loss = 0.63212790\n",
      "Iteration 609, loss = 0.63165177\n",
      "Iteration 610, loss = 0.63117640\n",
      "Iteration 611, loss = 0.63070177\n",
      "Iteration 612, loss = 0.63022786\n",
      "Iteration 613, loss = 0.62975464\n",
      "Iteration 614, loss = 0.62928209\n",
      "Iteration 615, loss = 0.62881020\n",
      "Iteration 616, loss = 0.62833894\n",
      "Iteration 617, loss = 0.62786830\n",
      "Iteration 618, loss = 0.62739826\n",
      "Iteration 619, loss = 0.62693038\n",
      "Iteration 620, loss = 0.62646376\n",
      "Iteration 621, loss = 0.62599949\n",
      "Iteration 622, loss = 0.62553668\n",
      "Iteration 623, loss = 0.62507475\n",
      "Iteration 624, loss = 0.62461366\n",
      "Iteration 625, loss = 0.62415340\n",
      "Iteration 626, loss = 0.62369394\n",
      "Iteration 627, loss = 0.62323580\n",
      "Iteration 628, loss = 0.62277940\n",
      "Iteration 629, loss = 0.62232379\n",
      "Iteration 630, loss = 0.62186895\n",
      "Iteration 631, loss = 0.62141486\n",
      "Iteration 632, loss = 0.62096151\n",
      "Iteration 633, loss = 0.62050948\n",
      "Iteration 634, loss = 0.62005954\n",
      "Iteration 635, loss = 0.61961045\n",
      "Iteration 636, loss = 0.61916217\n",
      "Iteration 637, loss = 0.61871469\n",
      "Iteration 638, loss = 0.61826933\n",
      "Iteration 639, loss = 0.61782738\n",
      "Iteration 640, loss = 0.61738656\n",
      "Iteration 641, loss = 0.61694755\n",
      "Iteration 642, loss = 0.61651106\n",
      "Iteration 643, loss = 0.61607561\n",
      "Iteration 644, loss = 0.61564115\n",
      "Iteration 645, loss = 0.61520766\n",
      "Iteration 646, loss = 0.61477514\n",
      "Iteration 647, loss = 0.61434357\n",
      "Iteration 648, loss = 0.61391296\n",
      "Iteration 649, loss = 0.61348328\n",
      "Iteration 650, loss = 0.61305452\n",
      "Iteration 651, loss = 0.61262668\n",
      "Iteration 652, loss = 0.61219975\n",
      "Iteration 653, loss = 0.61177370\n",
      "Iteration 654, loss = 0.61134853\n",
      "Iteration 655, loss = 0.61092421\n",
      "Iteration 656, loss = 0.61050074\n",
      "Iteration 657, loss = 0.61007909\n",
      "Iteration 658, loss = 0.60965885\n",
      "Iteration 659, loss = 0.60923954\n",
      "Iteration 660, loss = 0.60882115\n",
      "Iteration 661, loss = 0.60840367\n",
      "Iteration 662, loss = 0.60798708\n",
      "Iteration 663, loss = 0.60757136\n",
      "Iteration 664, loss = 0.60715699\n",
      "Iteration 665, loss = 0.60674459\n",
      "Iteration 666, loss = 0.60633314\n",
      "Iteration 667, loss = 0.60592263\n",
      "Iteration 668, loss = 0.60551303\n",
      "Iteration 669, loss = 0.60510434\n",
      "Iteration 670, loss = 0.60469652\n",
      "Iteration 671, loss = 0.60428958\n",
      "Iteration 672, loss = 0.60388348\n",
      "Iteration 673, loss = 0.60347822\n",
      "Iteration 674, loss = 0.60307377\n",
      "Iteration 675, loss = 0.60267012\n",
      "Iteration 676, loss = 0.60226724\n",
      "Iteration 677, loss = 0.60186513\n",
      "Iteration 678, loss = 0.60146377\n",
      "Iteration 679, loss = 0.60106314\n",
      "Iteration 680, loss = 0.60066323\n",
      "Iteration 681, loss = 0.60026401\n",
      "Iteration 682, loss = 0.59986549\n",
      "Iteration 683, loss = 0.59946763\n",
      "Iteration 684, loss = 0.59907044\n",
      "Iteration 685, loss = 0.59867389\n",
      "Iteration 686, loss = 0.59827798\n",
      "Iteration 687, loss = 0.59788269\n",
      "Iteration 688, loss = 0.59748802\n",
      "Iteration 689, loss = 0.59709394\n",
      "Iteration 690, loss = 0.59670046\n",
      "Iteration 691, loss = 0.59630756\n",
      "Iteration 692, loss = 0.59591524\n",
      "Iteration 693, loss = 0.59552407\n",
      "Iteration 694, loss = 0.59513462\n",
      "Iteration 695, loss = 0.59474589\n",
      "Iteration 696, loss = 0.59435783\n",
      "Iteration 697, loss = 0.59397045\n",
      "Iteration 698, loss = 0.59358373\n",
      "Iteration 699, loss = 0.59319765\n",
      "Iteration 700, loss = 0.59281221\n",
      "Iteration 701, loss = 0.59242738\n",
      "Iteration 702, loss = 0.59204316\n",
      "Iteration 703, loss = 0.59165969\n",
      "Iteration 704, loss = 0.59127745\n",
      "Iteration 705, loss = 0.59089581\n",
      "Iteration 706, loss = 0.59051478\n",
      "Iteration 707, loss = 0.59013433\n",
      "Iteration 708, loss = 0.58975446\n",
      "Iteration 709, loss = 0.58937517\n",
      "Iteration 710, loss = 0.58899644\n",
      "Iteration 711, loss = 0.58861828\n",
      "Iteration 712, loss = 0.58824066\n",
      "Iteration 713, loss = 0.58786486\n",
      "Iteration 714, loss = 0.58749260\n",
      "Iteration 715, loss = 0.58712114\n",
      "Iteration 716, loss = 0.58675046\n",
      "Iteration 717, loss = 0.58638056\n",
      "Iteration 718, loss = 0.58601140\n",
      "Iteration 719, loss = 0.58564299\n",
      "Iteration 720, loss = 0.58527530\n",
      "Iteration 721, loss = 0.58490833\n",
      "Iteration 722, loss = 0.58454206\n",
      "Iteration 723, loss = 0.58417648\n",
      "Iteration 724, loss = 0.58381284\n",
      "Iteration 725, loss = 0.58345028\n",
      "Iteration 726, loss = 0.58308848\n",
      "Iteration 727, loss = 0.58272744\n",
      "Iteration 728, loss = 0.58236715\n",
      "Iteration 729, loss = 0.58200759\n",
      "Iteration 730, loss = 0.58164877\n",
      "Iteration 731, loss = 0.58129067\n",
      "Iteration 732, loss = 0.58093328\n",
      "Iteration 733, loss = 0.58057659\n",
      "Iteration 734, loss = 0.58022059\n",
      "Iteration 735, loss = 0.57986526\n",
      "Iteration 736, loss = 0.57951060\n",
      "Iteration 737, loss = 0.57915659\n",
      "Iteration 738, loss = 0.57880322\n",
      "Iteration 739, loss = 0.57845047\n",
      "Iteration 740, loss = 0.57809834\n",
      "Iteration 741, loss = 0.57774680\n",
      "Iteration 742, loss = 0.57739585\n",
      "Iteration 743, loss = 0.57704548\n",
      "Iteration 744, loss = 0.57669568\n",
      "Iteration 745, loss = 0.57634757\n",
      "Iteration 746, loss = 0.57600027\n",
      "Iteration 747, loss = 0.57565364\n",
      "Iteration 748, loss = 0.57530767\n",
      "Iteration 749, loss = 0.57496233\n",
      "Iteration 750, loss = 0.57461763\n",
      "Iteration 751, loss = 0.57427355\n",
      "Iteration 752, loss = 0.57393008\n",
      "Iteration 753, loss = 0.57358721\n",
      "Iteration 754, loss = 0.57324528\n",
      "Iteration 755, loss = 0.57290405\n",
      "Iteration 756, loss = 0.57256335\n",
      "Iteration 757, loss = 0.57222320\n",
      "Iteration 758, loss = 0.57188358\n",
      "Iteration 759, loss = 0.57154449\n",
      "Iteration 760, loss = 0.57120594\n",
      "Iteration 761, loss = 0.57086792\n",
      "Iteration 762, loss = 0.57053044\n",
      "Iteration 763, loss = 0.57019349\n",
      "Iteration 764, loss = 0.56985841\n",
      "Iteration 765, loss = 0.56952416\n",
      "Iteration 766, loss = 0.56919057\n",
      "Iteration 767, loss = 0.56885761\n",
      "Iteration 768, loss = 0.56852527\n",
      "Iteration 769, loss = 0.56819356\n",
      "Iteration 770, loss = 0.56786244\n",
      "Iteration 771, loss = 0.56753193\n",
      "Iteration 772, loss = 0.56720201\n",
      "Iteration 773, loss = 0.56687267\n",
      "Iteration 774, loss = 0.56654390\n",
      "Iteration 775, loss = 0.56621569\n",
      "Iteration 776, loss = 0.56588804\n",
      "Iteration 777, loss = 0.56556094\n",
      "Iteration 778, loss = 0.56523438\n",
      "Iteration 779, loss = 0.56490835\n",
      "Iteration 780, loss = 0.56458285\n",
      "Iteration 781, loss = 0.56425786\n",
      "Iteration 782, loss = 0.56393339\n",
      "Iteration 783, loss = 0.56360943\n",
      "Iteration 784, loss = 0.56328596\n",
      "Iteration 785, loss = 0.56296298\n",
      "Iteration 786, loss = 0.56264049\n",
      "Iteration 787, loss = 0.56231847\n",
      "Iteration 788, loss = 0.56199693\n",
      "Iteration 789, loss = 0.56167499\n",
      "Iteration 790, loss = 0.56132940\n",
      "Iteration 791, loss = 0.56096813\n",
      "Iteration 792, loss = 0.56052140\n",
      "Iteration 793, loss = 0.55996284\n",
      "Iteration 794, loss = 0.55896129\n",
      "Iteration 795, loss = 0.55759320\n",
      "Iteration 796, loss = 0.55588308\n",
      "Iteration 797, loss = 0.55396307\n",
      "Iteration 798, loss = 0.55187884\n",
      "Iteration 799, loss = 0.54964802\n",
      "Iteration 800, loss = 0.54728658\n",
      "Iteration 801, loss = 0.54490652\n",
      "Iteration 802, loss = 0.54260339\n",
      "Iteration 803, loss = 0.54076399\n",
      "Iteration 804, loss = 0.54015050\n",
      "Iteration 805, loss = 0.53976899\n",
      "Iteration 806, loss = 0.53904704\n",
      "Iteration 807, loss = 0.53804927\n",
      "Iteration 808, loss = 0.53683713\n",
      "Iteration 809, loss = 0.53545470\n",
      "Iteration 810, loss = 0.53393378\n",
      "Iteration 811, loss = 0.53229633\n",
      "Iteration 812, loss = 0.53057532\n",
      "Iteration 813, loss = 0.52887062\n",
      "Iteration 814, loss = 0.52728336\n",
      "Iteration 815, loss = 0.52584685\n",
      "Iteration 816, loss = 0.52457162\n",
      "Iteration 817, loss = 0.52334561\n",
      "Iteration 818, loss = 0.52211485\n",
      "Iteration 819, loss = 0.52082890\n",
      "Iteration 820, loss = 0.51943764\n",
      "Iteration 821, loss = 0.51797122\n",
      "Iteration 822, loss = 0.51646653\n",
      "Iteration 823, loss = 0.51496472\n",
      "Iteration 824, loss = 0.51346571\n",
      "Iteration 825, loss = 0.51204171\n",
      "Iteration 826, loss = 0.51066217\n",
      "Iteration 827, loss = 0.50929286\n",
      "Iteration 828, loss = 0.50792252\n",
      "Iteration 829, loss = 0.50655206\n",
      "Iteration 830, loss = 0.50518956\n",
      "Iteration 831, loss = 0.50381791\n",
      "Iteration 832, loss = 0.50243893\n",
      "Iteration 833, loss = 0.50106158\n",
      "Iteration 834, loss = 0.49969380\n",
      "Iteration 835, loss = 0.49833132\n",
      "Iteration 836, loss = 0.49697478\n",
      "Iteration 837, loss = 0.49562474\n",
      "Iteration 838, loss = 0.49428175\n",
      "Iteration 839, loss = 0.49294624\n",
      "Iteration 840, loss = 0.49161862\n",
      "Iteration 841, loss = 0.49029925\n",
      "Iteration 842, loss = 0.48899164\n",
      "Iteration 843, loss = 0.48770704\n",
      "Iteration 844, loss = 0.48644066\n",
      "Iteration 845, loss = 0.48518668\n",
      "Iteration 846, loss = 0.48394408\n",
      "Iteration 847, loss = 0.48270671\n",
      "Iteration 848, loss = 0.48147202\n",
      "Iteration 849, loss = 0.48024421\n",
      "Iteration 850, loss = 0.47902818\n",
      "Iteration 851, loss = 0.47782632\n",
      "Iteration 852, loss = 0.47664182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 853, loss = 0.47547322\n",
      "Iteration 854, loss = 0.47431930\n",
      "Iteration 855, loss = 0.47317674\n",
      "Iteration 856, loss = 0.47204374\n",
      "Iteration 857, loss = 0.47092218\n",
      "Iteration 858, loss = 0.46981234\n",
      "Iteration 859, loss = 0.46872182\n",
      "Iteration 860, loss = 0.46764337\n",
      "Iteration 861, loss = 0.46657864\n",
      "Iteration 862, loss = 0.46552548\n",
      "Iteration 863, loss = 0.46448399\n",
      "Iteration 864, loss = 0.46345427\n",
      "Iteration 865, loss = 0.46243636\n",
      "Iteration 866, loss = 0.46143026\n",
      "Iteration 867, loss = 0.46043587\n",
      "Iteration 868, loss = 0.45945306\n",
      "Iteration 869, loss = 0.45848165\n",
      "Iteration 870, loss = 0.45752142\n",
      "Iteration 871, loss = 0.45657213\n",
      "Iteration 872, loss = 0.45563476\n",
      "Iteration 873, loss = 0.45470877\n",
      "Iteration 874, loss = 0.45379417\n",
      "Iteration 875, loss = 0.45289097\n",
      "Iteration 876, loss = 0.45199792\n",
      "Iteration 877, loss = 0.45111524\n",
      "Iteration 878, loss = 0.45024306\n",
      "Iteration 879, loss = 0.44938142\n",
      "Iteration 880, loss = 0.44853031\n",
      "Iteration 881, loss = 0.44768961\n",
      "Iteration 882, loss = 0.44685916\n",
      "Iteration 883, loss = 0.44604041\n",
      "Iteration 884, loss = 0.44523191\n",
      "Iteration 885, loss = 0.44443235\n",
      "Iteration 886, loss = 0.44364278\n",
      "Iteration 887, loss = 0.44286350\n",
      "Iteration 888, loss = 0.44209294\n",
      "Iteration 889, loss = 0.44133076\n",
      "Iteration 890, loss = 0.44057866\n",
      "Iteration 891, loss = 0.43983451\n",
      "Iteration 892, loss = 0.43909919\n",
      "Iteration 893, loss = 0.43837368\n",
      "Iteration 894, loss = 0.43766108\n",
      "Iteration 895, loss = 0.43695567\n",
      "Iteration 896, loss = 0.43625754\n",
      "Iteration 897, loss = 0.43556674\n",
      "Iteration 898, loss = 0.43488329\n",
      "Iteration 899, loss = 0.43420717\n",
      "Iteration 900, loss = 0.43353831\n",
      "Iteration 901, loss = 0.43287661\n",
      "Iteration 902, loss = 0.43222196\n",
      "Iteration 903, loss = 0.43157418\n",
      "Iteration 904, loss = 0.43093309\n",
      "Iteration 905, loss = 0.43029851\n",
      "Iteration 906, loss = 0.42967171\n",
      "Iteration 907, loss = 0.42905228\n",
      "Iteration 908, loss = 0.42843907\n",
      "Iteration 909, loss = 0.42783204\n",
      "Iteration 910, loss = 0.42723111\n",
      "Iteration 911, loss = 0.42663619\n",
      "Iteration 912, loss = 0.42604714\n",
      "Iteration 913, loss = 0.42546381\n",
      "Iteration 914, loss = 0.42488605\n",
      "Iteration 915, loss = 0.42431366\n",
      "Iteration 916, loss = 0.42374791\n",
      "Iteration 917, loss = 0.42318746\n",
      "Iteration 918, loss = 0.42263193\n",
      "Iteration 919, loss = 0.42208112\n",
      "Iteration 920, loss = 0.42153549\n",
      "Iteration 921, loss = 0.42099658\n",
      "Iteration 922, loss = 0.42046226\n",
      "Iteration 923, loss = 0.41993245\n",
      "Iteration 924, loss = 0.41940709\n",
      "Iteration 925, loss = 0.41888608\n",
      "Iteration 926, loss = 0.41836933\n",
      "Iteration 927, loss = 0.41785672\n",
      "Iteration 928, loss = 0.41734814\n",
      "Iteration 929, loss = 0.41684346\n",
      "Iteration 930, loss = 0.41634259\n",
      "Iteration 931, loss = 0.41584669\n",
      "Iteration 932, loss = 0.41535443\n",
      "Iteration 933, loss = 0.41486581\n",
      "Iteration 934, loss = 0.41438082\n",
      "Iteration 935, loss = 0.41389942\n",
      "Iteration 936, loss = 0.41342155\n",
      "Iteration 937, loss = 0.41294714\n",
      "Iteration 938, loss = 0.41247611\n",
      "Iteration 939, loss = 0.41200838\n",
      "Iteration 940, loss = 0.41154385\n",
      "Iteration 941, loss = 0.41108241\n",
      "Iteration 942, loss = 0.41062397\n",
      "Iteration 943, loss = 0.41016841\n",
      "Iteration 944, loss = 0.40971563\n",
      "Iteration 945, loss = 0.40926553\n",
      "Iteration 946, loss = 0.40881801\n",
      "Iteration 947, loss = 0.40837298\n",
      "Iteration 948, loss = 0.40793035\n",
      "Iteration 949, loss = 0.40749003\n",
      "Iteration 950, loss = 0.40705201\n",
      "Iteration 951, loss = 0.40661723\n",
      "Iteration 952, loss = 0.40618542\n",
      "Iteration 953, loss = 0.40575576\n",
      "Iteration 954, loss = 0.40532905\n",
      "Iteration 955, loss = 0.40490656\n",
      "Iteration 956, loss = 0.40448627\n",
      "Iteration 957, loss = 0.40406813\n",
      "Iteration 958, loss = 0.40365207\n",
      "Iteration 959, loss = 0.40323958\n",
      "Iteration 960, loss = 0.40283010\n",
      "Iteration 961, loss = 0.40242285\n",
      "Iteration 962, loss = 0.40201660\n",
      "Iteration 963, loss = 0.40161159\n",
      "Iteration 964, loss = 0.40120807\n",
      "Iteration 965, loss = 0.40080637\n",
      "Iteration 966, loss = 0.40040789\n",
      "Iteration 967, loss = 0.40001158\n",
      "Iteration 968, loss = 0.39961793\n",
      "Iteration 969, loss = 0.39922607\n",
      "Iteration 970, loss = 0.39883545\n",
      "Iteration 971, loss = 0.39844599\n",
      "Iteration 972, loss = 0.39805879\n",
      "Iteration 973, loss = 0.39767330\n",
      "Iteration 974, loss = 0.39729039\n",
      "Iteration 975, loss = 0.39690895\n",
      "Iteration 976, loss = 0.39652901\n",
      "Iteration 977, loss = 0.39615115\n",
      "Iteration 978, loss = 0.39577501\n",
      "Iteration 979, loss = 0.39540048\n",
      "Iteration 980, loss = 0.39502755\n",
      "Iteration 981, loss = 0.39465620\n",
      "Iteration 982, loss = 0.39428641\n",
      "Iteration 983, loss = 0.39391815\n",
      "Iteration 984, loss = 0.39355143\n",
      "Iteration 985, loss = 0.39318650\n",
      "Iteration 986, loss = 0.39282299\n",
      "Iteration 987, loss = 0.39246087\n",
      "Iteration 988, loss = 0.39210008\n",
      "Iteration 989, loss = 0.39174060\n",
      "Iteration 990, loss = 0.39138273\n",
      "Iteration 991, loss = 0.39102606\n",
      "Iteration 992, loss = 0.39067053\n",
      "Iteration 993, loss = 0.39031609\n",
      "Iteration 994, loss = 0.38996271\n",
      "Iteration 995, loss = 0.38961033\n",
      "Iteration 996, loss = 0.38925892\n",
      "Iteration 997, loss = 0.38890842\n",
      "Iteration 998, loss = 0.38855882\n",
      "Iteration 999, loss = 0.38821007\n",
      "Iteration 1000, loss = 0.38786215\n",
      "Iteration 1, loss = 2.14020736\n",
      "Iteration 2, loss = 2.12475445\n",
      "Iteration 3, loss = 2.10936026\n",
      "Iteration 4, loss = 2.09396273\n",
      "Iteration 5, loss = 2.07856020\n",
      "Iteration 6, loss = 2.06310077\n",
      "Iteration 7, loss = 2.04764752\n",
      "Iteration 8, loss = 2.03217010\n",
      "Iteration 9, loss = 2.01660377\n",
      "Iteration 10, loss = 2.00113382\n",
      "Iteration 11, loss = 1.98576877\n",
      "Iteration 12, loss = 1.97042756\n",
      "Iteration 13, loss = 1.95511943\n",
      "Iteration 14, loss = 1.93979712\n",
      "Iteration 15, loss = 1.92433246\n",
      "Iteration 16, loss = 1.90878213\n",
      "Iteration 17, loss = 1.89305085\n",
      "Iteration 18, loss = 1.87737231\n",
      "Iteration 19, loss = 1.86166983\n",
      "Iteration 20, loss = 1.84594972\n",
      "Iteration 21, loss = 1.83034201\n",
      "Iteration 22, loss = 1.81487075\n",
      "Iteration 23, loss = 1.79955261\n",
      "Iteration 24, loss = 1.78438191\n",
      "Iteration 25, loss = 1.76936498\n",
      "Iteration 26, loss = 1.75449991\n",
      "Iteration 27, loss = 1.73975881\n",
      "Iteration 28, loss = 1.72511449\n",
      "Iteration 29, loss = 1.71063888\n",
      "Iteration 30, loss = 1.69632624\n",
      "Iteration 31, loss = 1.68218017\n",
      "Iteration 32, loss = 1.66820388\n",
      "Iteration 33, loss = 1.65440021\n",
      "Iteration 34, loss = 1.64069844\n",
      "Iteration 35, loss = 1.62713138\n",
      "Iteration 36, loss = 1.61373768\n",
      "Iteration 37, loss = 1.60052000\n",
      "Iteration 38, loss = 1.58748059\n",
      "Iteration 39, loss = 1.57462135\n",
      "Iteration 40, loss = 1.56194385\n",
      "Iteration 41, loss = 1.54944987\n",
      "Iteration 42, loss = 1.53715988\n",
      "Iteration 43, loss = 1.52505633\n",
      "Iteration 44, loss = 1.51313957\n",
      "Iteration 45, loss = 1.50140970\n",
      "Iteration 46, loss = 1.48986666\n",
      "Iteration 47, loss = 1.47851020\n",
      "Iteration 48, loss = 1.46733991\n",
      "Iteration 49, loss = 1.45635522\n",
      "Iteration 50, loss = 1.44555544\n",
      "Iteration 51, loss = 1.43493975\n",
      "Iteration 52, loss = 1.42450722\n",
      "Iteration 53, loss = 1.41425682\n",
      "Iteration 54, loss = 1.40418745\n",
      "Iteration 55, loss = 1.39429789\n",
      "Iteration 56, loss = 1.38458689\n",
      "Iteration 57, loss = 1.37505309\n",
      "Iteration 58, loss = 1.36569509\n",
      "Iteration 59, loss = 1.35651140\n",
      "Iteration 60, loss = 1.34750051\n",
      "Iteration 61, loss = 1.33866082\n",
      "Iteration 62, loss = 1.32999066\n",
      "Iteration 63, loss = 1.32148833\n",
      "Iteration 64, loss = 1.31315207\n",
      "Iteration 65, loss = 1.30495789\n",
      "Iteration 66, loss = 1.29692091\n",
      "Iteration 67, loss = 1.28904338\n",
      "Iteration 68, loss = 1.28132358\n",
      "Iteration 69, loss = 1.27375971\n",
      "Iteration 70, loss = 1.26634985\n",
      "Iteration 71, loss = 1.25909203\n",
      "Iteration 72, loss = 1.25198417\n",
      "Iteration 73, loss = 1.24502415\n",
      "Iteration 74, loss = 1.23820975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 75, loss = 1.23153869\n",
      "Iteration 76, loss = 1.22500865\n",
      "Iteration 77, loss = 1.21861724\n",
      "Iteration 78, loss = 1.21236201\n",
      "Iteration 79, loss = 1.20624047\n",
      "Iteration 80, loss = 1.20025010\n",
      "Iteration 81, loss = 1.19438833\n",
      "Iteration 82, loss = 1.18864246\n",
      "Iteration 83, loss = 1.18301069\n",
      "Iteration 84, loss = 1.17748568\n",
      "Iteration 85, loss = 1.17207161\n",
      "Iteration 86, loss = 1.16676631\n",
      "Iteration 87, loss = 1.16154483\n",
      "Iteration 88, loss = 1.15641440\n",
      "Iteration 89, loss = 1.15136169\n",
      "Iteration 90, loss = 1.14636298\n",
      "Iteration 91, loss = 1.14146244\n",
      "Iteration 92, loss = 1.13665898\n",
      "Iteration 93, loss = 1.13195135\n",
      "Iteration 94, loss = 1.12733811\n",
      "Iteration 95, loss = 1.12281259\n",
      "Iteration 96, loss = 1.11836386\n",
      "Iteration 97, loss = 1.11399517\n",
      "Iteration 98, loss = 1.10970429\n",
      "Iteration 99, loss = 1.10548181\n",
      "Iteration 100, loss = 1.10133170\n",
      "Iteration 101, loss = 1.09724446\n",
      "Iteration 102, loss = 1.09320890\n",
      "Iteration 103, loss = 1.08924863\n",
      "Iteration 104, loss = 1.08535089\n",
      "Iteration 105, loss = 1.08149410\n",
      "Iteration 106, loss = 1.07769324\n",
      "Iteration 107, loss = 1.07395660\n",
      "Iteration 108, loss = 1.07027621\n",
      "Iteration 109, loss = 1.06666215\n",
      "Iteration 110, loss = 1.06309372\n",
      "Iteration 111, loss = 1.05957214\n",
      "Iteration 112, loss = 1.05610380\n",
      "Iteration 113, loss = 1.05267262\n",
      "Iteration 114, loss = 1.04929693\n",
      "Iteration 115, loss = 1.04596736\n",
      "Iteration 116, loss = 1.04269084\n",
      "Iteration 117, loss = 1.03945687\n",
      "Iteration 118, loss = 1.03626604\n",
      "Iteration 119, loss = 1.03311989\n",
      "Iteration 120, loss = 1.03002275\n",
      "Iteration 121, loss = 1.02697627\n",
      "Iteration 122, loss = 1.02397973\n",
      "Iteration 123, loss = 1.02103232\n",
      "Iteration 124, loss = 1.01813288\n",
      "Iteration 125, loss = 1.01526771\n",
      "Iteration 126, loss = 1.01244834\n",
      "Iteration 127, loss = 1.00967061\n",
      "Iteration 128, loss = 1.00693051\n",
      "Iteration 129, loss = 1.00422333\n",
      "Iteration 130, loss = 1.00155198\n",
      "Iteration 131, loss = 0.99891782\n",
      "Iteration 132, loss = 0.99632319\n",
      "Iteration 133, loss = 0.99376737\n",
      "Iteration 134, loss = 0.99124966\n",
      "Iteration 135, loss = 0.98876317\n",
      "Iteration 136, loss = 0.98630891\n",
      "Iteration 137, loss = 0.98389007\n",
      "Iteration 138, loss = 0.98150601\n",
      "Iteration 139, loss = 0.97915604\n",
      "Iteration 140, loss = 0.97683948\n",
      "Iteration 141, loss = 0.97455530\n",
      "Iteration 142, loss = 0.97229778\n",
      "Iteration 143, loss = 0.97007148\n",
      "Iteration 144, loss = 0.96787579\n",
      "Iteration 145, loss = 0.96571012\n",
      "Iteration 146, loss = 0.96357388\n",
      "Iteration 147, loss = 0.96146650\n",
      "Iteration 148, loss = 0.95938740\n",
      "Iteration 149, loss = 0.95733603\n",
      "Iteration 150, loss = 0.95531184\n",
      "Iteration 151, loss = 0.95331431\n",
      "Iteration 152, loss = 0.95134293\n",
      "Iteration 153, loss = 0.94939720\n",
      "Iteration 154, loss = 0.94747664\n",
      "Iteration 155, loss = 0.94558079\n",
      "Iteration 156, loss = 0.94370921\n",
      "Iteration 157, loss = 0.94186146\n",
      "Iteration 158, loss = 0.94003713\n",
      "Iteration 159, loss = 0.93823583\n",
      "Iteration 160, loss = 0.93645717\n",
      "Iteration 161, loss = 0.93470078\n",
      "Iteration 162, loss = 0.93296632\n",
      "Iteration 163, loss = 0.93125343\n",
      "Iteration 164, loss = 0.92956180\n",
      "Iteration 165, loss = 0.92789111\n",
      "Iteration 166, loss = 0.92624104\n",
      "Iteration 167, loss = 0.92461132\n",
      "Iteration 168, loss = 0.92300165\n",
      "Iteration 169, loss = 0.92141176\n",
      "Iteration 170, loss = 0.91984138\n",
      "Iteration 171, loss = 0.91829026\n",
      "Iteration 172, loss = 0.91675814\n",
      "Iteration 173, loss = 0.91524478\n",
      "Iteration 174, loss = 0.91374994\n",
      "Iteration 175, loss = 0.91227338\n",
      "Iteration 176, loss = 0.91081489\n",
      "Iteration 177, loss = 0.90937423\n",
      "Iteration 178, loss = 0.90795119\n",
      "Iteration 179, loss = 0.90654556\n",
      "Iteration 180, loss = 0.90515711\n",
      "Iteration 181, loss = 0.90378563\n",
      "Iteration 182, loss = 0.90243093\n",
      "Iteration 183, loss = 0.90109280\n",
      "Iteration 184, loss = 0.89977102\n",
      "Iteration 185, loss = 0.89846541\n",
      "Iteration 186, loss = 0.89717575\n",
      "Iteration 187, loss = 0.89590186\n",
      "Iteration 188, loss = 0.89464353\n",
      "Iteration 189, loss = 0.89340056\n",
      "Iteration 190, loss = 0.89217276\n",
      "Iteration 191, loss = 0.89095993\n",
      "Iteration 192, loss = 0.88976187\n",
      "Iteration 193, loss = 0.88857838\n",
      "Iteration 194, loss = 0.88740928\n",
      "Iteration 195, loss = 0.88625437\n",
      "Iteration 196, loss = 0.88511344\n",
      "Iteration 197, loss = 0.88398631\n",
      "Iteration 198, loss = 0.88287277\n",
      "Iteration 199, loss = 0.88177264\n",
      "Iteration 200, loss = 0.88068571\n",
      "Iteration 201, loss = 0.87961180\n",
      "Iteration 202, loss = 0.87855070\n",
      "Iteration 203, loss = 0.87750221\n",
      "Iteration 204, loss = 0.87646616\n",
      "Iteration 205, loss = 0.87544233\n",
      "Iteration 206, loss = 0.87443053\n",
      "Iteration 207, loss = 0.87343057\n",
      "Iteration 208, loss = 0.87244225\n",
      "Iteration 209, loss = 0.87146538\n",
      "Iteration 210, loss = 0.87049977\n",
      "Iteration 211, loss = 0.86954521\n",
      "Iteration 212, loss = 0.86860152\n",
      "Iteration 213, loss = 0.86766851\n",
      "Iteration 214, loss = 0.86674598\n",
      "Iteration 215, loss = 0.86583373\n",
      "Iteration 216, loss = 0.86493159\n",
      "Iteration 217, loss = 0.86403936\n",
      "Iteration 218, loss = 0.86315686\n",
      "Iteration 219, loss = 0.86228388\n",
      "Iteration 220, loss = 0.86142026\n",
      "Iteration 221, loss = 0.86056579\n",
      "Iteration 222, loss = 0.85972030\n",
      "Iteration 223, loss = 0.85888361\n",
      "Iteration 224, loss = 0.85805553\n",
      "Iteration 225, loss = 0.85723587\n",
      "Iteration 226, loss = 0.85642447\n",
      "Iteration 227, loss = 0.85562115\n",
      "Iteration 228, loss = 0.85482572\n",
      "Iteration 229, loss = 0.85403801\n",
      "Iteration 230, loss = 0.85325786\n",
      "Iteration 231, loss = 0.85248508\n",
      "Iteration 232, loss = 0.85171951\n",
      "Iteration 233, loss = 0.85096098\n",
      "Iteration 234, loss = 0.85020933\n",
      "Iteration 235, loss = 0.84946439\n",
      "Iteration 236, loss = 0.84872599\n",
      "Iteration 237, loss = 0.84799398\n",
      "Iteration 238, loss = 0.84726821\n",
      "Iteration 239, loss = 0.84654850\n",
      "Iteration 240, loss = 0.84583471\n",
      "Iteration 241, loss = 0.84512669\n",
      "Iteration 242, loss = 0.84442428\n",
      "Iteration 243, loss = 0.84372734\n",
      "Iteration 244, loss = 0.84303571\n",
      "Iteration 245, loss = 0.84234926\n",
      "Iteration 246, loss = 0.84166785\n",
      "Iteration 247, loss = 0.84099133\n",
      "Iteration 248, loss = 0.84031956\n",
      "Iteration 249, loss = 0.83965241\n",
      "Iteration 250, loss = 0.83898975\n",
      "Iteration 251, loss = 0.83833144\n",
      "Iteration 252, loss = 0.83767735\n",
      "Iteration 253, loss = 0.83702737\n",
      "Iteration 254, loss = 0.83638135\n",
      "Iteration 255, loss = 0.83573919\n",
      "Iteration 256, loss = 0.83510075\n",
      "Iteration 257, loss = 0.83446593\n",
      "Iteration 258, loss = 0.83383460\n",
      "Iteration 259, loss = 0.83320664\n",
      "Iteration 260, loss = 0.83258196\n",
      "Iteration 261, loss = 0.83196043\n",
      "Iteration 262, loss = 0.83134194\n",
      "Iteration 263, loss = 0.83072641\n",
      "Iteration 264, loss = 0.83011371\n",
      "Iteration 265, loss = 0.82950374\n",
      "Iteration 266, loss = 0.82889642\n",
      "Iteration 267, loss = 0.82829163\n",
      "Iteration 268, loss = 0.82768929\n",
      "Iteration 269, loss = 0.82708929\n",
      "Iteration 270, loss = 0.82649155\n",
      "Iteration 271, loss = 0.82589598\n",
      "Iteration 272, loss = 0.82530249\n",
      "Iteration 273, loss = 0.82471099\n",
      "Iteration 274, loss = 0.82412140\n",
      "Iteration 275, loss = 0.82353364\n",
      "Iteration 276, loss = 0.82294762\n",
      "Iteration 277, loss = 0.82236327\n",
      "Iteration 278, loss = 0.82178051\n",
      "Iteration 279, loss = 0.82119926\n",
      "Iteration 280, loss = 0.82061945\n",
      "Iteration 281, loss = 0.82004101\n",
      "Iteration 282, loss = 0.81946387\n",
      "Iteration 283, loss = 0.81888796\n",
      "Iteration 284, loss = 0.81831321\n",
      "Iteration 285, loss = 0.81773956\n",
      "Iteration 286, loss = 0.81716694\n",
      "Iteration 287, loss = 0.81659529\n",
      "Iteration 288, loss = 0.81602456\n",
      "Iteration 289, loss = 0.81545467\n",
      "Iteration 290, loss = 0.81488558\n",
      "Iteration 291, loss = 0.81431722\n",
      "Iteration 292, loss = 0.81374954\n",
      "Iteration 293, loss = 0.81318250\n",
      "Iteration 294, loss = 0.81261603\n",
      "Iteration 295, loss = 0.81205008\n",
      "Iteration 296, loss = 0.81148461\n",
      "Iteration 297, loss = 0.81091957\n",
      "Iteration 298, loss = 0.81035491\n",
      "Iteration 299, loss = 0.80979058\n",
      "Iteration 300, loss = 0.80922654\n",
      "Iteration 301, loss = 0.80866275\n",
      "Iteration 302, loss = 0.80809917\n",
      "Iteration 303, loss = 0.80753575\n",
      "Iteration 304, loss = 0.80697245\n",
      "Iteration 305, loss = 0.80640923\n",
      "Iteration 306, loss = 0.80584607\n",
      "Iteration 307, loss = 0.80528291\n",
      "Iteration 308, loss = 0.80471973\n",
      "Iteration 309, loss = 0.80415649\n",
      "Iteration 310, loss = 0.80359316\n",
      "Iteration 311, loss = 0.80302970\n",
      "Iteration 312, loss = 0.80246608\n",
      "Iteration 313, loss = 0.80190227\n",
      "Iteration 314, loss = 0.80133825\n",
      "Iteration 315, loss = 0.80077397\n",
      "Iteration 316, loss = 0.80020942\n",
      "Iteration 317, loss = 0.79964457\n",
      "Iteration 318, loss = 0.79907939\n",
      "Iteration 319, loss = 0.79851385\n",
      "Iteration 320, loss = 0.79794793\n",
      "Iteration 321, loss = 0.79738161\n",
      "Iteration 322, loss = 0.79681486\n",
      "Iteration 323, loss = 0.79624765\n",
      "Iteration 324, loss = 0.79567998\n",
      "Iteration 325, loss = 0.79511181\n",
      "Iteration 326, loss = 0.79454313\n",
      "Iteration 327, loss = 0.79397391\n",
      "Iteration 328, loss = 0.79340414\n",
      "Iteration 329, loss = 0.79283380\n",
      "Iteration 330, loss = 0.79226286\n",
      "Iteration 331, loss = 0.79169132\n",
      "Iteration 332, loss = 0.79111915\n",
      "Iteration 333, loss = 0.79054635\n",
      "Iteration 334, loss = 0.78997288\n",
      "Iteration 335, loss = 0.78939875\n",
      "Iteration 336, loss = 0.78882393\n",
      "Iteration 337, loss = 0.78824840\n",
      "Iteration 338, loss = 0.78767217\n",
      "Iteration 339, loss = 0.78710179\n",
      "Iteration 340, loss = 0.78653353\n",
      "Iteration 341, loss = 0.78596523\n",
      "Iteration 342, loss = 0.78539683\n",
      "Iteration 343, loss = 0.78483142\n",
      "Iteration 344, loss = 0.78426707\n",
      "Iteration 345, loss = 0.78370271\n",
      "Iteration 346, loss = 0.78313830\n",
      "Iteration 347, loss = 0.78257379\n",
      "Iteration 348, loss = 0.78200914\n",
      "Iteration 349, loss = 0.78144431\n",
      "Iteration 350, loss = 0.78087926\n",
      "Iteration 351, loss = 0.78031396\n",
      "Iteration 352, loss = 0.77974838\n",
      "Iteration 353, loss = 0.77918248\n",
      "Iteration 354, loss = 0.77861624\n",
      "Iteration 355, loss = 0.77804963\n",
      "Iteration 356, loss = 0.77748262\n",
      "Iteration 357, loss = 0.77691519\n",
      "Iteration 358, loss = 0.77634730\n",
      "Iteration 359, loss = 0.77577894\n",
      "Iteration 360, loss = 0.77521009\n",
      "Iteration 361, loss = 0.77464071\n",
      "Iteration 362, loss = 0.77407079\n",
      "Iteration 363, loss = 0.77350032\n",
      "Iteration 364, loss = 0.77292926\n",
      "Iteration 365, loss = 0.77235760\n",
      "Iteration 366, loss = 0.77178533\n",
      "Iteration 367, loss = 0.77121242\n",
      "Iteration 368, loss = 0.77063887\n",
      "Iteration 369, loss = 0.77006465\n",
      "Iteration 370, loss = 0.76948975\n",
      "Iteration 371, loss = 0.76891416\n",
      "Iteration 372, loss = 0.76833786\n",
      "Iteration 373, loss = 0.76776086\n",
      "Iteration 374, loss = 0.76718312\n",
      "Iteration 375, loss = 0.76660465\n",
      "Iteration 376, loss = 0.76602543\n",
      "Iteration 377, loss = 0.76544852\n",
      "Iteration 378, loss = 0.76487194\n",
      "Iteration 379, loss = 0.76429493\n",
      "Iteration 380, loss = 0.76371747\n",
      "Iteration 381, loss = 0.76313953\n",
      "Iteration 382, loss = 0.76256109\n",
      "Iteration 383, loss = 0.76198211\n",
      "Iteration 384, loss = 0.76140320\n",
      "Iteration 385, loss = 0.76082750\n",
      "Iteration 386, loss = 0.76025152\n",
      "Iteration 387, loss = 0.75967522\n",
      "Iteration 388, loss = 0.75909858\n",
      "Iteration 389, loss = 0.75852387\n",
      "Iteration 390, loss = 0.75795017\n",
      "Iteration 391, loss = 0.75737626\n",
      "Iteration 392, loss = 0.75680212\n",
      "Iteration 393, loss = 0.75622770\n",
      "Iteration 394, loss = 0.75565298\n",
      "Iteration 395, loss = 0.75507794\n",
      "Iteration 396, loss = 0.75450253\n",
      "Iteration 397, loss = 0.75392674\n",
      "Iteration 398, loss = 0.75335055\n",
      "Iteration 399, loss = 0.75277394\n",
      "Iteration 400, loss = 0.75219687\n",
      "Iteration 401, loss = 0.75161934\n",
      "Iteration 402, loss = 0.75104133\n",
      "Iteration 403, loss = 0.75046281\n",
      "Iteration 404, loss = 0.74988378\n",
      "Iteration 405, loss = 0.74930421\n",
      "Iteration 406, loss = 0.74872725\n",
      "Iteration 407, loss = 0.74815034\n",
      "Iteration 408, loss = 0.74757313\n",
      "Iteration 409, loss = 0.74699562\n",
      "Iteration 410, loss = 0.74641776\n",
      "Iteration 411, loss = 0.74583953\n",
      "Iteration 412, loss = 0.74526092\n",
      "Iteration 413, loss = 0.74468190\n",
      "Iteration 414, loss = 0.74410244\n",
      "Iteration 415, loss = 0.74352254\n",
      "Iteration 416, loss = 0.74294217\n",
      "Iteration 417, loss = 0.74236132\n",
      "Iteration 418, loss = 0.74177996\n",
      "Iteration 419, loss = 0.74119809\n",
      "Iteration 420, loss = 0.74061569\n",
      "Iteration 421, loss = 0.74003276\n",
      "Iteration 422, loss = 0.73944927\n",
      "Iteration 423, loss = 0.73886521\n",
      "Iteration 424, loss = 0.73828058\n",
      "Iteration 425, loss = 0.73769537\n",
      "Iteration 426, loss = 0.73710957\n",
      "Iteration 427, loss = 0.73652317\n",
      "Iteration 428, loss = 0.73593840\n",
      "Iteration 429, loss = 0.73535467\n",
      "Iteration 430, loss = 0.73477064\n",
      "Iteration 431, loss = 0.73418626\n",
      "Iteration 432, loss = 0.73360152\n",
      "Iteration 433, loss = 0.73301639\n",
      "Iteration 434, loss = 0.73243086\n",
      "Iteration 435, loss = 0.73184491\n",
      "Iteration 436, loss = 0.73125851\n",
      "Iteration 437, loss = 0.73067166\n",
      "Iteration 438, loss = 0.73008435\n",
      "Iteration 439, loss = 0.72949868\n",
      "Iteration 440, loss = 0.72891347\n",
      "Iteration 441, loss = 0.72832796\n",
      "Iteration 442, loss = 0.72774214\n",
      "Iteration 443, loss = 0.72715599\n",
      "Iteration 444, loss = 0.72656948\n",
      "Iteration 445, loss = 0.72598262\n",
      "Iteration 446, loss = 0.72539537\n",
      "Iteration 447, loss = 0.72480921\n",
      "Iteration 448, loss = 0.72422361\n",
      "Iteration 449, loss = 0.72363781\n",
      "Iteration 450, loss = 0.72305177\n",
      "Iteration 451, loss = 0.72246548\n",
      "Iteration 452, loss = 0.72187891\n",
      "Iteration 453, loss = 0.72129204\n",
      "Iteration 454, loss = 0.72070484\n",
      "Iteration 455, loss = 0.72011731\n",
      "Iteration 456, loss = 0.71952941\n",
      "Iteration 457, loss = 0.71894115\n",
      "Iteration 458, loss = 0.71835251\n",
      "Iteration 459, loss = 0.71776347\n",
      "Iteration 460, loss = 0.71717402\n",
      "Iteration 461, loss = 0.71658415\n",
      "Iteration 462, loss = 0.71599387\n",
      "Iteration 463, loss = 0.71540314\n",
      "Iteration 464, loss = 0.71481198\n",
      "Iteration 465, loss = 0.71422037\n",
      "Iteration 466, loss = 0.71362831\n",
      "Iteration 467, loss = 0.71303579\n",
      "Iteration 468, loss = 0.71244281\n",
      "Iteration 469, loss = 0.71184937\n",
      "Iteration 470, loss = 0.71125545\n",
      "Iteration 471, loss = 0.71066107\n",
      "Iteration 472, loss = 0.71006621\n",
      "Iteration 473, loss = 0.70947088\n",
      "Iteration 474, loss = 0.70887507\n",
      "Iteration 475, loss = 0.70827878\n",
      "Iteration 476, loss = 0.70768201\n",
      "Iteration 477, loss = 0.70708477\n",
      "Iteration 478, loss = 0.70648842\n",
      "Iteration 479, loss = 0.70589346\n",
      "Iteration 480, loss = 0.70530065\n",
      "Iteration 481, loss = 0.70470802\n",
      "Iteration 482, loss = 0.70411537\n",
      "Iteration 483, loss = 0.70352266\n",
      "Iteration 484, loss = 0.70292986\n",
      "Iteration 485, loss = 0.70233815\n",
      "Iteration 486, loss = 0.70174835\n",
      "Iteration 487, loss = 0.70115865\n",
      "Iteration 488, loss = 0.70056901\n",
      "Iteration 489, loss = 0.69997939\n",
      "Iteration 490, loss = 0.69938976\n",
      "Iteration 491, loss = 0.69880010\n",
      "Iteration 492, loss = 0.69821331\n",
      "Iteration 493, loss = 0.69762673\n",
      "Iteration 494, loss = 0.69704024\n",
      "Iteration 495, loss = 0.69645422\n",
      "Iteration 496, loss = 0.69587047\n",
      "Iteration 497, loss = 0.69528694\n",
      "Iteration 498, loss = 0.69470360\n",
      "Iteration 499, loss = 0.69412040\n",
      "Iteration 500, loss = 0.69353731\n",
      "Iteration 501, loss = 0.69295430\n",
      "Iteration 502, loss = 0.69237134\n",
      "Iteration 503, loss = 0.69178841\n",
      "Iteration 504, loss = 0.69120549\n",
      "Iteration 505, loss = 0.69062256\n",
      "Iteration 506, loss = 0.69004185\n",
      "Iteration 507, loss = 0.68946168\n",
      "Iteration 508, loss = 0.68888168\n",
      "Iteration 509, loss = 0.68830182\n",
      "Iteration 510, loss = 0.68772207\n",
      "Iteration 511, loss = 0.68714241\n",
      "Iteration 512, loss = 0.68656282\n",
      "Iteration 513, loss = 0.68598327\n",
      "Iteration 514, loss = 0.68540374\n",
      "Iteration 515, loss = 0.68482422\n",
      "Iteration 516, loss = 0.68424470\n",
      "Iteration 517, loss = 0.68366515\n",
      "Iteration 518, loss = 0.68308556\n",
      "Iteration 519, loss = 0.68250594\n",
      "Iteration 520, loss = 0.68192625\n",
      "Iteration 521, loss = 0.68134650\n",
      "Iteration 522, loss = 0.68076668\n",
      "Iteration 523, loss = 0.68018677\n",
      "Iteration 524, loss = 0.67960678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 525, loss = 0.67902669\n",
      "Iteration 526, loss = 0.67844650\n",
      "Iteration 527, loss = 0.67786621\n",
      "Iteration 528, loss = 0.67728581\n",
      "Iteration 529, loss = 0.67670529\n",
      "Iteration 530, loss = 0.67612466\n",
      "Iteration 531, loss = 0.67554391\n",
      "Iteration 532, loss = 0.67496304\n",
      "Iteration 533, loss = 0.67438205\n",
      "Iteration 534, loss = 0.67380093\n",
      "Iteration 535, loss = 0.67321969\n",
      "Iteration 536, loss = 0.67263832\n",
      "Iteration 537, loss = 0.67205948\n",
      "Iteration 538, loss = 0.67148268\n",
      "Iteration 539, loss = 0.67090751\n",
      "Iteration 540, loss = 0.67033272\n",
      "Iteration 541, loss = 0.66976029\n",
      "Iteration 542, loss = 0.66918935\n",
      "Iteration 543, loss = 0.66861891\n",
      "Iteration 544, loss = 0.66805125\n",
      "Iteration 545, loss = 0.66748487\n",
      "Iteration 546, loss = 0.66692379\n",
      "Iteration 547, loss = 0.66636497\n",
      "Iteration 548, loss = 0.66580707\n",
      "Iteration 549, loss = 0.66525259\n",
      "Iteration 550, loss = 0.66469929\n",
      "Iteration 551, loss = 0.66414692\n",
      "Iteration 552, loss = 0.66359545\n",
      "Iteration 553, loss = 0.66304484\n",
      "Iteration 554, loss = 0.66249506\n",
      "Iteration 555, loss = 0.66194607\n",
      "Iteration 556, loss = 0.66139944\n",
      "Iteration 557, loss = 0.66085436\n",
      "Iteration 558, loss = 0.66031015\n",
      "Iteration 559, loss = 0.65976679\n",
      "Iteration 560, loss = 0.65922423\n",
      "Iteration 561, loss = 0.65868244\n",
      "Iteration 562, loss = 0.65814250\n",
      "Iteration 563, loss = 0.65760456\n",
      "Iteration 564, loss = 0.65706749\n",
      "Iteration 565, loss = 0.65653124\n",
      "Iteration 566, loss = 0.65599578\n",
      "Iteration 567, loss = 0.65546108\n",
      "Iteration 568, loss = 0.65492709\n",
      "Iteration 569, loss = 0.65439428\n",
      "Iteration 570, loss = 0.65386443\n",
      "Iteration 571, loss = 0.65333532\n",
      "Iteration 572, loss = 0.65280861\n",
      "Iteration 573, loss = 0.65228341\n",
      "Iteration 574, loss = 0.65175905\n",
      "Iteration 575, loss = 0.65123549\n",
      "Iteration 576, loss = 0.65071269\n",
      "Iteration 577, loss = 0.65019063\n",
      "Iteration 578, loss = 0.64966925\n",
      "Iteration 579, loss = 0.64914853\n",
      "Iteration 580, loss = 0.64862845\n",
      "Iteration 581, loss = 0.64810898\n",
      "Iteration 582, loss = 0.64759009\n",
      "Iteration 583, loss = 0.64707177\n",
      "Iteration 584, loss = 0.64655399\n",
      "Iteration 585, loss = 0.64603674\n",
      "Iteration 586, loss = 0.64552000\n",
      "Iteration 587, loss = 0.64500375\n",
      "Iteration 588, loss = 0.64448971\n",
      "Iteration 589, loss = 0.64397921\n",
      "Iteration 590, loss = 0.64347118\n",
      "Iteration 591, loss = 0.64296769\n",
      "Iteration 592, loss = 0.64246527\n",
      "Iteration 593, loss = 0.64196389\n",
      "Iteration 594, loss = 0.64146352\n",
      "Iteration 595, loss = 0.64096411\n",
      "Iteration 596, loss = 0.64046565\n",
      "Iteration 597, loss = 0.63996980\n",
      "Iteration 598, loss = 0.63947508\n",
      "Iteration 599, loss = 0.63898455\n",
      "Iteration 600, loss = 0.63849552\n",
      "Iteration 601, loss = 0.63800876\n",
      "Iteration 602, loss = 0.63752361\n",
      "Iteration 603, loss = 0.63703995\n",
      "Iteration 604, loss = 0.63656108\n",
      "Iteration 605, loss = 0.63608420\n",
      "Iteration 606, loss = 0.63560822\n",
      "Iteration 607, loss = 0.63513351\n",
      "Iteration 608, loss = 0.63465964\n",
      "Iteration 609, loss = 0.63418821\n",
      "Iteration 610, loss = 0.63371803\n",
      "Iteration 611, loss = 0.63324895\n",
      "Iteration 612, loss = 0.63278075\n",
      "Iteration 613, loss = 0.63231346\n",
      "Iteration 614, loss = 0.63184814\n",
      "Iteration 615, loss = 0.63138407\n",
      "Iteration 616, loss = 0.63092146\n",
      "Iteration 617, loss = 0.63046015\n",
      "Iteration 618, loss = 0.63000009\n",
      "Iteration 619, loss = 0.62954128\n",
      "Iteration 620, loss = 0.62908370\n",
      "Iteration 621, loss = 0.62862734\n",
      "Iteration 622, loss = 0.62817219\n",
      "Iteration 623, loss = 0.62771823\n",
      "Iteration 624, loss = 0.62726544\n",
      "Iteration 625, loss = 0.62681395\n",
      "Iteration 626, loss = 0.62636385\n",
      "Iteration 627, loss = 0.62591479\n",
      "Iteration 628, loss = 0.62546679\n",
      "Iteration 629, loss = 0.62501985\n",
      "Iteration 630, loss = 0.62457366\n",
      "Iteration 631, loss = 0.62412837\n",
      "Iteration 632, loss = 0.62368515\n",
      "Iteration 633, loss = 0.62324332\n",
      "Iteration 634, loss = 0.62280276\n",
      "Iteration 635, loss = 0.62236322\n",
      "Iteration 636, loss = 0.62192468\n",
      "Iteration 637, loss = 0.62148711\n",
      "Iteration 638, loss = 0.62105051\n",
      "Iteration 639, loss = 0.62061484\n",
      "Iteration 640, loss = 0.62018009\n",
      "Iteration 641, loss = 0.61974625\n",
      "Iteration 642, loss = 0.61931328\n",
      "Iteration 643, loss = 0.61888119\n",
      "Iteration 644, loss = 0.61844995\n",
      "Iteration 645, loss = 0.61801954\n",
      "Iteration 646, loss = 0.61759003\n",
      "Iteration 647, loss = 0.61716135\n",
      "Iteration 648, loss = 0.61673342\n",
      "Iteration 649, loss = 0.61630624\n",
      "Iteration 650, loss = 0.61587981\n",
      "Iteration 651, loss = 0.61545441\n",
      "Iteration 652, loss = 0.61502977\n",
      "Iteration 653, loss = 0.61460583\n",
      "Iteration 654, loss = 0.61418260\n",
      "Iteration 655, loss = 0.61376008\n",
      "Iteration 656, loss = 0.61333824\n",
      "Iteration 657, loss = 0.61291706\n",
      "Iteration 658, loss = 0.61249744\n",
      "Iteration 659, loss = 0.61207932\n",
      "Iteration 660, loss = 0.61166200\n",
      "Iteration 661, loss = 0.61124558\n",
      "Iteration 662, loss = 0.61082996\n",
      "Iteration 663, loss = 0.61041508\n",
      "Iteration 664, loss = 0.61000092\n",
      "Iteration 665, loss = 0.60958747\n",
      "Iteration 666, loss = 0.60917474\n",
      "Iteration 667, loss = 0.60876270\n",
      "Iteration 668, loss = 0.60835134\n",
      "Iteration 669, loss = 0.60794067\n",
      "Iteration 670, loss = 0.60753066\n",
      "Iteration 671, loss = 0.60712132\n",
      "Iteration 672, loss = 0.60671262\n",
      "Iteration 673, loss = 0.60630466\n",
      "Iteration 674, loss = 0.60589724\n",
      "Iteration 675, loss = 0.60549036\n",
      "Iteration 676, loss = 0.60508416\n",
      "Iteration 677, loss = 0.60467855\n",
      "Iteration 678, loss = 0.60427353\n",
      "Iteration 679, loss = 0.60386908\n",
      "Iteration 680, loss = 0.60346520\n",
      "Iteration 681, loss = 0.60306188\n",
      "Iteration 682, loss = 0.60265912\n",
      "Iteration 683, loss = 0.60225689\n",
      "Iteration 684, loss = 0.60185849\n",
      "Iteration 685, loss = 0.60146395\n",
      "Iteration 686, loss = 0.60107032\n",
      "Iteration 687, loss = 0.60067757\n",
      "Iteration 688, loss = 0.60028569\n",
      "Iteration 689, loss = 0.59989562\n",
      "Iteration 690, loss = 0.59950717\n",
      "Iteration 691, loss = 0.59911960\n",
      "Iteration 692, loss = 0.59873291\n",
      "Iteration 693, loss = 0.59834711\n",
      "Iteration 694, loss = 0.59796220\n",
      "Iteration 695, loss = 0.59757817\n",
      "Iteration 696, loss = 0.59719501\n",
      "Iteration 697, loss = 0.59681274\n",
      "Iteration 698, loss = 0.59643132\n",
      "Iteration 699, loss = 0.59605076\n",
      "Iteration 700, loss = 0.59567103\n",
      "Iteration 701, loss = 0.59529213\n",
      "Iteration 702, loss = 0.59491403\n",
      "Iteration 703, loss = 0.59453672\n",
      "Iteration 704, loss = 0.59416018\n",
      "Iteration 705, loss = 0.59378478\n",
      "Iteration 706, loss = 0.59341085\n",
      "Iteration 707, loss = 0.59303775\n",
      "Iteration 708, loss = 0.59266546\n",
      "Iteration 709, loss = 0.59229397\n",
      "Iteration 710, loss = 0.59192326\n",
      "Iteration 711, loss = 0.59155332\n",
      "Iteration 712, loss = 0.59118412\n",
      "Iteration 713, loss = 0.59081565\n",
      "Iteration 714, loss = 0.59044789\n",
      "Iteration 715, loss = 0.59008083\n",
      "Iteration 716, loss = 0.58971445\n",
      "Iteration 717, loss = 0.58934874\n",
      "Iteration 718, loss = 0.58898368\n",
      "Iteration 719, loss = 0.58861926\n",
      "Iteration 720, loss = 0.58825547\n",
      "Iteration 721, loss = 0.58789229\n",
      "Iteration 722, loss = 0.58752972\n",
      "Iteration 723, loss = 0.58716774\n",
      "Iteration 724, loss = 0.58680634\n",
      "Iteration 725, loss = 0.58644551\n",
      "Iteration 726, loss = 0.58608526\n",
      "Iteration 727, loss = 0.58572555\n",
      "Iteration 728, loss = 0.58536721\n",
      "Iteration 729, loss = 0.58501026\n",
      "Iteration 730, loss = 0.58465400\n",
      "Iteration 731, loss = 0.58429855\n",
      "Iteration 732, loss = 0.58394390\n",
      "Iteration 733, loss = 0.58358987\n",
      "Iteration 734, loss = 0.58323645\n",
      "Iteration 735, loss = 0.58288364\n",
      "Iteration 736, loss = 0.58253144\n",
      "Iteration 737, loss = 0.58217984\n",
      "Iteration 738, loss = 0.58182883\n",
      "Iteration 739, loss = 0.58147842\n",
      "Iteration 740, loss = 0.58112937\n",
      "Iteration 741, loss = 0.58078175\n",
      "Iteration 742, loss = 0.58043568\n",
      "Iteration 743, loss = 0.58009067\n",
      "Iteration 744, loss = 0.57974636\n",
      "Iteration 745, loss = 0.57940273\n",
      "Iteration 746, loss = 0.57905980\n",
      "Iteration 747, loss = 0.57871755\n",
      "Iteration 748, loss = 0.57837598\n",
      "Iteration 749, loss = 0.57803519\n",
      "Iteration 750, loss = 0.57769501\n",
      "Iteration 751, loss = 0.57735542\n",
      "Iteration 752, loss = 0.57701650\n",
      "Iteration 753, loss = 0.57667830\n",
      "Iteration 754, loss = 0.57634075\n",
      "Iteration 755, loss = 0.57600384\n",
      "Iteration 756, loss = 0.57566757\n",
      "Iteration 757, loss = 0.57533192\n",
      "Iteration 758, loss = 0.57499689\n",
      "Iteration 759, loss = 0.57466247\n",
      "Iteration 760, loss = 0.57432864\n",
      "Iteration 761, loss = 0.57399541\n",
      "Iteration 762, loss = 0.57366275\n",
      "Iteration 763, loss = 0.57333066\n",
      "Iteration 764, loss = 0.57299913\n",
      "Iteration 765, loss = 0.57266816\n",
      "Iteration 766, loss = 0.57233772\n",
      "Iteration 767, loss = 0.57200783\n",
      "Iteration 768, loss = 0.57167846\n",
      "Iteration 769, loss = 0.57134961\n",
      "Iteration 770, loss = 0.57102128\n",
      "Iteration 771, loss = 0.57069345\n",
      "Iteration 772, loss = 0.57036613\n",
      "Iteration 773, loss = 0.57003929\n",
      "Iteration 774, loss = 0.56971429\n",
      "Iteration 775, loss = 0.56939007\n",
      "Iteration 776, loss = 0.56906646\n",
      "Iteration 777, loss = 0.56874344\n",
      "Iteration 778, loss = 0.56842102\n",
      "Iteration 779, loss = 0.56809918\n",
      "Iteration 780, loss = 0.56777790\n",
      "Iteration 781, loss = 0.56745719\n",
      "Iteration 782, loss = 0.56713702\n",
      "Iteration 783, loss = 0.56681740\n",
      "Iteration 784, loss = 0.56649831\n",
      "Iteration 785, loss = 0.56617976\n",
      "Iteration 786, loss = 0.56586171\n",
      "Iteration 787, loss = 0.56554419\n",
      "Iteration 788, loss = 0.56522716\n",
      "Iteration 789, loss = 0.56491063\n",
      "Iteration 790, loss = 0.56459459\n",
      "Iteration 791, loss = 0.56427903\n",
      "Iteration 792, loss = 0.56396395\n",
      "Iteration 793, loss = 0.56364934\n",
      "Iteration 794, loss = 0.56333519\n",
      "Iteration 795, loss = 0.56302030\n",
      "Iteration 796, loss = 0.56269097\n",
      "Iteration 797, loss = 0.56233909\n",
      "Iteration 798, loss = 0.56185931\n",
      "Iteration 799, loss = 0.56119405\n",
      "Iteration 800, loss = 0.56007975\n",
      "Iteration 801, loss = 0.55861701\n",
      "Iteration 802, loss = 0.55685416\n",
      "Iteration 803, loss = 0.55491677\n",
      "Iteration 804, loss = 0.55282685\n",
      "Iteration 805, loss = 0.55059937\n",
      "Iteration 806, loss = 0.54827849\n",
      "Iteration 807, loss = 0.54599670\n",
      "Iteration 808, loss = 0.54380890\n",
      "Iteration 809, loss = 0.54238408\n",
      "Iteration 810, loss = 0.54192690\n",
      "Iteration 811, loss = 0.54148820\n",
      "Iteration 812, loss = 0.54070005\n",
      "Iteration 813, loss = 0.53963939\n",
      "Iteration 814, loss = 0.53840023\n",
      "Iteration 815, loss = 0.53699072\n",
      "Iteration 816, loss = 0.53544957\n",
      "Iteration 817, loss = 0.53380767\n",
      "Iteration 818, loss = 0.53209727\n",
      "Iteration 819, loss = 0.53044744\n",
      "Iteration 820, loss = 0.52899431\n",
      "Iteration 821, loss = 0.52772839\n",
      "Iteration 822, loss = 0.52655715\n",
      "Iteration 823, loss = 0.52538415\n",
      "Iteration 824, loss = 0.52415794\n",
      "Iteration 825, loss = 0.52285317\n",
      "Iteration 826, loss = 0.52146926\n",
      "Iteration 827, loss = 0.52001757\n",
      "Iteration 828, loss = 0.51853649\n",
      "Iteration 829, loss = 0.51702421\n",
      "Iteration 830, loss = 0.51555688\n",
      "Iteration 831, loss = 0.51413310\n",
      "Iteration 832, loss = 0.51274532\n",
      "Iteration 833, loss = 0.51137181\n",
      "Iteration 834, loss = 0.51003011\n",
      "Iteration 835, loss = 0.50871772\n",
      "Iteration 836, loss = 0.50737135\n",
      "Iteration 837, loss = 0.50598927\n",
      "Iteration 838, loss = 0.50458424\n",
      "Iteration 839, loss = 0.50321108\n",
      "Iteration 840, loss = 0.50187069\n",
      "Iteration 841, loss = 0.50054454\n",
      "Iteration 842, loss = 0.49922399\n",
      "Iteration 843, loss = 0.49790949\n",
      "Iteration 844, loss = 0.49660151\n",
      "Iteration 845, loss = 0.49530058\n",
      "Iteration 846, loss = 0.49400721\n",
      "Iteration 847, loss = 0.49272192\n",
      "Iteration 848, loss = 0.49144521\n",
      "Iteration 849, loss = 0.49018008\n",
      "Iteration 850, loss = 0.48892389\n",
      "Iteration 851, loss = 0.48767716\n",
      "Iteration 852, loss = 0.48644050\n",
      "Iteration 853, loss = 0.48521589\n",
      "Iteration 854, loss = 0.48400284\n",
      "Iteration 855, loss = 0.48280140\n",
      "Iteration 856, loss = 0.48161182\n",
      "Iteration 857, loss = 0.48043429\n",
      "Iteration 858, loss = 0.47926893\n",
      "Iteration 859, loss = 0.47811578\n",
      "Iteration 860, loss = 0.47697480\n",
      "Iteration 861, loss = 0.47584592\n",
      "Iteration 862, loss = 0.47472901\n",
      "Iteration 863, loss = 0.47362389\n",
      "Iteration 864, loss = 0.47253036\n",
      "Iteration 865, loss = 0.47144822\n",
      "Iteration 866, loss = 0.47037720\n",
      "Iteration 867, loss = 0.46931708\n",
      "Iteration 868, loss = 0.46826761\n",
      "Iteration 869, loss = 0.46722853\n",
      "Iteration 870, loss = 0.46619959\n",
      "Iteration 871, loss = 0.46518128\n",
      "Iteration 872, loss = 0.46417796\n",
      "Iteration 873, loss = 0.46318507\n",
      "Iteration 874, loss = 0.46220696\n",
      "Iteration 875, loss = 0.46123906\n",
      "Iteration 876, loss = 0.46028156\n",
      "Iteration 877, loss = 0.45933669\n",
      "Iteration 878, loss = 0.45840141\n",
      "Iteration 879, loss = 0.45747602\n",
      "Iteration 880, loss = 0.45656032\n",
      "Iteration 881, loss = 0.45565513\n",
      "Iteration 882, loss = 0.45476048\n",
      "Iteration 883, loss = 0.45387632\n",
      "Iteration 884, loss = 0.45300255\n",
      "Iteration 885, loss = 0.45213899\n",
      "Iteration 886, loss = 0.45128543\n",
      "Iteration 887, loss = 0.45044159\n",
      "Iteration 888, loss = 0.44960721\n",
      "Iteration 889, loss = 0.44878195\n",
      "Iteration 890, loss = 0.44796552\n",
      "Iteration 891, loss = 0.44715822\n",
      "Iteration 892, loss = 0.44636112\n",
      "Iteration 893, loss = 0.44557261\n",
      "Iteration 894, loss = 0.44479312\n",
      "Iteration 895, loss = 0.44402347\n",
      "Iteration 896, loss = 0.44326224\n",
      "Iteration 897, loss = 0.44250924\n",
      "Iteration 898, loss = 0.44176425\n",
      "Iteration 899, loss = 0.44102706\n",
      "Iteration 900, loss = 0.44029881\n",
      "Iteration 901, loss = 0.43958041\n",
      "Iteration 902, loss = 0.43886932\n",
      "Iteration 903, loss = 0.43816525\n",
      "Iteration 904, loss = 0.43746794\n",
      "Iteration 905, loss = 0.43677856\n",
      "Iteration 906, loss = 0.43609713\n",
      "Iteration 907, loss = 0.43542226\n",
      "Iteration 908, loss = 0.43475379\n",
      "Iteration 909, loss = 0.43409161\n",
      "Iteration 910, loss = 0.43343555\n",
      "Iteration 911, loss = 0.43278546\n",
      "Iteration 912, loss = 0.43214117\n",
      "Iteration 913, loss = 0.43150253\n",
      "Iteration 914, loss = 0.43087196\n",
      "Iteration 915, loss = 0.43024685\n",
      "Iteration 916, loss = 0.42962720\n",
      "Iteration 917, loss = 0.42901295\n",
      "Iteration 918, loss = 0.42840486\n",
      "Iteration 919, loss = 0.42780289\n",
      "Iteration 920, loss = 0.42720583\n",
      "Iteration 921, loss = 0.42661435\n",
      "Iteration 922, loss = 0.42602893\n",
      "Iteration 923, loss = 0.42544881\n",
      "Iteration 924, loss = 0.42487400\n",
      "Iteration 925, loss = 0.42430459\n",
      "Iteration 926, loss = 0.42374060\n",
      "Iteration 927, loss = 0.42318223\n",
      "Iteration 928, loss = 0.42263005\n",
      "Iteration 929, loss = 0.42208281\n",
      "Iteration 930, loss = 0.42154041\n",
      "Iteration 931, loss = 0.42100223\n",
      "Iteration 932, loss = 0.42046842\n",
      "Iteration 933, loss = 0.41993887\n",
      "Iteration 934, loss = 0.41941437\n",
      "Iteration 935, loss = 0.41889376\n",
      "Iteration 936, loss = 0.41837753\n",
      "Iteration 937, loss = 0.41786542\n",
      "Iteration 938, loss = 0.41735840\n",
      "Iteration 939, loss = 0.41685584\n",
      "Iteration 940, loss = 0.41635679\n",
      "Iteration 941, loss = 0.41586120\n",
      "Iteration 942, loss = 0.41536901\n",
      "Iteration 943, loss = 0.41488017\n",
      "Iteration 944, loss = 0.41439502\n",
      "Iteration 945, loss = 0.41391368\n",
      "Iteration 946, loss = 0.41343551\n",
      "Iteration 947, loss = 0.41296042\n",
      "Iteration 948, loss = 0.41248831\n",
      "Iteration 949, loss = 0.41202025\n",
      "Iteration 950, loss = 0.41155521\n",
      "Iteration 951, loss = 0.41109284\n",
      "Iteration 952, loss = 0.41063318\n",
      "Iteration 953, loss = 0.41017625\n",
      "Iteration 954, loss = 0.40972273\n",
      "Iteration 955, loss = 0.40927234\n",
      "Iteration 956, loss = 0.40882471\n",
      "Iteration 957, loss = 0.40837980\n",
      "Iteration 958, loss = 0.40793754\n",
      "Iteration 959, loss = 0.40749786\n",
      "Iteration 960, loss = 0.40706068\n",
      "Iteration 961, loss = 0.40662654\n",
      "Iteration 962, loss = 0.40619508\n",
      "Iteration 963, loss = 0.40576592\n",
      "Iteration 964, loss = 0.40533938\n",
      "Iteration 965, loss = 0.40491542\n",
      "Iteration 966, loss = 0.40449354\n",
      "Iteration 967, loss = 0.40407365\n",
      "Iteration 968, loss = 0.40365641\n",
      "Iteration 969, loss = 0.40324140\n",
      "Iteration 970, loss = 0.40282823\n",
      "Iteration 971, loss = 0.40241692\n",
      "Iteration 972, loss = 0.40200745\n",
      "Iteration 973, loss = 0.40159981\n",
      "Iteration 974, loss = 0.40119398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 975, loss = 0.40078992\n",
      "Iteration 976, loss = 0.40038760\n",
      "Iteration 977, loss = 0.39998698\n",
      "Iteration 978, loss = 0.39958801\n",
      "Iteration 979, loss = 0.39919064\n",
      "Iteration 980, loss = 0.39879483\n",
      "Iteration 981, loss = 0.39840052\n",
      "Iteration 982, loss = 0.39800766\n",
      "Iteration 983, loss = 0.39761620\n",
      "Iteration 984, loss = 0.39722608\n",
      "Iteration 985, loss = 0.39683726\n",
      "Iteration 986, loss = 0.39644968\n",
      "Iteration 987, loss = 0.39606330\n",
      "Iteration 988, loss = 0.39567809\n",
      "Iteration 989, loss = 0.39529398\n",
      "Iteration 990, loss = 0.39491096\n",
      "Iteration 991, loss = 0.39452897\n",
      "Iteration 992, loss = 0.39414800\n",
      "Iteration 993, loss = 0.39376952\n",
      "Iteration 994, loss = 0.39339258\n",
      "Iteration 995, loss = 0.39301661\n",
      "Iteration 996, loss = 0.39264163\n",
      "Iteration 997, loss = 0.39226765\n",
      "Iteration 998, loss = 0.39189466\n",
      "Iteration 999, loss = 0.39152266\n",
      "Iteration 1000, loss = 0.39115166\n",
      "Iteration 1, loss = 2.13352491\n",
      "Iteration 2, loss = 2.11810970\n",
      "Iteration 3, loss = 2.10275324\n",
      "Iteration 4, loss = 2.08731218\n",
      "Iteration 5, loss = 2.07177272\n",
      "Iteration 6, loss = 2.05618262\n",
      "Iteration 7, loss = 2.04060877\n",
      "Iteration 8, loss = 2.02508809\n",
      "Iteration 9, loss = 2.00959822\n",
      "Iteration 10, loss = 1.99421452\n",
      "Iteration 11, loss = 1.97894124\n",
      "Iteration 12, loss = 1.96370323\n",
      "Iteration 13, loss = 1.94857276\n",
      "Iteration 14, loss = 1.93343740\n",
      "Iteration 15, loss = 1.91815944\n",
      "Iteration 16, loss = 1.90286604\n",
      "Iteration 17, loss = 1.88752887\n",
      "Iteration 18, loss = 1.87221640\n",
      "Iteration 19, loss = 1.85683921\n",
      "Iteration 20, loss = 1.84143236\n",
      "Iteration 21, loss = 1.82599611\n",
      "Iteration 22, loss = 1.81062059\n",
      "Iteration 23, loss = 1.79538258\n",
      "Iteration 24, loss = 1.78028105\n",
      "Iteration 25, loss = 1.76533609\n",
      "Iteration 26, loss = 1.75054185\n",
      "Iteration 27, loss = 1.73587255\n",
      "Iteration 28, loss = 1.72126616\n",
      "Iteration 29, loss = 1.70682203\n",
      "Iteration 30, loss = 1.69253779\n",
      "Iteration 31, loss = 1.67841542\n",
      "Iteration 32, loss = 1.66445844\n",
      "Iteration 33, loss = 1.65066992\n",
      "Iteration 34, loss = 1.63699046\n",
      "Iteration 35, loss = 1.62343115\n",
      "Iteration 36, loss = 1.61006072\n",
      "Iteration 37, loss = 1.59686440\n",
      "Iteration 38, loss = 1.58384439\n",
      "Iteration 39, loss = 1.57100250\n",
      "Iteration 40, loss = 1.55834020\n",
      "Iteration 41, loss = 1.54585866\n",
      "Iteration 42, loss = 1.53357735\n",
      "Iteration 43, loss = 1.52148095\n",
      "Iteration 44, loss = 1.50956900\n",
      "Iteration 45, loss = 1.49784153\n",
      "Iteration 46, loss = 1.48629841\n",
      "Iteration 47, loss = 1.47493932\n",
      "Iteration 48, loss = 1.46376381\n",
      "Iteration 49, loss = 1.45277131\n",
      "Iteration 50, loss = 1.44196111\n",
      "Iteration 51, loss = 1.43133241\n",
      "Iteration 52, loss = 1.42088432\n",
      "Iteration 53, loss = 1.41061586\n",
      "Iteration 54, loss = 1.40052598\n",
      "Iteration 55, loss = 1.39061356\n",
      "Iteration 56, loss = 1.38087741\n",
      "Iteration 57, loss = 1.37131629\n",
      "Iteration 58, loss = 1.36192888\n",
      "Iteration 59, loss = 1.35271383\n",
      "Iteration 60, loss = 1.34366970\n",
      "Iteration 61, loss = 1.33479502\n",
      "Iteration 62, loss = 1.32608824\n",
      "Iteration 63, loss = 1.31754775\n",
      "Iteration 64, loss = 1.30917190\n",
      "Iteration 65, loss = 1.30095895\n",
      "Iteration 66, loss = 1.29290712\n",
      "Iteration 67, loss = 1.28501456\n",
      "Iteration 68, loss = 1.27727935\n",
      "Iteration 69, loss = 1.26969952\n",
      "Iteration 70, loss = 1.26227304\n",
      "Iteration 71, loss = 1.25499779\n",
      "Iteration 72, loss = 1.24787164\n",
      "Iteration 73, loss = 1.24089236\n",
      "Iteration 74, loss = 1.23405769\n",
      "Iteration 75, loss = 1.22736532\n",
      "Iteration 76, loss = 1.22081288\n",
      "Iteration 77, loss = 1.21439797\n",
      "Iteration 78, loss = 1.20811813\n",
      "Iteration 79, loss = 1.20197087\n",
      "Iteration 80, loss = 1.19595368\n",
      "Iteration 81, loss = 1.19006400\n",
      "Iteration 82, loss = 1.18429926\n",
      "Iteration 83, loss = 1.17865686\n",
      "Iteration 84, loss = 1.17311851\n",
      "Iteration 85, loss = 1.16769308\n",
      "Iteration 86, loss = 1.16237339\n",
      "Iteration 87, loss = 1.15712294\n",
      "Iteration 88, loss = 1.15196117\n",
      "Iteration 89, loss = 1.14688766\n",
      "Iteration 90, loss = 1.14188277\n",
      "Iteration 91, loss = 1.13699055\n",
      "Iteration 92, loss = 1.13219596\n",
      "Iteration 93, loss = 1.12749737\n",
      "Iteration 94, loss = 1.12289071\n",
      "Iteration 95, loss = 1.11836132\n",
      "Iteration 96, loss = 1.11392170\n",
      "Iteration 97, loss = 1.10956108\n",
      "Iteration 98, loss = 1.10527785\n",
      "Iteration 99, loss = 1.10106235\n",
      "Iteration 100, loss = 1.09691855\n",
      "Iteration 101, loss = 1.09283637\n",
      "Iteration 102, loss = 1.08880519\n",
      "Iteration 103, loss = 1.08484800\n",
      "Iteration 104, loss = 1.08096381\n",
      "Iteration 105, loss = 1.07713497\n",
      "Iteration 106, loss = 1.07336474\n",
      "Iteration 107, loss = 1.06965951\n",
      "Iteration 108, loss = 1.06600600\n",
      "Iteration 109, loss = 1.06241749\n",
      "Iteration 110, loss = 1.05887211\n",
      "Iteration 111, loss = 1.05537067\n",
      "Iteration 112, loss = 1.05191952\n",
      "Iteration 113, loss = 1.04849351\n",
      "Iteration 114, loss = 1.04511444\n",
      "Iteration 115, loss = 1.04177201\n",
      "Iteration 116, loss = 1.03847087\n",
      "Iteration 117, loss = 1.03522139\n",
      "Iteration 118, loss = 1.03201383\n",
      "Iteration 119, loss = 1.02885521\n",
      "Iteration 120, loss = 1.02574648\n",
      "Iteration 121, loss = 1.02267887\n",
      "Iteration 122, loss = 1.01965690\n",
      "Iteration 123, loss = 1.01668245\n",
      "Iteration 124, loss = 1.01375485\n",
      "Iteration 125, loss = 1.01086561\n",
      "Iteration 126, loss = 1.00801647\n",
      "Iteration 127, loss = 1.00521122\n",
      "Iteration 128, loss = 1.00244837\n",
      "Iteration 129, loss = 0.99972246\n",
      "Iteration 130, loss = 0.99703064\n",
      "Iteration 131, loss = 0.99437124\n",
      "Iteration 132, loss = 0.99175073\n",
      "Iteration 133, loss = 0.98916847\n",
      "Iteration 134, loss = 0.98662376\n",
      "Iteration 135, loss = 0.98411510\n",
      "Iteration 136, loss = 0.98163277\n",
      "Iteration 137, loss = 0.97918540\n",
      "Iteration 138, loss = 0.97677239\n",
      "Iteration 139, loss = 0.97439099\n",
      "Iteration 140, loss = 0.97203724\n",
      "Iteration 141, loss = 0.96971567\n",
      "Iteration 142, loss = 0.96742571\n",
      "Iteration 143, loss = 0.96516680\n",
      "Iteration 144, loss = 0.96293836\n",
      "Iteration 145, loss = 0.96073982\n",
      "Iteration 146, loss = 0.95857061\n",
      "Iteration 147, loss = 0.95643016\n",
      "Iteration 148, loss = 0.95431774\n",
      "Iteration 149, loss = 0.95222745\n",
      "Iteration 150, loss = 0.95016416\n",
      "Iteration 151, loss = 0.94812740\n",
      "Iteration 152, loss = 0.94611674\n",
      "Iteration 153, loss = 0.94413174\n",
      "Iteration 154, loss = 0.94217197\n",
      "Iteration 155, loss = 0.94023701\n",
      "Iteration 156, loss = 0.93832645\n",
      "Iteration 157, loss = 0.93643991\n",
      "Iteration 158, loss = 0.93457697\n",
      "Iteration 159, loss = 0.93273541\n",
      "Iteration 160, loss = 0.93091396\n",
      "Iteration 161, loss = 0.92911494\n",
      "Iteration 162, loss = 0.92733806\n",
      "Iteration 163, loss = 0.92558302\n",
      "Iteration 164, loss = 0.92384955\n",
      "Iteration 165, loss = 0.92213737\n",
      "Iteration 166, loss = 0.92044621\n",
      "Iteration 167, loss = 0.91877579\n",
      "Iteration 168, loss = 0.91712585\n",
      "Iteration 169, loss = 0.91549614\n",
      "Iteration 170, loss = 0.91388639\n",
      "Iteration 171, loss = 0.91229637\n",
      "Iteration 172, loss = 0.91072582\n",
      "Iteration 173, loss = 0.90917450\n",
      "Iteration 174, loss = 0.90764218\n",
      "Iteration 175, loss = 0.90612862\n",
      "Iteration 176, loss = 0.90463359\n",
      "Iteration 177, loss = 0.90315686\n",
      "Iteration 178, loss = 0.90169822\n",
      "Iteration 179, loss = 0.90025744\n",
      "Iteration 180, loss = 0.89883429\n",
      "Iteration 181, loss = 0.89742858\n",
      "Iteration 182, loss = 0.89604007\n",
      "Iteration 183, loss = 0.89466856\n",
      "Iteration 184, loss = 0.89331384\n",
      "Iteration 185, loss = 0.89197570\n",
      "Iteration 186, loss = 0.89065393\n",
      "Iteration 187, loss = 0.88934832\n",
      "Iteration 188, loss = 0.88805867\n",
      "Iteration 189, loss = 0.88678477\n",
      "Iteration 190, loss = 0.88552642\n",
      "Iteration 191, loss = 0.88428341\n",
      "Iteration 192, loss = 0.88305555\n",
      "Iteration 193, loss = 0.88184262\n",
      "Iteration 194, loss = 0.88064442"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 195, loss = 0.87946076\n",
      "Iteration 196, loss = 0.87829142\n",
      "Iteration 197, loss = 0.87713622\n",
      "Iteration 198, loss = 0.87599494\n",
      "Iteration 199, loss = 0.87486738\n",
      "Iteration 200, loss = 0.87375334\n",
      "Iteration 201, loss = 0.87265263\n",
      "Iteration 202, loss = 0.87156503\n",
      "Iteration 203, loss = 0.87049035\n",
      "Iteration 204, loss = 0.86942839\n",
      "Iteration 205, loss = 0.86837895\n",
      "Iteration 206, loss = 0.86734182\n",
      "Iteration 207, loss = 0.86631681\n",
      "Iteration 208, loss = 0.86530371\n",
      "Iteration 209, loss = 0.86430233\n",
      "Iteration 210, loss = 0.86331247\n",
      "Iteration 211, loss = 0.86233392\n",
      "Iteration 212, loss = 0.86136650\n",
      "Iteration 213, loss = 0.86041000\n",
      "Iteration 214, loss = 0.85946423\n",
      "Iteration 215, loss = 0.85852899\n",
      "Iteration 216, loss = 0.85760409\n",
      "Iteration 217, loss = 0.85668933\n",
      "Iteration 218, loss = 0.85578452\n",
      "Iteration 219, loss = 0.85488948\n",
      "Iteration 220, loss = 0.85400399\n",
      "Iteration 221, loss = 0.85312789\n",
      "Iteration 222, loss = 0.85226097\n",
      "Iteration 223, loss = 0.85140306\n",
      "Iteration 224, loss = 0.85055396\n",
      "Iteration 225, loss = 0.84971350\n",
      "Iteration 226, loss = 0.84888148\n",
      "Iteration 227, loss = 0.84805773\n",
      "Iteration 228, loss = 0.84724206\n",
      "Iteration 229, loss = 0.84643431\n",
      "Iteration 230, loss = 0.84563428\n",
      "Iteration 231, loss = 0.84484181\n",
      "Iteration 232, loss = 0.84405673\n",
      "Iteration 233, loss = 0.84327886\n",
      "Iteration 234, loss = 0.84250803\n",
      "Iteration 235, loss = 0.84174407\n",
      "Iteration 236, loss = 0.84098682\n",
      "Iteration 237, loss = 0.84023612\n",
      "Iteration 238, loss = 0.83949180\n",
      "Iteration 239, loss = 0.83875371\n",
      "Iteration 240, loss = 0.83802167\n",
      "Iteration 241, loss = 0.83729555\n",
      "Iteration 242, loss = 0.83657519\n",
      "Iteration 243, loss = 0.83586042\n",
      "Iteration 244, loss = 0.83515112\n",
      "Iteration 245, loss = 0.83444712\n",
      "Iteration 246, loss = 0.83374828\n",
      "Iteration 247, loss = 0.83305446\n",
      "Iteration 248, loss = 0.83236552\n",
      "Iteration 249, loss = 0.83168132\n",
      "Iteration 250, loss = 0.83100172\n",
      "Iteration 251, loss = 0.83032659\n",
      "Iteration 252, loss = 0.82965580\n",
      "Iteration 253, loss = 0.82898921\n",
      "Iteration 254, loss = 0.82832670\n",
      "Iteration 255, loss = 0.82766815\n",
      "Iteration 256, loss = 0.82701343\n",
      "Iteration 257, loss = 0.82636242\n",
      "Iteration 258, loss = 0.82571499\n",
      "Iteration 259, loss = 0.82507105\n",
      "Iteration 260, loss = 0.82443046\n",
      "Iteration 261, loss = 0.82379311\n",
      "Iteration 262, loss = 0.82315891\n",
      "Iteration 263, loss = 0.82252773\n",
      "Iteration 264, loss = 0.82189947\n",
      "Iteration 265, loss = 0.82127403\n",
      "Iteration 266, loss = 0.82065131\n",
      "Iteration 267, loss = 0.82003120\n",
      "Iteration 268, loss = 0.81941361\n",
      "Iteration 269, loss = 0.81879844\n",
      "Iteration 270, loss = 0.81818560\n",
      "Iteration 271, loss = 0.81757500\n",
      "Iteration 272, loss = 0.81696654\n",
      "Iteration 273, loss = 0.81636014\n",
      "Iteration 274, loss = 0.81575572\n",
      "Iteration 275, loss = 0.81515318\n",
      "Iteration 276, loss = 0.81455245\n",
      "Iteration 277, loss = 0.81395344\n",
      "Iteration 278, loss = 0.81335608\n",
      "Iteration 279, loss = 0.81276029\n",
      "Iteration 280, loss = 0.81216599\n",
      "Iteration 281, loss = 0.81157312\n",
      "Iteration 282, loss = 0.81098159\n",
      "Iteration 283, loss = 0.81039135\n",
      "Iteration 284, loss = 0.80980231\n",
      "Iteration 285, loss = 0.80921442\n",
      "Iteration 286, loss = 0.80862761\n",
      "Iteration 287, loss = 0.80804181\n",
      "Iteration 288, loss = 0.80745697\n",
      "Iteration 289, loss = 0.80687302\n",
      "Iteration 290, loss = 0.80628990\n",
      "Iteration 291, loss = 0.80570756\n",
      "Iteration 292, loss = 0.80512594\n",
      "Iteration 293, loss = 0.80454499\n",
      "Iteration 294, loss = 0.80396464\n",
      "Iteration 295, loss = 0.80338486\n",
      "Iteration 296, loss = 0.80280559\n",
      "Iteration 297, loss = 0.80222679\n",
      "Iteration 298, loss = 0.80164839\n",
      "Iteration 299, loss = 0.80107036\n",
      "Iteration 300, loss = 0.80049265\n",
      "Iteration 301, loss = 0.79991522\n",
      "Iteration 302, loss = 0.79933802\n",
      "Iteration 303, loss = 0.79876102\n",
      "Iteration 304, loss = 0.79818416\n",
      "Iteration 305, loss = 0.79760742\n",
      "Iteration 306, loss = 0.79703344\n",
      "Iteration 307, loss = 0.79646009\n",
      "Iteration 308, loss = 0.79588704\n",
      "Iteration 309, loss = 0.79531425\n",
      "Iteration 310, loss = 0.79474167\n",
      "Iteration 311, loss = 0.79416925\n",
      "Iteration 312, loss = 0.79359695\n",
      "Iteration 313, loss = 0.79302472\n",
      "Iteration 314, loss = 0.79245253\n",
      "Iteration 315, loss = 0.79188033\n",
      "Iteration 316, loss = 0.79130809\n",
      "Iteration 317, loss = 0.79073938\n",
      "Iteration 318, loss = 0.79017114\n",
      "Iteration 319, loss = 0.78960311\n",
      "Iteration 320, loss = 0.78903522\n",
      "Iteration 321, loss = 0.78846744\n",
      "Iteration 322, loss = 0.78789971\n",
      "Iteration 323, loss = 0.78733200\n",
      "Iteration 324, loss = 0.78676427\n",
      "Iteration 325, loss = 0.78619648\n",
      "Iteration 326, loss = 0.78562859\n",
      "Iteration 327, loss = 0.78506056\n",
      "Iteration 328, loss = 0.78449237\n",
      "Iteration 329, loss = 0.78392399\n",
      "Iteration 330, loss = 0.78335537\n",
      "Iteration 331, loss = 0.78278649\n",
      "Iteration 332, loss = 0.78221732\n",
      "Iteration 333, loss = 0.78164784\n",
      "Iteration 334, loss = 0.78107801\n",
      "Iteration 335, loss = 0.78050781\n",
      "Iteration 336, loss = 0.77993722\n",
      "Iteration 337, loss = 0.77936621\n",
      "Iteration 338, loss = 0.77879476\n",
      "Iteration 339, loss = 0.77822284\n",
      "Iteration 340, loss = 0.77765044\n",
      "Iteration 341, loss = 0.77707754\n",
      "Iteration 342, loss = 0.77650411\n",
      "Iteration 343, loss = 0.77593013\n",
      "Iteration 344, loss = 0.77535559\n",
      "Iteration 345, loss = 0.77478048\n",
      "Iteration 346, loss = 0.77420477\n",
      "Iteration 347, loss = 0.77362845\n",
      "Iteration 348, loss = 0.77305150\n",
      "Iteration 349, loss = 0.77247391\n",
      "Iteration 350, loss = 0.77189566\n",
      "Iteration 351, loss = 0.77131675\n",
      "Iteration 352, loss = 0.77073715\n",
      "Iteration 353, loss = 0.77015686\n",
      "Iteration 354, loss = 0.76957587\n",
      "Iteration 355, loss = 0.76899692\n",
      "Iteration 356, loss = 0.76841868\n",
      "Iteration 357, loss = 0.76784007\n",
      "Iteration 358, loss = 0.76726104\n",
      "Iteration 359, loss = 0.76668282\n",
      "Iteration 360, loss = 0.76610695\n",
      "Iteration 361, loss = 0.76553086\n",
      "Iteration 362, loss = 0.76495451\n",
      "Iteration 363, loss = 0.76437785\n",
      "Iteration 364, loss = 0.76380086\n",
      "Iteration 365, loss = 0.76322351\n",
      "Iteration 366, loss = 0.76264576\n",
      "Iteration 367, loss = 0.76206760\n",
      "Iteration 368, loss = 0.76148899\n",
      "Iteration 369, loss = 0.76090991\n",
      "Iteration 370, loss = 0.76033035\n",
      "Iteration 371, loss = 0.75975028\n",
      "Iteration 372, loss = 0.75916967\n",
      "Iteration 373, loss = 0.75858852\n",
      "Iteration 374, loss = 0.75800681\n",
      "Iteration 375, loss = 0.75742452\n",
      "Iteration 376, loss = 0.75684163\n",
      "Iteration 377, loss = 0.75625812\n",
      "Iteration 378, loss = 0.75567399\n",
      "Iteration 379, loss = 0.75508923\n",
      "Iteration 380, loss = 0.75450381\n",
      "Iteration 381, loss = 0.75391773\n",
      "Iteration 382, loss = 0.75333097\n",
      "Iteration 383, loss = 0.75274353\n",
      "Iteration 384, loss = 0.75215539\n",
      "Iteration 385, loss = 0.75156655\n",
      "Iteration 386, loss = 0.75097700\n",
      "Iteration 387, loss = 0.75038673\n",
      "Iteration 388, loss = 0.74979573\n",
      "Iteration 389, loss = 0.74920399\n",
      "Iteration 390, loss = 0.74861151\n",
      "Iteration 391, loss = 0.74801829\n",
      "Iteration 392, loss = 0.74742431\n",
      "Iteration 393, loss = 0.74682958\n",
      "Iteration 394, loss = 0.74623408\n",
      "Iteration 395, loss = 0.74563782\n",
      "Iteration 396, loss = 0.74504238\n",
      "Iteration 397, loss = 0.74444860\n",
      "Iteration 398, loss = 0.74385437\n",
      "Iteration 399, loss = 0.74325966\n",
      "Iteration 400, loss = 0.74266445\n",
      "Iteration 401, loss = 0.74206871\n",
      "Iteration 402, loss = 0.74147244\n",
      "Iteration 403, loss = 0.74087560\n",
      "Iteration 404, loss = 0.74028009\n",
      "Iteration 405, loss = 0.73968624\n",
      "Iteration 406, loss = 0.73909207\n",
      "Iteration 407, loss = 0.73849755\n",
      "Iteration 408, loss = 0.73790265\n",
      "Iteration 409, loss = 0.73730933\n",
      "Iteration 410, loss = 0.73671707\n",
      "Iteration 411, loss = 0.73612456\n",
      "Iteration 412, loss = 0.73553178\n",
      "Iteration 413, loss = 0.73493869\n",
      "Iteration 414, loss = 0.73434528\n",
      "Iteration 415, loss = 0.73375342\n",
      "Iteration 416, loss = 0.73316161\n",
      "Iteration 417, loss = 0.73256962\n",
      "Iteration 418, loss = 0.73197741\n",
      "Iteration 419, loss = 0.73138496\n",
      "Iteration 420, loss = 0.73079224\n",
      "Iteration 421, loss = 0.73019921\n",
      "Iteration 422, loss = 0.72960586\n",
      "Iteration 423, loss = 0.72901216\n",
      "Iteration 424, loss = 0.72841808\n",
      "Iteration 425, loss = 0.72782362\n",
      "Iteration 426, loss = 0.72722874\n",
      "Iteration 427, loss = 0.72663439\n",
      "Iteration 428, loss = 0.72604202\n",
      "Iteration 429, loss = 0.72544944\n",
      "Iteration 430, loss = 0.72485663\n",
      "Iteration 431, loss = 0.72426357\n",
      "Iteration 432, loss = 0.72367023\n",
      "Iteration 433, loss = 0.72307659\n",
      "Iteration 434, loss = 0.72248263\n",
      "Iteration 435, loss = 0.72188833\n",
      "Iteration 436, loss = 0.72129369\n",
      "Iteration 437, loss = 0.72069867\n",
      "Iteration 438, loss = 0.72010327\n",
      "Iteration 439, loss = 0.71950748\n",
      "Iteration 440, loss = 0.71891127\n",
      "Iteration 441, loss = 0.71831465\n",
      "Iteration 442, loss = 0.71771759\n",
      "Iteration 443, loss = 0.71712010\n",
      "Iteration 444, loss = 0.71652216\n",
      "Iteration 445, loss = 0.71592376\n",
      "Iteration 446, loss = 0.71532489\n",
      "Iteration 447, loss = 0.71472556\n",
      "Iteration 448, loss = 0.71412574\n",
      "Iteration 449, loss = 0.71352544\n",
      "Iteration 450, loss = 0.71292466\n",
      "Iteration 451, loss = 0.71232389\n",
      "Iteration 452, loss = 0.71172467\n",
      "Iteration 453, loss = 0.71112518\n",
      "Iteration 454, loss = 0.71052817\n",
      "Iteration 455, loss = 0.70993155\n",
      "Iteration 456, loss = 0.70933486\n",
      "Iteration 457, loss = 0.70873806\n",
      "Iteration 458, loss = 0.70814113\n",
      "Iteration 459, loss = 0.70754404\n",
      "Iteration 460, loss = 0.70694676\n",
      "Iteration 461, loss = 0.70634927\n",
      "Iteration 462, loss = 0.70575155\n",
      "Iteration 463, loss = 0.70515359\n",
      "Iteration 464, loss = 0.70455537\n",
      "Iteration 465, loss = 0.70395787\n",
      "Iteration 466, loss = 0.70336171\n",
      "Iteration 467, loss = 0.70276541\n",
      "Iteration 468, loss = 0.70216895\n",
      "Iteration 469, loss = 0.70157230\n",
      "Iteration 470, loss = 0.70097547\n",
      "Iteration 471, loss = 0.70037843\n",
      "Iteration 472, loss = 0.69978118\n",
      "Iteration 473, loss = 0.69918370\n",
      "Iteration 474, loss = 0.69858649\n",
      "Iteration 475, loss = 0.69799164\n",
      "Iteration 476, loss = 0.69739679\n",
      "Iteration 477, loss = 0.69680193\n",
      "Iteration 478, loss = 0.69620723\n",
      "Iteration 479, loss = 0.69561493\n",
      "Iteration 480, loss = 0.69502276\n",
      "Iteration 481, loss = 0.69443068\n",
      "Iteration 482, loss = 0.69383865\n",
      "Iteration 483, loss = 0.69324665\n",
      "Iteration 484, loss = 0.69265679\n",
      "Iteration 485, loss = 0.69206786\n",
      "Iteration 486, loss = 0.69147912\n",
      "Iteration 487, loss = 0.69089055\n",
      "Iteration 488, loss = 0.69030210\n",
      "Iteration 489, loss = 0.68971376\n",
      "Iteration 490, loss = 0.68912549\n",
      "Iteration 491, loss = 0.68853727\n",
      "Iteration 492, loss = 0.68795079\n",
      "Iteration 493, loss = 0.68736540\n",
      "Iteration 494, loss = 0.68678017\n",
      "Iteration 495, loss = 0.68619620\n",
      "Iteration 496, loss = 0.68561371\n",
      "Iteration 497, loss = 0.68503154\n",
      "Iteration 498, loss = 0.68444963\n",
      "Iteration 499, loss = 0.68386796\n",
      "Iteration 500, loss = 0.68328650\n",
      "Iteration 501, loss = 0.68270521\n",
      "Iteration 502, loss = 0.68212409\n",
      "Iteration 503, loss = 0.68154310\n",
      "Iteration 504, loss = 0.68096222\n",
      "Iteration 505, loss = 0.68038144\n",
      "Iteration 506, loss = 0.67980160\n",
      "Iteration 507, loss = 0.67922365\n",
      "Iteration 508, loss = 0.67864656\n",
      "Iteration 509, loss = 0.67807122\n",
      "Iteration 510, loss = 0.67749621\n",
      "Iteration 511, loss = 0.67692149\n",
      "Iteration 512, loss = 0.67634706\n",
      "Iteration 513, loss = 0.67577287\n",
      "Iteration 514, loss = 0.67519892\n",
      "Iteration 515, loss = 0.67462517\n",
      "Iteration 516, loss = 0.67405162\n",
      "Iteration 517, loss = 0.67347825\n",
      "Iteration 518, loss = 0.67290503\n",
      "Iteration 519, loss = 0.67233197\n",
      "Iteration 520, loss = 0.67175904\n",
      "Iteration 521, loss = 0.67118623\n",
      "Iteration 522, loss = 0.67061354\n",
      "Iteration 523, loss = 0.67004094\n",
      "Iteration 524, loss = 0.66946844\n",
      "Iteration 525, loss = 0.66889661\n",
      "Iteration 526, loss = 0.66832570\n",
      "Iteration 527, loss = 0.66775492\n",
      "Iteration 528, loss = 0.66718429\n",
      "Iteration 529, loss = 0.66661379\n",
      "Iteration 530, loss = 0.66604342\n",
      "Iteration 531, loss = 0.66547317\n",
      "Iteration 532, loss = 0.66490304\n",
      "Iteration 533, loss = 0.66433303\n",
      "Iteration 534, loss = 0.66376537\n",
      "Iteration 535, loss = 0.66319858\n",
      "Iteration 536, loss = 0.66263214\n",
      "Iteration 537, loss = 0.66206602\n",
      "Iteration 538, loss = 0.66150062\n",
      "Iteration 539, loss = 0.66093788\n",
      "Iteration 540, loss = 0.66037665\n",
      "Iteration 541, loss = 0.65981760\n",
      "Iteration 542, loss = 0.65925916\n",
      "Iteration 543, loss = 0.65870183\n",
      "Iteration 544, loss = 0.65814995\n",
      "Iteration 545, loss = 0.65759920\n",
      "Iteration 546, loss = 0.65704965\n",
      "Iteration 547, loss = 0.65650328\n",
      "Iteration 548, loss = 0.65595918\n",
      "Iteration 549, loss = 0.65541758\n",
      "Iteration 550, loss = 0.65487696\n",
      "Iteration 551, loss = 0.65433731\n",
      "Iteration 552, loss = 0.65379858\n",
      "Iteration 553, loss = 0.65326296\n",
      "Iteration 554, loss = 0.65272848\n",
      "Iteration 555, loss = 0.65219667\n",
      "Iteration 556, loss = 0.65166648\n",
      "Iteration 557, loss = 0.65113738\n",
      "Iteration 558, loss = 0.65061060\n",
      "Iteration 559, loss = 0.65008497\n",
      "Iteration 560, loss = 0.64956039\n",
      "Iteration 561, loss = 0.64903771\n",
      "Iteration 562, loss = 0.64851709\n",
      "Iteration 563, loss = 0.64799757\n",
      "Iteration 564, loss = 0.64747911\n",
      "Iteration 565, loss = 0.64696167\n",
      "Iteration 566, loss = 0.64644521\n",
      "Iteration 567, loss = 0.64592971\n",
      "Iteration 568, loss = 0.64541513\n",
      "Iteration 569, loss = 0.64490153\n",
      "Iteration 570, loss = 0.64439067\n",
      "Iteration 571, loss = 0.64388082\n",
      "Iteration 572, loss = 0.64337193\n",
      "Iteration 573, loss = 0.64286398\n",
      "Iteration 574, loss = 0.64235692\n",
      "Iteration 575, loss = 0.64185072\n",
      "Iteration 576, loss = 0.64134536\n",
      "Iteration 577, loss = 0.64084081\n",
      "Iteration 578, loss = 0.64033703\n",
      "Iteration 579, loss = 0.63983401\n",
      "Iteration 580, loss = 0.63933170\n",
      "Iteration 581, loss = 0.63883009\n",
      "Iteration 582, loss = 0.63832916\n",
      "Iteration 583, loss = 0.63782889\n",
      "Iteration 584, loss = 0.63732924\n",
      "Iteration 585, loss = 0.63683021\n",
      "Iteration 586, loss = 0.63633177\n",
      "Iteration 587, loss = 0.63583473\n",
      "Iteration 588, loss = 0.63533987\n",
      "Iteration 589, loss = 0.63484578\n",
      "Iteration 590, loss = 0.63435244\n",
      "Iteration 591, loss = 0.63385981\n",
      "Iteration 592, loss = 0.63336789\n",
      "Iteration 593, loss = 0.63287663\n",
      "Iteration 594, loss = 0.63238603\n",
      "Iteration 595, loss = 0.63189606\n",
      "Iteration 596, loss = 0.63140670\n",
      "Iteration 597, loss = 0.63091794\n",
      "Iteration 598, loss = 0.63042975\n",
      "Iteration 599, loss = 0.62994253\n",
      "Iteration 600, loss = 0.62945775\n",
      "Iteration 601, loss = 0.62897487\n",
      "Iteration 602, loss = 0.62849391\n",
      "Iteration 603, loss = 0.62801826\n",
      "Iteration 604, loss = 0.62754366\n",
      "Iteration 605, loss = 0.62707008\n",
      "Iteration 606, loss = 0.62659749\n",
      "Iteration 607, loss = 0.62612587\n",
      "Iteration 608, loss = 0.62565520\n",
      "Iteration 609, loss = 0.62518547\n",
      "Iteration 610, loss = 0.62471665\n",
      "Iteration 611, loss = 0.62424873\n",
      "Iteration 612, loss = 0.62378168\n",
      "Iteration 613, loss = 0.62331657\n",
      "Iteration 614, loss = 0.62285307\n",
      "Iteration 615, loss = 0.62239209\n",
      "Iteration 616, loss = 0.62193226\n",
      "Iteration 617, loss = 0.62147485\n",
      "Iteration 618, loss = 0.62101994\n",
      "Iteration 619, loss = 0.62056613\n",
      "Iteration 620, loss = 0.62011344\n",
      "Iteration 621, loss = 0.61966185\n",
      "Iteration 622, loss = 0.61921209\n",
      "Iteration 623, loss = 0.61876418\n",
      "Iteration 624, loss = 0.61831743\n",
      "Iteration 625, loss = 0.61787184\n",
      "Iteration 626, loss = 0.61742740\n",
      "Iteration 627, loss = 0.61698409\n",
      "Iteration 628, loss = 0.61654277\n",
      "Iteration 629, loss = 0.61610283\n",
      "Iteration 630, loss = 0.61566405\n",
      "Iteration 631, loss = 0.61522642\n",
      "Iteration 632, loss = 0.61478992\n",
      "Iteration 633, loss = 0.61435453\n",
      "Iteration 634, loss = 0.61392022\n",
      "Iteration 635, loss = 0.61348697\n",
      "Iteration 636, loss = 0.61305474\n",
      "Iteration 637, loss = 0.61262350\n",
      "Iteration 638, loss = 0.61219323\n",
      "Iteration 639, loss = 0.61176390\n",
      "Iteration 640, loss = 0.61133548\n",
      "Iteration 641, loss = 0.61090794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 642, loss = 0.61048126\n",
      "Iteration 643, loss = 0.61005541\n",
      "Iteration 644, loss = 0.60963037\n",
      "Iteration 645, loss = 0.60920611\n",
      "Iteration 646, loss = 0.60878263\n",
      "Iteration 647, loss = 0.60835989\n",
      "Iteration 648, loss = 0.60793907\n",
      "Iteration 649, loss = 0.60751927\n",
      "Iteration 650, loss = 0.60710032\n",
      "Iteration 651, loss = 0.60668222\n",
      "Iteration 652, loss = 0.60626493\n",
      "Iteration 653, loss = 0.60584845\n",
      "Iteration 654, loss = 0.60543275\n",
      "Iteration 655, loss = 0.60501782\n",
      "Iteration 656, loss = 0.60460418\n",
      "Iteration 657, loss = 0.60419233\n",
      "Iteration 658, loss = 0.60378135\n",
      "Iteration 659, loss = 0.60337121\n",
      "Iteration 660, loss = 0.60296189\n",
      "Iteration 661, loss = 0.60255338\n",
      "Iteration 662, loss = 0.60214566\n",
      "Iteration 663, loss = 0.60173871\n",
      "Iteration 664, loss = 0.60133253\n",
      "Iteration 665, loss = 0.60092710\n",
      "Iteration 666, loss = 0.60052239\n",
      "Iteration 667, loss = 0.60011841\n",
      "Iteration 668, loss = 0.59971513\n",
      "Iteration 669, loss = 0.59931255\n",
      "Iteration 670, loss = 0.59891066\n",
      "Iteration 671, loss = 0.59850943\n",
      "Iteration 672, loss = 0.59810887\n",
      "Iteration 673, loss = 0.59770895\n",
      "Iteration 674, loss = 0.59730968\n",
      "Iteration 675, loss = 0.59691104\n",
      "Iteration 676, loss = 0.59651301\n",
      "Iteration 677, loss = 0.59611560\n",
      "Iteration 678, loss = 0.59571880\n",
      "Iteration 679, loss = 0.59532259\n",
      "Iteration 680, loss = 0.59492696\n",
      "Iteration 681, loss = 0.59453192\n",
      "Iteration 682, loss = 0.59413745\n",
      "Iteration 683, loss = 0.59374354\n",
      "Iteration 684, loss = 0.59335019\n",
      "Iteration 685, loss = 0.59295740\n",
      "Iteration 686, loss = 0.59256533\n",
      "Iteration 687, loss = 0.59217468\n",
      "Iteration 688, loss = 0.59178458\n",
      "Iteration 689, loss = 0.59139504\n",
      "Iteration 690, loss = 0.59100605\n",
      "Iteration 691, loss = 0.59061762\n",
      "Iteration 692, loss = 0.59022974\n",
      "Iteration 693, loss = 0.58984242\n",
      "Iteration 694, loss = 0.58945564\n",
      "Iteration 695, loss = 0.58906942\n",
      "Iteration 696, loss = 0.58868374\n",
      "Iteration 697, loss = 0.58829861\n",
      "Iteration 698, loss = 0.58791402\n",
      "Iteration 699, loss = 0.58752996\n",
      "Iteration 700, loss = 0.58714643\n",
      "Iteration 701, loss = 0.58676343\n",
      "Iteration 702, loss = 0.58638095\n",
      "Iteration 703, loss = 0.58599898\n",
      "Iteration 704, loss = 0.58561753\n",
      "Iteration 705, loss = 0.58523952\n",
      "Iteration 706, loss = 0.58486538\n",
      "Iteration 707, loss = 0.58449200\n",
      "Iteration 708, loss = 0.58411938\n",
      "Iteration 709, loss = 0.58374750\n",
      "Iteration 710, loss = 0.58337635\n",
      "Iteration 711, loss = 0.58300594\n",
      "Iteration 712, loss = 0.58263626\n",
      "Iteration 713, loss = 0.58226731\n",
      "Iteration 714, loss = 0.58189910\n",
      "Iteration 715, loss = 0.58153161\n",
      "Iteration 716, loss = 0.58116484\n",
      "Iteration 717, loss = 0.58079879\n",
      "Iteration 718, loss = 0.58043345\n",
      "Iteration 719, loss = 0.58006882\n",
      "Iteration 720, loss = 0.57970488\n",
      "Iteration 721, loss = 0.57934163\n",
      "Iteration 722, loss = 0.57897905\n",
      "Iteration 723, loss = 0.57861714\n",
      "Iteration 724, loss = 0.57825587\n",
      "Iteration 725, loss = 0.57789525\n",
      "Iteration 726, loss = 0.57753524\n",
      "Iteration 727, loss = 0.57717585\n",
      "Iteration 728, loss = 0.57681706\n",
      "Iteration 729, loss = 0.57645885\n",
      "Iteration 730, loss = 0.57610122\n",
      "Iteration 731, loss = 0.57574415\n",
      "Iteration 732, loss = 0.57538764\n",
      "Iteration 733, loss = 0.57503167\n",
      "Iteration 734, loss = 0.57467624\n",
      "Iteration 735, loss = 0.57432164\n",
      "Iteration 736, loss = 0.57396865\n",
      "Iteration 737, loss = 0.57361629\n",
      "Iteration 738, loss = 0.57326456\n",
      "Iteration 739, loss = 0.57291346\n",
      "Iteration 740, loss = 0.57256296\n",
      "Iteration 741, loss = 0.57221306\n",
      "Iteration 742, loss = 0.57186374\n",
      "Iteration 743, loss = 0.57151501\n",
      "Iteration 744, loss = 0.57116684\n",
      "Iteration 745, loss = 0.57081924\n",
      "Iteration 746, loss = 0.57047218\n",
      "Iteration 747, loss = 0.57012567\n",
      "Iteration 748, loss = 0.56977969\n",
      "Iteration 749, loss = 0.56943424\n",
      "Iteration 750, loss = 0.56908959\n",
      "Iteration 751, loss = 0.56874673\n",
      "Iteration 752, loss = 0.56840450\n",
      "Iteration 753, loss = 0.56806290\n",
      "Iteration 754, loss = 0.56772190\n",
      "Iteration 755, loss = 0.56738176\n",
      "Iteration 756, loss = 0.56704341\n",
      "Iteration 757, loss = 0.56670571\n",
      "Iteration 758, loss = 0.56636867\n",
      "Iteration 759, loss = 0.56603228\n",
      "Iteration 760, loss = 0.56569652\n",
      "Iteration 761, loss = 0.56536140\n",
      "Iteration 762, loss = 0.56502690\n",
      "Iteration 763, loss = 0.56469301\n",
      "Iteration 764, loss = 0.56435973\n",
      "Iteration 765, loss = 0.56402705\n",
      "Iteration 766, loss = 0.56369583\n",
      "Iteration 767, loss = 0.56336554\n",
      "Iteration 768, loss = 0.56303591\n",
      "Iteration 769, loss = 0.56270692\n",
      "Iteration 770, loss = 0.56237859\n",
      "Iteration 771, loss = 0.56205088\n",
      "Iteration 772, loss = 0.56172381\n",
      "Iteration 773, loss = 0.56139736\n",
      "Iteration 774, loss = 0.56107151\n",
      "Iteration 775, loss = 0.56074627\n",
      "Iteration 776, loss = 0.56042162\n",
      "Iteration 777, loss = 0.56009755\n",
      "Iteration 778, loss = 0.55977405\n",
      "Iteration 779, loss = 0.55945112\n",
      "Iteration 780, loss = 0.55912873\n",
      "Iteration 781, loss = 0.55880689\n",
      "Iteration 782, loss = 0.55848558\n",
      "Iteration 783, loss = 0.55816480\n",
      "Iteration 784, loss = 0.55784453\n",
      "Iteration 785, loss = 0.55752478\n",
      "Iteration 786, loss = 0.55720552\n",
      "Iteration 787, loss = 0.55688677\n",
      "Iteration 788, loss = 0.55656850\n",
      "Iteration 789, loss = 0.55624984\n",
      "Iteration 790, loss = 0.55591736\n",
      "Iteration 791, loss = 0.55557888\n",
      "Iteration 792, loss = 0.55513577\n",
      "Iteration 793, loss = 0.55452594\n",
      "Iteration 794, loss = 0.55345591\n",
      "Iteration 795, loss = 0.55200845\n",
      "Iteration 796, loss = 0.55027436\n",
      "Iteration 797, loss = 0.54833064\n",
      "Iteration 798, loss = 0.54622858\n",
      "Iteration 799, loss = 0.54398444\n",
      "Iteration 800, loss = 0.54161298\n",
      "Iteration 801, loss = 0.53924936\n",
      "Iteration 802, loss = 0.53699566\n",
      "Iteration 803, loss = 0.53523200\n",
      "Iteration 804, loss = 0.53460350\n",
      "Iteration 805, loss = 0.53425598\n",
      "Iteration 806, loss = 0.53358820\n",
      "Iteration 807, loss = 0.53254348\n",
      "Iteration 808, loss = 0.53123499\n",
      "Iteration 809, loss = 0.52971249\n",
      "Iteration 810, loss = 0.52802614\n",
      "Iteration 811, loss = 0.52626574\n",
      "Iteration 812, loss = 0.52456211\n",
      "Iteration 813, loss = 0.52300445\n",
      "Iteration 814, loss = 0.52167212\n",
      "Iteration 815, loss = 0.52046079\n",
      "Iteration 816, loss = 0.51929995\n",
      "Iteration 817, loss = 0.51812047\n",
      "Iteration 818, loss = 0.51682109\n",
      "Iteration 819, loss = 0.51541262\n",
      "Iteration 820, loss = 0.51390926\n",
      "Iteration 821, loss = 0.51238089\n",
      "Iteration 822, loss = 0.51086268\n",
      "Iteration 823, loss = 0.50936160\n",
      "Iteration 824, loss = 0.50793210\n",
      "Iteration 825, loss = 0.50654444\n",
      "Iteration 826, loss = 0.50516990\n",
      "Iteration 827, loss = 0.50379478\n",
      "Iteration 828, loss = 0.50242738\n",
      "Iteration 829, loss = 0.50105127\n",
      "Iteration 830, loss = 0.49966062\n",
      "Iteration 831, loss = 0.49825817\n",
      "Iteration 832, loss = 0.49685281\n",
      "Iteration 833, loss = 0.49545784\n",
      "Iteration 834, loss = 0.49407275\n",
      "Iteration 835, loss = 0.49269321\n",
      "Iteration 836, loss = 0.49131976\n",
      "Iteration 837, loss = 0.48995291\n",
      "Iteration 838, loss = 0.48859306\n",
      "Iteration 839, loss = 0.48724660\n",
      "Iteration 840, loss = 0.48591795\n",
      "Iteration 841, loss = 0.48459723\n",
      "Iteration 842, loss = 0.48328692\n",
      "Iteration 843, loss = 0.48198748\n",
      "Iteration 844, loss = 0.48069710\n",
      "Iteration 845, loss = 0.47941443\n",
      "Iteration 846, loss = 0.47813845\n",
      "Iteration 847, loss = 0.47687046\n",
      "Iteration 848, loss = 0.47561473\n",
      "Iteration 849, loss = 0.47437101\n",
      "Iteration 850, loss = 0.47313925\n",
      "Iteration 851, loss = 0.47191968\n",
      "Iteration 852, loss = 0.47071242\n",
      "Iteration 853, loss = 0.46951750\n",
      "Iteration 854, loss = 0.46833543\n",
      "Iteration 855, loss = 0.46716770\n",
      "Iteration 856, loss = 0.46601231\n",
      "Iteration 857, loss = 0.46486907\n",
      "Iteration 858, loss = 0.46373774\n",
      "Iteration 859, loss = 0.46261811\n",
      "Iteration 860, loss = 0.46150990\n",
      "Iteration 861, loss = 0.46041487\n",
      "Iteration 862, loss = 0.45933113\n",
      "Iteration 863, loss = 0.45825822\n",
      "Iteration 864, loss = 0.45719702\n",
      "Iteration 865, loss = 0.45615140\n",
      "Iteration 866, loss = 0.45512308\n",
      "Iteration 867, loss = 0.45410714\n",
      "Iteration 868, loss = 0.45310333\n",
      "Iteration 869, loss = 0.45211143\n",
      "Iteration 870, loss = 0.45113093\n",
      "Iteration 871, loss = 0.45016361\n",
      "Iteration 872, loss = 0.44920746\n",
      "Iteration 873, loss = 0.44826252\n",
      "Iteration 874, loss = 0.44732877\n",
      "Iteration 875, loss = 0.44640612\n",
      "Iteration 876, loss = 0.44549442\n",
      "Iteration 877, loss = 0.44459347\n",
      "Iteration 878, loss = 0.44370303\n",
      "Iteration 879, loss = 0.44282282\n",
      "Iteration 880, loss = 0.44195253\n",
      "Iteration 881, loss = 0.44109183\n",
      "Iteration 882, loss = 0.44024040\n",
      "Iteration 883, loss = 0.43939789\n",
      "Iteration 884, loss = 0.43856397\n",
      "Iteration 885, loss = 0.43773831\n",
      "Iteration 886, loss = 0.43692158\n",
      "Iteration 887, loss = 0.43611900\n",
      "Iteration 888, loss = 0.43532682\n",
      "Iteration 889, loss = 0.43454242\n",
      "Iteration 890, loss = 0.43376588\n",
      "Iteration 891, loss = 0.43299724\n",
      "Iteration 892, loss = 0.43223650\n",
      "Iteration 893, loss = 0.43148363\n",
      "Iteration 894, loss = 0.43073853\n",
      "Iteration 895, loss = 0.43000110\n",
      "Iteration 896, loss = 0.42927312\n",
      "Iteration 897, loss = 0.42855272\n",
      "Iteration 898, loss = 0.42783959\n",
      "Iteration 899, loss = 0.42713435\n",
      "Iteration 900, loss = 0.42643746\n",
      "Iteration 901, loss = 0.42574889\n",
      "Iteration 902, loss = 0.42506724\n",
      "Iteration 903, loss = 0.42439274\n",
      "Iteration 904, loss = 0.42372562\n",
      "Iteration 905, loss = 0.42306681\n",
      "Iteration 906, loss = 0.42241487\n",
      "Iteration 907, loss = 0.42176837\n",
      "Iteration 908, loss = 0.42112864\n",
      "Iteration 909, loss = 0.42049541\n",
      "Iteration 910, loss = 0.41986841\n",
      "Iteration 911, loss = 0.41924805\n",
      "Iteration 912, loss = 0.41863389\n",
      "Iteration 913, loss = 0.41802541\n",
      "Iteration 914, loss = 0.41742247\n",
      "Iteration 915, loss = 0.41682491\n",
      "Iteration 916, loss = 0.41623255\n",
      "Iteration 917, loss = 0.41564523\n",
      "Iteration 918, loss = 0.41506275\n",
      "Iteration 919, loss = 0.41448491\n",
      "Iteration 920, loss = 0.41391154\n",
      "Iteration 921, loss = 0.41334245\n",
      "Iteration 922, loss = 0.41277744\n",
      "Iteration 923, loss = 0.41221636\n",
      "Iteration 924, loss = 0.41166092\n",
      "Iteration 925, loss = 0.41111157\n",
      "Iteration 926, loss = 0.41056618\n",
      "Iteration 927, loss = 0.41002466\n",
      "Iteration 928, loss = 0.40948695\n",
      "Iteration 929, loss = 0.40895296\n",
      "Iteration 930, loss = 0.40842260\n",
      "Iteration 931, loss = 0.40789576\n",
      "Iteration 932, loss = 0.40737279\n",
      "Iteration 933, loss = 0.40685389\n",
      "Iteration 934, loss = 0.40633829\n",
      "Iteration 935, loss = 0.40582743\n",
      "Iteration 936, loss = 0.40531966\n",
      "Iteration 937, loss = 0.40481488\n",
      "Iteration 938, loss = 0.40431311\n",
      "Iteration 939, loss = 0.40381436\n",
      "Iteration 940, loss = 0.40331860\n",
      "Iteration 941, loss = 0.40282580\n",
      "Iteration 942, loss = 0.40233609\n",
      "Iteration 943, loss = 0.40184915\n",
      "Iteration 944, loss = 0.40136516\n",
      "Iteration 945, loss = 0.40088457\n",
      "Iteration 946, loss = 0.40040675\n",
      "Iteration 947, loss = 0.39993160\n",
      "Iteration 948, loss = 0.39945903\n",
      "Iteration 949, loss = 0.39898932\n",
      "Iteration 950, loss = 0.39852213\n",
      "Iteration 951, loss = 0.39805746\n",
      "Iteration 952, loss = 0.39759543\n",
      "Iteration 953, loss = 0.39713560\n",
      "Iteration 954, loss = 0.39667826\n",
      "Iteration 955, loss = 0.39622322\n",
      "Iteration 956, loss = 0.39577058\n",
      "Iteration 957, loss = 0.39532091\n",
      "Iteration 958, loss = 0.39487343\n",
      "Iteration 959, loss = 0.39442806\n",
      "Iteration 960, loss = 0.39398472\n",
      "Iteration 961, loss = 0.39354334\n",
      "Iteration 962, loss = 0.39310383\n",
      "Iteration 963, loss = 0.39266613\n",
      "Iteration 964, loss = 0.39223016\n",
      "Iteration 965, loss = 0.39179585\n",
      "Iteration 966, loss = 0.39136313\n",
      "Iteration 967, loss = 0.39093194\n",
      "Iteration 968, loss = 0.39050223\n",
      "Iteration 969, loss = 0.39007446\n",
      "Iteration 970, loss = 0.38964811\n",
      "Iteration 971, loss = 0.38922312\n",
      "Iteration 972, loss = 0.38879943\n",
      "Iteration 973, loss = 0.38837701\n",
      "Iteration 974, loss = 0.38795640\n",
      "Iteration 975, loss = 0.38753897\n",
      "Iteration 976, loss = 0.38712286\n",
      "Iteration 977, loss = 0.38670805\n",
      "Iteration 978, loss = 0.38629450\n",
      "Iteration 979, loss = 0.38588220\n",
      "Iteration 980, loss = 0.38547111\n",
      "Iteration 981, loss = 0.38506121\n",
      "Iteration 982, loss = 0.38465247\n",
      "Iteration 983, loss = 0.38424523\n",
      "Iteration 984, loss = 0.38384014\n",
      "Iteration 985, loss = 0.38343653\n",
      "Iteration 986, loss = 0.38303345\n",
      "Iteration 987, loss = 0.38263075\n",
      "Iteration 988, loss = 0.38222939\n",
      "Iteration 989, loss = 0.38182989\n",
      "Iteration 990, loss = 0.38143188\n",
      "Iteration 991, loss = 0.38103547\n",
      "Iteration 992, loss = 0.38063990\n",
      "Iteration 993, loss = 0.38024509\n",
      "Iteration 994, loss = 0.37985120\n",
      "Iteration 995, loss = 0.37945876\n",
      "Iteration 996, loss = 0.37906730\n",
      "Iteration 997, loss = 0.37867734\n",
      "Iteration 998, loss = 0.37828812\n",
      "Iteration 999, loss = 0.37789965\n",
      "Iteration 1000, loss = 0.37751200\n",
      "Iteration 1, loss = 2.13108299\n",
      "Iteration 2, loss = 2.11564431\n",
      "Iteration 3, loss = 2.10026384\n",
      "Iteration 4, loss = 2.08479315\n",
      "Iteration 5, loss = 2.06929299\n",
      "Iteration 6, loss = 2.05379713\n",
      "Iteration 7, loss = 2.03839797\n",
      "Iteration 8, loss = 2.02298256\n",
      "Iteration 9, loss = 2.00748600\n",
      "Iteration 10, loss = 1.99209199\n",
      "Iteration 11, loss = 1.97680491\n",
      "Iteration 12, loss = 1.96154633\n",
      "Iteration 13, loss = 1.94632699\n",
      "Iteration 14, loss = 1.93101786\n",
      "Iteration 15, loss = 1.91581469\n",
      "Iteration 16, loss = 1.90055748\n",
      "Iteration 17, loss = 1.88510857\n",
      "Iteration 18, loss = 1.86967617\n",
      "Iteration 19, loss = 1.85421653\n",
      "Iteration 20, loss = 1.83879159\n",
      "Iteration 21, loss = 1.82333684\n",
      "Iteration 22, loss = 1.80792823\n",
      "Iteration 23, loss = 1.79264538\n",
      "Iteration 24, loss = 1.77749863\n",
      "Iteration 25, loss = 1.76250607\n",
      "Iteration 26, loss = 1.74766397\n",
      "Iteration 27, loss = 1.73295011\n",
      "Iteration 28, loss = 1.71831495\n",
      "Iteration 29, loss = 1.70385594\n",
      "Iteration 30, loss = 1.68955747\n",
      "Iteration 31, loss = 1.67542182\n",
      "Iteration 32, loss = 1.66145231\n",
      "Iteration 33, loss = 1.64765189\n",
      "Iteration 34, loss = 1.63395677\n",
      "Iteration 35, loss = 1.62038741\n",
      "Iteration 36, loss = 1.60700657\n",
      "Iteration 37, loss = 1.59380024\n",
      "Iteration 38, loss = 1.58077054\n",
      "Iteration 39, loss = 1.56791930\n",
      "Iteration 40, loss = 1.55524797\n",
      "Iteration 41, loss = 1.54275774\n",
      "Iteration 42, loss = 1.53046873\n",
      "Iteration 43, loss = 1.51836432\n",
      "Iteration 44, loss = 1.50644474\n",
      "Iteration 45, loss = 1.49471004\n",
      "Iteration 46, loss = 1.48316009\n",
      "Iteration 47, loss = 1.47179460\n",
      "Iteration 48, loss = 1.46061314\n",
      "Iteration 49, loss = 1.44961514\n",
      "Iteration 50, loss = 1.43879991\n",
      "Iteration 51, loss = 1.42816665\n",
      "Iteration 52, loss = 1.41771448\n",
      "Iteration 53, loss = 1.40744243\n",
      "Iteration 54, loss = 1.39734943\n",
      "Iteration 55, loss = 1.38743437\n",
      "Iteration 56, loss = 1.37769605\n",
      "Iteration 57, loss = 1.36813323\n",
      "Iteration 58, loss = 1.35874459\n",
      "Iteration 59, loss = 1.34952876\n",
      "Iteration 60, loss = 1.34048431\n",
      "Iteration 61, loss = 1.33160975\n",
      "Iteration 62, loss = 1.32290354\n",
      "Iteration 63, loss = 1.31436407\n",
      "Iteration 64, loss = 1.30598968\n",
      "Iteration 65, loss = 1.29775391\n",
      "Iteration 66, loss = 1.28967701\n",
      "Iteration 67, loss = 1.28175887\n",
      "Iteration 68, loss = 1.27399788\n",
      "Iteration 69, loss = 1.26639231\n",
      "Iteration 70, loss = 1.25894034\n",
      "Iteration 71, loss = 1.25164007\n",
      "Iteration 72, loss = 1.24448952\n",
      "Iteration 73, loss = 1.23748663\n",
      "Iteration 74, loss = 1.23062925\n",
      "Iteration 75, loss = 1.22391519\n",
      "Iteration 76, loss = 1.21734217\n",
      "Iteration 77, loss = 1.21090786\n",
      "Iteration 78, loss = 1.20460988\n",
      "Iteration 79, loss = 1.19844579\n",
      "Iteration 80, loss = 1.19241311\n",
      "Iteration 81, loss = 1.18650932\n",
      "Iteration 82, loss = 1.18071756\n",
      "Iteration 83, loss = 1.17504438\n",
      "Iteration 84, loss = 1.16949168\n",
      "Iteration 85, loss = 1.16405704\n",
      "Iteration 86, loss = 1.15872516\n",
      "Iteration 87, loss = 1.15346366\n",
      "Iteration 88, loss = 1.14829454\n",
      "Iteration 89, loss = 1.14320468\n",
      "Iteration 90, loss = 1.13821546\n",
      "Iteration 91, loss = 1.13334038\n",
      "Iteration 92, loss = 1.12856523\n",
      "Iteration 93, loss = 1.12388812\n",
      "Iteration 94, loss = 1.11930121\n",
      "Iteration 95, loss = 1.11478678\n",
      "Iteration 96, loss = 1.11035418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 97, loss = 1.10599795\n",
      "Iteration 98, loss = 1.10172611\n",
      "Iteration 99, loss = 1.09753017\n",
      "Iteration 100, loss = 1.09340723\n",
      "Iteration 101, loss = 1.08934318\n",
      "Iteration 102, loss = 1.08533788\n",
      "Iteration 103, loss = 1.08140746\n",
      "Iteration 104, loss = 1.07752758\n",
      "Iteration 105, loss = 1.07369449\n",
      "Iteration 106, loss = 1.06992166\n",
      "Iteration 107, loss = 1.06621587\n",
      "Iteration 108, loss = 1.06256422\n",
      "Iteration 109, loss = 1.05896493\n",
      "Iteration 110, loss = 1.05540594\n",
      "Iteration 111, loss = 1.05189385\n",
      "Iteration 112, loss = 1.04842901\n",
      "Iteration 113, loss = 1.04499151\n",
      "Iteration 114, loss = 1.04159987\n",
      "Iteration 115, loss = 1.03824495\n",
      "Iteration 116, loss = 1.03493432\n",
      "Iteration 117, loss = 1.03166371\n",
      "Iteration 118, loss = 1.02843424\n",
      "Iteration 119, loss = 1.02524900\n",
      "Iteration 120, loss = 1.02211085\n",
      "Iteration 121, loss = 1.01901143\n",
      "Iteration 122, loss = 1.01596127\n",
      "Iteration 123, loss = 1.01295999\n",
      "Iteration 124, loss = 1.01000710\n",
      "Iteration 125, loss = 1.00710197\n",
      "Iteration 126, loss = 1.00424389\n",
      "Iteration 127, loss = 1.00143071\n",
      "Iteration 128, loss = 0.99866275\n",
      "Iteration 129, loss = 0.99593898\n",
      "Iteration 130, loss = 0.99325848\n",
      "Iteration 131, loss = 0.99062029\n",
      "Iteration 132, loss = 0.98802345\n",
      "Iteration 133, loss = 0.98546697\n",
      "Iteration 134, loss = 0.98294987\n",
      "Iteration 135, loss = 0.98047117\n",
      "Iteration 136, loss = 0.97802989\n",
      "Iteration 137, loss = 0.97562507\n",
      "Iteration 138, loss = 0.97325577\n",
      "Iteration 139, loss = 0.97091587\n",
      "Iteration 140, loss = 0.96860759\n",
      "Iteration 141, loss = 0.96633194\n",
      "Iteration 142, loss = 0.96408347\n",
      "Iteration 143, loss = 0.96186538\n",
      "Iteration 144, loss = 0.95967755\n",
      "Iteration 145, loss = 0.95751936\n",
      "Iteration 146, loss = 0.95539016\n",
      "Iteration 147, loss = 0.95328935\n",
      "Iteration 148, loss = 0.95121487\n",
      "Iteration 149, loss = 0.94916355\n",
      "Iteration 150, loss = 0.94713876\n",
      "Iteration 151, loss = 0.94514002\n",
      "Iteration 152, loss = 0.94316686\n",
      "Iteration 153, loss = 0.94121883\n",
      "Iteration 154, loss = 0.93929549\n",
      "Iteration 155, loss = 0.93739641\n",
      "Iteration 156, loss = 0.93552119\n",
      "Iteration 157, loss = 0.93366940\n",
      "Iteration 158, loss = 0.93184068\n",
      "Iteration 159, loss = 0.93003337\n",
      "Iteration 160, loss = 0.92824518\n",
      "Iteration 161, loss = 0.92647886\n",
      "Iteration 162, loss = 0.92473414\n",
      "Iteration 163, loss = 0.92301072\n",
      "Iteration 164, loss = 0.92130833\n",
      "Iteration 165, loss = 0.91962670\n",
      "Iteration 166, loss = 0.91796556\n",
      "Iteration 167, loss = 0.91632464\n",
      "Iteration 168, loss = 0.91470370\n",
      "Iteration 169, loss = 0.91310247\n",
      "Iteration 170, loss = 0.91152072\n",
      "Iteration 171, loss = 0.90995820\n",
      "Iteration 172, loss = 0.90841468\n",
      "Iteration 173, loss = 0.90688991\n",
      "Iteration 174, loss = 0.90538368\n",
      "Iteration 175, loss = 0.90389575\n",
      "Iteration 176, loss = 0.90242590\n",
      "Iteration 177, loss = 0.90097392\n",
      "Iteration 178, loss = 0.89953958\n",
      "Iteration 179, loss = 0.89812268\n",
      "Iteration 180, loss = 0.89672300\n",
      "Iteration 181, loss = 0.89534033\n",
      "Iteration 182, loss = 0.89397447\n",
      "Iteration 183, loss = 0.89262521\n",
      "Iteration 184, loss = 0.89129235\n",
      "Iteration 185, loss = 0.88997568\n",
      "Iteration 186, loss = 0.88867500\n",
      "Iteration 187, loss = 0.88739011\n",
      "Iteration 188, loss = 0.88612081\n",
      "Iteration 189, loss = 0.88486689\n",
      "Iteration 190, loss = 0.88362817\n",
      "Iteration 191, loss = 0.88240444\n",
      "Iteration 192, loss = 0.88119550\n",
      "Iteration 193, loss = 0.88000116\n",
      "Iteration 194, loss = 0.87882122\n",
      "Iteration 195, loss = 0.87765547\n",
      "Iteration 196, loss = 0.87650373\n",
      "Iteration 197, loss = 0.87536579\n",
      "Iteration 198, loss = 0.87424146\n",
      "Iteration 199, loss = 0.87313054\n",
      "Iteration 200, loss = 0.87203283\n",
      "Iteration 201, loss = 0.87094813\n",
      "Iteration 202, loss = 0.86987625\n",
      "Iteration 203, loss = 0.86881699\n",
      "Iteration 204, loss = 0.86777015\n",
      "Iteration 205, loss = 0.86673554\n",
      "Iteration 206, loss = 0.86571296\n",
      "Iteration 207, loss = 0.86470221\n",
      "Iteration 208, loss = 0.86370310\n",
      "Iteration 209, loss = 0.86271543\n",
      "Iteration 210, loss = 0.86173901\n",
      "Iteration 211, loss = 0.86077364\n",
      "Iteration 212, loss = 0.85981913\n",
      "Iteration 213, loss = 0.85887528\n",
      "Iteration 214, loss = 0.85794190\n",
      "Iteration 215, loss = 0.85701880\n",
      "Iteration 216, loss = 0.85610579\n",
      "Iteration 217, loss = 0.85520267\n",
      "Iteration 218, loss = 0.85430927\n",
      "Iteration 219, loss = 0.85342538\n",
      "Iteration 220, loss = 0.85255082\n",
      "Iteration 221, loss = 0.85168541\n",
      "Iteration 222, loss = 0.85082895\n",
      "Iteration 223, loss = 0.84998128\n",
      "Iteration 224, loss = 0.84914219\n",
      "Iteration 225, loss = 0.84831152\n",
      "Iteration 226, loss = 0.84748908\n",
      "Iteration 227, loss = 0.84667470\n",
      "Iteration 228, loss = 0.84586819\n",
      "Iteration 229, loss = 0.84506938\n",
      "Iteration 230, loss = 0.84427810\n",
      "Iteration 231, loss = 0.84349418\n",
      "Iteration 232, loss = 0.84271744\n",
      "Iteration 233, loss = 0.84194772\n",
      "Iteration 234, loss = 0.84118485\n",
      "Iteration 235, loss = 0.84042867\n",
      "Iteration 236, loss = 0.83967901\n",
      "Iteration 237, loss = 0.83893571\n",
      "Iteration 238, loss = 0.83819861\n",
      "Iteration 239, loss = 0.83746756\n",
      "Iteration 240, loss = 0.83674241\n",
      "Iteration 241, loss = 0.83602299\n",
      "Iteration 242, loss = 0.83530915\n",
      "Iteration 243, loss = 0.83460076\n",
      "Iteration 244, loss = 0.83389765\n",
      "Iteration 245, loss = 0.83319970\n",
      "Iteration 246, loss = 0.83250674\n",
      "Iteration 247, loss = 0.83181865\n",
      "Iteration 248, loss = 0.83113529\n",
      "Iteration 249, loss = 0.83045651\n",
      "Iteration 250, loss = 0.82978219\n",
      "Iteration 251, loss = 0.82911219\n",
      "Iteration 252, loss = 0.82844638\n",
      "Iteration 253, loss = 0.82778464\n",
      "Iteration 254, loss = 0.82712683\n",
      "Iteration 255, loss = 0.82647285\n",
      "Iteration 256, loss = 0.82582256\n",
      "Iteration 257, loss = 0.82517585\n",
      "Iteration 258, loss = 0.82453259\n",
      "Iteration 259, loss = 0.82389268\n",
      "Iteration 260, loss = 0.82325601\n",
      "Iteration 261, loss = 0.82262245\n",
      "Iteration 262, loss = 0.82199191\n",
      "Iteration 263, loss = 0.82136428\n",
      "Iteration 264, loss = 0.82073945\n",
      "Iteration 265, loss = 0.82011732\n",
      "Iteration 266, loss = 0.81949779\n",
      "Iteration 267, loss = 0.81888076\n",
      "Iteration 268, loss = 0.81826614\n",
      "Iteration 269, loss = 0.81765383\n",
      "Iteration 270, loss = 0.81704374\n",
      "Iteration 271, loss = 0.81643578\n",
      "Iteration 272, loss = 0.81582986\n",
      "Iteration 273, loss = 0.81522590\n",
      "Iteration 274, loss = 0.81462381\n",
      "Iteration 275, loss = 0.81402350\n",
      "Iteration 276, loss = 0.81342491\n",
      "Iteration 277, loss = 0.81282794\n",
      "Iteration 278, loss = 0.81223252\n",
      "Iteration 279, loss = 0.81163857\n",
      "Iteration 280, loss = 0.81104603\n",
      "Iteration 281, loss = 0.81045481\n",
      "Iteration 282, loss = 0.80986486\n",
      "Iteration 283, loss = 0.80927609\n",
      "Iteration 284, loss = 0.80868845\n",
      "Iteration 285, loss = 0.80810187\n",
      "Iteration 286, loss = 0.80751628\n",
      "Iteration 287, loss = 0.80693162\n",
      "Iteration 288, loss = 0.80634783\n",
      "Iteration 289, loss = 0.80576485\n",
      "Iteration 290, loss = 0.80518262\n",
      "Iteration 291, loss = 0.80460110\n",
      "Iteration 292, loss = 0.80402021\n",
      "Iteration 293, loss = 0.80343991\n",
      "Iteration 294, loss = 0.80286015\n",
      "Iteration 295, loss = 0.80228088\n",
      "Iteration 296, loss = 0.80170258\n",
      "Iteration 297, loss = 0.80112715\n",
      "Iteration 298, loss = 0.80055235\n",
      "Iteration 299, loss = 0.79997812\n",
      "Iteration 300, loss = 0.79940442\n",
      "Iteration 301, loss = 0.79883118\n",
      "Iteration 302, loss = 0.79825835\n",
      "Iteration 303, loss = 0.79768589\n",
      "Iteration 304, loss = 0.79711374\n",
      "Iteration 305, loss = 0.79654186\n",
      "Iteration 306, loss = 0.79597019\n",
      "Iteration 307, loss = 0.79539871\n",
      "Iteration 308, loss = 0.79482736\n",
      "Iteration 309, loss = 0.79425611\n",
      "Iteration 310, loss = 0.79368772\n",
      "Iteration 311, loss = 0.79312060\n",
      "Iteration 312, loss = 0.79255382\n",
      "Iteration 313, loss = 0.79198731\n",
      "Iteration 314, loss = 0.79142103\n",
      "Iteration 315, loss = 0.79085493\n",
      "Iteration 316, loss = 0.79028896\n",
      "Iteration 317, loss = 0.78972308\n",
      "Iteration 318, loss = 0.78915725\n",
      "Iteration 319, loss = 0.78859144\n",
      "Iteration 320, loss = 0.78802559\n",
      "Iteration 321, loss = 0.78745967\n",
      "Iteration 322, loss = 0.78689365\n",
      "Iteration 323, loss = 0.78632749\n",
      "Iteration 324, loss = 0.78576116\n",
      "Iteration 325, loss = 0.78519463\n",
      "Iteration 326, loss = 0.78462786\n",
      "Iteration 327, loss = 0.78406083\n",
      "Iteration 328, loss = 0.78349350\n",
      "Iteration 329, loss = 0.78292585\n",
      "Iteration 330, loss = 0.78235785\n",
      "Iteration 331, loss = 0.78178947\n",
      "Iteration 332, loss = 0.78122070\n",
      "Iteration 333, loss = 0.78065150\n",
      "Iteration 334, loss = 0.78008185\n",
      "Iteration 335, loss = 0.77951173\n",
      "Iteration 336, loss = 0.77894112\n",
      "Iteration 337, loss = 0.77837000\n",
      "Iteration 338, loss = 0.77779835\n",
      "Iteration 339, loss = 0.77722614\n",
      "Iteration 340, loss = 0.77665337\n",
      "Iteration 341, loss = 0.77608001\n",
      "Iteration 342, loss = 0.77550604\n",
      "Iteration 343, loss = 0.77493146\n",
      "Iteration 344, loss = 0.77435624\n",
      "Iteration 345, loss = 0.77378037\n",
      "Iteration 346, loss = 0.77320436\n",
      "Iteration 347, loss = 0.77263386\n",
      "Iteration 348, loss = 0.77206443\n",
      "Iteration 349, loss = 0.77149489\n",
      "Iteration 350, loss = 0.77092519\n",
      "Iteration 351, loss = 0.77035666\n",
      "Iteration 352, loss = 0.76979024\n",
      "Iteration 353, loss = 0.76922374\n",
      "Iteration 354, loss = 0.76865712\n",
      "Iteration 355, loss = 0.76809035\n",
      "Iteration 356, loss = 0.76752340\n",
      "Iteration 357, loss = 0.76695624\n",
      "Iteration 358, loss = 0.76638883\n",
      "Iteration 359, loss = 0.76582117\n",
      "Iteration 360, loss = 0.76525320\n",
      "Iteration 361, loss = 0.76468492\n",
      "Iteration 362, loss = 0.76411630\n",
      "Iteration 363, loss = 0.76354732\n",
      "Iteration 364, loss = 0.76297795\n",
      "Iteration 365, loss = 0.76240816\n",
      "Iteration 366, loss = 0.76183795\n",
      "Iteration 367, loss = 0.76126729\n",
      "Iteration 368, loss = 0.76069615\n",
      "Iteration 369, loss = 0.76012452\n",
      "Iteration 370, loss = 0.75955238\n",
      "Iteration 371, loss = 0.75897971\n",
      "Iteration 372, loss = 0.75840649\n",
      "Iteration 373, loss = 0.75783272\n",
      "Iteration 374, loss = 0.75725836\n",
      "Iteration 375, loss = 0.75668341\n",
      "Iteration 376, loss = 0.75610785\n",
      "Iteration 377, loss = 0.75553167\n",
      "Iteration 378, loss = 0.75495485\n",
      "Iteration 379, loss = 0.75437739\n",
      "Iteration 380, loss = 0.75379927\n",
      "Iteration 381, loss = 0.75322048\n",
      "Iteration 382, loss = 0.75264101\n",
      "Iteration 383, loss = 0.75206086\n",
      "Iteration 384, loss = 0.75148000\n",
      "Iteration 385, loss = 0.75089844\n",
      "Iteration 386, loss = 0.75031617\n",
      "Iteration 387, loss = 0.74973318\n",
      "Iteration 388, loss = 0.74914946\n",
      "Iteration 389, loss = 0.74856501\n",
      "Iteration 390, loss = 0.74797982\n",
      "Iteration 391, loss = 0.74739408\n",
      "Iteration 392, loss = 0.74681114\n",
      "Iteration 393, loss = 0.74622774\n",
      "Iteration 394, loss = 0.74564388\n",
      "Iteration 395, loss = 0.74505952\n",
      "Iteration 396, loss = 0.74447465\n",
      "Iteration 397, loss = 0.74388924\n",
      "Iteration 398, loss = 0.74330327\n",
      "Iteration 399, loss = 0.74271674\n",
      "Iteration 400, loss = 0.74212963\n",
      "Iteration 401, loss = 0.74154278\n",
      "Iteration 402, loss = 0.74095835\n",
      "Iteration 403, loss = 0.74037358\n",
      "Iteration 404, loss = 0.73978843\n",
      "Iteration 405, loss = 0.73920289\n",
      "Iteration 406, loss = 0.73861977\n",
      "Iteration 407, loss = 0.73803663\n",
      "Iteration 408, loss = 0.73745462\n",
      "Iteration 409, loss = 0.73687307\n",
      "Iteration 410, loss = 0.73629140\n",
      "Iteration 411, loss = 0.73570957\n",
      "Iteration 412, loss = 0.73512755\n",
      "Iteration 413, loss = 0.73454530\n",
      "Iteration 414, loss = 0.73396279\n",
      "Iteration 415, loss = 0.73338001\n",
      "Iteration 416, loss = 0.73279692\n",
      "Iteration 417, loss = 0.73221351\n",
      "Iteration 418, loss = 0.73162974\n",
      "Iteration 419, loss = 0.73104561\n",
      "Iteration 420, loss = 0.73046108\n",
      "Iteration 421, loss = 0.72987615\n",
      "Iteration 422, loss = 0.72929080\n",
      "Iteration 423, loss = 0.72870502\n",
      "Iteration 424, loss = 0.72811878\n",
      "Iteration 425, loss = 0.72753208\n",
      "Iteration 426, loss = 0.72694574\n",
      "Iteration 427, loss = 0.72636130\n",
      "Iteration 428, loss = 0.72577660\n",
      "Iteration 429, loss = 0.72519163\n",
      "Iteration 430, loss = 0.72460637\n",
      "Iteration 431, loss = 0.72402078\n",
      "Iteration 432, loss = 0.72343487\n",
      "Iteration 433, loss = 0.72284860\n",
      "Iteration 434, loss = 0.72226197\n",
      "Iteration 435, loss = 0.72167496\n",
      "Iteration 436, loss = 0.72108756\n",
      "Iteration 437, loss = 0.72049975\n",
      "Iteration 438, loss = 0.71991152\n",
      "Iteration 439, loss = 0.71932286\n",
      "Iteration 440, loss = 0.71873560\n",
      "Iteration 441, loss = 0.71814838\n",
      "Iteration 442, loss = 0.71756090\n",
      "Iteration 443, loss = 0.71697314\n",
      "Iteration 444, loss = 0.71638509\n",
      "Iteration 445, loss = 0.71579769\n",
      "Iteration 446, loss = 0.71521138\n",
      "Iteration 447, loss = 0.71462492\n",
      "Iteration 448, loss = 0.71403828\n",
      "Iteration 449, loss = 0.71345143\n",
      "Iteration 450, loss = 0.71286435\n",
      "Iteration 451, loss = 0.71227702\n",
      "Iteration 452, loss = 0.71168943\n",
      "Iteration 453, loss = 0.71110155\n",
      "Iteration 454, loss = 0.71051394\n",
      "Iteration 455, loss = 0.70992881\n",
      "Iteration 456, loss = 0.70934359\n",
      "Iteration 457, loss = 0.70875826\n",
      "Iteration 458, loss = 0.70817279\n",
      "Iteration 459, loss = 0.70758716\n",
      "Iteration 460, loss = 0.70700135\n",
      "Iteration 461, loss = 0.70641534\n",
      "Iteration 462, loss = 0.70582910\n",
      "Iteration 463, loss = 0.70524263\n",
      "Iteration 464, loss = 0.70465663\n",
      "Iteration 465, loss = 0.70407215\n",
      "Iteration 466, loss = 0.70348753\n",
      "Iteration 467, loss = 0.70290276\n",
      "Iteration 468, loss = 0.70231783\n",
      "Iteration 469, loss = 0.70173416\n",
      "Iteration 470, loss = 0.70115179\n",
      "Iteration 471, loss = 0.70056948\n",
      "Iteration 472, loss = 0.69998719\n",
      "Iteration 473, loss = 0.69940490\n",
      "Iteration 474, loss = 0.69882258\n",
      "Iteration 475, loss = 0.69824022\n",
      "Iteration 476, loss = 0.69765778\n",
      "Iteration 477, loss = 0.69707526\n",
      "Iteration 478, loss = 0.69649263\n",
      "Iteration 479, loss = 0.69590989\n",
      "Iteration 480, loss = 0.69532700\n",
      "Iteration 481, loss = 0.69474397\n",
      "Iteration 482, loss = 0.69416077\n",
      "Iteration 483, loss = 0.69357740\n",
      "Iteration 484, loss = 0.69299385\n",
      "Iteration 485, loss = 0.69241010\n",
      "Iteration 486, loss = 0.69182615\n",
      "Iteration 487, loss = 0.69124199\n",
      "Iteration 488, loss = 0.69065762\n",
      "Iteration 489, loss = 0.69007302\n",
      "Iteration 490, loss = 0.68948819\n",
      "Iteration 491, loss = 0.68890313\n",
      "Iteration 492, loss = 0.68831782\n",
      "Iteration 493, loss = 0.68773228\n",
      "Iteration 494, loss = 0.68714648\n",
      "Iteration 495, loss = 0.68656044\n",
      "Iteration 496, loss = 0.68597606\n",
      "Iteration 497, loss = 0.68539224\n",
      "Iteration 498, loss = 0.68480834\n",
      "Iteration 499, loss = 0.68422436\n",
      "Iteration 500, loss = 0.68364167\n",
      "Iteration 501, loss = 0.68306036\n",
      "Iteration 502, loss = 0.68247917\n",
      "Iteration 503, loss = 0.68189808\n",
      "Iteration 504, loss = 0.68131707\n",
      "Iteration 505, loss = 0.68073611\n",
      "Iteration 506, loss = 0.68015518\n",
      "Iteration 507, loss = 0.67957426\n",
      "Iteration 508, loss = 0.67899334\n",
      "Iteration 509, loss = 0.67841239\n",
      "Iteration 510, loss = 0.67783140\n",
      "Iteration 511, loss = 0.67725098\n",
      "Iteration 512, loss = 0.67667201\n",
      "Iteration 513, loss = 0.67609308\n",
      "Iteration 514, loss = 0.67551419\n",
      "Iteration 515, loss = 0.67493531\n",
      "Iteration 516, loss = 0.67435646\n",
      "Iteration 517, loss = 0.67377760\n",
      "Iteration 518, loss = 0.67319874\n",
      "Iteration 519, loss = 0.67261986\n",
      "Iteration 520, loss = 0.67204096\n",
      "Iteration 521, loss = 0.67146204\n",
      "Iteration 522, loss = 0.67088308\n",
      "Iteration 523, loss = 0.67030408\n",
      "Iteration 524, loss = 0.66972507\n",
      "Iteration 525, loss = 0.66914904\n",
      "Iteration 526, loss = 0.66857457\n",
      "Iteration 527, loss = 0.66800036\n",
      "Iteration 528, loss = 0.66742640\n",
      "Iteration 529, loss = 0.66685267\n",
      "Iteration 530, loss = 0.66627914\n",
      "Iteration 531, loss = 0.66570848\n",
      "Iteration 532, loss = 0.66514122\n",
      "Iteration 533, loss = 0.66457455\n",
      "Iteration 534, loss = 0.66401083\n",
      "Iteration 535, loss = 0.66345135\n",
      "Iteration 536, loss = 0.66289271\n",
      "Iteration 537, loss = 0.66233512\n",
      "Iteration 538, loss = 0.66178085\n",
      "Iteration 539, loss = 0.66122743\n",
      "Iteration 540, loss = 0.66067635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 541, loss = 0.66012706\n",
      "Iteration 542, loss = 0.65957890\n",
      "Iteration 543, loss = 0.65903407\n",
      "Iteration 544, loss = 0.65848997\n",
      "Iteration 545, loss = 0.65794846\n",
      "Iteration 546, loss = 0.65740871\n",
      "Iteration 547, loss = 0.65686995\n",
      "Iteration 548, loss = 0.65633217\n",
      "Iteration 549, loss = 0.65579534\n",
      "Iteration 550, loss = 0.65525944\n",
      "Iteration 551, loss = 0.65472544\n",
      "Iteration 552, loss = 0.65419338\n",
      "Iteration 553, loss = 0.65366233\n",
      "Iteration 554, loss = 0.65313225\n",
      "Iteration 555, loss = 0.65260310\n",
      "Iteration 556, loss = 0.65207487\n",
      "Iteration 557, loss = 0.65154751\n",
      "Iteration 558, loss = 0.65102254\n",
      "Iteration 559, loss = 0.65049891\n",
      "Iteration 560, loss = 0.64997623\n",
      "Iteration 561, loss = 0.64945447\n",
      "Iteration 562, loss = 0.64893359\n",
      "Iteration 563, loss = 0.64841411\n",
      "Iteration 564, loss = 0.64789601\n",
      "Iteration 565, loss = 0.64737869\n",
      "Iteration 566, loss = 0.64686211\n",
      "Iteration 567, loss = 0.64634623\n",
      "Iteration 568, loss = 0.64583112\n",
      "Iteration 569, loss = 0.64531886\n",
      "Iteration 570, loss = 0.64480744\n",
      "Iteration 571, loss = 0.64429683\n",
      "Iteration 572, loss = 0.64378779\n",
      "Iteration 573, loss = 0.64328105\n",
      "Iteration 574, loss = 0.64277522\n",
      "Iteration 575, loss = 0.64227028\n",
      "Iteration 576, loss = 0.64176620\n",
      "Iteration 577, loss = 0.64126295\n",
      "Iteration 578, loss = 0.64076049\n",
      "Iteration 579, loss = 0.64025880\n",
      "Iteration 580, loss = 0.63975785\n",
      "Iteration 581, loss = 0.63925763\n",
      "Iteration 582, loss = 0.63875810\n",
      "Iteration 583, loss = 0.63825924\n",
      "Iteration 584, loss = 0.63776103\n",
      "Iteration 585, loss = 0.63726345\n",
      "Iteration 586, loss = 0.63676649\n",
      "Iteration 587, loss = 0.63627012\n",
      "Iteration 588, loss = 0.63577587\n",
      "Iteration 589, loss = 0.63528524\n",
      "Iteration 590, loss = 0.63479585\n",
      "Iteration 591, loss = 0.63430913\n",
      "Iteration 592, loss = 0.63382340\n",
      "Iteration 593, loss = 0.63333861\n",
      "Iteration 594, loss = 0.63285475\n",
      "Iteration 595, loss = 0.63237178\n",
      "Iteration 596, loss = 0.63188968\n",
      "Iteration 597, loss = 0.63140841\n",
      "Iteration 598, loss = 0.63092823\n",
      "Iteration 599, loss = 0.63045050\n",
      "Iteration 600, loss = 0.62997660\n",
      "Iteration 601, loss = 0.62950402\n",
      "Iteration 602, loss = 0.62903249\n",
      "Iteration 603, loss = 0.62856228\n",
      "Iteration 604, loss = 0.62809458\n",
      "Iteration 605, loss = 0.62762797\n",
      "Iteration 606, loss = 0.62716245\n",
      "Iteration 607, loss = 0.62669801\n",
      "Iteration 608, loss = 0.62623495\n",
      "Iteration 609, loss = 0.62577432\n",
      "Iteration 610, loss = 0.62531482\n",
      "Iteration 611, loss = 0.62485645\n",
      "Iteration 612, loss = 0.62439920\n",
      "Iteration 613, loss = 0.62394305\n",
      "Iteration 614, loss = 0.62348798\n",
      "Iteration 615, loss = 0.62303397\n",
      "Iteration 616, loss = 0.62258099\n",
      "Iteration 617, loss = 0.62212902\n",
      "Iteration 618, loss = 0.62167802\n",
      "Iteration 619, loss = 0.62122797\n",
      "Iteration 620, loss = 0.62077884\n",
      "Iteration 621, loss = 0.62033060\n",
      "Iteration 622, loss = 0.61988322\n",
      "Iteration 623, loss = 0.61943666\n",
      "Iteration 624, loss = 0.61899092\n",
      "Iteration 625, loss = 0.61854594\n",
      "Iteration 626, loss = 0.61810172\n",
      "Iteration 627, loss = 0.61765824\n",
      "Iteration 628, loss = 0.61721545\n",
      "Iteration 629, loss = 0.61677358\n",
      "Iteration 630, loss = 0.61633389\n",
      "Iteration 631, loss = 0.61589503\n",
      "Iteration 632, loss = 0.61545699\n",
      "Iteration 633, loss = 0.61501972\n",
      "Iteration 634, loss = 0.61458322\n",
      "Iteration 635, loss = 0.61414835\n",
      "Iteration 636, loss = 0.61371514\n",
      "Iteration 637, loss = 0.61328281\n",
      "Iteration 638, loss = 0.61285132\n",
      "Iteration 639, loss = 0.61242065\n",
      "Iteration 640, loss = 0.61199077\n",
      "Iteration 641, loss = 0.61156168\n",
      "Iteration 642, loss = 0.61113334\n",
      "Iteration 643, loss = 0.61070574\n",
      "Iteration 644, loss = 0.61027886\n",
      "Iteration 645, loss = 0.60985269\n",
      "Iteration 646, loss = 0.60942720\n",
      "Iteration 647, loss = 0.60900239\n",
      "Iteration 648, loss = 0.60857824\n",
      "Iteration 649, loss = 0.60815473\n",
      "Iteration 650, loss = 0.60773186\n",
      "Iteration 651, loss = 0.60730960\n",
      "Iteration 652, loss = 0.60688795\n",
      "Iteration 653, loss = 0.60646763\n",
      "Iteration 654, loss = 0.60604892\n",
      "Iteration 655, loss = 0.60563093\n",
      "Iteration 656, loss = 0.60521365\n",
      "Iteration 657, loss = 0.60479707\n",
      "Iteration 658, loss = 0.60438117\n",
      "Iteration 659, loss = 0.60396594\n",
      "Iteration 660, loss = 0.60355135\n",
      "Iteration 661, loss = 0.60313740\n",
      "Iteration 662, loss = 0.60272408\n",
      "Iteration 663, loss = 0.60231137\n",
      "Iteration 664, loss = 0.60189926\n",
      "Iteration 665, loss = 0.60148774\n",
      "Iteration 666, loss = 0.60107680\n",
      "Iteration 667, loss = 0.60066643\n",
      "Iteration 668, loss = 0.60025661\n",
      "Iteration 669, loss = 0.59984735\n",
      "Iteration 670, loss = 0.59943863\n",
      "Iteration 671, loss = 0.59903045\n",
      "Iteration 672, loss = 0.59862279\n",
      "Iteration 673, loss = 0.59821564\n",
      "Iteration 674, loss = 0.59780901\n",
      "Iteration 675, loss = 0.59740287\n",
      "Iteration 676, loss = 0.59699724\n",
      "Iteration 677, loss = 0.59659267\n",
      "Iteration 678, loss = 0.59619025\n",
      "Iteration 679, loss = 0.59578848\n",
      "Iteration 680, loss = 0.59538737\n",
      "Iteration 681, loss = 0.59498773\n",
      "Iteration 682, loss = 0.59459002\n",
      "Iteration 683, loss = 0.59419306\n",
      "Iteration 684, loss = 0.59379682\n",
      "Iteration 685, loss = 0.59340130\n",
      "Iteration 686, loss = 0.59300648\n",
      "Iteration 687, loss = 0.59261234\n",
      "Iteration 688, loss = 0.59221887\n",
      "Iteration 689, loss = 0.59182606\n",
      "Iteration 690, loss = 0.59143390\n",
      "Iteration 691, loss = 0.59104237\n",
      "Iteration 692, loss = 0.59065145\n",
      "Iteration 693, loss = 0.59026115\n",
      "Iteration 694, loss = 0.58987270\n",
      "Iteration 695, loss = 0.58948512\n",
      "Iteration 696, loss = 0.58909883\n",
      "Iteration 697, loss = 0.58871378\n",
      "Iteration 698, loss = 0.58832941\n",
      "Iteration 699, loss = 0.58794570\n",
      "Iteration 700, loss = 0.58756263\n",
      "Iteration 701, loss = 0.58718020\n",
      "Iteration 702, loss = 0.58679839\n",
      "Iteration 703, loss = 0.58641721\n",
      "Iteration 704, loss = 0.58603663\n",
      "Iteration 705, loss = 0.58565666\n",
      "Iteration 706, loss = 0.58527727\n",
      "Iteration 707, loss = 0.58489848\n",
      "Iteration 708, loss = 0.58452026\n",
      "Iteration 709, loss = 0.58414262\n",
      "Iteration 710, loss = 0.58376555\n",
      "Iteration 711, loss = 0.58338904\n",
      "Iteration 712, loss = 0.58301309\n",
      "Iteration 713, loss = 0.58263768\n",
      "Iteration 714, loss = 0.58226359\n",
      "Iteration 715, loss = 0.58189107\n",
      "Iteration 716, loss = 0.58151922\n",
      "Iteration 717, loss = 0.58114914\n",
      "Iteration 718, loss = 0.58078037\n",
      "Iteration 719, loss = 0.58041234\n",
      "Iteration 720, loss = 0.58004502\n",
      "Iteration 721, loss = 0.57967841\n",
      "Iteration 722, loss = 0.57931249\n",
      "Iteration 723, loss = 0.57894726\n",
      "Iteration 724, loss = 0.57858317\n",
      "Iteration 725, loss = 0.57822114\n",
      "Iteration 726, loss = 0.57786054\n",
      "Iteration 727, loss = 0.57750068\n",
      "Iteration 728, loss = 0.57714156\n",
      "Iteration 729, loss = 0.57678319\n",
      "Iteration 730, loss = 0.57642558\n",
      "Iteration 731, loss = 0.57606873\n",
      "Iteration 732, loss = 0.57571264\n",
      "Iteration 733, loss = 0.57535730\n",
      "Iteration 734, loss = 0.57500273\n",
      "Iteration 735, loss = 0.57464890\n",
      "Iteration 736, loss = 0.57429582\n",
      "Iteration 737, loss = 0.57394346\n",
      "Iteration 738, loss = 0.57359182\n",
      "Iteration 739, loss = 0.57324089\n",
      "Iteration 740, loss = 0.57289064\n",
      "Iteration 741, loss = 0.57254107\n",
      "Iteration 742, loss = 0.57219216\n",
      "Iteration 743, loss = 0.57184390\n",
      "Iteration 744, loss = 0.57149627\n",
      "Iteration 745, loss = 0.57114925\n",
      "Iteration 746, loss = 0.57080284\n",
      "Iteration 747, loss = 0.57045703\n",
      "Iteration 748, loss = 0.57011179\n",
      "Iteration 749, loss = 0.56976712\n",
      "Iteration 750, loss = 0.56942301\n",
      "Iteration 751, loss = 0.56907946\n",
      "Iteration 752, loss = 0.56873644\n",
      "Iteration 753, loss = 0.56839396\n",
      "Iteration 754, loss = 0.56805201\n",
      "Iteration 755, loss = 0.56771058\n",
      "Iteration 756, loss = 0.56736967\n",
      "Iteration 757, loss = 0.56702957\n",
      "Iteration 758, loss = 0.56669124\n",
      "Iteration 759, loss = 0.56635354\n",
      "Iteration 760, loss = 0.56601647\n",
      "Iteration 761, loss = 0.56568000\n",
      "Iteration 762, loss = 0.56534413\n",
      "Iteration 763, loss = 0.56500885\n",
      "Iteration 764, loss = 0.56467414\n",
      "Iteration 765, loss = 0.56434000\n",
      "Iteration 766, loss = 0.56400643\n",
      "Iteration 767, loss = 0.56367340\n",
      "Iteration 768, loss = 0.56334092\n",
      "Iteration 769, loss = 0.56300897\n",
      "Iteration 770, loss = 0.56267756\n",
      "Iteration 771, loss = 0.56234666\n",
      "Iteration 772, loss = 0.56201628\n",
      "Iteration 773, loss = 0.56168640\n",
      "Iteration 774, loss = 0.56135702\n",
      "Iteration 775, loss = 0.56102814\n",
      "Iteration 776, loss = 0.56069975\n",
      "Iteration 777, loss = 0.56037183\n",
      "Iteration 778, loss = 0.56004440\n",
      "Iteration 779, loss = 0.55971743\n",
      "Iteration 780, loss = 0.55939093\n",
      "Iteration 781, loss = 0.55906488\n",
      "Iteration 782, loss = 0.55873930\n",
      "Iteration 783, loss = 0.55841416\n",
      "Iteration 784, loss = 0.55808946\n",
      "Iteration 785, loss = 0.55776521\n",
      "Iteration 786, loss = 0.55744139\n",
      "Iteration 787, loss = 0.55711800\n",
      "Iteration 788, loss = 0.55679504\n",
      "Iteration 789, loss = 0.55647162\n",
      "Iteration 790, loss = 0.55612423\n",
      "Iteration 791, loss = 0.55576098\n",
      "Iteration 792, loss = 0.55532354\n",
      "Iteration 793, loss = 0.55470837\n",
      "Iteration 794, loss = 0.55370096\n",
      "Iteration 795, loss = 0.55229925\n",
      "Iteration 796, loss = 0.55054155\n",
      "Iteration 797, loss = 0.54858386\n",
      "Iteration 798, loss = 0.54646711\n",
      "Iteration 799, loss = 0.54420678\n",
      "Iteration 800, loss = 0.54182241\n",
      "Iteration 801, loss = 0.53944944\n",
      "Iteration 802, loss = 0.53722588\n",
      "Iteration 803, loss = 0.53536828\n",
      "Iteration 804, loss = 0.53457414\n",
      "Iteration 805, loss = 0.53422631\n",
      "Iteration 806, loss = 0.53355507\n",
      "Iteration 807, loss = 0.53255711\n",
      "Iteration 808, loss = 0.53133175\n",
      "Iteration 809, loss = 0.52991415\n",
      "Iteration 810, loss = 0.52834415\n",
      "Iteration 811, loss = 0.52663023\n",
      "Iteration 812, loss = 0.52485922\n",
      "Iteration 813, loss = 0.52315990\n",
      "Iteration 814, loss = 0.52164999\n",
      "Iteration 815, loss = 0.52035541\n",
      "Iteration 816, loss = 0.51915395\n",
      "Iteration 817, loss = 0.51795683\n",
      "Iteration 818, loss = 0.51670476\n",
      "Iteration 819, loss = 0.51536320\n",
      "Iteration 820, loss = 0.51394015\n",
      "Iteration 821, loss = 0.51244671\n",
      "Iteration 822, loss = 0.51092410\n",
      "Iteration 823, loss = 0.50937667\n",
      "Iteration 824, loss = 0.50787521\n",
      "Iteration 825, loss = 0.50642691\n",
      "Iteration 826, loss = 0.50501268\n",
      "Iteration 827, loss = 0.50361079\n",
      "Iteration 828, loss = 0.50223597\n",
      "Iteration 829, loss = 0.50089258\n",
      "Iteration 830, loss = 0.49951723\n",
      "Iteration 831, loss = 0.49810180\n",
      "Iteration 832, loss = 0.49665960\n",
      "Iteration 833, loss = 0.49524580\n",
      "Iteration 834, loss = 0.49386268\n",
      "Iteration 835, loss = 0.49249660\n",
      "Iteration 836, loss = 0.49113644\n",
      "Iteration 837, loss = 0.48978254\n",
      "Iteration 838, loss = 0.48843531\n",
      "Iteration 839, loss = 0.48709516\n",
      "Iteration 840, loss = 0.48576254\n",
      "Iteration 841, loss = 0.48444134\n",
      "Iteration 842, loss = 0.48312872\n",
      "Iteration 843, loss = 0.48182433\n",
      "Iteration 844, loss = 0.48052880\n",
      "Iteration 845, loss = 0.47924269\n",
      "Iteration 846, loss = 0.47796655\n",
      "Iteration 847, loss = 0.47670082\n",
      "Iteration 848, loss = 0.47544591\n",
      "Iteration 849, loss = 0.47420214\n",
      "Iteration 850, loss = 0.47296977\n",
      "Iteration 851, loss = 0.47174990\n",
      "Iteration 852, loss = 0.47054345\n",
      "Iteration 853, loss = 0.46934897\n",
      "Iteration 854, loss = 0.46816643\n",
      "Iteration 855, loss = 0.46699577\n",
      "Iteration 856, loss = 0.46583687\n",
      "Iteration 857, loss = 0.46468959\n",
      "Iteration 858, loss = 0.46355376\n",
      "Iteration 859, loss = 0.46242918\n",
      "Iteration 860, loss = 0.46131928\n",
      "Iteration 861, loss = 0.46022304\n",
      "Iteration 862, loss = 0.45914226\n",
      "Iteration 863, loss = 0.45807360\n",
      "Iteration 864, loss = 0.45701637\n",
      "Iteration 865, loss = 0.45597072\n",
      "Iteration 866, loss = 0.45493756\n",
      "Iteration 867, loss = 0.45391846\n",
      "Iteration 868, loss = 0.45291299\n",
      "Iteration 869, loss = 0.45191933\n",
      "Iteration 870, loss = 0.45093731\n",
      "Iteration 871, loss = 0.44996669\n",
      "Iteration 872, loss = 0.44900721\n",
      "Iteration 873, loss = 0.44805855\n",
      "Iteration 874, loss = 0.44712041\n",
      "Iteration 875, loss = 0.44619243\n",
      "Iteration 876, loss = 0.44527429\n",
      "Iteration 877, loss = 0.44436563\n",
      "Iteration 878, loss = 0.44346612\n",
      "Iteration 879, loss = 0.44257585\n",
      "Iteration 880, loss = 0.44170019\n",
      "Iteration 881, loss = 0.44083357\n",
      "Iteration 882, loss = 0.43997595\n",
      "Iteration 883, loss = 0.43912761\n",
      "Iteration 884, loss = 0.43828973\n",
      "Iteration 885, loss = 0.43746075\n",
      "Iteration 886, loss = 0.43664076\n",
      "Iteration 887, loss = 0.43582925\n",
      "Iteration 888, loss = 0.43502598\n",
      "Iteration 889, loss = 0.43423150\n",
      "Iteration 890, loss = 0.43344790\n",
      "Iteration 891, loss = 0.43267247\n",
      "Iteration 892, loss = 0.43190421\n",
      "Iteration 893, loss = 0.43114342\n",
      "Iteration 894, loss = 0.43039034\n",
      "Iteration 895, loss = 0.42964648\n",
      "Iteration 896, loss = 0.42891246\n",
      "Iteration 897, loss = 0.42818610\n",
      "Iteration 898, loss = 0.42746767\n",
      "Iteration 899, loss = 0.42675721\n",
      "Iteration 900, loss = 0.42605350\n",
      "Iteration 901, loss = 0.42535715\n",
      "Iteration 902, loss = 0.42466808\n",
      "Iteration 903, loss = 0.42398698\n",
      "Iteration 904, loss = 0.42331245\n",
      "Iteration 905, loss = 0.42264422\n",
      "Iteration 906, loss = 0.42198207\n",
      "Iteration 907, loss = 0.42132581\n",
      "Iteration 908, loss = 0.42067522\n",
      "Iteration 909, loss = 0.42003216\n",
      "Iteration 910, loss = 0.41939911\n",
      "Iteration 911, loss = 0.41877106\n",
      "Iteration 912, loss = 0.41814773\n",
      "Iteration 913, loss = 0.41752937\n",
      "Iteration 914, loss = 0.41691618\n",
      "Iteration 915, loss = 0.41630833\n",
      "Iteration 916, loss = 0.41570618\n",
      "Iteration 917, loss = 0.41511057\n",
      "Iteration 918, loss = 0.41451992\n",
      "Iteration 919, loss = 0.41393396\n",
      "Iteration 920, loss = 0.41335242\n",
      "Iteration 921, loss = 0.41277506\n",
      "Iteration 922, loss = 0.41220398\n",
      "Iteration 923, loss = 0.41163796\n",
      "Iteration 924, loss = 0.41107627\n",
      "Iteration 925, loss = 0.41051879\n",
      "Iteration 926, loss = 0.40996541\n",
      "Iteration 927, loss = 0.40941602\n",
      "Iteration 928, loss = 0.40887049\n",
      "Iteration 929, loss = 0.40832937\n",
      "Iteration 930, loss = 0.40779353\n",
      "Iteration 931, loss = 0.40726261\n",
      "Iteration 932, loss = 0.40673528\n",
      "Iteration 933, loss = 0.40621151\n",
      "Iteration 934, loss = 0.40569184\n",
      "Iteration 935, loss = 0.40517497\n",
      "Iteration 936, loss = 0.40466135\n",
      "Iteration 937, loss = 0.40415170\n",
      "Iteration 938, loss = 0.40364545\n",
      "Iteration 939, loss = 0.40314274\n",
      "Iteration 940, loss = 0.40264249\n",
      "Iteration 941, loss = 0.40214557\n",
      "Iteration 942, loss = 0.40165229\n",
      "Iteration 943, loss = 0.40116170\n",
      "Iteration 944, loss = 0.40067384\n",
      "Iteration 945, loss = 0.40018875\n",
      "Iteration 946, loss = 0.39970641\n",
      "Iteration 947, loss = 0.39922682\n",
      "Iteration 948, loss = 0.39875044\n",
      "Iteration 949, loss = 0.39827634\n",
      "Iteration 950, loss = 0.39780427\n",
      "Iteration 951, loss = 0.39733484\n",
      "Iteration 952, loss = 0.39686788\n",
      "Iteration 953, loss = 0.39640316\n",
      "Iteration 954, loss = 0.39594064\n",
      "Iteration 955, loss = 0.39548104\n",
      "Iteration 956, loss = 0.39502366\n",
      "Iteration 957, loss = 0.39456835\n",
      "Iteration 958, loss = 0.39411504\n",
      "Iteration 959, loss = 0.39366367\n",
      "Iteration 960, loss = 0.39321418\n",
      "Iteration 961, loss = 0.39276648\n",
      "Iteration 962, loss = 0.39232052\n",
      "Iteration 963, loss = 0.39187623\n",
      "Iteration 964, loss = 0.39143371\n",
      "Iteration 965, loss = 0.39099319\n",
      "Iteration 966, loss = 0.39055423\n",
      "Iteration 967, loss = 0.39011801\n",
      "Iteration 968, loss = 0.38968568\n",
      "Iteration 969, loss = 0.38925447\n",
      "Iteration 970, loss = 0.38882396\n",
      "Iteration 971, loss = 0.38839436\n",
      "Iteration 972, loss = 0.38796584\n",
      "Iteration 973, loss = 0.38753855\n",
      "Iteration 974, loss = 0.38711417\n",
      "Iteration 975, loss = 0.38669163\n",
      "Iteration 976, loss = 0.38627067\n",
      "Iteration 977, loss = 0.38585132\n",
      "Iteration 978, loss = 0.38543294\n",
      "Iteration 979, loss = 0.38501599\n",
      "Iteration 980, loss = 0.38460029\n",
      "Iteration 981, loss = 0.38418578\n",
      "Iteration 982, loss = 0.38377241\n",
      "Iteration 983, loss = 0.38336012\n",
      "Iteration 984, loss = 0.38294888\n",
      "Iteration 985, loss = 0.38253930\n",
      "Iteration 986, loss = 0.38213207\n",
      "Iteration 987, loss = 0.38172588\n",
      "Iteration 988, loss = 0.38132075\n",
      "Iteration 989, loss = 0.38091667\n",
      "Iteration 990, loss = 0.38051366\n",
      "Iteration 991, loss = 0.38011172\n",
      "Iteration 992, loss = 0.37971083\n",
      "Iteration 993, loss = 0.37931100\n",
      "Iteration 994, loss = 0.37891220\n",
      "Iteration 995, loss = 0.37851442\n",
      "Iteration 996, loss = 0.37811763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 997, loss = 0.37772182\n",
      "Iteration 998, loss = 0.37732695\n",
      "Iteration 999, loss = 0.37693300\n",
      "Iteration 1000, loss = 0.37653993\n",
      "Iteration 1, loss = 2.15205734\n",
      "Iteration 2, loss = 2.13625730\n",
      "Iteration 3, loss = 2.12053503\n",
      "Iteration 4, loss = 2.10472782\n",
      "Iteration 5, loss = 2.08882000\n",
      "Iteration 6, loss = 2.07285903\n",
      "Iteration 7, loss = 2.05691201\n",
      "Iteration 8, loss = 2.04098178\n",
      "Iteration 9, loss = 2.02503288\n",
      "Iteration 10, loss = 2.00917658\n",
      "Iteration 11, loss = 1.99341413\n",
      "Iteration 12, loss = 1.97776718\n",
      "Iteration 13, loss = 1.96216538\n",
      "Iteration 14, loss = 1.94644776\n",
      "Iteration 15, loss = 1.93058188\n",
      "Iteration 16, loss = 1.91475165\n",
      "Iteration 17, loss = 1.89880884\n",
      "Iteration 18, loss = 1.88290629\n",
      "Iteration 19, loss = 1.86702468\n",
      "Iteration 20, loss = 1.85120100\n",
      "Iteration 21, loss = 1.83534798\n",
      "Iteration 22, loss = 1.81955685\n",
      "Iteration 23, loss = 1.80391254\n",
      "Iteration 24, loss = 1.78840632\n",
      "Iteration 25, loss = 1.77305669\n",
      "Iteration 26, loss = 1.75785934\n",
      "Iteration 27, loss = 1.74282038\n",
      "Iteration 28, loss = 1.72794540\n",
      "Iteration 29, loss = 1.71325390\n",
      "Iteration 30, loss = 1.69872969\n",
      "Iteration 31, loss = 1.68437277\n",
      "Iteration 32, loss = 1.67018568\n",
      "Iteration 33, loss = 1.65617073\n",
      "Iteration 34, loss = 1.64232993\n",
      "Iteration 35, loss = 1.62866510\n",
      "Iteration 36, loss = 1.61519339\n",
      "Iteration 37, loss = 1.60190549\n",
      "Iteration 38, loss = 1.58879941\n",
      "Iteration 39, loss = 1.57587597\n",
      "Iteration 40, loss = 1.56313576\n",
      "Iteration 41, loss = 1.55057923\n",
      "Iteration 42, loss = 1.53820663\n",
      "Iteration 43, loss = 1.52601805\n",
      "Iteration 44, loss = 1.51401343\n",
      "Iteration 45, loss = 1.50219258\n",
      "Iteration 46, loss = 1.49055514\n",
      "Iteration 47, loss = 1.47910066\n",
      "Iteration 48, loss = 1.46782855\n",
      "Iteration 49, loss = 1.45673813\n",
      "Iteration 50, loss = 1.44582863\n",
      "Iteration 51, loss = 1.43509918\n",
      "Iteration 52, loss = 1.42454885\n",
      "Iteration 53, loss = 1.41417663\n",
      "Iteration 54, loss = 1.40398146\n",
      "Iteration 55, loss = 1.39396221\n",
      "Iteration 56, loss = 1.38411771\n",
      "Iteration 57, loss = 1.37444672\n",
      "Iteration 58, loss = 1.36494796\n",
      "Iteration 59, loss = 1.35562011\n",
      "Iteration 60, loss = 1.34646180\n",
      "Iteration 61, loss = 1.33747159\n",
      "Iteration 62, loss = 1.32864801\n",
      "Iteration 63, loss = 1.31998955\n",
      "Iteration 64, loss = 1.31149461\n",
      "Iteration 65, loss = 1.30316157\n",
      "Iteration 66, loss = 1.29497544\n",
      "Iteration 67, loss = 1.28693475\n",
      "Iteration 68, loss = 1.27904979\n",
      "Iteration 69, loss = 1.27131898\n",
      "Iteration 70, loss = 1.26374064\n",
      "Iteration 71, loss = 1.25631301\n",
      "Iteration 72, loss = 1.24903426\n",
      "Iteration 73, loss = 1.24190246\n",
      "Iteration 74, loss = 1.23491562\n",
      "Iteration 75, loss = 1.22807168\n",
      "Iteration 76, loss = 1.22136851\n",
      "Iteration 77, loss = 1.21480392\n",
      "Iteration 78, loss = 1.20837567\n",
      "Iteration 79, loss = 1.20208146\n",
      "Iteration 80, loss = 1.19591894\n",
      "Iteration 81, loss = 1.18988572\n",
      "Iteration 82, loss = 1.18397937\n",
      "Iteration 83, loss = 1.17819744\n",
      "Iteration 84, loss = 1.17252646\n",
      "Iteration 85, loss = 1.16696750\n",
      "Iteration 86, loss = 1.16151185\n",
      "Iteration 87, loss = 1.15616402\n",
      "Iteration 88, loss = 1.15092698\n",
      "Iteration 89, loss = 1.14577741\n",
      "Iteration 90, loss = 1.14072425\n",
      "Iteration 91, loss = 1.13576363\n",
      "Iteration 92, loss = 1.13084469\n",
      "Iteration 93, loss = 1.12602128\n",
      "Iteration 94, loss = 1.12129220\n",
      "Iteration 95, loss = 1.11665610\n",
      "Iteration 96, loss = 1.11211150\n",
      "Iteration 97, loss = 1.10764561\n",
      "Iteration 98, loss = 1.10324742\n",
      "Iteration 99, loss = 1.09893198\n",
      "Iteration 100, loss = 1.09469980\n",
      "Iteration 101, loss = 1.09054011\n",
      "Iteration 102, loss = 1.08645668\n",
      "Iteration 103, loss = 1.08245673\n",
      "Iteration 104, loss = 1.07853832\n",
      "Iteration 105, loss = 1.07468724\n",
      "Iteration 106, loss = 1.07090848\n",
      "Iteration 107, loss = 1.06718450\n",
      "Iteration 108, loss = 1.06351290\n",
      "Iteration 109, loss = 1.05990026\n",
      "Iteration 110, loss = 1.05634776\n",
      "Iteration 111, loss = 1.05284640\n",
      "Iteration 112, loss = 1.04939281\n",
      "Iteration 113, loss = 1.04598692\n",
      "Iteration 114, loss = 1.04262979\n",
      "Iteration 115, loss = 1.03932143\n",
      "Iteration 116, loss = 1.03604752\n",
      "Iteration 117, loss = 1.03282071\n",
      "Iteration 118, loss = 1.02963170\n",
      "Iteration 119, loss = 1.02647961\n",
      "Iteration 120, loss = 1.02336870\n",
      "Iteration 121, loss = 1.02030221\n",
      "Iteration 122, loss = 1.01728240\n",
      "Iteration 123, loss = 1.01430075\n",
      "Iteration 124, loss = 1.01136199\n",
      "Iteration 125, loss = 1.00846127\n",
      "Iteration 126, loss = 1.00561318\n",
      "Iteration 127, loss = 1.00280906\n",
      "Iteration 128, loss = 1.00004810\n",
      "Iteration 129, loss = 0.99731936\n",
      "Iteration 130, loss = 0.99463055\n",
      "Iteration 131, loss = 0.99198189\n",
      "Iteration 132, loss = 0.98937262\n",
      "Iteration 133, loss = 0.98679720\n",
      "Iteration 134, loss = 0.98425388\n",
      "Iteration 135, loss = 0.98174018\n",
      "Iteration 136, loss = 0.97926214\n",
      "Iteration 137, loss = 0.97681915\n",
      "Iteration 138, loss = 0.97441057\n",
      "Iteration 139, loss = 0.97203576\n",
      "Iteration 140, loss = 0.96968812\n",
      "Iteration 141, loss = 0.96736960\n",
      "Iteration 142, loss = 0.96508252\n",
      "Iteration 143, loss = 0.96282633\n",
      "Iteration 144, loss = 0.96059693\n",
      "Iteration 145, loss = 0.95839426\n",
      "Iteration 146, loss = 0.95622062\n",
      "Iteration 147, loss = 0.95407309\n",
      "Iteration 148, loss = 0.95195110\n",
      "Iteration 149, loss = 0.94985655\n",
      "Iteration 150, loss = 0.94778902\n",
      "Iteration 151, loss = 0.94574811\n",
      "Iteration 152, loss = 0.94373338\n",
      "Iteration 153, loss = 0.94174442\n",
      "Iteration 154, loss = 0.93977861\n",
      "Iteration 155, loss = 0.93783505\n",
      "Iteration 156, loss = 0.93591591\n",
      "Iteration 157, loss = 0.93402084\n",
      "Iteration 158, loss = 0.93214951\n",
      "Iteration 159, loss = 0.93030176\n",
      "Iteration 160, loss = 0.92847762\n",
      "Iteration 161, loss = 0.92667601\n",
      "Iteration 162, loss = 0.92489660\n",
      "Iteration 163, loss = 0.92313906\n",
      "Iteration 164, loss = 0.92140309\n",
      "Iteration 165, loss = 0.91968838\n",
      "Iteration 166, loss = 0.91799154\n",
      "Iteration 167, loss = 0.91631467\n",
      "Iteration 168, loss = 0.91465811\n",
      "Iteration 169, loss = 0.91302165\n",
      "Iteration 170, loss = 0.91140506\n",
      "Iteration 171, loss = 0.90980810\n",
      "Iteration 172, loss = 0.90823057\n",
      "Iteration 173, loss = 0.90667287\n",
      "Iteration 174, loss = 0.90513445\n",
      "Iteration 175, loss = 0.90361501\n",
      "Iteration 176, loss = 0.90211430\n",
      "Iteration 177, loss = 0.90063211\n",
      "Iteration 178, loss = 0.89916822\n",
      "Iteration 179, loss = 0.89772241\n",
      "Iteration 180, loss = 0.89629445\n",
      "Iteration 181, loss = 0.89488414\n",
      "Iteration 182, loss = 0.89349124\n",
      "Iteration 183, loss = 0.89211555\n",
      "Iteration 184, loss = 0.89075685\n",
      "Iteration 185, loss = 0.88941492\n",
      "Iteration 186, loss = 0.88808956\n",
      "Iteration 187, loss = 0.88678055\n",
      "Iteration 188, loss = 0.88548768\n",
      "Iteration 189, loss = 0.88421075\n",
      "Iteration 190, loss = 0.88294954\n",
      "Iteration 191, loss = 0.88170384\n",
      "Iteration 192, loss = 0.88047345\n",
      "Iteration 193, loss = 0.87925816\n",
      "Iteration 194, loss = 0.87805777\n",
      "Iteration 195, loss = 0.87687207\n",
      "Iteration 196, loss = 0.87570085\n",
      "Iteration 197, loss = 0.87454392\n",
      "Iteration 198, loss = 0.87340106\n",
      "Iteration 199, loss = 0.87227209\n",
      "Iteration 200, loss = 0.87115678\n",
      "Iteration 201, loss = 0.87005495\n",
      "Iteration 202, loss = 0.86896638\n",
      "Iteration 203, loss = 0.86789089\n",
      "Iteration 204, loss = 0.86682827\n",
      "Iteration 205, loss = 0.86577831\n",
      "Iteration 206, loss = 0.86474083\n",
      "Iteration 207, loss = 0.86371562\n",
      "Iteration 208, loss = 0.86270248\n",
      "Iteration 209, loss = 0.86170122\n",
      "Iteration 210, loss = 0.86071164\n",
      "Iteration 211, loss = 0.85973355\n",
      "Iteration 212, loss = 0.85876675\n",
      "Iteration 213, loss = 0.85781104\n",
      "Iteration 214, loss = 0.85686623\n",
      "Iteration 215, loss = 0.85593213\n",
      "Iteration 216, loss = 0.85500854\n",
      "Iteration 217, loss = 0.85409529\n",
      "Iteration 218, loss = 0.85319217\n",
      "Iteration 219, loss = 0.85229899\n",
      "Iteration 220, loss = 0.85141558\n",
      "Iteration 221, loss = 0.85054174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 222, loss = 0.84967729\n",
      "Iteration 223, loss = 0.84882204\n",
      "Iteration 224, loss = 0.84797581\n",
      "Iteration 225, loss = 0.84713842\n",
      "Iteration 226, loss = 0.84630969\n",
      "Iteration 227, loss = 0.84548944\n",
      "Iteration 228, loss = 0.84467750\n",
      "Iteration 229, loss = 0.84387368\n",
      "Iteration 230, loss = 0.84307782\n",
      "Iteration 231, loss = 0.84228975\n",
      "Iteration 232, loss = 0.84150928\n",
      "Iteration 233, loss = 0.84073625\n",
      "Iteration 234, loss = 0.83997050\n",
      "Iteration 235, loss = 0.83921187\n",
      "Iteration 236, loss = 0.83846017\n",
      "Iteration 237, loss = 0.83771527\n",
      "Iteration 238, loss = 0.83697698\n",
      "Iteration 239, loss = 0.83624517\n",
      "Iteration 240, loss = 0.83551966\n",
      "Iteration 241, loss = 0.83480031\n",
      "Iteration 242, loss = 0.83408697\n",
      "Iteration 243, loss = 0.83337948\n",
      "Iteration 244, loss = 0.83267769\n",
      "Iteration 245, loss = 0.83198147\n",
      "Iteration 246, loss = 0.83129066\n",
      "Iteration 247, loss = 0.83060513\n",
      "Iteration 248, loss = 0.82992473\n",
      "Iteration 249, loss = 0.82924932\n",
      "Iteration 250, loss = 0.82857878\n",
      "Iteration 251, loss = 0.82791296\n",
      "Iteration 252, loss = 0.82725174\n",
      "Iteration 253, loss = 0.82659499\n",
      "Iteration 254, loss = 0.82594257\n",
      "Iteration 255, loss = 0.82529437\n",
      "Iteration 256, loss = 0.82465025\n",
      "Iteration 257, loss = 0.82401011\n",
      "Iteration 258, loss = 0.82337381\n",
      "Iteration 259, loss = 0.82274125\n",
      "Iteration 260, loss = 0.82211230\n",
      "Iteration 261, loss = 0.82148687\n",
      "Iteration 262, loss = 0.82086482\n",
      "Iteration 263, loss = 0.82024606\n",
      "Iteration 264, loss = 0.81963048\n",
      "Iteration 265, loss = 0.81901798\n",
      "Iteration 266, loss = 0.81840845\n",
      "Iteration 267, loss = 0.81780178\n",
      "Iteration 268, loss = 0.81719789\n",
      "Iteration 269, loss = 0.81659668\n",
      "Iteration 270, loss = 0.81599805\n",
      "Iteration 271, loss = 0.81540190\n",
      "Iteration 272, loss = 0.81480815\n",
      "Iteration 273, loss = 0.81421672\n",
      "Iteration 274, loss = 0.81362750\n",
      "Iteration 275, loss = 0.81304041\n",
      "Iteration 276, loss = 0.81245538\n",
      "Iteration 277, loss = 0.81187232\n",
      "Iteration 278, loss = 0.81129115\n",
      "Iteration 279, loss = 0.81071178\n",
      "Iteration 280, loss = 0.81013416\n",
      "Iteration 281, loss = 0.80955819\n",
      "Iteration 282, loss = 0.80898381\n",
      "Iteration 283, loss = 0.80841095\n",
      "Iteration 284, loss = 0.80783953\n",
      "Iteration 285, loss = 0.80726949\n",
      "Iteration 286, loss = 0.80670076\n",
      "Iteration 287, loss = 0.80613327\n",
      "Iteration 288, loss = 0.80556697\n",
      "Iteration 289, loss = 0.80500179\n",
      "Iteration 290, loss = 0.80443766\n",
      "Iteration 291, loss = 0.80387453\n",
      "Iteration 292, loss = 0.80331235\n",
      "Iteration 293, loss = 0.80275105\n",
      "Iteration 294, loss = 0.80219058\n",
      "Iteration 295, loss = 0.80163089\n",
      "Iteration 296, loss = 0.80107192\n",
      "Iteration 297, loss = 0.80051363\n",
      "Iteration 298, loss = 0.79995596\n",
      "Iteration 299, loss = 0.79939886\n",
      "Iteration 300, loss = 0.79884230\n",
      "Iteration 301, loss = 0.79828621\n",
      "Iteration 302, loss = 0.79773057\n",
      "Iteration 303, loss = 0.79717532\n",
      "Iteration 304, loss = 0.79662041\n",
      "Iteration 305, loss = 0.79606582\n",
      "Iteration 306, loss = 0.79551150\n",
      "Iteration 307, loss = 0.79495741\n",
      "Iteration 308, loss = 0.79440351\n",
      "Iteration 309, loss = 0.79384977\n",
      "Iteration 310, loss = 0.79329614\n",
      "Iteration 311, loss = 0.79274260\n",
      "Iteration 312, loss = 0.79218912\n",
      "Iteration 313, loss = 0.79163564\n",
      "Iteration 314, loss = 0.79108215\n",
      "Iteration 315, loss = 0.79052922\n",
      "Iteration 316, loss = 0.78997879\n",
      "Iteration 317, loss = 0.78942854\n",
      "Iteration 318, loss = 0.78887844\n",
      "Iteration 319, loss = 0.78832843\n",
      "Iteration 320, loss = 0.78777849\n",
      "Iteration 321, loss = 0.78722858\n",
      "Iteration 322, loss = 0.78667865\n",
      "Iteration 323, loss = 0.78612867\n",
      "Iteration 324, loss = 0.78557861\n",
      "Iteration 325, loss = 0.78502844\n",
      "Iteration 326, loss = 0.78448087\n",
      "Iteration 327, loss = 0.78393444\n",
      "Iteration 328, loss = 0.78338814\n",
      "Iteration 329, loss = 0.78284192\n",
      "Iteration 330, loss = 0.78229575\n",
      "Iteration 331, loss = 0.78174958\n",
      "Iteration 332, loss = 0.78120337\n",
      "Iteration 333, loss = 0.78065711\n",
      "Iteration 334, loss = 0.78011074\n",
      "Iteration 335, loss = 0.77956424\n",
      "Iteration 336, loss = 0.77901757\n",
      "Iteration 337, loss = 0.77847072\n",
      "Iteration 338, loss = 0.77792364\n",
      "Iteration 339, loss = 0.77737632\n",
      "Iteration 340, loss = 0.77682873\n",
      "Iteration 341, loss = 0.77628083\n",
      "Iteration 342, loss = 0.77573262\n",
      "Iteration 343, loss = 0.77518406\n",
      "Iteration 344, loss = 0.77463513\n",
      "Iteration 345, loss = 0.77408581\n",
      "Iteration 346, loss = 0.77353608\n",
      "Iteration 347, loss = 0.77298592\n",
      "Iteration 348, loss = 0.77243531\n",
      "Iteration 349, loss = 0.77188423\n",
      "Iteration 350, loss = 0.77133267\n",
      "Iteration 351, loss = 0.77078061\n",
      "Iteration 352, loss = 0.77022802\n",
      "Iteration 353, loss = 0.76967490\n",
      "Iteration 354, loss = 0.76912124\n",
      "Iteration 355, loss = 0.76856701\n",
      "Iteration 356, loss = 0.76801220\n",
      "Iteration 357, loss = 0.76745680\n",
      "Iteration 358, loss = 0.76690080\n",
      "Iteration 359, loss = 0.76634419\n",
      "Iteration 360, loss = 0.76578695\n",
      "Iteration 361, loss = 0.76522907\n",
      "Iteration 362, loss = 0.76467055\n",
      "Iteration 363, loss = 0.76411137\n",
      "Iteration 364, loss = 0.76355152\n",
      "Iteration 365, loss = 0.76299100\n",
      "Iteration 366, loss = 0.76242979\n",
      "Iteration 367, loss = 0.76186796\n",
      "Iteration 368, loss = 0.76130940\n",
      "Iteration 369, loss = 0.76075047\n",
      "Iteration 370, loss = 0.76019113\n",
      "Iteration 371, loss = 0.75963137\n",
      "Iteration 372, loss = 0.75907197\n",
      "Iteration 373, loss = 0.75851518\n",
      "Iteration 374, loss = 0.75795815\n",
      "Iteration 375, loss = 0.75740085\n",
      "Iteration 376, loss = 0.75684324\n",
      "Iteration 377, loss = 0.75628530\n",
      "Iteration 378, loss = 0.75572699\n",
      "Iteration 379, loss = 0.75516831\n",
      "Iteration 380, loss = 0.75460922\n",
      "Iteration 381, loss = 0.75404970\n",
      "Iteration 382, loss = 0.75348974\n",
      "Iteration 383, loss = 0.75292931\n",
      "Iteration 384, loss = 0.75236840\n",
      "Iteration 385, loss = 0.75180699\n",
      "Iteration 386, loss = 0.75124506\n",
      "Iteration 387, loss = 0.75068261\n",
      "Iteration 388, loss = 0.75011961\n",
      "Iteration 389, loss = 0.74955606\n",
      "Iteration 390, loss = 0.74899194\n",
      "Iteration 391, loss = 0.74842724\n",
      "Iteration 392, loss = 0.74786195\n",
      "Iteration 393, loss = 0.74729605\n",
      "Iteration 394, loss = 0.74672954\n",
      "Iteration 395, loss = 0.74616240\n",
      "Iteration 396, loss = 0.74559464\n",
      "Iteration 397, loss = 0.74502624\n",
      "Iteration 398, loss = 0.74445719\n",
      "Iteration 399, loss = 0.74388748\n",
      "Iteration 400, loss = 0.74331711\n",
      "Iteration 401, loss = 0.74274608\n",
      "Iteration 402, loss = 0.74217437\n",
      "Iteration 403, loss = 0.74160199\n",
      "Iteration 404, loss = 0.74102892\n",
      "Iteration 405, loss = 0.74045516\n",
      "Iteration 406, loss = 0.73988071\n",
      "Iteration 407, loss = 0.73930557\n",
      "Iteration 408, loss = 0.73872973\n",
      "Iteration 409, loss = 0.73815319\n",
      "Iteration 410, loss = 0.73757666\n",
      "Iteration 411, loss = 0.73700061\n",
      "Iteration 412, loss = 0.73642403\n",
      "Iteration 413, loss = 0.73584690\n",
      "Iteration 414, loss = 0.73526921\n",
      "Iteration 415, loss = 0.73469094\n",
      "Iteration 416, loss = 0.73411386\n",
      "Iteration 417, loss = 0.73353882\n",
      "Iteration 418, loss = 0.73296352\n",
      "Iteration 419, loss = 0.73238793\n",
      "Iteration 420, loss = 0.73181202\n",
      "Iteration 421, loss = 0.73123575\n",
      "Iteration 422, loss = 0.73066060\n",
      "Iteration 423, loss = 0.73008728\n",
      "Iteration 424, loss = 0.72951377\n",
      "Iteration 425, loss = 0.72894004\n",
      "Iteration 426, loss = 0.72836606\n",
      "Iteration 427, loss = 0.72779180\n",
      "Iteration 428, loss = 0.72721723\n",
      "Iteration 429, loss = 0.72664234\n",
      "Iteration 430, loss = 0.72606710\n",
      "Iteration 431, loss = 0.72549149\n",
      "Iteration 432, loss = 0.72491549\n",
      "Iteration 433, loss = 0.72433910\n",
      "Iteration 434, loss = 0.72376230\n",
      "Iteration 435, loss = 0.72318507\n",
      "Iteration 436, loss = 0.72260740\n",
      "Iteration 437, loss = 0.72202929\n",
      "Iteration 438, loss = 0.72145071\n",
      "Iteration 439, loss = 0.72087166\n",
      "Iteration 440, loss = 0.72029214\n",
      "Iteration 441, loss = 0.71971213\n",
      "Iteration 442, loss = 0.71913163\n",
      "Iteration 443, loss = 0.71855227\n",
      "Iteration 444, loss = 0.71797290\n",
      "Iteration 445, loss = 0.71739320\n",
      "Iteration 446, loss = 0.71681316\n",
      "Iteration 447, loss = 0.71623437\n",
      "Iteration 448, loss = 0.71565581\n",
      "Iteration 449, loss = 0.71507704\n",
      "Iteration 450, loss = 0.71449803\n",
      "Iteration 451, loss = 0.71391877\n",
      "Iteration 452, loss = 0.71333923\n",
      "Iteration 453, loss = 0.71275940\n",
      "Iteration 454, loss = 0.71217925\n",
      "Iteration 455, loss = 0.71159878\n",
      "Iteration 456, loss = 0.71101796\n",
      "Iteration 457, loss = 0.71043680\n",
      "Iteration 458, loss = 0.70985528\n",
      "Iteration 459, loss = 0.70927338\n",
      "Iteration 460, loss = 0.70869111\n",
      "Iteration 461, loss = 0.70810845\n",
      "Iteration 462, loss = 0.70752540\n",
      "Iteration 463, loss = 0.70694196\n",
      "Iteration 464, loss = 0.70635811\n",
      "Iteration 465, loss = 0.70577386\n",
      "Iteration 466, loss = 0.70518920\n",
      "Iteration 467, loss = 0.70460412\n",
      "Iteration 468, loss = 0.70401863\n",
      "Iteration 469, loss = 0.70343273\n",
      "Iteration 470, loss = 0.70284640\n",
      "Iteration 471, loss = 0.70226097\n",
      "Iteration 472, loss = 0.70167651\n",
      "Iteration 473, loss = 0.70109187\n",
      "Iteration 474, loss = 0.70050703\n",
      "Iteration 475, loss = 0.69992196\n",
      "Iteration 476, loss = 0.69933765\n",
      "Iteration 477, loss = 0.69875446\n",
      "Iteration 478, loss = 0.69817122\n",
      "Iteration 479, loss = 0.69758790\n",
      "Iteration 480, loss = 0.69700447\n",
      "Iteration 481, loss = 0.69642093\n",
      "Iteration 482, loss = 0.69583749\n",
      "Iteration 483, loss = 0.69525644\n",
      "Iteration 484, loss = 0.69467678\n",
      "Iteration 485, loss = 0.69410037\n",
      "Iteration 486, loss = 0.69352429\n",
      "Iteration 487, loss = 0.69294846\n",
      "Iteration 488, loss = 0.69237285\n",
      "Iteration 489, loss = 0.69179741\n",
      "Iteration 490, loss = 0.69122211\n",
      "Iteration 491, loss = 0.69064691\n",
      "Iteration 492, loss = 0.69007177\n",
      "Iteration 493, loss = 0.68949719\n",
      "Iteration 494, loss = 0.68892437\n",
      "Iteration 495, loss = 0.68835249\n",
      "Iteration 496, loss = 0.68778370\n",
      "Iteration 497, loss = 0.68721524\n",
      "Iteration 498, loss = 0.68664708\n",
      "Iteration 499, loss = 0.68607916\n",
      "Iteration 500, loss = 0.68551147\n",
      "Iteration 501, loss = 0.68494397\n",
      "Iteration 502, loss = 0.68437663\n",
      "Iteration 503, loss = 0.68380943\n",
      "Iteration 504, loss = 0.68324234\n",
      "Iteration 505, loss = 0.68267536\n",
      "Iteration 506, loss = 0.68210845\n",
      "Iteration 507, loss = 0.68154160\n",
      "Iteration 508, loss = 0.68097480\n",
      "Iteration 509, loss = 0.68040804\n",
      "Iteration 510, loss = 0.67984131\n",
      "Iteration 511, loss = 0.67927458\n",
      "Iteration 512, loss = 0.67870785\n",
      "Iteration 513, loss = 0.67814112\n",
      "Iteration 514, loss = 0.67757438\n",
      "Iteration 515, loss = 0.67700761\n",
      "Iteration 516, loss = 0.67644329\n",
      "Iteration 517, loss = 0.67587956\n",
      "Iteration 518, loss = 0.67531598\n",
      "Iteration 519, loss = 0.67475255\n",
      "Iteration 520, loss = 0.67418926\n",
      "Iteration 521, loss = 0.67362608\n",
      "Iteration 522, loss = 0.67306300\n",
      "Iteration 523, loss = 0.67250001\n",
      "Iteration 524, loss = 0.67193711\n",
      "Iteration 525, loss = 0.67137427\n",
      "Iteration 526, loss = 0.67081150\n",
      "Iteration 527, loss = 0.67024878\n",
      "Iteration 528, loss = 0.66968610\n",
      "Iteration 529, loss = 0.66912346\n",
      "Iteration 530, loss = 0.66856145\n",
      "Iteration 531, loss = 0.66800165\n",
      "Iteration 532, loss = 0.66744211\n",
      "Iteration 533, loss = 0.66688278\n",
      "Iteration 534, loss = 0.66632390\n",
      "Iteration 535, loss = 0.66576756\n",
      "Iteration 536, loss = 0.66521157\n",
      "Iteration 537, loss = 0.66465592\n",
      "Iteration 538, loss = 0.66410279\n",
      "Iteration 539, loss = 0.66355294\n",
      "Iteration 540, loss = 0.66300371\n",
      "Iteration 541, loss = 0.66245763\n",
      "Iteration 542, loss = 0.66191492\n",
      "Iteration 543, loss = 0.66137315\n",
      "Iteration 544, loss = 0.66083195\n",
      "Iteration 545, loss = 0.66029440\n",
      "Iteration 546, loss = 0.65975958\n",
      "Iteration 547, loss = 0.65922570\n",
      "Iteration 548, loss = 0.65869411\n",
      "Iteration 549, loss = 0.65816462\n",
      "Iteration 550, loss = 0.65763613\n",
      "Iteration 551, loss = 0.65710967\n",
      "Iteration 552, loss = 0.65658731\n",
      "Iteration 553, loss = 0.65606680\n",
      "Iteration 554, loss = 0.65554744\n",
      "Iteration 555, loss = 0.65502920\n",
      "Iteration 556, loss = 0.65451202\n",
      "Iteration 557, loss = 0.65399588\n",
      "Iteration 558, loss = 0.65348072\n",
      "Iteration 559, loss = 0.65296652\n",
      "Iteration 560, loss = 0.65245419\n",
      "Iteration 561, loss = 0.65194359\n",
      "Iteration 562, loss = 0.65143399\n",
      "Iteration 563, loss = 0.65092534\n",
      "Iteration 564, loss = 0.65041762\n",
      "Iteration 565, loss = 0.64991077\n",
      "Iteration 566, loss = 0.64940478\n",
      "Iteration 567, loss = 0.64889960\n",
      "Iteration 568, loss = 0.64839521\n",
      "Iteration 569, loss = 0.64789157\n",
      "Iteration 570, loss = 0.64738866\n",
      "Iteration 571, loss = 0.64688645\n",
      "Iteration 572, loss = 0.64638490\n",
      "Iteration 573, loss = 0.64588401\n",
      "Iteration 574, loss = 0.64538374\n",
      "Iteration 575, loss = 0.64488407\n",
      "Iteration 576, loss = 0.64438499\n",
      "Iteration 577, loss = 0.64388647\n",
      "Iteration 578, loss = 0.64338850\n",
      "Iteration 579, loss = 0.64289267\n",
      "Iteration 580, loss = 0.64239797\n",
      "Iteration 581, loss = 0.64190438\n",
      "Iteration 582, loss = 0.64141332\n",
      "Iteration 583, loss = 0.64092312\n",
      "Iteration 584, loss = 0.64043372\n",
      "Iteration 585, loss = 0.63994511\n",
      "Iteration 586, loss = 0.63945726\n",
      "Iteration 587, loss = 0.63897014\n",
      "Iteration 588, loss = 0.63848372\n",
      "Iteration 589, loss = 0.63799799\n",
      "Iteration 590, loss = 0.63751292\n",
      "Iteration 591, loss = 0.63702850\n",
      "Iteration 592, loss = 0.63654470\n",
      "Iteration 593, loss = 0.63606151\n",
      "Iteration 594, loss = 0.63557891\n",
      "Iteration 595, loss = 0.63509689\n",
      "Iteration 596, loss = 0.63461544\n",
      "Iteration 597, loss = 0.63413799\n",
      "Iteration 598, loss = 0.63366205\n",
      "Iteration 599, loss = 0.63318696\n",
      "Iteration 600, loss = 0.63271354\n",
      "Iteration 601, loss = 0.63224211\n",
      "Iteration 602, loss = 0.63177158\n",
      "Iteration 603, loss = 0.63130194\n",
      "Iteration 604, loss = 0.63083315\n",
      "Iteration 605, loss = 0.63036550\n",
      "Iteration 606, loss = 0.62990014\n",
      "Iteration 607, loss = 0.62943567\n",
      "Iteration 608, loss = 0.62897204\n",
      "Iteration 609, loss = 0.62850925\n",
      "Iteration 610, loss = 0.62804859\n",
      "Iteration 611, loss = 0.62758915\n",
      "Iteration 612, loss = 0.62713063\n",
      "Iteration 613, loss = 0.62667298\n",
      "Iteration 614, loss = 0.62621618\n",
      "Iteration 615, loss = 0.62576143\n",
      "Iteration 616, loss = 0.62530813\n",
      "Iteration 617, loss = 0.62485577\n",
      "Iteration 618, loss = 0.62440430\n",
      "Iteration 619, loss = 0.62395373\n",
      "Iteration 620, loss = 0.62350401\n",
      "Iteration 621, loss = 0.62305634\n",
      "Iteration 622, loss = 0.62260953\n",
      "Iteration 623, loss = 0.62216354\n",
      "Iteration 624, loss = 0.62171881\n",
      "Iteration 625, loss = 0.62127606\n",
      "Iteration 626, loss = 0.62083423\n",
      "Iteration 627, loss = 0.62039329\n",
      "Iteration 628, loss = 0.61995323\n",
      "Iteration 629, loss = 0.61951403\n",
      "Iteration 630, loss = 0.61907567\n",
      "Iteration 631, loss = 0.61863814\n",
      "Iteration 632, loss = 0.61820141\n",
      "Iteration 633, loss = 0.61776548\n",
      "Iteration 634, loss = 0.61733034\n",
      "Iteration 635, loss = 0.61689596\n",
      "Iteration 636, loss = 0.61646233\n",
      "Iteration 637, loss = 0.61602944\n",
      "Iteration 638, loss = 0.61559727\n",
      "Iteration 639, loss = 0.61516582\n",
      "Iteration 640, loss = 0.61473506\n",
      "Iteration 641, loss = 0.61430500\n",
      "Iteration 642, loss = 0.61387560\n",
      "Iteration 643, loss = 0.61344687\n",
      "Iteration 644, loss = 0.61301918\n",
      "Iteration 645, loss = 0.61259365\n",
      "Iteration 646, loss = 0.61216891\n",
      "Iteration 647, loss = 0.61174494\n",
      "Iteration 648, loss = 0.61132173\n",
      "Iteration 649, loss = 0.61089925\n",
      "Iteration 650, loss = 0.61047750\n",
      "Iteration 651, loss = 0.61005645\n",
      "Iteration 652, loss = 0.60963646\n",
      "Iteration 653, loss = 0.60921762\n",
      "Iteration 654, loss = 0.60879982\n",
      "Iteration 655, loss = 0.60838408\n",
      "Iteration 656, loss = 0.60796916\n",
      "Iteration 657, loss = 0.60755503\n",
      "Iteration 658, loss = 0.60714169\n",
      "Iteration 659, loss = 0.60672912\n",
      "Iteration 660, loss = 0.60631730\n",
      "Iteration 661, loss = 0.60590622\n",
      "Iteration 662, loss = 0.60549586\n",
      "Iteration 663, loss = 0.60508622\n",
      "Iteration 664, loss = 0.60467727\n",
      "Iteration 665, loss = 0.60426900\n",
      "Iteration 666, loss = 0.60386141\n",
      "Iteration 667, loss = 0.60345447\n",
      "Iteration 668, loss = 0.60304818\n",
      "Iteration 669, loss = 0.60264253\n",
      "Iteration 670, loss = 0.60223751\n",
      "Iteration 671, loss = 0.60183309\n",
      "Iteration 672, loss = 0.60142928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 673, loss = 0.60102607\n",
      "Iteration 674, loss = 0.60062343\n",
      "Iteration 675, loss = 0.60022137\n",
      "Iteration 676, loss = 0.59981988\n",
      "Iteration 677, loss = 0.59941894\n",
      "Iteration 678, loss = 0.59901854\n",
      "Iteration 679, loss = 0.59861868\n",
      "Iteration 680, loss = 0.59821936\n",
      "Iteration 681, loss = 0.59782055\n",
      "Iteration 682, loss = 0.59742226\n",
      "Iteration 683, loss = 0.59702447\n",
      "Iteration 684, loss = 0.59662718\n",
      "Iteration 685, loss = 0.59623296\n",
      "Iteration 686, loss = 0.59584311\n",
      "Iteration 687, loss = 0.59545525\n",
      "Iteration 688, loss = 0.59506903\n",
      "Iteration 689, loss = 0.59468368\n",
      "Iteration 690, loss = 0.59429918\n",
      "Iteration 691, loss = 0.59391555\n",
      "Iteration 692, loss = 0.59353277\n",
      "Iteration 693, loss = 0.59315085\n",
      "Iteration 694, loss = 0.59276977\n",
      "Iteration 695, loss = 0.59238954\n",
      "Iteration 696, loss = 0.59201014\n",
      "Iteration 697, loss = 0.59163157\n",
      "Iteration 698, loss = 0.59125382\n",
      "Iteration 699, loss = 0.59087686\n",
      "Iteration 700, loss = 0.59050070\n",
      "Iteration 701, loss = 0.59012530\n",
      "Iteration 702, loss = 0.58975065\n",
      "Iteration 703, loss = 0.58937673\n",
      "Iteration 704, loss = 0.58900353\n",
      "Iteration 705, loss = 0.58863103\n",
      "Iteration 706, loss = 0.58825920\n",
      "Iteration 707, loss = 0.58788803\n",
      "Iteration 708, loss = 0.58751750\n",
      "Iteration 709, loss = 0.58714759\n",
      "Iteration 710, loss = 0.58677830\n",
      "Iteration 711, loss = 0.58640959\n",
      "Iteration 712, loss = 0.58604147\n",
      "Iteration 713, loss = 0.58567392\n",
      "Iteration 714, loss = 0.58530692\n",
      "Iteration 715, loss = 0.58494046\n",
      "Iteration 716, loss = 0.58457516\n",
      "Iteration 717, loss = 0.58421050\n",
      "Iteration 718, loss = 0.58384635\n",
      "Iteration 719, loss = 0.58348270\n",
      "Iteration 720, loss = 0.58311956\n",
      "Iteration 721, loss = 0.58275694\n",
      "Iteration 722, loss = 0.58239483\n",
      "Iteration 723, loss = 0.58203483\n",
      "Iteration 724, loss = 0.58167716\n",
      "Iteration 725, loss = 0.58132023\n",
      "Iteration 726, loss = 0.58096537\n",
      "Iteration 727, loss = 0.58061154\n",
      "Iteration 728, loss = 0.58025845\n",
      "Iteration 729, loss = 0.57990611\n",
      "Iteration 730, loss = 0.57955449\n",
      "Iteration 731, loss = 0.57920360\n",
      "Iteration 732, loss = 0.57885342\n",
      "Iteration 733, loss = 0.57850395\n",
      "Iteration 734, loss = 0.57815564\n",
      "Iteration 735, loss = 0.57780883\n",
      "Iteration 736, loss = 0.57746277\n",
      "Iteration 737, loss = 0.57711745\n",
      "Iteration 738, loss = 0.57677287\n",
      "Iteration 739, loss = 0.57642903\n",
      "Iteration 740, loss = 0.57608593\n",
      "Iteration 741, loss = 0.57574356\n",
      "Iteration 742, loss = 0.57540191\n",
      "Iteration 743, loss = 0.57506098\n",
      "Iteration 744, loss = 0.57472075\n",
      "Iteration 745, loss = 0.57438122\n",
      "Iteration 746, loss = 0.57404237\n",
      "Iteration 747, loss = 0.57370419\n",
      "Iteration 748, loss = 0.57336667\n",
      "Iteration 749, loss = 0.57302979\n",
      "Iteration 750, loss = 0.57269356\n",
      "Iteration 751, loss = 0.57235794\n",
      "Iteration 752, loss = 0.57202293\n",
      "Iteration 753, loss = 0.57168852\n",
      "Iteration 754, loss = 0.57135469\n",
      "Iteration 755, loss = 0.57102144\n",
      "Iteration 756, loss = 0.57068875\n",
      "Iteration 757, loss = 0.57035662\n",
      "Iteration 758, loss = 0.57002503\n",
      "Iteration 759, loss = 0.56969398\n",
      "Iteration 760, loss = 0.56936346\n",
      "Iteration 761, loss = 0.56903346\n",
      "Iteration 762, loss = 0.56870396\n",
      "Iteration 763, loss = 0.56837498\n",
      "Iteration 764, loss = 0.56804649\n",
      "Iteration 765, loss = 0.56771872\n",
      "Iteration 766, loss = 0.56739277\n",
      "Iteration 767, loss = 0.56706743\n",
      "Iteration 768, loss = 0.56674270\n",
      "Iteration 769, loss = 0.56641855\n",
      "Iteration 770, loss = 0.56609498\n",
      "Iteration 771, loss = 0.56577198\n",
      "Iteration 772, loss = 0.56544954\n",
      "Iteration 773, loss = 0.56512765\n",
      "Iteration 774, loss = 0.56480652\n",
      "Iteration 775, loss = 0.56448590\n",
      "Iteration 776, loss = 0.56416578\n",
      "Iteration 777, loss = 0.56384617\n",
      "Iteration 778, loss = 0.56352705\n",
      "Iteration 779, loss = 0.56320843\n",
      "Iteration 780, loss = 0.56289030\n",
      "Iteration 781, loss = 0.56257267\n",
      "Iteration 782, loss = 0.56225553\n",
      "Iteration 783, loss = 0.56193891\n",
      "Iteration 784, loss = 0.56162283\n",
      "Iteration 785, loss = 0.56130720\n",
      "Iteration 786, loss = 0.56099199\n",
      "Iteration 787, loss = 0.56067726\n",
      "Iteration 788, loss = 0.56036303\n",
      "Iteration 789, loss = 0.56004838\n",
      "Iteration 790, loss = 0.55971001\n",
      "Iteration 791, loss = 0.55935589\n",
      "Iteration 792, loss = 0.55894102\n",
      "Iteration 793, loss = 0.55839181\n",
      "Iteration 794, loss = 0.55733323\n",
      "Iteration 795, loss = 0.55598511\n",
      "Iteration 796, loss = 0.55428644\n",
      "Iteration 797, loss = 0.55238877\n",
      "Iteration 798, loss = 0.55032764\n",
      "Iteration 799, loss = 0.54812048\n",
      "Iteration 800, loss = 0.54578244\n",
      "Iteration 801, loss = 0.54335004\n",
      "Iteration 802, loss = 0.54098315\n",
      "Iteration 803, loss = 0.53911784\n",
      "Iteration 804, loss = 0.53820835\n",
      "Iteration 805, loss = 0.53785720\n",
      "Iteration 806, loss = 0.53724513\n",
      "Iteration 807, loss = 0.53630713\n",
      "Iteration 808, loss = 0.53513807\n",
      "Iteration 809, loss = 0.53379203\n",
      "Iteration 810, loss = 0.53229014\n",
      "Iteration 811, loss = 0.53065508\n",
      "Iteration 812, loss = 0.52891930\n",
      "Iteration 813, loss = 0.52714694\n",
      "Iteration 814, loss = 0.52548709\n",
      "Iteration 815, loss = 0.52403691\n",
      "Iteration 816, loss = 0.52276322\n",
      "Iteration 817, loss = 0.52157489\n",
      "Iteration 818, loss = 0.52036019\n",
      "Iteration 819, loss = 0.51907738\n",
      "Iteration 820, loss = 0.51771272\n",
      "Iteration 821, loss = 0.51627268\n",
      "Iteration 822, loss = 0.51477464\n",
      "Iteration 823, loss = 0.51323702\n",
      "Iteration 824, loss = 0.51169224\n",
      "Iteration 825, loss = 0.51020081\n",
      "Iteration 826, loss = 0.50875725\n",
      "Iteration 827, loss = 0.50733731\n",
      "Iteration 828, loss = 0.50595222\n",
      "Iteration 829, loss = 0.50460995\n",
      "Iteration 830, loss = 0.50324971\n",
      "Iteration 831, loss = 0.50186750\n",
      "Iteration 832, loss = 0.50046660\n",
      "Iteration 833, loss = 0.49905133\n",
      "Iteration 834, loss = 0.49763341\n",
      "Iteration 835, loss = 0.49624929\n",
      "Iteration 836, loss = 0.49490578\n",
      "Iteration 837, loss = 0.49357752\n",
      "Iteration 838, loss = 0.49225416\n",
      "Iteration 839, loss = 0.49093643\n",
      "Iteration 840, loss = 0.48962512\n",
      "Iteration 841, loss = 0.48832105\n",
      "Iteration 842, loss = 0.48702507\n",
      "Iteration 843, loss = 0.48573800\n",
      "Iteration 844, loss = 0.48446061\n",
      "Iteration 845, loss = 0.48319361\n",
      "Iteration 846, loss = 0.48193762\n",
      "Iteration 847, loss = 0.48069316\n",
      "Iteration 848, loss = 0.47946067\n",
      "Iteration 849, loss = 0.47824049\n",
      "Iteration 850, loss = 0.47703284\n",
      "Iteration 851, loss = 0.47583788\n",
      "Iteration 852, loss = 0.47465576\n",
      "Iteration 853, loss = 0.47348680\n",
      "Iteration 854, loss = 0.47233366\n",
      "Iteration 855, loss = 0.47119530\n",
      "Iteration 856, loss = 0.47006948\n",
      "Iteration 857, loss = 0.46895588\n",
      "Iteration 858, loss = 0.46785419\n",
      "Iteration 859, loss = 0.46676409\n",
      "Iteration 860, loss = 0.46568529\n",
      "Iteration 861, loss = 0.46461747\n",
      "Iteration 862, loss = 0.46356276\n",
      "Iteration 863, loss = 0.46252004\n",
      "Iteration 864, loss = 0.46148852\n",
      "Iteration 865, loss = 0.46046802\n",
      "Iteration 866, loss = 0.45945835\n",
      "Iteration 867, loss = 0.45845931\n",
      "Iteration 868, loss = 0.45747069\n",
      "Iteration 869, loss = 0.45649226\n",
      "Iteration 870, loss = 0.45552378\n",
      "Iteration 871, loss = 0.45456922\n",
      "Iteration 872, loss = 0.45362758\n",
      "Iteration 873, loss = 0.45269892\n",
      "Iteration 874, loss = 0.45177971\n",
      "Iteration 875, loss = 0.45087067\n",
      "Iteration 876, loss = 0.44997226\n",
      "Iteration 877, loss = 0.44908232\n",
      "Iteration 878, loss = 0.44820214\n",
      "Iteration 879, loss = 0.44733333\n",
      "Iteration 880, loss = 0.44647651\n",
      "Iteration 881, loss = 0.44562988\n",
      "Iteration 882, loss = 0.44479308\n",
      "Iteration 883, loss = 0.44396578\n",
      "Iteration 884, loss = 0.44314778\n",
      "Iteration 885, loss = 0.44234032\n",
      "Iteration 886, loss = 0.44154318\n",
      "Iteration 887, loss = 0.44075537\n",
      "Iteration 888, loss = 0.43997626\n",
      "Iteration 889, loss = 0.43920572\n",
      "Iteration 890, loss = 0.43844360\n",
      "Iteration 891, loss = 0.43768970\n",
      "Iteration 892, loss = 0.43694381\n",
      "Iteration 893, loss = 0.43620770\n",
      "Iteration 894, loss = 0.43548017\n",
      "Iteration 895, loss = 0.43476035\n",
      "Iteration 896, loss = 0.43404808\n",
      "Iteration 897, loss = 0.43334319\n",
      "Iteration 898, loss = 0.43264632\n",
      "Iteration 899, loss = 0.43195641\n",
      "Iteration 900, loss = 0.43127335\n",
      "Iteration 901, loss = 0.43059714\n",
      "Iteration 902, loss = 0.42992774\n",
      "Iteration 903, loss = 0.42926507\n",
      "Iteration 904, loss = 0.42860902\n",
      "Iteration 905, loss = 0.42795943\n",
      "Iteration 906, loss = 0.42731614\n",
      "Iteration 907, loss = 0.42667896\n",
      "Iteration 908, loss = 0.42604768\n",
      "Iteration 909, loss = 0.42542211\n",
      "Iteration 910, loss = 0.42480201\n",
      "Iteration 911, loss = 0.42418719\n",
      "Iteration 912, loss = 0.42357744\n",
      "Iteration 913, loss = 0.42297254\n",
      "Iteration 914, loss = 0.42237374\n",
      "Iteration 915, loss = 0.42178141\n",
      "Iteration 916, loss = 0.42119387\n",
      "Iteration 917, loss = 0.42061106\n",
      "Iteration 918, loss = 0.42003292\n",
      "Iteration 919, loss = 0.41945938\n",
      "Iteration 920, loss = 0.41889042\n",
      "Iteration 921, loss = 0.41832725\n",
      "Iteration 922, loss = 0.41776846\n",
      "Iteration 923, loss = 0.41721393\n",
      "Iteration 924, loss = 0.41666599\n",
      "Iteration 925, loss = 0.41612203\n",
      "Iteration 926, loss = 0.41558199\n",
      "Iteration 927, loss = 0.41504592\n",
      "Iteration 928, loss = 0.41451381\n",
      "Iteration 929, loss = 0.41398565\n",
      "Iteration 930, loss = 0.41346237\n",
      "Iteration 931, loss = 0.41294318\n",
      "Iteration 932, loss = 0.41242787\n",
      "Iteration 933, loss = 0.41191633\n",
      "Iteration 934, loss = 0.41140845\n",
      "Iteration 935, loss = 0.41090412\n",
      "Iteration 936, loss = 0.41040323\n",
      "Iteration 937, loss = 0.40990612\n",
      "Iteration 938, loss = 0.40941301\n",
      "Iteration 939, loss = 0.40892306\n",
      "Iteration 940, loss = 0.40843611\n",
      "Iteration 941, loss = 0.40795206\n",
      "Iteration 942, loss = 0.40747077\n",
      "Iteration 943, loss = 0.40699232\n",
      "Iteration 944, loss = 0.40651739\n",
      "Iteration 945, loss = 0.40604498\n",
      "Iteration 946, loss = 0.40557498\n",
      "Iteration 947, loss = 0.40510812\n",
      "Iteration 948, loss = 0.40464560\n",
      "Iteration 949, loss = 0.40418547\n",
      "Iteration 950, loss = 0.40372770\n",
      "Iteration 951, loss = 0.40327226\n",
      "Iteration 952, loss = 0.40281910\n",
      "Iteration 953, loss = 0.40236818\n",
      "Iteration 954, loss = 0.40191947\n",
      "Iteration 955, loss = 0.40147292\n",
      "Iteration 956, loss = 0.40102847\n",
      "Iteration 957, loss = 0.40058608\n",
      "Iteration 958, loss = 0.40014568\n",
      "Iteration 959, loss = 0.39970723\n",
      "Iteration 960, loss = 0.39927066\n",
      "Iteration 961, loss = 0.39883727\n",
      "Iteration 962, loss = 0.39840581\n",
      "Iteration 963, loss = 0.39797598\n",
      "Iteration 964, loss = 0.39754784\n",
      "Iteration 965, loss = 0.39712144\n",
      "Iteration 966, loss = 0.39669681\n",
      "Iteration 967, loss = 0.39627397\n",
      "Iteration 968, loss = 0.39585292\n",
      "Iteration 969, loss = 0.39543366\n",
      "Iteration 970, loss = 0.39501617\n",
      "Iteration 971, loss = 0.39460080\n",
      "Iteration 972, loss = 0.39418757\n",
      "Iteration 973, loss = 0.39377608\n",
      "Iteration 974, loss = 0.39336626\n",
      "Iteration 975, loss = 0.39295806\n",
      "Iteration 976, loss = 0.39255143\n",
      "Iteration 977, loss = 0.39214630\n",
      "Iteration 978, loss = 0.39174262\n",
      "Iteration 979, loss = 0.39134034\n",
      "Iteration 980, loss = 0.39093939\n",
      "Iteration 981, loss = 0.39053972\n",
      "Iteration 982, loss = 0.39014128\n",
      "Iteration 983, loss = 0.38974402\n",
      "Iteration 984, loss = 0.38934795\n",
      "Iteration 985, loss = 0.38895339\n",
      "Iteration 986, loss = 0.38855990\n",
      "Iteration 987, loss = 0.38816746\n",
      "Iteration 988, loss = 0.38777602\n",
      "Iteration 989, loss = 0.38738554\n",
      "Iteration 990, loss = 0.38699746\n",
      "Iteration 991, loss = 0.38661115\n",
      "Iteration 992, loss = 0.38622586\n",
      "Iteration 993, loss = 0.38584158\n",
      "Iteration 994, loss = 0.38545828\n",
      "Iteration 995, loss = 0.38507595\n",
      "Iteration 996, loss = 0.38469459\n",
      "Iteration 997, loss = 0.38431417\n",
      "Iteration 998, loss = 0.38393468\n",
      "Iteration 999, loss = 0.38355609\n",
      "Iteration 1000, loss = 0.38317886\n",
      "Iteration 1, loss = 2.13218766\n",
      "Iteration 2, loss = 2.07016128\n",
      "Iteration 3, loss = 1.98533254\n",
      "Iteration 4, loss = 1.88387948\n",
      "Iteration 5, loss = 1.77198295\n",
      "Iteration 6, loss = 1.65610238\n",
      "Iteration 7, loss = 1.54249417\n",
      "Iteration 8, loss = 1.43709992\n",
      "Iteration 9, loss = 1.34661954\n",
      "Iteration 10, loss = 1.27552877\n",
      "Iteration 11, loss = 1.22711051\n",
      "Iteration 12, loss = 1.20100118\n",
      "Iteration 13, loss = 1.19021944\n",
      "Iteration 14, loss = 1.18929941\n",
      "Iteration 15, loss = 1.19433679\n",
      "Iteration 16, loss = 1.20130903\n",
      "Iteration 17, loss = 1.20936405\n",
      "Iteration 18, loss = 1.21343685\n",
      "Iteration 19, loss = 1.21626976\n",
      "Iteration 20, loss = 1.21712734\n",
      "Iteration 21, loss = 1.21688334\n",
      "Iteration 22, loss = 1.21628987\n",
      "Iteration 23, loss = 1.21546924\n",
      "Iteration 24, loss = 1.21453162\n",
      "Iteration 25, loss = 1.21330125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14020736\n",
      "Iteration 2, loss = 2.07783837\n",
      "Iteration 3, loss = 1.99241651\n",
      "Iteration 4, loss = 1.89013953\n",
      "Iteration 5, loss = 1.77720328\n",
      "Iteration 6, loss = 1.65989286\n",
      "Iteration 7, loss = 1.54454319\n",
      "Iteration 8, loss = 1.43776203\n",
      "Iteration 9, loss = 1.34517215\n",
      "Iteration 10, loss = 1.27169774\n",
      "Iteration 11, loss = 1.22239632\n",
      "Iteration 12, loss = 1.19731928\n",
      "Iteration 13, loss = 1.18957168\n",
      "Iteration 14, loss = 1.19139445\n",
      "Iteration 15, loss = 1.19859592\n",
      "Iteration 16, loss = 1.20694193\n",
      "Iteration 17, loss = 1.21542070\n",
      "Iteration 18, loss = 1.21906582\n",
      "Iteration 19, loss = 1.22092740\n",
      "Iteration 20, loss = 1.22056241\n",
      "Iteration 21, loss = 1.21921610\n",
      "Iteration 22, loss = 1.21735583\n",
      "Iteration 23, loss = 1.21497950\n",
      "Iteration 24, loss = 1.21269909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13352491\n",
      "Iteration 2, loss = 2.07150108\n",
      "Iteration 3, loss = 1.98657585\n",
      "Iteration 4, loss = 1.88482142\n",
      "Iteration 5, loss = 1.77217349\n",
      "Iteration 6, loss = 1.65520654\n",
      "Iteration 7, loss = 1.54063512\n",
      "Iteration 8, loss = 1.43478493\n",
      "Iteration 9, loss = 1.34339938\n",
      "Iteration 10, loss = 1.27177299\n",
      "Iteration 11, loss = 1.22436676\n",
      "Iteration 12, loss = 1.19882870\n",
      "Iteration 13, loss = 1.19015250\n",
      "Iteration 14, loss = 1.19095510\n",
      "Iteration 15, loss = 1.19745614\n",
      "Iteration 16, loss = 1.20535218\n",
      "Iteration 17, loss = 1.21235805\n",
      "Iteration 18, loss = 1.21630662\n",
      "Iteration 19, loss = 1.21864928\n",
      "Iteration 20, loss = 1.21894175\n",
      "Iteration 21, loss = 1.21826441\n",
      "Iteration 22, loss = 1.21711728\n",
      "Iteration 23, loss = 1.21567426\n",
      "Iteration 24, loss = 1.21403284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13108299\n",
      "Iteration 2, loss = 2.06915449\n",
      "Iteration 3, loss = 1.98433623\n",
      "Iteration 4, loss = 1.88265254\n",
      "Iteration 5, loss = 1.77018932\n",
      "Iteration 6, loss = 1.65367616\n",
      "Iteration 7, loss = 1.53919545\n",
      "Iteration 8, loss = 1.43324636\n",
      "Iteration 9, loss = 1.34127426\n",
      "Iteration 10, loss = 1.26903523\n",
      "Iteration 11, loss = 1.22180097\n",
      "Iteration 12, loss = 1.19679969\n",
      "Iteration 13, loss = 1.18791695\n",
      "Iteration 14, loss = 1.18906109\n",
      "Iteration 15, loss = 1.19629915\n",
      "Iteration 16, loss = 1.20506056\n",
      "Iteration 17, loss = 1.21365775\n",
      "Iteration 18, loss = 1.21753259\n",
      "Iteration 19, loss = 1.21953029\n",
      "Iteration 20, loss = 1.21944621\n",
      "Iteration 21, loss = 1.21851367\n",
      "Iteration 22, loss = 1.21736433\n",
      "Iteration 23, loss = 1.21541666\n",
      "Iteration 24, loss = 1.21339425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15205734\n",
      "Iteration 2, loss = 2.08823184\n",
      "Iteration 3, loss = 2.00089280\n",
      "Iteration 4, loss = 1.89594640\n",
      "Iteration 5, loss = 1.77966245\n",
      "Iteration 6, loss = 1.65894064\n",
      "Iteration 7, loss = 1.54076792\n",
      "Iteration 8, loss = 1.43136570\n",
      "Iteration 9, loss = 1.33726562\n",
      "Iteration 10, loss = 1.26453209\n",
      "Iteration 11, loss = 1.21647338\n",
      "Iteration 12, loss = 1.19327549\n",
      "Iteration 13, loss = 1.18739358\n",
      "Iteration 14, loss = 1.19084499\n",
      "Iteration 15, loss = 1.19886692\n",
      "Iteration 16, loss = 1.20699230\n",
      "Iteration 17, loss = 1.21463027\n",
      "Iteration 18, loss = 1.21865056\n",
      "Iteration 19, loss = 1.22073971\n",
      "Iteration 20, loss = 1.22108306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 21, loss = 1.22037861\n",
      "Iteration 22, loss = 1.21895241\n",
      "Iteration 23, loss = 1.21682562\n",
      "Iteration 24, loss = 1.21448047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50048972\n",
      "Iteration 2, loss = 1.08809198\n",
      "Iteration 3, loss = 1.21258203\n",
      "Iteration 4, loss = 1.03933360\n",
      "Iteration 5, loss = 0.84143905\n",
      "Iteration 6, loss = 0.80932356\n",
      "Iteration 7, loss = 0.78971091\n",
      "Iteration 8, loss = 0.75558943\n",
      "Iteration 9, loss = 0.76216997\n",
      "Iteration 10, loss = 0.75189560\n",
      "Iteration 11, loss = 0.66604954\n",
      "Iteration 12, loss = 0.60752747\n",
      "Iteration 13, loss = 0.55946565\n",
      "Iteration 14, loss = 0.50906301\n",
      "Iteration 15, loss = 0.45573885\n",
      "Iteration 16, loss = 0.41494050\n",
      "Iteration 17, loss = 0.36960807\n",
      "Iteration 18, loss = 0.33520697\n",
      "Iteration 19, loss = 0.29950740\n",
      "Iteration 20, loss = 0.25936543\n",
      "Iteration 21, loss = 0.23156232\n",
      "Iteration 22, loss = 0.21229660\n",
      "Iteration 23, loss = 0.17985741\n",
      "Iteration 24, loss = 0.15519751\n",
      "Iteration 25, loss = 0.14659861\n",
      "Iteration 26, loss = 0.13563377\n",
      "Iteration 27, loss = 0.11913737\n",
      "Iteration 28, loss = 0.10706478\n",
      "Iteration 29, loss = 0.10714835\n",
      "Iteration 30, loss = 0.10308200\n",
      "Iteration 31, loss = 0.08972612\n",
      "Iteration 32, loss = 0.09232065\n",
      "Iteration 33, loss = 0.09325155\n",
      "Iteration 34, loss = 0.08048391\n",
      "Iteration 35, loss = 0.08675946\n",
      "Iteration 36, loss = 0.08726712\n",
      "Iteration 37, loss = 0.07524104\n",
      "Iteration 38, loss = 0.08675130\n",
      "Iteration 39, loss = 0.08150668\n",
      "Iteration 40, loss = 0.07446169\n",
      "Iteration 41, loss = 0.08579678\n",
      "Iteration 42, loss = 0.07316037\n",
      "Iteration 43, loss = 0.07596174\n",
      "Iteration 44, loss = 0.07575657\n",
      "Iteration 45, loss = 0.06858443\n",
      "Iteration 46, loss = 0.07421387\n",
      "Iteration 47, loss = 0.06779841\n",
      "Iteration 48, loss = 0.07127080\n",
      "Iteration 49, loss = 0.07062607\n",
      "Iteration 50, loss = 0.06717271\n",
      "Iteration 51, loss = 0.07179124\n",
      "Iteration 52, loss = 0.06681596\n",
      "Iteration 53, loss = 0.06769176\n",
      "Iteration 54, loss = 0.06835403\n",
      "Iteration 55, loss = 0.06481294\n",
      "Iteration 56, loss = 0.06702069\n",
      "Iteration 57, loss = 0.06512742\n",
      "Iteration 58, loss = 0.06466534\n",
      "Iteration 59, loss = 0.06577291\n",
      "Iteration 60, loss = 0.06367759\n",
      "Iteration 61, loss = 0.06476808\n",
      "Iteration 62, loss = 0.06455561\n",
      "Iteration 63, loss = 0.06308438\n",
      "Iteration 64, loss = 0.06442447\n",
      "Iteration 65, loss = 0.06356296\n",
      "Iteration 66, loss = 0.06261447\n",
      "Iteration 67, loss = 0.06377339\n",
      "Iteration 68, loss = 0.06280854\n",
      "Iteration 69, loss = 0.06208845\n",
      "Iteration 70, loss = 0.06305228\n",
      "Iteration 71, loss = 0.06224555\n",
      "Iteration 72, loss = 0.06153364\n",
      "Iteration 73, loss = 0.06236282\n",
      "Iteration 74, loss = 0.06182880\n",
      "Iteration 75, loss = 0.06100694\n",
      "Iteration 76, loss = 0.06169867\n",
      "Iteration 77, loss = 0.06149457\n",
      "Iteration 78, loss = 0.06055369\n",
      "Iteration 79, loss = 0.06102535\n",
      "Iteration 80, loss = 0.06115861\n",
      "Iteration 81, loss = 0.06020939\n",
      "Iteration 82, loss = 0.06032807\n",
      "Iteration 83, loss = 0.06068974\n",
      "Iteration 84, loss = 0.05997275\n",
      "Iteration 85, loss = 0.05969775\n",
      "Iteration 86, loss = 0.06003341\n",
      "Iteration 87, loss = 0.05975589\n",
      "Iteration 88, loss = 0.05928757\n",
      "Iteration 89, loss = 0.05930737\n",
      "Iteration 90, loss = 0.05939781\n",
      "Iteration 91, loss = 0.05913467\n",
      "Iteration 92, loss = 0.05881769\n",
      "Iteration 93, loss = 0.05883985\n",
      "Iteration 94, loss = 0.05888980\n",
      "Iteration 95, loss = 0.05864375\n",
      "Iteration 96, loss = 0.05839223\n",
      "Iteration 97, loss = 0.05835244\n",
      "Iteration 98, loss = 0.05836378\n",
      "Iteration 99, loss = 0.05823925\n",
      "Iteration 100, loss = 0.05801854\n",
      "Iteration 101, loss = 0.05789025\n",
      "Iteration 102, loss = 0.05786941\n",
      "Iteration 103, loss = 0.05782548\n",
      "Iteration 104, loss = 0.05770288\n",
      "Iteration 105, loss = 0.05753382\n",
      "Iteration 106, loss = 0.05741387\n",
      "Iteration 107, loss = 0.05735808\n",
      "Iteration 108, loss = 0.05731467\n",
      "Iteration 109, loss = 0.05723968\n",
      "Iteration 110, loss = 0.05711901\n",
      "Iteration 111, loss = 0.05699161\n",
      "Iteration 112, loss = 0.05688422\n",
      "Iteration 113, loss = 0.05680656\n",
      "Iteration 114, loss = 0.05674758\n",
      "Iteration 115, loss = 0.05668810\n",
      "Iteration 116, loss = 0.05662264\n",
      "Iteration 117, loss = 0.05654481\n",
      "Iteration 118, loss = 0.05646337\n",
      "Iteration 119, loss = 0.05637508\n",
      "Iteration 120, loss = 0.05628910\n",
      "Iteration 121, loss = 0.05620491\n",
      "Iteration 122, loss = 0.05612542\n",
      "Iteration 123, loss = 0.05604772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51234105\n",
      "Iteration 2, loss = 1.09537332\n",
      "Iteration 3, loss = 1.22009750\n",
      "Iteration 4, loss = 1.04337485\n",
      "Iteration 5, loss = 0.84749591\n",
      "Iteration 6, loss = 0.81988422\n",
      "Iteration 7, loss = 0.79575583\n",
      "Iteration 8, loss = 0.75993072\n",
      "Iteration 9, loss = 0.76700396\n",
      "Iteration 10, loss = 0.75874061\n",
      "Iteration 11, loss = 0.67236672\n",
      "Iteration 12, loss = 0.61330743\n",
      "Iteration 13, loss = 0.56437891\n",
      "Iteration 14, loss = 0.50713046\n",
      "Iteration 15, loss = 0.45241838\n",
      "Iteration 16, loss = 0.40261375\n",
      "Iteration 17, loss = 0.35509609\n",
      "Iteration 18, loss = 0.30784197\n",
      "Iteration 19, loss = 0.26274230\n",
      "Iteration 20, loss = 0.22573879\n",
      "Iteration 21, loss = 0.22040566\n",
      "Iteration 22, loss = 0.17371406\n",
      "Iteration 23, loss = 0.15338813\n",
      "Iteration 24, loss = 0.15354790\n",
      "Iteration 25, loss = 0.11870624\n",
      "Iteration 26, loss = 0.12966003\n",
      "Iteration 27, loss = 0.10596495\n",
      "Iteration 28, loss = 0.10557542\n",
      "Iteration 29, loss = 0.09584198\n",
      "Iteration 30, loss = 0.08877387\n",
      "Iteration 31, loss = 0.08968677\n",
      "Iteration 32, loss = 0.07988633\n",
      "Iteration 33, loss = 0.08456532\n",
      "Iteration 34, loss = 0.07286588\n",
      "Iteration 35, loss = 0.07911082\n",
      "Iteration 36, loss = 0.06956051\n",
      "Iteration 37, loss = 0.07600512\n",
      "Iteration 38, loss = 0.06786589\n",
      "Iteration 39, loss = 0.07159351\n",
      "Iteration 40, loss = 0.06642671\n",
      "Iteration 41, loss = 0.06829892\n",
      "Iteration 42, loss = 0.06656110\n",
      "Iteration 43, loss = 0.06553381\n",
      "Iteration 44, loss = 0.06630362\n",
      "Iteration 45, loss = 0.06313755\n",
      "Iteration 46, loss = 0.06543057\n",
      "Iteration 47, loss = 0.06218529\n",
      "Iteration 48, loss = 0.06459744\n",
      "Iteration 49, loss = 0.06224677\n",
      "Iteration 50, loss = 0.06272205\n",
      "Iteration 51, loss = 0.06245380\n",
      "Iteration 52, loss = 0.06109340\n",
      "Iteration 53, loss = 0.06224289\n",
      "Iteration 54, loss = 0.06051003\n",
      "Iteration 55, loss = 0.06154409\n",
      "Iteration 56, loss = 0.06078280\n",
      "Iteration 57, loss = 0.06031771\n",
      "Iteration 58, loss = 0.06095382\n",
      "Iteration 59, loss = 0.05970515\n",
      "Iteration 60, loss = 0.06017620\n",
      "Iteration 61, loss = 0.05982258\n",
      "Iteration 62, loss = 0.05929327\n",
      "Iteration 63, loss = 0.05970618\n",
      "Iteration 64, loss = 0.05904020\n",
      "Iteration 65, loss = 0.05909904\n",
      "Iteration 66, loss = 0.05914079\n",
      "Iteration 67, loss = 0.05859292\n",
      "Iteration 68, loss = 0.05883102\n",
      "Iteration 69, loss = 0.05863322\n",
      "Iteration 70, loss = 0.05826013\n",
      "Iteration 71, loss = 0.05847975\n",
      "Iteration 72, loss = 0.05821098\n",
      "Iteration 73, loss = 0.05794232\n",
      "Iteration 74, loss = 0.05810597\n",
      "Iteration 75, loss = 0.05785646\n",
      "Iteration 76, loss = 0.05762368\n",
      "Iteration 77, loss = 0.05774057\n",
      "Iteration 78, loss = 0.05754785\n",
      "Iteration 79, loss = 0.05731334\n",
      "Iteration 80, loss = 0.05738710\n",
      "Iteration 81, loss = 0.05726629\n",
      "Iteration 82, loss = 0.05702350\n",
      "Iteration 83, loss = 0.05704265\n",
      "Iteration 84, loss = 0.05699454\n",
      "Iteration 85, loss = 0.05676358\n",
      "Iteration 86, loss = 0.05670861\n",
      "Iteration 87, loss = 0.05670555\n",
      "Iteration 88, loss = 0.05652965\n",
      "Iteration 89, loss = 0.05640207\n",
      "Iteration 90, loss = 0.05639048\n",
      "Iteration 91, loss = 0.05630074\n",
      "Iteration 92, loss = 0.05615038\n",
      "Iteration 93, loss = 0.05607641\n",
      "Iteration 94, loss = 0.05603928\n",
      "Iteration 95, loss = 0.05593941\n",
      "Iteration 96, loss = 0.05581606\n",
      "Iteration 97, loss = 0.05575034\n",
      "Iteration 98, loss = 0.05570195\n",
      "Iteration 99, loss = 0.05560736\n",
      "Iteration 100, loss = 0.05550050\n",
      "Iteration 101, loss = 0.05542804\n",
      "Iteration 102, loss = 0.05537328\n",
      "Iteration 103, loss = 0.05529761\n",
      "Iteration 104, loss = 0.05520180\n",
      "Iteration 105, loss = 0.05511880\n",
      "Iteration 106, loss = 0.05505644\n",
      "Iteration 107, loss = 0.05499381\n",
      "Iteration 108, loss = 0.05491685\n",
      "Iteration 109, loss = 0.05483213\n",
      "Iteration 110, loss = 0.05475571\n",
      "Iteration 111, loss = 0.05469091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50852130\n",
      "Iteration 2, loss = 1.08753901\n",
      "Iteration 3, loss = 1.21090026\n",
      "Iteration 4, loss = 1.04084440\n",
      "Iteration 5, loss = 0.83958870\n",
      "Iteration 6, loss = 0.80677200\n",
      "Iteration 7, loss = 0.79369168\n",
      "Iteration 8, loss = 0.75782105\n",
      "Iteration 9, loss = 0.75599888\n",
      "Iteration 10, loss = 0.75383767\n",
      "Iteration 11, loss = 0.66640736\n",
      "Iteration 12, loss = 0.60564203\n",
      "Iteration 13, loss = 0.55364532\n",
      "Iteration 14, loss = 0.48935702\n",
      "Iteration 15, loss = 0.42681490\n",
      "Iteration 16, loss = 0.37357451\n",
      "Iteration 17, loss = 0.32324446\n",
      "Iteration 18, loss = 0.29806447\n",
      "Iteration 19, loss = 0.24745353\n",
      "Iteration 20, loss = 0.20509344\n",
      "Iteration 21, loss = 0.19171830\n",
      "Iteration 22, loss = 0.16120418\n",
      "Iteration 23, loss = 0.13331306\n",
      "Iteration 24, loss = 0.12445073\n",
      "Iteration 25, loss = 0.10535098\n",
      "Iteration 26, loss = 0.09112703\n",
      "Iteration 27, loss = 0.08677504\n",
      "Iteration 28, loss = 0.07381421\n",
      "Iteration 29, loss = 0.07096374\n",
      "Iteration 30, loss = 0.06569410\n",
      "Iteration 31, loss = 0.05984705\n",
      "Iteration 32, loss = 0.05964074\n",
      "Iteration 33, loss = 0.05360261\n",
      "Iteration 34, loss = 0.05407283\n",
      "Iteration 35, loss = 0.05030905\n",
      "Iteration 36, loss = 0.05004688\n",
      "Iteration 37, loss = 0.04860214\n",
      "Iteration 38, loss = 0.04727687\n",
      "Iteration 39, loss = 0.04721279\n",
      "Iteration 40, loss = 0.04533031\n",
      "Iteration 41, loss = 0.04596946\n",
      "Iteration 42, loss = 0.04430417\n",
      "Iteration 43, loss = 0.04501034\n",
      "Iteration 44, loss = 0.04372701\n",
      "Iteration 45, loss = 0.04397362\n",
      "Iteration 46, loss = 0.04328061\n",
      "Iteration 47, loss = 0.04311425\n",
      "Iteration 48, loss = 0.04296047\n",
      "Iteration 49, loss = 0.04245867\n",
      "Iteration 50, loss = 0.04259165\n",
      "Iteration 51, loss = 0.04192709\n",
      "Iteration 52, loss = 0.04214002\n",
      "Iteration 53, loss = 0.04156444\n",
      "Iteration 54, loss = 0.04171438\n",
      "Iteration 55, loss = 0.04129946\n",
      "Iteration 56, loss = 0.04127514\n",
      "Iteration 57, loss = 0.04105323\n",
      "Iteration 58, loss = 0.04088393\n",
      "Iteration 59, loss = 0.04081885\n",
      "Iteration 60, loss = 0.04056080\n",
      "Iteration 61, loss = 0.04057077\n",
      "Iteration 62, loss = 0.04028757\n",
      "Iteration 63, loss = 0.04029702\n",
      "Iteration 64, loss = 0.04006090\n",
      "Iteration 65, loss = 0.04002748\n",
      "Iteration 66, loss = 0.03986312\n",
      "Iteration 67, loss = 0.03976587\n",
      "Iteration 68, loss = 0.03967347\n",
      "Iteration 69, loss = 0.03952603\n",
      "Iteration 70, loss = 0.03947714\n",
      "Iteration 71, loss = 0.03931928\n",
      "Iteration 72, loss = 0.03927661\n",
      "Iteration 73, loss = 0.03913712\n",
      "Iteration 74, loss = 0.03907130\n",
      "Iteration 75, loss = 0.03896919\n",
      "Iteration 76, loss = 0.03887413\n",
      "Iteration 77, loss = 0.03880440\n",
      "Iteration 78, loss = 0.03869326\n",
      "Iteration 79, loss = 0.03863578\n",
      "Iteration 80, loss = 0.03852985\n",
      "Iteration 81, loss = 0.03846277\n",
      "Iteration 82, loss = 0.03837798\n",
      "Iteration 83, loss = 0.03829415\n",
      "Iteration 84, loss = 0.03822821\n",
      "Iteration 85, loss = 0.03813792\n",
      "Iteration 86, loss = 0.03807566\n",
      "Iteration 87, loss = 0.03799571\n",
      "Iteration 88, loss = 0.03792438\n",
      "Iteration 89, loss = 0.03785860\n",
      "Iteration 90, loss = 0.03778105\n",
      "Iteration 91, loss = 0.03771996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50041490\n",
      "Iteration 2, loss = 1.08594107\n",
      "Iteration 3, loss = 1.21305314\n",
      "Iteration 4, loss = 1.03612878\n",
      "Iteration 5, loss = 0.83965719\n",
      "Iteration 6, loss = 0.81124393\n",
      "Iteration 7, loss = 0.79115780\n",
      "Iteration 8, loss = 0.75560012\n",
      "Iteration 9, loss = 0.76067290\n",
      "Iteration 10, loss = 0.75024966\n",
      "Iteration 11, loss = 0.66530210\n",
      "Iteration 12, loss = 0.61341066\n",
      "Iteration 13, loss = 0.57291829\n",
      "Iteration 14, loss = 0.51943222\n",
      "Iteration 15, loss = 0.46796048\n",
      "Iteration 16, loss = 0.41600894\n",
      "Iteration 17, loss = 0.36476173\n",
      "Iteration 18, loss = 0.31613781\n",
      "Iteration 19, loss = 0.27654638\n",
      "Iteration 20, loss = 0.23561019\n",
      "Iteration 21, loss = 0.19489605\n",
      "Iteration 22, loss = 0.16311863\n",
      "Iteration 23, loss = 0.14030087\n",
      "Iteration 24, loss = 0.12684239\n",
      "Iteration 25, loss = 0.12169067\n",
      "Iteration 26, loss = 0.08984737\n",
      "Iteration 27, loss = 0.08516141\n",
      "Iteration 28, loss = 0.08369842\n",
      "Iteration 29, loss = 0.06385834\n",
      "Iteration 30, loss = 0.07332354\n",
      "Iteration 31, loss = 0.05731032\n",
      "Iteration 32, loss = 0.05957558\n",
      "Iteration 33, loss = 0.05206157\n",
      "Iteration 34, loss = 0.05012127\n",
      "Iteration 35, loss = 0.04912647\n",
      "Iteration 36, loss = 0.04462088\n",
      "Iteration 37, loss = 0.04608643\n",
      "Iteration 38, loss = 0.03977427\n",
      "Iteration 39, loss = 0.04302375\n",
      "Iteration 40, loss = 0.03729267\n",
      "Iteration 41, loss = 0.04100486\n",
      "Iteration 42, loss = 0.03545587\n",
      "Iteration 43, loss = 0.03798926\n",
      "Iteration 44, loss = 0.03421776\n",
      "Iteration 45, loss = 0.03559247\n",
      "Iteration 46, loss = 0.03389105\n",
      "Iteration 47, loss = 0.03323009\n",
      "Iteration 48, loss = 0.03337104\n",
      "Iteration 49, loss = 0.03119117\n",
      "Iteration 50, loss = 0.03247513\n",
      "Iteration 51, loss = 0.03015847\n",
      "Iteration 52, loss = 0.03131620\n",
      "Iteration 53, loss = 0.02991108\n",
      "Iteration 54, loss = 0.02965140\n",
      "Iteration 55, loss = 0.02976493\n",
      "Iteration 56, loss = 0.02834445\n",
      "Iteration 57, loss = 0.02900864\n",
      "Iteration 58, loss = 0.02788931\n",
      "Iteration 59, loss = 0.02782042\n",
      "Iteration 60, loss = 0.02779766\n",
      "Iteration 61, loss = 0.02684268\n",
      "Iteration 62, loss = 0.02722532\n",
      "Iteration 63, loss = 0.02662255\n",
      "Iteration 64, loss = 0.02614918\n",
      "Iteration 65, loss = 0.02639048\n",
      "Iteration 66, loss = 0.02568949\n",
      "Iteration 67, loss = 0.02546794\n",
      "Iteration 68, loss = 0.02554160\n",
      "Iteration 69, loss = 0.02493639\n",
      "Iteration 70, loss = 0.02475981\n",
      "Iteration 71, loss = 0.02476874\n",
      "Iteration 72, loss = 0.02428766\n",
      "Iteration 73, loss = 0.02405994\n",
      "Iteration 74, loss = 0.02406425\n",
      "Iteration 75, loss = 0.02371569\n",
      "Iteration 76, loss = 0.02340287\n",
      "Iteration 77, loss = 0.02338125\n",
      "Iteration 78, loss = 0.02318900\n",
      "Iteration 79, loss = 0.02283196\n",
      "Iteration 80, loss = 0.02270133\n",
      "Iteration 81, loss = 0.02263119\n",
      "Iteration 82, loss = 0.02235978\n",
      "Iteration 83, loss = 0.02209613\n",
      "Iteration 84, loss = 0.02198644\n",
      "Iteration 85, loss = 0.02187364\n",
      "Iteration 86, loss = 0.02165377\n",
      "Iteration 87, loss = 0.02141602\n",
      "Iteration 88, loss = 0.02127059\n",
      "Iteration 89, loss = 0.02116742\n",
      "Iteration 90, loss = 0.02100778\n",
      "Iteration 91, loss = 0.02080161\n",
      "Iteration 92, loss = 0.02060731\n",
      "Iteration 93, loss = 0.02046266\n",
      "Iteration 94, loss = 0.02034554\n",
      "Iteration 95, loss = 0.02021367\n",
      "Iteration 96, loss = 0.02005667\n",
      "Iteration 97, loss = 0.01988030\n",
      "Iteration 98, loss = 0.01970935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 0.01955437\n",
      "Iteration 100, loss = 0.01941637\n",
      "Iteration 101, loss = 0.01929004\n",
      "Iteration 102, loss = 0.01916904\n",
      "Iteration 103, loss = 0.01905304\n",
      "Iteration 104, loss = 0.01893880\n",
      "Iteration 105, loss = 0.01883929\n",
      "Iteration 106, loss = 0.01875186\n",
      "Iteration 107, loss = 0.01871811\n",
      "Iteration 108, loss = 0.01873735\n",
      "Iteration 109, loss = 0.01897520\n",
      "Iteration 110, loss = 0.01941153\n",
      "Iteration 111, loss = 0.02086452\n",
      "Iteration 112, loss = 0.02251719\n",
      "Iteration 113, loss = 0.02772386\n",
      "Iteration 114, loss = 0.02708354\n",
      "Iteration 115, loss = 0.02929466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52120330\n",
      "Iteration 2, loss = 1.08488337\n",
      "Iteration 3, loss = 1.21601445\n",
      "Iteration 4, loss = 1.03426898\n",
      "Iteration 5, loss = 0.83157607\n",
      "Iteration 6, loss = 0.80338392\n",
      "Iteration 7, loss = 0.79130715\n",
      "Iteration 8, loss = 0.75566026\n",
      "Iteration 9, loss = 0.75707790\n",
      "Iteration 10, loss = 0.74926979\n",
      "Iteration 11, loss = 0.66262751\n",
      "Iteration 12, loss = 0.60526919\n",
      "Iteration 13, loss = 0.55790307\n",
      "Iteration 14, loss = 0.50048188\n",
      "Iteration 15, loss = 0.44746703\n",
      "Iteration 16, loss = 0.39844589\n",
      "Iteration 17, loss = 0.35041464\n",
      "Iteration 18, loss = 0.30289950\n",
      "Iteration 19, loss = 0.25758140\n",
      "Iteration 20, loss = 0.21988993\n",
      "Iteration 21, loss = 0.21854508\n",
      "Iteration 22, loss = 0.17839846\n",
      "Iteration 23, loss = 0.15058408\n",
      "Iteration 24, loss = 0.15636517\n",
      "Iteration 25, loss = 0.12336953\n",
      "Iteration 26, loss = 0.13252131\n",
      "Iteration 27, loss = 0.11499425\n",
      "Iteration 28, loss = 0.10930627\n",
      "Iteration 29, loss = 0.10682956\n",
      "Iteration 30, loss = 0.09233643\n",
      "Iteration 31, loss = 0.09956770\n",
      "Iteration 32, loss = 0.08440281\n",
      "Iteration 33, loss = 0.09391739\n",
      "Iteration 34, loss = 0.08136752\n",
      "Iteration 35, loss = 0.08524034\n",
      "Iteration 36, loss = 0.07973639\n",
      "Iteration 37, loss = 0.07779983\n",
      "Iteration 38, loss = 0.07968988\n",
      "Iteration 39, loss = 0.07340077\n",
      "Iteration 40, loss = 0.07883102\n",
      "Iteration 41, loss = 0.07166547\n",
      "Iteration 42, loss = 0.07433527\n",
      "Iteration 43, loss = 0.07186293\n",
      "Iteration 44, loss = 0.07005186\n",
      "Iteration 45, loss = 0.07211457\n",
      "Iteration 46, loss = 0.06806191\n",
      "Iteration 47, loss = 0.07091367\n",
      "Iteration 48, loss = 0.06872729\n",
      "Iteration 49, loss = 0.06769209\n",
      "Iteration 50, loss = 0.06941014\n",
      "Iteration 51, loss = 0.06632453\n",
      "Iteration 52, loss = 0.06725458\n",
      "Iteration 53, loss = 0.06693797\n",
      "Iteration 54, loss = 0.06518687\n",
      "Iteration 55, loss = 0.06626807\n",
      "Iteration 56, loss = 0.06510894\n",
      "Iteration 57, loss = 0.06452608\n",
      "Iteration 58, loss = 0.06502759\n",
      "Iteration 59, loss = 0.06381615\n",
      "Iteration 60, loss = 0.06398133\n",
      "Iteration 61, loss = 0.06427590\n",
      "Iteration 62, loss = 0.06321464\n",
      "Iteration 63, loss = 0.06333851\n",
      "Iteration 64, loss = 0.06336484\n",
      "Iteration 65, loss = 0.06254610\n",
      "Iteration 66, loss = 0.06278902\n",
      "Iteration 67, loss = 0.06288413\n",
      "Iteration 68, loss = 0.06208703\n",
      "Iteration 69, loss = 0.06204730\n",
      "Iteration 70, loss = 0.06213302\n",
      "Iteration 71, loss = 0.06158570\n",
      "Iteration 72, loss = 0.06143672\n",
      "Iteration 73, loss = 0.06152887\n",
      "Iteration 74, loss = 0.06116372\n",
      "Iteration 75, loss = 0.06081409\n",
      "Iteration 76, loss = 0.06088179\n",
      "Iteration 77, loss = 0.06078554\n",
      "Iteration 78, loss = 0.06042874\n",
      "Iteration 79, loss = 0.06025777\n",
      "Iteration 80, loss = 0.06026154\n",
      "Iteration 81, loss = 0.06009999\n",
      "Iteration 82, loss = 0.05984430\n",
      "Iteration 83, loss = 0.05968772\n",
      "Iteration 84, loss = 0.05964713\n",
      "Iteration 85, loss = 0.05950374\n",
      "Iteration 86, loss = 0.05930744\n",
      "Iteration 87, loss = 0.05916506\n",
      "Iteration 88, loss = 0.05907803\n",
      "Iteration 89, loss = 0.05895439\n",
      "Iteration 90, loss = 0.05879690\n",
      "Iteration 91, loss = 0.05868197\n",
      "Iteration 92, loss = 0.05855444\n",
      "Iteration 93, loss = 0.05853505\n",
      "Iteration 94, loss = 0.05841080\n",
      "Iteration 95, loss = 0.05820482\n",
      "Iteration 96, loss = 0.05804194\n",
      "Iteration 97, loss = 0.05803255\n",
      "Iteration 98, loss = 0.05782934\n",
      "Iteration 99, loss = 0.05777237\n",
      "Iteration 100, loss = 0.05765788\n",
      "Iteration 101, loss = 0.05748031\n",
      "Iteration 102, loss = 0.05736420\n",
      "Iteration 103, loss = 0.05738225\n",
      "Iteration 104, loss = 0.05721084\n",
      "Iteration 105, loss = 0.05731450\n",
      "Iteration 106, loss = 0.05719171\n",
      "Iteration 107, loss = 0.05698657\n",
      "Iteration 108, loss = 0.05683066\n",
      "Iteration 109, loss = 0.05707124\n",
      "Iteration 110, loss = 0.05677035\n",
      "Iteration 111, loss = 0.05691180\n",
      "Iteration 112, loss = 0.05715539\n",
      "Iteration 113, loss = 0.05707749\n",
      "Iteration 114, loss = 0.05716392\n",
      "Iteration 115, loss = 0.05751570\n",
      "Iteration 116, loss = 0.05834462\n",
      "Iteration 117, loss = 0.05946307\n",
      "Iteration 118, loss = 0.06129237\n",
      "Iteration 119, loss = 0.06152835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50048972\n",
      "Iteration 2, loss = 1.45290957\n",
      "Iteration 3, loss = 1.07169716\n",
      "Iteration 4, loss = 0.79855901\n",
      "Iteration 5, loss = 0.77382246\n",
      "Iteration 6, loss = 0.71366116\n",
      "Iteration 7, loss = 0.64556081\n",
      "Iteration 8, loss = 0.58275436\n",
      "Iteration 9, loss = 0.52208268\n",
      "Iteration 10, loss = 0.47622592\n",
      "Iteration 11, loss = 0.43923404\n",
      "Iteration 12, loss = 0.40679823\n",
      "Iteration 13, loss = 0.37712595\n",
      "Iteration 14, loss = 0.34840921\n",
      "Iteration 15, loss = 0.32514186\n",
      "Iteration 16, loss = 0.35680784\n",
      "Iteration 17, loss = 1.09239591\n",
      "Iteration 18, loss = 2.76870863\n",
      "Iteration 19, loss = 0.45553835\n",
      "Iteration 20, loss = 0.45044592\n",
      "Iteration 21, loss = 0.50761491\n",
      "Iteration 22, loss = 0.40347532\n",
      "Iteration 23, loss = 0.42590796\n",
      "Iteration 24, loss = 0.41180959\n",
      "Iteration 25, loss = 0.41246572\n",
      "Iteration 26, loss = 0.41131202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51234105\n",
      "Iteration 2, loss = 1.47504904\n",
      "Iteration 3, loss = 1.07258459\n",
      "Iteration 4, loss = 0.80232006\n",
      "Iteration 5, loss = 0.78400041\n",
      "Iteration 6, loss = 0.72508474\n",
      "Iteration 7, loss = 0.65613480\n",
      "Iteration 8, loss = 0.59469205\n",
      "Iteration 9, loss = 0.53098358\n",
      "Iteration 10, loss = 0.48057752\n",
      "Iteration 11, loss = 0.44076114\n",
      "Iteration 12, loss = 0.40579584\n",
      "Iteration 13, loss = 0.37376292\n",
      "Iteration 14, loss = 0.34251164\n",
      "Iteration 15, loss = 0.31566356\n",
      "Iteration 16, loss = 0.33429253\n",
      "Iteration 17, loss = 1.03767966\n",
      "Iteration 18, loss = 3.08477278\n",
      "Iteration 19, loss = 0.55797295\n",
      "Iteration 20, loss = 0.80468445\n",
      "Iteration 21, loss = 0.43162298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 22, loss = 0.47932181\n",
      "Iteration 23, loss = 0.42640178\n",
      "Iteration 24, loss = 0.33144734\n",
      "Iteration 25, loss = 0.26823515\n",
      "Iteration 26, loss = 0.24106740\n",
      "Iteration 27, loss = 0.21609556\n",
      "Iteration 28, loss = 0.19387018\n",
      "Iteration 29, loss = 0.17784280\n",
      "Iteration 30, loss = 0.17925314\n",
      "Iteration 31, loss = 0.30745921\n",
      "Iteration 32, loss = 1.44686793\n",
      "Iteration 33, loss = 2.29384247\n",
      "Iteration 34, loss = 2.18768845\n",
      "Iteration 35, loss = 0.27796641\n",
      "Iteration 36, loss = 0.27701026\n",
      "Iteration 37, loss = 0.31711901\n",
      "Iteration 38, loss = 0.25745315\n",
      "Iteration 39, loss = 0.23180904\n",
      "Iteration 40, loss = 0.21257200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50852130\n",
      "Iteration 2, loss = 1.45271745\n",
      "Iteration 3, loss = 1.06591894\n",
      "Iteration 4, loss = 0.79241983\n",
      "Iteration 5, loss = 0.76680061\n",
      "Iteration 6, loss = 0.70709840\n",
      "Iteration 7, loss = 0.64307640\n",
      "Iteration 8, loss = 0.57968811\n",
      "Iteration 9, loss = 0.51761862\n",
      "Iteration 10, loss = 0.46927552\n",
      "Iteration 11, loss = 0.43072508\n",
      "Iteration 12, loss = 0.39520866\n",
      "Iteration 13, loss = 0.36267276\n",
      "Iteration 14, loss = 0.33160988\n",
      "Iteration 15, loss = 0.32049375\n",
      "Iteration 16, loss = 0.53365415\n",
      "Iteration 17, loss = 1.93644277\n",
      "Iteration 18, loss = 1.78752418\n",
      "Iteration 19, loss = 0.33773296\n",
      "Iteration 20, loss = 0.31191260\n",
      "Iteration 21, loss = 0.33489116\n",
      "Iteration 22, loss = 0.32594759\n",
      "Iteration 23, loss = 0.29661852\n",
      "Iteration 24, loss = 0.26530804\n",
      "Iteration 25, loss = 0.24071878\n",
      "Iteration 26, loss = 0.21826685\n",
      "Iteration 27, loss = 0.19651035\n",
      "Iteration 28, loss = 0.17626989\n",
      "Iteration 29, loss = 0.15854582\n",
      "Iteration 30, loss = 0.14342253\n",
      "Iteration 31, loss = 0.13274228\n",
      "Iteration 32, loss = 0.13223549\n",
      "Iteration 33, loss = 0.18699237\n",
      "Iteration 34, loss = 0.71203460\n",
      "Iteration 35, loss = 2.92530670\n",
      "Iteration 36, loss = 2.20105400\n",
      "Iteration 37, loss = 0.16854730\n",
      "Iteration 38, loss = 0.31518308\n",
      "Iteration 39, loss = 0.25022481\n",
      "Iteration 40, loss = 0.22048166\n",
      "Iteration 41, loss = 0.19023775\n",
      "Iteration 42, loss = 0.16843095\n",
      "Iteration 43, loss = 0.15606061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50041490\n",
      "Iteration 2, loss = 1.45919144\n",
      "Iteration 3, loss = 1.06738605\n",
      "Iteration 4, loss = 0.79760009\n",
      "Iteration 5, loss = 0.77515296\n",
      "Iteration 6, loss = 0.71410890\n",
      "Iteration 7, loss = 0.64904621\n",
      "Iteration 8, loss = 0.58678673\n",
      "Iteration 9, loss = 0.52586912\n",
      "Iteration 10, loss = 0.47723925\n",
      "Iteration 11, loss = 0.43790468\n",
      "Iteration 12, loss = 0.40281751\n",
      "Iteration 13, loss = 0.37038660\n",
      "Iteration 14, loss = 0.33852414\n",
      "Iteration 15, loss = 0.31285250\n",
      "Iteration 16, loss = 0.35384462\n",
      "Iteration 17, loss = 1.21251711\n",
      "Iteration 18, loss = 2.77552630\n",
      "Iteration 19, loss = 0.44897287\n",
      "Iteration 20, loss = 0.46884458\n",
      "Iteration 21, loss = 0.48106404\n",
      "Iteration 22, loss = 0.39692651\n",
      "Iteration 23, loss = 0.41569808\n",
      "Iteration 24, loss = 0.40431799\n",
      "Iteration 25, loss = 0.40323099\n",
      "Iteration 26, loss = 0.40030179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52120330\n",
      "Iteration 2, loss = 1.49768499\n",
      "Iteration 3, loss = 1.07139089\n",
      "Iteration 4, loss = 0.79820472\n",
      "Iteration 5, loss = 0.77575810\n",
      "Iteration 6, loss = 0.71581461\n",
      "Iteration 7, loss = 0.65064998\n",
      "Iteration 8, loss = 0.58839506\n",
      "Iteration 9, loss = 0.52559341\n",
      "Iteration 10, loss = 0.47619367\n",
      "Iteration 11, loss = 0.43734865\n",
      "Iteration 12, loss = 0.40356571\n",
      "Iteration 13, loss = 0.37257495\n",
      "Iteration 14, loss = 0.34239982\n",
      "Iteration 15, loss = 0.31749475\n",
      "Iteration 16, loss = 0.34830276\n",
      "Iteration 17, loss = 1.14289514\n",
      "Iteration 18, loss = 2.93541846\n",
      "Iteration 19, loss = 0.46204332\n",
      "Iteration 20, loss = 0.47878660\n",
      "Iteration 21, loss = 0.47699851\n",
      "Iteration 22, loss = 0.40873667\n",
      "Iteration 23, loss = 0.41799775\n",
      "Iteration 24, loss = 0.40648327\n",
      "Iteration 25, loss = 0.40547820\n",
      "Iteration 26, loss = 0.40240225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50048972\n",
      "Iteration 2, loss = 1.34462279\n",
      "Iteration 3, loss = 1.21557774\n",
      "Iteration 4, loss = 1.11018636\n",
      "Iteration 5, loss = 1.02505800\n",
      "Iteration 6, loss = 0.95254331\n",
      "Iteration 7, loss = 0.89265886\n",
      "Iteration 8, loss = 0.84657812\n",
      "Iteration 9, loss = 0.81641825\n",
      "Iteration 10, loss = 0.80113253\n",
      "Iteration 11, loss = 0.79693968\n",
      "Iteration 12, loss = 0.79881932\n",
      "Iteration 13, loss = 0.80198879\n",
      "Iteration 14, loss = 0.80189153\n",
      "Iteration 15, loss = 0.79796949\n",
      "Iteration 16, loss = 0.79087047\n",
      "Iteration 17, loss = 0.78139222\n",
      "Iteration 18, loss = 0.77124100\n",
      "Iteration 19, loss = 0.76101336\n",
      "Iteration 20, loss = 0.75080087\n",
      "Iteration 21, loss = 0.74011136\n",
      "Iteration 22, loss = 0.72855441\n",
      "Iteration 23, loss = 0.71601945\n",
      "Iteration 24, loss = 0.70310290\n",
      "Iteration 25, loss = 0.68979201\n",
      "Iteration 26, loss = 0.67747387\n",
      "Iteration 27, loss = 0.66730129\n",
      "Iteration 28, loss = 0.65939055\n",
      "Iteration 29, loss = 0.65345075\n",
      "Iteration 30, loss = 0.64905549\n",
      "Iteration 31, loss = 0.64545424\n",
      "Iteration 32, loss = 0.64181565\n",
      "Iteration 33, loss = 0.63729801\n",
      "Iteration 34, loss = 0.63154352\n",
      "Iteration 35, loss = 0.62456624\n",
      "Iteration 36, loss = 0.61670355\n",
      "Iteration 37, loss = 0.60845195\n",
      "Iteration 38, loss = 0.60042865\n",
      "Iteration 39, loss = 0.59297356\n",
      "Iteration 40, loss = 0.58620331\n",
      "Iteration 41, loss = 0.58004127\n",
      "Iteration 42, loss = 0.57428255\n",
      "Iteration 43, loss = 0.56886280\n",
      "Iteration 44, loss = 0.56363067\n",
      "Iteration 45, loss = 0.55859042\n",
      "Iteration 46, loss = 0.55374239\n",
      "Iteration 47, loss = 0.54910937\n",
      "Iteration 48, loss = 0.54461055\n",
      "Iteration 49, loss = 0.54016420\n",
      "Iteration 50, loss = 0.53567544\n",
      "Iteration 51, loss = 0.53108700\n",
      "Iteration 52, loss = 0.52638373\n",
      "Iteration 53, loss = 0.52158626\n",
      "Iteration 54, loss = 0.51643594\n",
      "Iteration 55, loss = 0.51108573\n",
      "Iteration 56, loss = 0.50577293\n",
      "Iteration 57, loss = 0.50256712\n",
      "Iteration 58, loss = 0.49783676\n",
      "Iteration 59, loss = 0.49330352\n",
      "Iteration 60, loss = 0.48958655\n",
      "Iteration 61, loss = 0.48574509\n",
      "Iteration 62, loss = 0.48162637\n",
      "Iteration 63, loss = 0.47727307\n",
      "Iteration 64, loss = 0.47284567\n",
      "Iteration 65, loss = 0.46853727\n",
      "Iteration 66, loss = 0.46478519\n",
      "Iteration 67, loss = 0.46083300\n",
      "Iteration 68, loss = 0.45645611\n",
      "Iteration 69, loss = 0.45230149\n",
      "Iteration 70, loss = 0.44841287\n",
      "Iteration 71, loss = 0.44446375\n",
      "Iteration 72, loss = 0.44042414\n",
      "Iteration 73, loss = 0.43633453\n",
      "Iteration 74, loss = 0.43240198\n",
      "Iteration 75, loss = 0.42868164\n",
      "Iteration 76, loss = 0.42494055\n",
      "Iteration 77, loss = 0.42104696\n",
      "Iteration 78, loss = 0.41724330\n",
      "Iteration 79, loss = 0.41359402\n",
      "Iteration 80, loss = 0.40998184\n",
      "Iteration 81, loss = 0.40631565\n",
      "Iteration 82, loss = 0.40266150\n",
      "Iteration 83, loss = 0.39913232\n",
      "Iteration 84, loss = 0.39565446\n",
      "Iteration 85, loss = 0.39219123\n",
      "Iteration 86, loss = 0.38873581\n",
      "Iteration 87, loss = 0.38528890\n",
      "Iteration 88, loss = 0.38189549\n",
      "Iteration 89, loss = 0.37857845\n",
      "Iteration 90, loss = 0.37529453\n",
      "Iteration 91, loss = 0.37200305\n",
      "Iteration 92, loss = 0.36872325\n",
      "Iteration 93, loss = 0.36547749\n",
      "Iteration 94, loss = 0.36229016\n",
      "Iteration 95, loss = 0.35911564\n",
      "Iteration 96, loss = 0.35594341\n",
      "Iteration 97, loss = 0.35280400\n",
      "Iteration 98, loss = 0.34969029\n",
      "Iteration 99, loss = 0.34659657\n",
      "Iteration 100, loss = 0.34351557\n",
      "Iteration 101, loss = 0.34044927\n",
      "Iteration 102, loss = 0.33739967\n",
      "Iteration 103, loss = 0.33437309\n",
      "Iteration 104, loss = 0.33136381\n",
      "Iteration 105, loss = 0.32836952\n",
      "Iteration 106, loss = 0.32538672\n",
      "Iteration 107, loss = 0.32241787\n",
      "Iteration 108, loss = 0.31946459\n",
      "Iteration 109, loss = 0.31652939\n",
      "Iteration 110, loss = 0.31360710\n",
      "Iteration 111, loss = 0.31069738\n",
      "Iteration 112, loss = 0.30780044\n",
      "Iteration 113, loss = 0.30491894\n",
      "Iteration 114, loss = 0.30205150\n",
      "Iteration 115, loss = 0.29919770\n",
      "Iteration 116, loss = 0.29635750\n",
      "Iteration 117, loss = 0.29353083\n",
      "Iteration 118, loss = 0.29071784\n",
      "Iteration 119, loss = 0.28791926\n",
      "Iteration 120, loss = 0.28513622\n",
      "Iteration 121, loss = 0.28236874\n",
      "Iteration 122, loss = 0.27961633\n",
      "Iteration 123, loss = 0.27687909\n",
      "Iteration 124, loss = 0.27415767\n",
      "Iteration 125, loss = 0.27145195\n",
      "Iteration 126, loss = 0.26876439\n",
      "Iteration 127, loss = 0.26609651\n",
      "Iteration 128, loss = 0.26344594\n",
      "Iteration 129, loss = 0.26081324\n",
      "Iteration 130, loss = 0.25819864\n",
      "Iteration 131, loss = 0.25560271\n",
      "Iteration 132, loss = 0.25302610\n",
      "Iteration 133, loss = 0.25046874\n",
      "Iteration 134, loss = 0.24793142\n",
      "Iteration 135, loss = 0.24541444\n",
      "Iteration 136, loss = 0.24291839\n",
      "Iteration 137, loss = 0.24044435\n",
      "Iteration 138, loss = 0.23799205\n",
      "Iteration 139, loss = 0.23556188\n",
      "Iteration 140, loss = 0.23315425\n",
      "Iteration 141, loss = 0.23076957\n",
      "Iteration 142, loss = 0.22840824\n",
      "Iteration 143, loss = 0.22607067\n",
      "Iteration 144, loss = 0.22375719\n",
      "Iteration 145, loss = 0.22146813\n",
      "Iteration 146, loss = 0.21920383\n",
      "Iteration 147, loss = 0.21696461\n",
      "Iteration 148, loss = 0.21475076\n",
      "Iteration 149, loss = 0.21256253\n",
      "Iteration 150, loss = 0.21040016\n",
      "Iteration 151, loss = 0.20826388\n",
      "Iteration 152, loss = 0.20615388\n",
      "Iteration 153, loss = 0.20407036\n",
      "Iteration 154, loss = 0.20201346\n",
      "Iteration 155, loss = 0.19998331\n",
      "Iteration 156, loss = 0.19798002\n",
      "Iteration 157, loss = 0.19600367\n",
      "Iteration 158, loss = 0.19405432\n",
      "Iteration 159, loss = 0.19213201\n",
      "Iteration 160, loss = 0.19023675\n",
      "Iteration 161, loss = 0.18836853\n",
      "Iteration 162, loss = 0.18652733\n",
      "Iteration 163, loss = 0.18471309\n",
      "Iteration 164, loss = 0.18292576\n",
      "Iteration 165, loss = 0.18116523\n",
      "Iteration 166, loss = 0.17943139\n",
      "Iteration 167, loss = 0.17772412\n",
      "Iteration 168, loss = 0.17604327\n",
      "Iteration 169, loss = 0.17438868\n",
      "Iteration 170, loss = 0.17276017\n",
      "Iteration 171, loss = 0.17115754\n",
      "Iteration 172, loss = 0.16958060\n",
      "Iteration 173, loss = 0.16802911\n",
      "Iteration 174, loss = 0.16650284\n",
      "Iteration 175, loss = 0.16500155\n",
      "Iteration 176, loss = 0.16352498\n",
      "Iteration 177, loss = 0.16207286\n",
      "Iteration 178, loss = 0.16064491\n",
      "Iteration 179, loss = 0.15924086\n",
      "Iteration 180, loss = 0.15786040\n",
      "Iteration 181, loss = 0.15650323\n",
      "Iteration 182, loss = 0.15516906\n",
      "Iteration 183, loss = 0.15385757\n",
      "Iteration 184, loss = 0.15256844\n",
      "Iteration 185, loss = 0.15130135\n",
      "Iteration 186, loss = 0.15005598\n",
      "Iteration 187, loss = 0.14883200\n",
      "Iteration 188, loss = 0.14762908\n",
      "Iteration 189, loss = 0.14644689\n",
      "Iteration 190, loss = 0.14528510\n",
      "Iteration 191, loss = 0.14414338\n",
      "Iteration 192, loss = 0.14302138\n",
      "Iteration 193, loss = 0.14191879\n",
      "Iteration 194, loss = 0.14083527\n",
      "Iteration 195, loss = 0.13977048\n",
      "Iteration 196, loss = 0.13872410\n",
      "Iteration 197, loss = 0.13769580\n",
      "Iteration 198, loss = 0.13668525\n",
      "Iteration 199, loss = 0.13569213\n",
      "Iteration 200, loss = 0.13471613\n",
      "Iteration 201, loss = 0.13375691\n",
      "Iteration 202, loss = 0.13281417\n",
      "Iteration 203, loss = 0.13188759\n",
      "Iteration 204, loss = 0.13097687\n",
      "Iteration 205, loss = 0.13008171\n",
      "Iteration 206, loss = 0.12920179\n",
      "Iteration 207, loss = 0.12833684\n",
      "Iteration 208, loss = 0.12748655\n",
      "Iteration 209, loss = 0.12665063\n",
      "Iteration 210, loss = 0.12582881\n",
      "Iteration 211, loss = 0.12502080\n",
      "Iteration 212, loss = 0.12422633\n",
      "Iteration 213, loss = 0.12344513\n",
      "Iteration 214, loss = 0.12267693\n",
      "Iteration 215, loss = 0.12192148\n",
      "Iteration 216, loss = 0.12117851\n",
      "Iteration 217, loss = 0.12044777\n",
      "Iteration 218, loss = 0.11972902\n",
      "Iteration 219, loss = 0.11902201\n",
      "Iteration 220, loss = 0.11832650\n",
      "Iteration 221, loss = 0.11764226\n",
      "Iteration 222, loss = 0.11696907\n",
      "Iteration 223, loss = 0.11630669\n",
      "Iteration 224, loss = 0.11565491\n",
      "Iteration 225, loss = 0.11501351\n",
      "Iteration 226, loss = 0.11438228\n",
      "Iteration 227, loss = 0.11376101\n",
      "Iteration 228, loss = 0.11314950\n",
      "Iteration 229, loss = 0.11254756\n",
      "Iteration 230, loss = 0.11195497\n",
      "Iteration 231, loss = 0.11137157\n",
      "Iteration 232, loss = 0.11079716\n",
      "Iteration 233, loss = 0.11023155\n",
      "Iteration 234, loss = 0.10967457\n",
      "Iteration 235, loss = 0.10912605\n",
      "Iteration 236, loss = 0.10858581\n",
      "Iteration 237, loss = 0.10805370\n",
      "Iteration 238, loss = 0.10752953\n",
      "Iteration 239, loss = 0.10701317\n",
      "Iteration 240, loss = 0.10650444\n",
      "Iteration 241, loss = 0.10600320\n",
      "Iteration 242, loss = 0.10550929\n",
      "Iteration 243, loss = 0.10502258\n",
      "Iteration 244, loss = 0.10454310\n",
      "Iteration 245, loss = 0.10407050\n",
      "Iteration 246, loss = 0.10360464\n",
      "Iteration 247, loss = 0.10314540\n",
      "Iteration 248, loss = 0.10269268\n",
      "Iteration 249, loss = 0.10224636\n",
      "Iteration 250, loss = 0.10180633\n",
      "Iteration 251, loss = 0.10137246\n",
      "Iteration 252, loss = 0.10094464\n",
      "Iteration 253, loss = 0.10052274\n",
      "Iteration 254, loss = 0.10010665\n",
      "Iteration 255, loss = 0.09969625\n",
      "Iteration 256, loss = 0.09929142\n",
      "Iteration 257, loss = 0.09889204\n",
      "Iteration 258, loss = 0.09849802\n",
      "Iteration 259, loss = 0.09810924\n",
      "Iteration 260, loss = 0.09772561\n",
      "Iteration 261, loss = 0.09734703\n",
      "Iteration 262, loss = 0.09697341\n",
      "Iteration 263, loss = 0.09660466\n",
      "Iteration 264, loss = 0.09624069\n",
      "Iteration 265, loss = 0.09588142\n",
      "Iteration 266, loss = 0.09552675\n",
      "Iteration 267, loss = 0.09517662\n",
      "Iteration 268, loss = 0.09483093\n",
      "Iteration 269, loss = 0.09448960\n",
      "Iteration 270, loss = 0.09415255\n",
      "Iteration 271, loss = 0.09381971\n",
      "Iteration 272, loss = 0.09349100\n",
      "Iteration 273, loss = 0.09316634\n",
      "Iteration 274, loss = 0.09284569\n",
      "Iteration 275, loss = 0.09252899\n",
      "Iteration 276, loss = 0.09221612\n",
      "Iteration 277, loss = 0.09190700\n",
      "Iteration 278, loss = 0.09160159\n",
      "Iteration 279, loss = 0.09129983\n",
      "Iteration 280, loss = 0.09100165\n",
      "Iteration 281, loss = 0.09070701\n",
      "Iteration 282, loss = 0.09041583\n",
      "Iteration 283, loss = 0.09012806\n",
      "Iteration 284, loss = 0.08984365\n",
      "Iteration 285, loss = 0.08956252\n",
      "Iteration 286, loss = 0.08928463\n",
      "Iteration 287, loss = 0.08900992\n",
      "Iteration 288, loss = 0.08873832\n",
      "Iteration 289, loss = 0.08846980\n",
      "Iteration 290, loss = 0.08820428\n",
      "Iteration 291, loss = 0.08794173\n",
      "Iteration 292, loss = 0.08768210\n",
      "Iteration 293, loss = 0.08742533\n",
      "Iteration 294, loss = 0.08717139\n",
      "Iteration 295, loss = 0.08692022\n",
      "Iteration 296, loss = 0.08667179\n",
      "Iteration 297, loss = 0.08642604\n",
      "Iteration 298, loss = 0.08618294\n",
      "Iteration 299, loss = 0.08594245\n",
      "Iteration 300, loss = 0.08570453\n",
      "Iteration 301, loss = 0.08546913\n",
      "Iteration 302, loss = 0.08523621\n",
      "Iteration 303, loss = 0.08500573\n",
      "Iteration 304, loss = 0.08477767\n",
      "Iteration 305, loss = 0.08455197\n",
      "Iteration 306, loss = 0.08432860\n",
      "Iteration 307, loss = 0.08410753\n",
      "Iteration 308, loss = 0.08388872\n",
      "Iteration 309, loss = 0.08367216\n",
      "Iteration 310, loss = 0.08345783\n",
      "Iteration 311, loss = 0.08324564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 312, loss = 0.08303556\n",
      "Iteration 313, loss = 0.08282757\n",
      "Iteration 314, loss = 0.08262166\n",
      "Iteration 315, loss = 0.08241784\n",
      "Iteration 316, loss = 0.08221603\n",
      "Iteration 317, loss = 0.08201619\n",
      "Iteration 318, loss = 0.08181828\n",
      "Iteration 319, loss = 0.08162228\n",
      "Iteration 320, loss = 0.08142822\n",
      "Iteration 321, loss = 0.08123602\n",
      "Iteration 322, loss = 0.08104566\n",
      "Iteration 323, loss = 0.08085711\n",
      "Iteration 324, loss = 0.08067035\n",
      "Iteration 325, loss = 0.08048535\n",
      "Iteration 326, loss = 0.08030209\n",
      "Iteration 327, loss = 0.08012054\n",
      "Iteration 328, loss = 0.07994068\n",
      "Iteration 329, loss = 0.07976248\n",
      "Iteration 330, loss = 0.07958592\n",
      "Iteration 331, loss = 0.07941098\n",
      "Iteration 332, loss = 0.07923763\n",
      "Iteration 333, loss = 0.07906586\n",
      "Iteration 334, loss = 0.07889564\n",
      "Iteration 335, loss = 0.07872694\n",
      "Iteration 336, loss = 0.07855976\n",
      "Iteration 337, loss = 0.07839407\n",
      "Iteration 338, loss = 0.07822985\n",
      "Iteration 339, loss = 0.07806708\n",
      "Iteration 340, loss = 0.07790574\n",
      "Iteration 341, loss = 0.07774582\n",
      "Iteration 342, loss = 0.07758729\n",
      "Iteration 343, loss = 0.07743014\n",
      "Iteration 344, loss = 0.07727434\n",
      "Iteration 345, loss = 0.07711989\n",
      "Iteration 346, loss = 0.07696676\n",
      "Iteration 347, loss = 0.07681493\n",
      "Iteration 348, loss = 0.07666440\n",
      "Iteration 349, loss = 0.07651514\n",
      "Iteration 350, loss = 0.07636714\n",
      "Iteration 351, loss = 0.07622037\n",
      "Iteration 352, loss = 0.07607484\n",
      "Iteration 353, loss = 0.07593052\n",
      "Iteration 354, loss = 0.07578739\n",
      "Iteration 355, loss = 0.07564544\n",
      "Iteration 356, loss = 0.07550466\n",
      "Iteration 357, loss = 0.07536504\n",
      "Iteration 358, loss = 0.07522658\n",
      "Iteration 359, loss = 0.07508924\n",
      "Iteration 360, loss = 0.07495301\n",
      "Iteration 361, loss = 0.07481788\n",
      "Iteration 362, loss = 0.07468384\n",
      "Iteration 363, loss = 0.07455087\n",
      "Iteration 364, loss = 0.07441897\n",
      "Iteration 365, loss = 0.07428812\n",
      "Iteration 366, loss = 0.07415832\n",
      "Iteration 367, loss = 0.07402954\n",
      "Iteration 368, loss = 0.07390177\n",
      "Iteration 369, loss = 0.07377500\n",
      "Iteration 370, loss = 0.07364923\n",
      "Iteration 371, loss = 0.07352442\n",
      "Iteration 372, loss = 0.07340059\n",
      "Iteration 373, loss = 0.07327771\n",
      "Iteration 374, loss = 0.07315577\n",
      "Iteration 375, loss = 0.07303476\n",
      "Iteration 376, loss = 0.07291468\n",
      "Iteration 377, loss = 0.07279551\n",
      "Iteration 378, loss = 0.07267723\n",
      "Iteration 379, loss = 0.07255985\n",
      "Iteration 380, loss = 0.07244335\n",
      "Iteration 381, loss = 0.07232772\n",
      "Iteration 382, loss = 0.07221300\n",
      "Iteration 383, loss = 0.07209912\n",
      "Iteration 384, loss = 0.07198608\n",
      "Iteration 385, loss = 0.07187387\n",
      "Iteration 386, loss = 0.07176249\n",
      "Iteration 387, loss = 0.07165191\n",
      "Iteration 388, loss = 0.07154215\n",
      "Iteration 389, loss = 0.07143319\n",
      "Iteration 390, loss = 0.07132503\n",
      "Iteration 391, loss = 0.07121765\n",
      "Iteration 392, loss = 0.07111105\n",
      "Iteration 393, loss = 0.07100522\n",
      "Iteration 394, loss = 0.07090015\n",
      "Iteration 395, loss = 0.07079583\n",
      "Iteration 396, loss = 0.07069226\n",
      "Iteration 397, loss = 0.07058942\n",
      "Iteration 398, loss = 0.07048731\n",
      "Iteration 399, loss = 0.07038592\n",
      "Iteration 400, loss = 0.07028524\n",
      "Iteration 401, loss = 0.07018527\n",
      "Iteration 402, loss = 0.07008599\n",
      "Iteration 403, loss = 0.06998739\n",
      "Iteration 404, loss = 0.06988949\n",
      "Iteration 405, loss = 0.06979225\n",
      "Iteration 406, loss = 0.06969569\n",
      "Iteration 407, loss = 0.06959979\n",
      "Iteration 408, loss = 0.06950455\n",
      "Iteration 409, loss = 0.06940995\n",
      "Iteration 410, loss = 0.06931600\n",
      "Iteration 411, loss = 0.06922269\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51234105\n",
      "Iteration 2, loss = 1.35417734\n",
      "Iteration 3, loss = 1.22270138\n",
      "Iteration 4, loss = 1.11604341\n",
      "Iteration 5, loss = 1.03039070\n",
      "Iteration 6, loss = 0.95758028\n",
      "Iteration 7, loss = 0.89708426\n",
      "Iteration 8, loss = 0.85127818\n",
      "Iteration 9, loss = 0.82216206\n",
      "Iteration 10, loss = 0.80863708\n",
      "Iteration 11, loss = 0.80675292\n",
      "Iteration 12, loss = 0.81167279\n",
      "Iteration 13, loss = 0.81839498\n",
      "Iteration 14, loss = 0.82263451\n",
      "Iteration 15, loss = 0.82136725\n",
      "Iteration 16, loss = 0.81385808\n",
      "Iteration 17, loss = 0.80048268\n",
      "Iteration 18, loss = 0.78492653\n",
      "Iteration 19, loss = 0.76985910\n",
      "Iteration 20, loss = 0.75678390\n",
      "Iteration 21, loss = 0.74635825\n",
      "Iteration 22, loss = 0.73706310\n",
      "Iteration 23, loss = 0.72743700\n",
      "Iteration 24, loss = 0.71691808\n",
      "Iteration 25, loss = 0.70488221\n",
      "Iteration 26, loss = 0.69263824\n",
      "Iteration 27, loss = 0.68159971\n",
      "Iteration 28, loss = 0.67225910\n",
      "Iteration 29, loss = 0.66494696\n",
      "Iteration 30, loss = 0.65933084\n",
      "Iteration 31, loss = 0.65519882\n",
      "Iteration 32, loss = 0.65153750\n",
      "Iteration 33, loss = 0.64750239\n",
      "Iteration 34, loss = 0.64243571\n",
      "Iteration 35, loss = 0.63602187\n",
      "Iteration 36, loss = 0.62842230\n",
      "Iteration 37, loss = 0.62008493\n",
      "Iteration 38, loss = 0.61155612\n",
      "Iteration 39, loss = 0.60352794\n",
      "Iteration 40, loss = 0.59624767\n",
      "Iteration 41, loss = 0.58974434\n",
      "Iteration 42, loss = 0.58394522\n",
      "Iteration 43, loss = 0.57870805\n",
      "Iteration 44, loss = 0.57382507\n",
      "Iteration 45, loss = 0.56894791\n",
      "Iteration 46, loss = 0.56402241\n",
      "Iteration 47, loss = 0.55902459\n",
      "Iteration 48, loss = 0.55403370\n",
      "Iteration 49, loss = 0.54914465\n",
      "Iteration 50, loss = 0.54435451\n",
      "Iteration 51, loss = 0.53964889\n",
      "Iteration 52, loss = 0.53499045\n",
      "Iteration 53, loss = 0.53030893\n",
      "Iteration 54, loss = 0.52525562\n",
      "Iteration 55, loss = 0.51990071\n",
      "Iteration 56, loss = 0.51447705\n",
      "Iteration 57, loss = 0.51063927\n",
      "Iteration 58, loss = 0.50589447\n",
      "Iteration 59, loss = 0.50137865\n",
      "Iteration 60, loss = 0.49745700\n",
      "Iteration 61, loss = 0.49346937\n",
      "Iteration 62, loss = 0.48925333\n",
      "Iteration 63, loss = 0.48481587\n",
      "Iteration 64, loss = 0.48026684\n",
      "Iteration 65, loss = 0.47599276\n",
      "Iteration 66, loss = 0.47175928\n",
      "Iteration 67, loss = 0.46736834\n",
      "Iteration 68, loss = 0.46311699\n",
      "Iteration 69, loss = 0.45908277\n",
      "Iteration 70, loss = 0.45500776\n",
      "Iteration 71, loss = 0.45084593\n",
      "Iteration 72, loss = 0.44670182\n",
      "Iteration 73, loss = 0.44269684\n",
      "Iteration 74, loss = 0.43872723\n",
      "Iteration 75, loss = 0.43477181\n",
      "Iteration 76, loss = 0.43086481\n",
      "Iteration 77, loss = 0.42706072\n",
      "Iteration 78, loss = 0.42332209\n",
      "Iteration 79, loss = 0.41960102\n",
      "Iteration 80, loss = 0.41590773\n",
      "Iteration 81, loss = 0.41227361\n",
      "Iteration 82, loss = 0.40869890\n",
      "Iteration 83, loss = 0.40518515\n",
      "Iteration 84, loss = 0.40172660\n",
      "Iteration 85, loss = 0.39831046\n",
      "Iteration 86, loss = 0.39492657\n",
      "Iteration 87, loss = 0.39158147\n",
      "Iteration 88, loss = 0.38828115\n",
      "Iteration 89, loss = 0.38503403\n",
      "Iteration 90, loss = 0.38181429\n",
      "Iteration 91, loss = 0.37862746\n",
      "Iteration 92, loss = 0.37547591\n",
      "Iteration 93, loss = 0.37236394\n",
      "Iteration 94, loss = 0.36927901\n",
      "Iteration 95, loss = 0.36622273\n",
      "Iteration 96, loss = 0.36319224\n",
      "Iteration 97, loss = 0.36018638\n",
      "Iteration 98, loss = 0.35720356\n",
      "Iteration 99, loss = 0.35424186\n",
      "Iteration 100, loss = 0.35130352\n",
      "Iteration 101, loss = 0.34838352\n",
      "Iteration 102, loss = 0.34548227\n",
      "Iteration 103, loss = 0.34259637\n",
      "Iteration 104, loss = 0.33972603\n",
      "Iteration 105, loss = 0.33687125\n",
      "Iteration 106, loss = 0.33403136\n",
      "Iteration 107, loss = 0.33120594\n",
      "Iteration 108, loss = 0.32839482\n",
      "Iteration 109, loss = 0.32559654\n",
      "Iteration 110, loss = 0.32281055\n",
      "Iteration 111, loss = 0.32003574\n",
      "Iteration 112, loss = 0.31727252\n",
      "Iteration 113, loss = 0.31452186\n",
      "Iteration 114, loss = 0.31178293\n",
      "Iteration 115, loss = 0.30905471\n",
      "Iteration 116, loss = 0.30633700\n",
      "Iteration 117, loss = 0.30362951\n",
      "Iteration 118, loss = 0.30093245\n",
      "Iteration 119, loss = 0.29824638\n",
      "Iteration 120, loss = 0.29557463\n",
      "Iteration 121, loss = 0.29291318\n",
      "Iteration 122, loss = 0.29026208\n",
      "Iteration 123, loss = 0.28762159\n",
      "Iteration 124, loss = 0.28499203\n",
      "Iteration 125, loss = 0.28237358\n",
      "Iteration 126, loss = 0.27976636\n",
      "Iteration 127, loss = 0.27717093\n",
      "Iteration 128, loss = 0.27458778\n",
      "Iteration 129, loss = 0.27201669\n",
      "Iteration 130, loss = 0.26945804\n",
      "Iteration 131, loss = 0.26691218\n",
      "Iteration 132, loss = 0.26437938\n",
      "Iteration 133, loss = 0.26185992\n",
      "Iteration 134, loss = 0.25935416\n",
      "Iteration 135, loss = 0.25686255\n",
      "Iteration 136, loss = 0.25438554\n",
      "Iteration 137, loss = 0.25192356\n",
      "Iteration 138, loss = 0.24947700\n",
      "Iteration 139, loss = 0.24704623\n",
      "Iteration 140, loss = 0.24463169\n",
      "Iteration 141, loss = 0.24223383\n",
      "Iteration 142, loss = 0.23985310\n",
      "Iteration 143, loss = 0.23748990\n",
      "Iteration 144, loss = 0.23514462\n",
      "Iteration 145, loss = 0.23281763\n",
      "Iteration 146, loss = 0.23050936\n",
      "Iteration 147, loss = 0.22822020\n",
      "Iteration 148, loss = 0.22595056\n",
      "Iteration 149, loss = 0.22370080\n",
      "Iteration 150, loss = 0.22147125\n",
      "Iteration 151, loss = 0.21926226\n",
      "Iteration 152, loss = 0.21707416\n",
      "Iteration 153, loss = 0.21490726\n",
      "Iteration 154, loss = 0.21276186\n",
      "Iteration 155, loss = 0.21063821\n",
      "Iteration 156, loss = 0.20853658\n",
      "Iteration 157, loss = 0.20645721\n",
      "Iteration 158, loss = 0.20440033\n",
      "Iteration 159, loss = 0.20236615\n",
      "Iteration 160, loss = 0.20035485\n",
      "Iteration 161, loss = 0.19836659\n",
      "Iteration 162, loss = 0.19640153\n",
      "Iteration 163, loss = 0.19445978\n",
      "Iteration 164, loss = 0.19254147\n",
      "Iteration 165, loss = 0.19064668\n",
      "Iteration 166, loss = 0.18877548\n",
      "Iteration 167, loss = 0.18692792\n",
      "Iteration 168, loss = 0.18510405\n",
      "Iteration 169, loss = 0.18330387\n",
      "Iteration 170, loss = 0.18152740\n",
      "Iteration 171, loss = 0.17977462\n",
      "Iteration 172, loss = 0.17804623\n",
      "Iteration 173, loss = 0.17634058\n",
      "Iteration 174, loss = 0.17465830\n",
      "Iteration 175, loss = 0.17300009\n",
      "Iteration 176, loss = 0.17136524\n",
      "Iteration 177, loss = 0.16975358\n",
      "Iteration 178, loss = 0.16816496\n",
      "Iteration 179, loss = 0.16659923\n",
      "Iteration 180, loss = 0.16505626\n",
      "Iteration 181, loss = 0.16353592\n",
      "Iteration 182, loss = 0.16203865\n",
      "Iteration 183, loss = 0.16056352\n",
      "Iteration 184, loss = 0.15910998\n",
      "Iteration 185, loss = 0.15767905\n",
      "Iteration 186, loss = 0.15626993\n",
      "Iteration 187, loss = 0.15488224\n",
      "Iteration 188, loss = 0.15351577\n",
      "Iteration 189, loss = 0.15217033\n",
      "Iteration 190, loss = 0.15084571\n",
      "Iteration 191, loss = 0.14954173\n",
      "Iteration 192, loss = 0.14825821\n",
      "Iteration 193, loss = 0.14699518\n",
      "Iteration 194, loss = 0.14575198\n",
      "Iteration 195, loss = 0.14452825\n",
      "Iteration 196, loss = 0.14332421\n",
      "Iteration 197, loss = 0.14213935\n",
      "Iteration 198, loss = 0.14097341\n",
      "Iteration 199, loss = 0.13982615\n",
      "Iteration 200, loss = 0.13869729\n",
      "Iteration 201, loss = 0.13758659\n",
      "Iteration 202, loss = 0.13649380\n",
      "Iteration 203, loss = 0.13541864\n",
      "Iteration 204, loss = 0.13436087\n",
      "Iteration 205, loss = 0.13332020\n",
      "Iteration 206, loss = 0.13229638\n",
      "Iteration 207, loss = 0.13128915\n",
      "Iteration 208, loss = 0.13029823\n",
      "Iteration 209, loss = 0.12932337\n",
      "Iteration 210, loss = 0.12836430\n",
      "Iteration 211, loss = 0.12742077\n",
      "Iteration 212, loss = 0.12649252\n",
      "Iteration 213, loss = 0.12557930\n",
      "Iteration 214, loss = 0.12468086\n",
      "Iteration 215, loss = 0.12379695\n",
      "Iteration 216, loss = 0.12292733\n",
      "Iteration 217, loss = 0.12207174\n",
      "Iteration 218, loss = 0.12122996\n",
      "Iteration 219, loss = 0.12040174\n",
      "Iteration 220, loss = 0.11958684\n",
      "Iteration 221, loss = 0.11878504\n",
      "Iteration 222, loss = 0.11799610\n",
      "Iteration 223, loss = 0.11721979\n",
      "Iteration 224, loss = 0.11645589\n",
      "Iteration 225, loss = 0.11570417\n",
      "Iteration 226, loss = 0.11496443\n",
      "Iteration 227, loss = 0.11423643\n",
      "Iteration 228, loss = 0.11351998\n",
      "Iteration 229, loss = 0.11281486\n",
      "Iteration 230, loss = 0.11212087\n",
      "Iteration 231, loss = 0.11143780\n",
      "Iteration 232, loss = 0.11076546\n",
      "Iteration 233, loss = 0.11010365\n",
      "Iteration 234, loss = 0.10945217\n",
      "Iteration 235, loss = 0.10881085\n",
      "Iteration 236, loss = 0.10817949\n",
      "Iteration 237, loss = 0.10755791\n",
      "Iteration 238, loss = 0.10694593\n",
      "Iteration 239, loss = 0.10634338\n",
      "Iteration 240, loss = 0.10575007\n",
      "Iteration 241, loss = 0.10516585\n",
      "Iteration 242, loss = 0.10459054\n",
      "Iteration 243, loss = 0.10402397\n",
      "Iteration 244, loss = 0.10346600\n",
      "Iteration 245, loss = 0.10291645\n",
      "Iteration 246, loss = 0.10237518\n",
      "Iteration 247, loss = 0.10184203\n",
      "Iteration 248, loss = 0.10131685\n",
      "Iteration 249, loss = 0.10079950\n",
      "Iteration 250, loss = 0.10028983\n",
      "Iteration 251, loss = 0.09978771\n",
      "Iteration 252, loss = 0.09929298\n",
      "Iteration 253, loss = 0.09880553\n",
      "Iteration 254, loss = 0.09832520\n",
      "Iteration 255, loss = 0.09785189\n",
      "Iteration 256, loss = 0.09738544\n",
      "Iteration 257, loss = 0.09692575\n",
      "Iteration 258, loss = 0.09647269\n",
      "Iteration 259, loss = 0.09602613\n",
      "Iteration 260, loss = 0.09558597\n",
      "Iteration 261, loss = 0.09515207\n",
      "Iteration 262, loss = 0.09472434\n",
      "Iteration 263, loss = 0.09430266\n",
      "Iteration 264, loss = 0.09388692\n",
      "Iteration 265, loss = 0.09347701\n",
      "Iteration 266, loss = 0.09307283\n",
      "Iteration 267, loss = 0.09267428\n",
      "Iteration 268, loss = 0.09228125\n",
      "Iteration 269, loss = 0.09189365\n",
      "Iteration 270, loss = 0.09151138\n",
      "Iteration 271, loss = 0.09113435\n",
      "Iteration 272, loss = 0.09076246\n",
      "Iteration 273, loss = 0.09039562\n",
      "Iteration 274, loss = 0.09003375\n",
      "Iteration 275, loss = 0.08967675\n",
      "Iteration 276, loss = 0.08932455\n",
      "Iteration 277, loss = 0.08897706\n",
      "Iteration 278, loss = 0.08863418\n",
      "Iteration 279, loss = 0.08829586\n",
      "Iteration 280, loss = 0.08796200\n",
      "Iteration 281, loss = 0.08763253\n",
      "Iteration 282, loss = 0.08730738\n",
      "Iteration 283, loss = 0.08698646\n",
      "Iteration 284, loss = 0.08666971\n",
      "Iteration 285, loss = 0.08635706\n",
      "Iteration 286, loss = 0.08604843\n",
      "Iteration 287, loss = 0.08574376\n",
      "Iteration 288, loss = 0.08544299\n",
      "Iteration 289, loss = 0.08514603\n",
      "Iteration 290, loss = 0.08485284\n",
      "Iteration 291, loss = 0.08456334\n",
      "Iteration 292, loss = 0.08427748\n",
      "Iteration 293, loss = 0.08399520\n",
      "Iteration 294, loss = 0.08371643\n",
      "Iteration 295, loss = 0.08344112\n",
      "Iteration 296, loss = 0.08316921\n",
      "Iteration 297, loss = 0.08290064\n",
      "Iteration 298, loss = 0.08263536\n",
      "Iteration 299, loss = 0.08237332\n",
      "Iteration 300, loss = 0.08211446\n",
      "Iteration 301, loss = 0.08185873\n",
      "Iteration 302, loss = 0.08160608\n",
      "Iteration 303, loss = 0.08135646\n",
      "Iteration 304, loss = 0.08110982\n",
      "Iteration 305, loss = 0.08086612\n",
      "Iteration 306, loss = 0.08062531\n",
      "Iteration 307, loss = 0.08038733\n",
      "Iteration 308, loss = 0.08015215\n",
      "Iteration 309, loss = 0.07991973\n",
      "Iteration 310, loss = 0.07969001\n",
      "Iteration 311, loss = 0.07946296\n",
      "Iteration 312, loss = 0.07923853\n",
      "Iteration 313, loss = 0.07901668\n",
      "Iteration 314, loss = 0.07879743\n",
      "Iteration 315, loss = 0.07858071\n",
      "Iteration 316, loss = 0.07836644\n",
      "Iteration 317, loss = 0.07815460\n",
      "Iteration 318, loss = 0.07794514\n",
      "Iteration 319, loss = 0.07773804\n",
      "Iteration 320, loss = 0.07753325\n",
      "Iteration 321, loss = 0.07733076\n",
      "Iteration 322, loss = 0.07713052\n",
      "Iteration 323, loss = 0.07693251\n",
      "Iteration 324, loss = 0.07673667\n",
      "Iteration 325, loss = 0.07654299\n",
      "Iteration 326, loss = 0.07635143\n",
      "Iteration 327, loss = 0.07616195\n",
      "Iteration 328, loss = 0.07597452\n",
      "Iteration 329, loss = 0.07578911\n",
      "Iteration 330, loss = 0.07560570\n",
      "Iteration 331, loss = 0.07542424\n",
      "Iteration 332, loss = 0.07524471\n",
      "Iteration 333, loss = 0.07506709\n",
      "Iteration 334, loss = 0.07489134\n",
      "Iteration 335, loss = 0.07471744\n",
      "Iteration 336, loss = 0.07454536\n",
      "Iteration 337, loss = 0.07437508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 338, loss = 0.07420657\n",
      "Iteration 339, loss = 0.07403980\n",
      "Iteration 340, loss = 0.07387476\n",
      "Iteration 341, loss = 0.07371142\n",
      "Iteration 342, loss = 0.07354975\n",
      "Iteration 343, loss = 0.07338973\n",
      "Iteration 344, loss = 0.07323134\n",
      "Iteration 345, loss = 0.07307455\n",
      "Iteration 346, loss = 0.07291934\n",
      "Iteration 347, loss = 0.07276569\n",
      "Iteration 348, loss = 0.07261358\n",
      "Iteration 349, loss = 0.07246299\n",
      "Iteration 350, loss = 0.07231389\n",
      "Iteration 351, loss = 0.07216627\n",
      "Iteration 352, loss = 0.07202010\n",
      "Iteration 353, loss = 0.07187536\n",
      "Iteration 354, loss = 0.07173205\n",
      "Iteration 355, loss = 0.07159013\n",
      "Iteration 356, loss = 0.07144959\n",
      "Iteration 357, loss = 0.07131041\n",
      "Iteration 358, loss = 0.07117257\n",
      "Iteration 359, loss = 0.07103605\n",
      "Iteration 360, loss = 0.07090085\n",
      "Iteration 361, loss = 0.07076693\n",
      "Iteration 362, loss = 0.07063428\n",
      "Iteration 363, loss = 0.07050289\n",
      "Iteration 364, loss = 0.07037274\n",
      "Iteration 365, loss = 0.07024381\n",
      "Iteration 366, loss = 0.07011609\n",
      "Iteration 367, loss = 0.06998957\n",
      "Iteration 368, loss = 0.06986425\n",
      "Iteration 369, loss = 0.06974010\n",
      "Iteration 370, loss = 0.06961708\n",
      "Iteration 371, loss = 0.06949519\n",
      "Iteration 372, loss = 0.06937442\n",
      "Iteration 373, loss = 0.06925476\n",
      "Iteration 374, loss = 0.06913619\n",
      "Iteration 375, loss = 0.06901870\n",
      "Iteration 376, loss = 0.06890228\n",
      "Iteration 377, loss = 0.06878691\n",
      "Iteration 378, loss = 0.06867260\n",
      "Iteration 379, loss = 0.06855930\n",
      "Iteration 380, loss = 0.06844701\n",
      "Iteration 381, loss = 0.06833572\n",
      "Iteration 382, loss = 0.06822542\n",
      "Iteration 383, loss = 0.06811609\n",
      "Iteration 384, loss = 0.06800773\n",
      "Iteration 385, loss = 0.06790031\n",
      "Iteration 386, loss = 0.06779384\n",
      "Iteration 387, loss = 0.06768829\n",
      "Iteration 388, loss = 0.06758366\n",
      "Iteration 389, loss = 0.06747993\n",
      "Iteration 390, loss = 0.06737709\n",
      "Iteration 391, loss = 0.06727514\n",
      "Iteration 392, loss = 0.06717406\n",
      "Iteration 393, loss = 0.06707385\n",
      "Iteration 394, loss = 0.06697449\n",
      "Iteration 395, loss = 0.06687596\n",
      "Iteration 396, loss = 0.06677828\n",
      "Iteration 397, loss = 0.06668141\n",
      "Iteration 398, loss = 0.06658536\n",
      "Iteration 399, loss = 0.06649012\n",
      "Iteration 400, loss = 0.06639566\n",
      "Iteration 401, loss = 0.06630200\n",
      "Iteration 402, loss = 0.06620911\n",
      "Iteration 403, loss = 0.06611698\n",
      "Iteration 404, loss = 0.06602562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50852130\n",
      "Iteration 2, loss = 1.35174986\n",
      "Iteration 3, loss = 1.22122299\n",
      "Iteration 4, loss = 1.11477232\n",
      "Iteration 5, loss = 1.02890721\n",
      "Iteration 6, loss = 0.95536448\n",
      "Iteration 7, loss = 0.89410386\n",
      "Iteration 8, loss = 0.84708579\n",
      "Iteration 9, loss = 0.81643382\n",
      "Iteration 10, loss = 0.80137155\n",
      "Iteration 11, loss = 0.79770198\n",
      "Iteration 12, loss = 0.80009629\n",
      "Iteration 13, loss = 0.80369521\n",
      "Iteration 14, loss = 0.80396081\n",
      "Iteration 15, loss = 0.79971127\n",
      "Iteration 16, loss = 0.79228490\n",
      "Iteration 17, loss = 0.78259895\n",
      "Iteration 18, loss = 0.77221535\n",
      "Iteration 19, loss = 0.76188987\n",
      "Iteration 20, loss = 0.75162038\n",
      "Iteration 21, loss = 0.74096797\n",
      "Iteration 22, loss = 0.72953481\n",
      "Iteration 23, loss = 0.71713104\n",
      "Iteration 24, loss = 0.70431245\n",
      "Iteration 25, loss = 0.69097672\n",
      "Iteration 26, loss = 0.67876162\n",
      "Iteration 27, loss = 0.66873734\n",
      "Iteration 28, loss = 0.66060914\n",
      "Iteration 29, loss = 0.65470122\n",
      "Iteration 30, loss = 0.65022978\n",
      "Iteration 31, loss = 0.64646155\n",
      "Iteration 32, loss = 0.64264354\n",
      "Iteration 33, loss = 0.63806377\n",
      "Iteration 34, loss = 0.63229140\n",
      "Iteration 35, loss = 0.62529730\n",
      "Iteration 36, loss = 0.61733667\n",
      "Iteration 37, loss = 0.60888517\n",
      "Iteration 38, loss = 0.60052429\n",
      "Iteration 39, loss = 0.59265688\n",
      "Iteration 40, loss = 0.58549058\n",
      "Iteration 41, loss = 0.57912705\n",
      "Iteration 42, loss = 0.57343521\n",
      "Iteration 43, loss = 0.56813902\n",
      "Iteration 44, loss = 0.56299394\n",
      "Iteration 45, loss = 0.55798776\n",
      "Iteration 46, loss = 0.55311993\n",
      "Iteration 47, loss = 0.54831351\n",
      "Iteration 48, loss = 0.54355254\n",
      "Iteration 49, loss = 0.53883338\n",
      "Iteration 50, loss = 0.53411185\n",
      "Iteration 51, loss = 0.52933982\n",
      "Iteration 52, loss = 0.52451731\n",
      "Iteration 53, loss = 0.51961526\n",
      "Iteration 54, loss = 0.51430448\n",
      "Iteration 55, loss = 0.50877977\n",
      "Iteration 56, loss = 0.50329478\n",
      "Iteration 57, loss = 0.49986108\n",
      "Iteration 58, loss = 0.49498544\n",
      "Iteration 59, loss = 0.49023343\n",
      "Iteration 60, loss = 0.48629435\n",
      "Iteration 61, loss = 0.48224517\n",
      "Iteration 62, loss = 0.47793085\n",
      "Iteration 63, loss = 0.47334424\n",
      "Iteration 64, loss = 0.46865769\n",
      "Iteration 65, loss = 0.46401294\n",
      "Iteration 66, loss = 0.45988126\n",
      "Iteration 67, loss = 0.45578201\n",
      "Iteration 68, loss = 0.45119477\n",
      "Iteration 69, loss = 0.44677387\n",
      "Iteration 70, loss = 0.44266221\n",
      "Iteration 71, loss = 0.43847640\n",
      "Iteration 72, loss = 0.43418121\n",
      "Iteration 73, loss = 0.42981639\n",
      "Iteration 74, loss = 0.42557385\n",
      "Iteration 75, loss = 0.42158312\n",
      "Iteration 76, loss = 0.41762003\n",
      "Iteration 77, loss = 0.41350519\n",
      "Iteration 78, loss = 0.40941248\n",
      "Iteration 79, loss = 0.40543777\n",
      "Iteration 80, loss = 0.40156666\n",
      "Iteration 81, loss = 0.39768757\n",
      "Iteration 82, loss = 0.39380360\n",
      "Iteration 83, loss = 0.38998621\n",
      "Iteration 84, loss = 0.38625504\n",
      "Iteration 85, loss = 0.38255474\n",
      "Iteration 86, loss = 0.37887451\n",
      "Iteration 87, loss = 0.37520704\n",
      "Iteration 88, loss = 0.37156196\n",
      "Iteration 89, loss = 0.36797996\n",
      "Iteration 90, loss = 0.36444865\n",
      "Iteration 91, loss = 0.36095530\n",
      "Iteration 92, loss = 0.35746991\n",
      "Iteration 93, loss = 0.35399936\n",
      "Iteration 94, loss = 0.35055496\n",
      "Iteration 95, loss = 0.34716327\n",
      "Iteration 96, loss = 0.34379620\n",
      "Iteration 97, loss = 0.34043854\n",
      "Iteration 98, loss = 0.33709418\n",
      "Iteration 99, loss = 0.33378094\n",
      "Iteration 100, loss = 0.33048705\n",
      "Iteration 101, loss = 0.32721169\n",
      "Iteration 102, loss = 0.32395330\n",
      "Iteration 103, loss = 0.32071133\n",
      "Iteration 104, loss = 0.31748572\n",
      "Iteration 105, loss = 0.31428085\n",
      "Iteration 106, loss = 0.31109291\n",
      "Iteration 107, loss = 0.30792122\n",
      "Iteration 108, loss = 0.30476311\n",
      "Iteration 109, loss = 0.30161794\n",
      "Iteration 110, loss = 0.29848661\n",
      "Iteration 111, loss = 0.29537353\n",
      "Iteration 112, loss = 0.29227404\n",
      "Iteration 113, loss = 0.28918836\n",
      "Iteration 114, loss = 0.28611704\n",
      "Iteration 115, loss = 0.28306048\n",
      "Iteration 116, loss = 0.28001878\n",
      "Iteration 117, loss = 0.27699179\n",
      "Iteration 118, loss = 0.27398154\n",
      "Iteration 119, loss = 0.27098531\n",
      "Iteration 120, loss = 0.26800401\n",
      "Iteration 121, loss = 0.26503915\n",
      "Iteration 122, loss = 0.26209327\n",
      "Iteration 123, loss = 0.25916454\n",
      "Iteration 124, loss = 0.25625230\n",
      "Iteration 125, loss = 0.25336008\n",
      "Iteration 126, loss = 0.25048706\n",
      "Iteration 127, loss = 0.24763482\n",
      "Iteration 128, loss = 0.24480181\n",
      "Iteration 129, loss = 0.24198840\n",
      "Iteration 130, loss = 0.23919508\n",
      "Iteration 131, loss = 0.23642244\n",
      "Iteration 132, loss = 0.23367107\n",
      "Iteration 133, loss = 0.23094148\n",
      "Iteration 134, loss = 0.22823410\n",
      "Iteration 135, loss = 0.22554936\n",
      "Iteration 136, loss = 0.22288770\n",
      "Iteration 137, loss = 0.22024965\n",
      "Iteration 138, loss = 0.21763567\n",
      "Iteration 139, loss = 0.21504620\n",
      "Iteration 140, loss = 0.21248162\n",
      "Iteration 141, loss = 0.20994233\n",
      "Iteration 142, loss = 0.20742875\n",
      "Iteration 143, loss = 0.20494132\n",
      "Iteration 144, loss = 0.20248043\n",
      "Iteration 145, loss = 0.20004641\n",
      "Iteration 146, loss = 0.19763959\n",
      "Iteration 147, loss = 0.19526025\n",
      "Iteration 148, loss = 0.19290870\n",
      "Iteration 149, loss = 0.19058520\n",
      "Iteration 150, loss = 0.18828998\n",
      "Iteration 151, loss = 0.18602322\n",
      "Iteration 152, loss = 0.18378509\n",
      "Iteration 153, loss = 0.18157576\n",
      "Iteration 154, loss = 0.17939537\n",
      "Iteration 155, loss = 0.17724404\n",
      "Iteration 156, loss = 0.17512185\n",
      "Iteration 157, loss = 0.17302884\n",
      "Iteration 158, loss = 0.17096506\n",
      "Iteration 159, loss = 0.16893053\n",
      "Iteration 160, loss = 0.16692523\n",
      "Iteration 161, loss = 0.16494913\n",
      "Iteration 162, loss = 0.16300216\n",
      "Iteration 163, loss = 0.16108425\n",
      "Iteration 164, loss = 0.15919530\n",
      "Iteration 165, loss = 0.15733519\n",
      "Iteration 166, loss = 0.15550378\n",
      "Iteration 167, loss = 0.15370092\n",
      "Iteration 168, loss = 0.15192642\n",
      "Iteration 169, loss = 0.15018010\n",
      "Iteration 170, loss = 0.14846175\n",
      "Iteration 171, loss = 0.14677116\n",
      "Iteration 172, loss = 0.14510808\n",
      "Iteration 173, loss = 0.14347226\n",
      "Iteration 174, loss = 0.14186345\n",
      "Iteration 175, loss = 0.14028136\n",
      "Iteration 176, loss = 0.13872573\n",
      "Iteration 177, loss = 0.13719626\n",
      "Iteration 178, loss = 0.13569263\n",
      "Iteration 179, loss = 0.13421456\n",
      "Iteration 180, loss = 0.13276170\n",
      "Iteration 181, loss = 0.13133376\n",
      "Iteration 182, loss = 0.12993039\n",
      "Iteration 183, loss = 0.12855125\n",
      "Iteration 184, loss = 0.12719602\n",
      "Iteration 185, loss = 0.12586435\n",
      "Iteration 186, loss = 0.12455590\n",
      "Iteration 187, loss = 0.12327031\n",
      "Iteration 188, loss = 0.12200724\n",
      "Iteration 189, loss = 0.12076634\n",
      "Iteration 190, loss = 0.11954726\n",
      "Iteration 191, loss = 0.11834964\n",
      "Iteration 192, loss = 0.11717313\n",
      "Iteration 193, loss = 0.11601738\n",
      "Iteration 194, loss = 0.11488205\n",
      "Iteration 195, loss = 0.11376678\n",
      "Iteration 196, loss = 0.11267123\n",
      "Iteration 197, loss = 0.11159505\n",
      "Iteration 198, loss = 0.11053789\n",
      "Iteration 199, loss = 0.10949943\n",
      "Iteration 200, loss = 0.10847932\n",
      "Iteration 201, loss = 0.10747723\n",
      "Iteration 202, loss = 0.10649282\n",
      "Iteration 203, loss = 0.10552578\n",
      "Iteration 204, loss = 0.10457577\n",
      "Iteration 205, loss = 0.10364248\n",
      "Iteration 206, loss = 0.10272560\n",
      "Iteration 207, loss = 0.10182481\n",
      "Iteration 208, loss = 0.10093980\n",
      "Iteration 209, loss = 0.10007027\n",
      "Iteration 210, loss = 0.09921593\n",
      "Iteration 211, loss = 0.09837648\n",
      "Iteration 212, loss = 0.09755163\n",
      "Iteration 213, loss = 0.09674110\n",
      "Iteration 214, loss = 0.09594461\n",
      "Iteration 215, loss = 0.09516187\n",
      "Iteration 216, loss = 0.09439263\n",
      "Iteration 217, loss = 0.09363662\n",
      "Iteration 218, loss = 0.09289357\n",
      "Iteration 219, loss = 0.09216322\n",
      "Iteration 220, loss = 0.09144534\n",
      "Iteration 221, loss = 0.09073966\n",
      "Iteration 222, loss = 0.09004595\n",
      "Iteration 223, loss = 0.08936397\n",
      "Iteration 224, loss = 0.08869349\n",
      "Iteration 225, loss = 0.08803427\n",
      "Iteration 226, loss = 0.08738610\n",
      "Iteration 227, loss = 0.08674875\n",
      "Iteration 228, loss = 0.08612200\n",
      "Iteration 229, loss = 0.08550565\n",
      "Iteration 230, loss = 0.08489950\n",
      "Iteration 231, loss = 0.08430332\n",
      "Iteration 232, loss = 0.08371694\n",
      "Iteration 233, loss = 0.08314015\n",
      "Iteration 234, loss = 0.08257276\n",
      "Iteration 235, loss = 0.08201459\n",
      "Iteration 236, loss = 0.08146546\n",
      "Iteration 237, loss = 0.08092518\n",
      "Iteration 238, loss = 0.08039358\n",
      "Iteration 239, loss = 0.07987049\n",
      "Iteration 240, loss = 0.07935574\n",
      "Iteration 241, loss = 0.07884917\n",
      "Iteration 242, loss = 0.07835062\n",
      "Iteration 243, loss = 0.07785993\n",
      "Iteration 244, loss = 0.07737694\n",
      "Iteration 245, loss = 0.07690152\n",
      "Iteration 246, loss = 0.07643350\n",
      "Iteration 247, loss = 0.07597274\n",
      "Iteration 248, loss = 0.07551920\n",
      "Iteration 249, loss = 0.07507270\n",
      "Iteration 250, loss = 0.07463302\n",
      "Iteration 251, loss = 0.07420005\n",
      "Iteration 252, loss = 0.07377367\n",
      "Iteration 253, loss = 0.07335376\n",
      "Iteration 254, loss = 0.07294021\n",
      "Iteration 255, loss = 0.07253290\n",
      "Iteration 256, loss = 0.07213172\n",
      "Iteration 257, loss = 0.07173653\n",
      "Iteration 258, loss = 0.07134724\n",
      "Iteration 259, loss = 0.07096372\n",
      "Iteration 260, loss = 0.07058585\n",
      "Iteration 261, loss = 0.07021352\n",
      "Iteration 262, loss = 0.06984664\n",
      "Iteration 263, loss = 0.06948508\n",
      "Iteration 264, loss = 0.06912877\n",
      "Iteration 265, loss = 0.06877758\n",
      "Iteration 266, loss = 0.06843144\n",
      "Iteration 267, loss = 0.06809025\n",
      "Iteration 268, loss = 0.06775392\n",
      "Iteration 269, loss = 0.06742236\n",
      "Iteration 270, loss = 0.06709550\n",
      "Iteration 271, loss = 0.06677324\n",
      "Iteration 272, loss = 0.06645550\n",
      "Iteration 273, loss = 0.06614220\n",
      "Iteration 274, loss = 0.06583327\n",
      "Iteration 275, loss = 0.06552861\n",
      "Iteration 276, loss = 0.06522816\n",
      "Iteration 277, loss = 0.06493184\n",
      "Iteration 278, loss = 0.06463958\n",
      "Iteration 279, loss = 0.06435129\n",
      "Iteration 280, loss = 0.06406692\n",
      "Iteration 281, loss = 0.06378640\n",
      "Iteration 282, loss = 0.06350964\n",
      "Iteration 283, loss = 0.06323660\n",
      "Iteration 284, loss = 0.06296721\n",
      "Iteration 285, loss = 0.06270143\n",
      "Iteration 286, loss = 0.06243918\n",
      "Iteration 287, loss = 0.06218037\n",
      "Iteration 288, loss = 0.06192497\n",
      "Iteration 289, loss = 0.06167291\n",
      "Iteration 290, loss = 0.06142415\n",
      "Iteration 291, loss = 0.06117862\n",
      "Iteration 292, loss = 0.06093628\n",
      "Iteration 293, loss = 0.06069706\n",
      "Iteration 294, loss = 0.06046092\n",
      "Iteration 295, loss = 0.06022781\n",
      "Iteration 296, loss = 0.05999766\n",
      "Iteration 297, loss = 0.05977044\n",
      "Iteration 298, loss = 0.05954609\n",
      "Iteration 299, loss = 0.05932455\n",
      "Iteration 300, loss = 0.05910580\n",
      "Iteration 301, loss = 0.05888977\n",
      "Iteration 302, loss = 0.05867643\n",
      "Iteration 303, loss = 0.05846572\n",
      "Iteration 304, loss = 0.05825762\n",
      "Iteration 305, loss = 0.05805207\n",
      "Iteration 306, loss = 0.05784904\n",
      "Iteration 307, loss = 0.05764849\n",
      "Iteration 308, loss = 0.05745037\n",
      "Iteration 309, loss = 0.05725465\n",
      "Iteration 310, loss = 0.05706129\n",
      "Iteration 311, loss = 0.05687025\n",
      "Iteration 312, loss = 0.05668150\n",
      "Iteration 313, loss = 0.05649501\n",
      "Iteration 314, loss = 0.05631072\n",
      "Iteration 315, loss = 0.05612862\n",
      "Iteration 316, loss = 0.05594866\n",
      "Iteration 317, loss = 0.05577082\n",
      "Iteration 318, loss = 0.05559506\n",
      "Iteration 319, loss = 0.05542134\n",
      "Iteration 320, loss = 0.05524964\n",
      "Iteration 321, loss = 0.05507993\n",
      "Iteration 322, loss = 0.05491217\n",
      "Iteration 323, loss = 0.05474634\n",
      "Iteration 324, loss = 0.05458241\n",
      "Iteration 325, loss = 0.05442039\n",
      "Iteration 326, loss = 0.05426020\n",
      "Iteration 327, loss = 0.05410180\n",
      "Iteration 328, loss = 0.05394518\n",
      "Iteration 329, loss = 0.05379033\n",
      "Iteration 330, loss = 0.05363721\n",
      "Iteration 331, loss = 0.05348583\n",
      "Iteration 332, loss = 0.05333614\n",
      "Iteration 333, loss = 0.05318809\n",
      "Iteration 334, loss = 0.05304167\n",
      "Iteration 335, loss = 0.05289689\n",
      "Iteration 336, loss = 0.05275369\n",
      "Iteration 337, loss = 0.05261206\n",
      "Iteration 338, loss = 0.05247197\n",
      "Iteration 339, loss = 0.05233341\n",
      "Iteration 340, loss = 0.05219634\n",
      "Iteration 341, loss = 0.05206075\n",
      "Iteration 342, loss = 0.05192662\n",
      "Iteration 343, loss = 0.05179392\n",
      "Iteration 344, loss = 0.05166264\n",
      "Iteration 345, loss = 0.05153275\n",
      "Iteration 346, loss = 0.05140424\n",
      "Iteration 347, loss = 0.05127709\n",
      "Iteration 348, loss = 0.05115126\n",
      "Iteration 349, loss = 0.05102676\n",
      "Iteration 350, loss = 0.05090356\n",
      "Iteration 351, loss = 0.05078163\n",
      "Iteration 352, loss = 0.05066098\n",
      "Iteration 353, loss = 0.05054156\n",
      "Iteration 354, loss = 0.05042338\n",
      "Iteration 355, loss = 0.05030641\n",
      "Iteration 356, loss = 0.05019064\n",
      "Iteration 357, loss = 0.05007604\n",
      "Iteration 358, loss = 0.04996262\n",
      "Iteration 359, loss = 0.04985034\n",
      "Iteration 360, loss = 0.04973919\n",
      "Iteration 361, loss = 0.04962916\n",
      "Iteration 362, loss = 0.04952024\n",
      "Iteration 363, loss = 0.04941241\n",
      "Iteration 364, loss = 0.04930565\n",
      "Iteration 365, loss = 0.04919995\n",
      "Iteration 366, loss = 0.04909529\n",
      "Iteration 367, loss = 0.04899167\n",
      "Iteration 368, loss = 0.04888907\n",
      "Iteration 369, loss = 0.04878748\n",
      "Iteration 370, loss = 0.04868688\n",
      "Iteration 371, loss = 0.04858725\n",
      "Iteration 372, loss = 0.04848860\n",
      "Iteration 373, loss = 0.04839090\n",
      "Iteration 374, loss = 0.04829414\n",
      "Iteration 375, loss = 0.04819832\n",
      "Iteration 376, loss = 0.04810341\n",
      "Iteration 377, loss = 0.04800941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 378, loss = 0.04791631\n",
      "Iteration 379, loss = 0.04782409\n",
      "Iteration 380, loss = 0.04773274\n",
      "Iteration 381, loss = 0.04764226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50041490\n",
      "Iteration 2, loss = 1.34375614\n",
      "Iteration 3, loss = 1.21380135\n",
      "Iteration 4, loss = 1.10832778\n",
      "Iteration 5, loss = 1.02311503\n",
      "Iteration 6, loss = 0.95009842\n",
      "Iteration 7, loss = 0.88970252\n",
      "Iteration 8, loss = 0.84383501\n",
      "Iteration 9, loss = 0.81458007\n",
      "Iteration 10, loss = 0.80092492\n",
      "Iteration 11, loss = 0.79863431\n",
      "Iteration 12, loss = 0.80282257\n",
      "Iteration 13, loss = 0.80844410\n",
      "Iteration 14, loss = 0.81107470\n",
      "Iteration 15, loss = 0.80783328\n",
      "Iteration 16, loss = 0.79853363\n",
      "Iteration 17, loss = 0.78627483\n",
      "Iteration 18, loss = 0.77300742\n",
      "Iteration 19, loss = 0.76075556\n",
      "Iteration 20, loss = 0.75006210\n",
      "Iteration 21, loss = 0.74024009\n",
      "Iteration 22, loss = 0.73022711\n",
      "Iteration 23, loss = 0.71925579\n",
      "Iteration 24, loss = 0.70719121\n",
      "Iteration 25, loss = 0.69412283\n",
      "Iteration 26, loss = 0.68154850\n",
      "Iteration 27, loss = 0.67087803\n",
      "Iteration 28, loss = 0.66229102\n",
      "Iteration 29, loss = 0.65582466\n",
      "Iteration 30, loss = 0.65114783\n",
      "Iteration 31, loss = 0.64736453\n",
      "Iteration 32, loss = 0.64380821\n",
      "Iteration 33, loss = 0.63950746\n",
      "Iteration 34, loss = 0.63399944\n",
      "Iteration 35, loss = 0.62716846\n",
      "Iteration 36, loss = 0.61928983\n",
      "Iteration 37, loss = 0.61087528\n",
      "Iteration 38, loss = 0.60258302\n",
      "Iteration 39, loss = 0.59480005\n",
      "Iteration 40, loss = 0.58768585\n",
      "Iteration 41, loss = 0.58137377\n",
      "Iteration 42, loss = 0.57562667\n",
      "Iteration 43, loss = 0.57031621\n",
      "Iteration 44, loss = 0.56514229\n",
      "Iteration 45, loss = 0.56006160\n",
      "Iteration 46, loss = 0.55505971\n",
      "Iteration 47, loss = 0.55017179\n",
      "Iteration 48, loss = 0.54539083\n",
      "Iteration 49, loss = 0.54066714\n",
      "Iteration 50, loss = 0.53598672\n",
      "Iteration 51, loss = 0.53128045\n",
      "Iteration 52, loss = 0.52650908\n",
      "Iteration 53, loss = 0.52165153\n",
      "Iteration 54, loss = 0.51638317\n",
      "Iteration 55, loss = 0.51085577\n",
      "Iteration 56, loss = 0.50535664\n",
      "Iteration 57, loss = 0.50176847\n",
      "Iteration 58, loss = 0.49697549\n",
      "Iteration 59, loss = 0.49239452\n",
      "Iteration 60, loss = 0.48846490\n",
      "Iteration 61, loss = 0.48447833\n",
      "Iteration 62, loss = 0.48021479\n",
      "Iteration 63, loss = 0.47569184\n",
      "Iteration 64, loss = 0.47106279\n",
      "Iteration 65, loss = 0.46649660\n",
      "Iteration 66, loss = 0.46229932\n",
      "Iteration 67, loss = 0.45827138\n",
      "Iteration 68, loss = 0.45375204\n",
      "Iteration 69, loss = 0.44932509\n",
      "Iteration 70, loss = 0.44517886\n",
      "Iteration 71, loss = 0.44103851\n",
      "Iteration 72, loss = 0.43683041\n",
      "Iteration 73, loss = 0.43256442\n",
      "Iteration 74, loss = 0.42833275\n",
      "Iteration 75, loss = 0.42430062\n",
      "Iteration 76, loss = 0.42033474\n",
      "Iteration 77, loss = 0.41631536\n",
      "Iteration 78, loss = 0.41229639\n",
      "Iteration 79, loss = 0.40836773\n",
      "Iteration 80, loss = 0.40449930\n",
      "Iteration 81, loss = 0.40067077\n",
      "Iteration 82, loss = 0.39683694\n",
      "Iteration 83, loss = 0.39306256\n",
      "Iteration 84, loss = 0.38935175\n",
      "Iteration 85, loss = 0.38568473\n",
      "Iteration 86, loss = 0.38205568\n",
      "Iteration 87, loss = 0.37843356\n",
      "Iteration 88, loss = 0.37484921\n",
      "Iteration 89, loss = 0.37131146\n",
      "Iteration 90, loss = 0.36781558\n",
      "Iteration 91, loss = 0.36434701\n",
      "Iteration 92, loss = 0.36091148\n",
      "Iteration 93, loss = 0.35748809\n",
      "Iteration 94, loss = 0.35408317\n",
      "Iteration 95, loss = 0.35071770\n",
      "Iteration 96, loss = 0.34738028\n",
      "Iteration 97, loss = 0.34406367\n",
      "Iteration 98, loss = 0.34076006\n",
      "Iteration 99, loss = 0.33747781\n",
      "Iteration 100, loss = 0.33421610\n",
      "Iteration 101, loss = 0.33097041\n",
      "Iteration 102, loss = 0.32774124\n",
      "Iteration 103, loss = 0.32452887\n",
      "Iteration 104, loss = 0.32133487\n",
      "Iteration 105, loss = 0.31815528\n",
      "Iteration 106, loss = 0.31498837\n",
      "Iteration 107, loss = 0.31183397\n",
      "Iteration 108, loss = 0.30869535\n",
      "Iteration 109, loss = 0.30557193\n",
      "Iteration 110, loss = 0.30246204\n",
      "Iteration 111, loss = 0.29936507\n",
      "Iteration 112, loss = 0.29628031\n",
      "Iteration 113, loss = 0.29320751\n",
      "Iteration 114, loss = 0.29014826\n",
      "Iteration 115, loss = 0.28710309\n",
      "Iteration 116, loss = 0.28407174\n",
      "Iteration 117, loss = 0.28105370\n",
      "Iteration 118, loss = 0.27804875\n",
      "Iteration 119, loss = 0.27505690\n",
      "Iteration 120, loss = 0.27207937\n",
      "Iteration 121, loss = 0.26912021\n",
      "Iteration 122, loss = 0.26617598\n",
      "Iteration 123, loss = 0.26324698\n",
      "Iteration 124, loss = 0.26033331\n",
      "Iteration 125, loss = 0.25743522\n",
      "Iteration 126, loss = 0.25455320\n",
      "Iteration 127, loss = 0.25168793\n",
      "Iteration 128, loss = 0.24883998\n",
      "Iteration 129, loss = 0.24601057\n",
      "Iteration 130, loss = 0.24319959\n",
      "Iteration 131, loss = 0.24040717\n",
      "Iteration 132, loss = 0.23763389\n",
      "Iteration 133, loss = 0.23488043\n",
      "Iteration 134, loss = 0.23214739\n",
      "Iteration 135, loss = 0.22943525\n",
      "Iteration 136, loss = 0.22674447\n",
      "Iteration 137, loss = 0.22407556\n",
      "Iteration 138, loss = 0.22142913\n",
      "Iteration 139, loss = 0.21880575\n",
      "Iteration 140, loss = 0.21620594\n",
      "Iteration 141, loss = 0.21363013\n",
      "Iteration 142, loss = 0.21107877\n",
      "Iteration 143, loss = 0.20855233\n",
      "Iteration 144, loss = 0.20605132\n",
      "Iteration 145, loss = 0.20357620\n",
      "Iteration 146, loss = 0.20112737\n",
      "Iteration 147, loss = 0.19870518\n",
      "Iteration 148, loss = 0.19630999\n",
      "Iteration 149, loss = 0.19394217\n",
      "Iteration 150, loss = 0.19160205\n",
      "Iteration 151, loss = 0.18928991\n",
      "Iteration 152, loss = 0.18700600\n",
      "Iteration 153, loss = 0.18475054\n",
      "Iteration 154, loss = 0.18252375\n",
      "Iteration 155, loss = 0.18032583\n",
      "Iteration 156, loss = 0.17815693\n",
      "Iteration 157, loss = 0.17601718\n",
      "Iteration 158, loss = 0.17390666\n",
      "Iteration 159, loss = 0.17182545\n",
      "Iteration 160, loss = 0.16977360\n",
      "Iteration 161, loss = 0.16775114\n",
      "Iteration 162, loss = 0.16575806\n",
      "Iteration 163, loss = 0.16379433\n",
      "Iteration 164, loss = 0.16185988\n",
      "Iteration 165, loss = 0.15995465\n",
      "Iteration 166, loss = 0.15807854\n",
      "Iteration 167, loss = 0.15623143\n",
      "Iteration 168, loss = 0.15441318\n",
      "Iteration 169, loss = 0.15262362\n",
      "Iteration 170, loss = 0.15086257\n",
      "Iteration 171, loss = 0.14912984\n",
      "Iteration 172, loss = 0.14742521\n",
      "Iteration 173, loss = 0.14574845\n",
      "Iteration 174, loss = 0.14409930\n",
      "Iteration 175, loss = 0.14247752\n",
      "Iteration 176, loss = 0.14088283\n",
      "Iteration 177, loss = 0.13931494\n",
      "Iteration 178, loss = 0.13777355\n",
      "Iteration 179, loss = 0.13625857\n",
      "Iteration 180, loss = 0.13476906\n",
      "Iteration 181, loss = 0.13330533\n",
      "Iteration 182, loss = 0.13186682\n",
      "Iteration 183, loss = 0.13045320\n",
      "Iteration 184, loss = 0.12906411\n",
      "Iteration 185, loss = 0.12769920\n",
      "Iteration 186, loss = 0.12635812\n",
      "Iteration 187, loss = 0.12504051\n",
      "Iteration 188, loss = 0.12374602\n",
      "Iteration 189, loss = 0.12247430\n",
      "Iteration 190, loss = 0.12122497\n",
      "Iteration 191, loss = 0.11999769\n",
      "Iteration 192, loss = 0.11879209\n",
      "Iteration 193, loss = 0.11760780\n",
      "Iteration 194, loss = 0.11644445\n",
      "Iteration 195, loss = 0.11530169\n",
      "Iteration 196, loss = 0.11417915\n",
      "Iteration 197, loss = 0.11307646\n",
      "Iteration 198, loss = 0.11199326\n",
      "Iteration 199, loss = 0.11092921\n",
      "Iteration 200, loss = 0.10988394\n",
      "Iteration 201, loss = 0.10885710\n",
      "Iteration 202, loss = 0.10784835\n",
      "Iteration 203, loss = 0.10685735\n",
      "Iteration 204, loss = 0.10588376\n",
      "Iteration 205, loss = 0.10492725\n",
      "Iteration 206, loss = 0.10398748\n",
      "Iteration 207, loss = 0.10306412\n",
      "Iteration 208, loss = 0.10215686\n",
      "Iteration 209, loss = 0.10126537\n",
      "Iteration 210, loss = 0.10038935\n",
      "Iteration 211, loss = 0.09952848\n",
      "Iteration 212, loss = 0.09868246\n",
      "Iteration 213, loss = 0.09785099\n",
      "Iteration 214, loss = 0.09703378\n",
      "Iteration 215, loss = 0.09623054\n",
      "Iteration 216, loss = 0.09544098\n",
      "Iteration 217, loss = 0.09466482\n",
      "Iteration 218, loss = 0.09390180\n",
      "Iteration 219, loss = 0.09315164\n",
      "Iteration 220, loss = 0.09241408\n",
      "Iteration 221, loss = 0.09168886\n",
      "Iteration 222, loss = 0.09097572\n",
      "Iteration 223, loss = 0.09027443\n",
      "Iteration 224, loss = 0.08958472\n",
      "Iteration 225, loss = 0.08890637\n",
      "Iteration 226, loss = 0.08823914\n",
      "Iteration 227, loss = 0.08758281\n",
      "Iteration 228, loss = 0.08693713\n",
      "Iteration 229, loss = 0.08630190\n",
      "Iteration 230, loss = 0.08567690\n",
      "Iteration 231, loss = 0.08506192\n",
      "Iteration 232, loss = 0.08445675\n",
      "Iteration 233, loss = 0.08386119\n",
      "Iteration 234, loss = 0.08327504\n",
      "Iteration 235, loss = 0.08269811\n",
      "Iteration 236, loss = 0.08213020\n",
      "Iteration 237, loss = 0.08157114\n",
      "Iteration 238, loss = 0.08102074\n",
      "Iteration 239, loss = 0.08047882\n",
      "Iteration 240, loss = 0.07994521\n",
      "Iteration 241, loss = 0.07941975\n",
      "Iteration 242, loss = 0.07890226\n",
      "Iteration 243, loss = 0.07839258\n",
      "Iteration 244, loss = 0.07789056\n",
      "Iteration 245, loss = 0.07739603\n",
      "Iteration 246, loss = 0.07690886\n",
      "Iteration 247, loss = 0.07642888\n",
      "Iteration 248, loss = 0.07595595\n",
      "Iteration 249, loss = 0.07548994\n",
      "Iteration 250, loss = 0.07503071\n",
      "Iteration 251, loss = 0.07457811\n",
      "Iteration 252, loss = 0.07413202\n",
      "Iteration 253, loss = 0.07369231\n",
      "Iteration 254, loss = 0.07325885\n",
      "Iteration 255, loss = 0.07283152\n",
      "Iteration 256, loss = 0.07241019\n",
      "Iteration 257, loss = 0.07199475\n",
      "Iteration 258, loss = 0.07158509\n",
      "Iteration 259, loss = 0.07118109\n",
      "Iteration 260, loss = 0.07078264\n",
      "Iteration 261, loss = 0.07038963\n",
      "Iteration 262, loss = 0.07000196\n",
      "Iteration 263, loss = 0.06961952\n",
      "Iteration 264, loss = 0.06924222\n",
      "Iteration 265, loss = 0.06886995\n",
      "Iteration 266, loss = 0.06850263\n",
      "Iteration 267, loss = 0.06814015\n",
      "Iteration 268, loss = 0.06778242\n",
      "Iteration 269, loss = 0.06742936\n",
      "Iteration 270, loss = 0.06708087\n",
      "Iteration 271, loss = 0.06673687\n",
      "Iteration 272, loss = 0.06639728\n",
      "Iteration 273, loss = 0.06606201\n",
      "Iteration 274, loss = 0.06573099\n",
      "Iteration 275, loss = 0.06540413\n",
      "Iteration 276, loss = 0.06508136\n",
      "Iteration 277, loss = 0.06476260\n",
      "Iteration 278, loss = 0.06444778\n",
      "Iteration 279, loss = 0.06413683\n",
      "Iteration 280, loss = 0.06382968\n",
      "Iteration 281, loss = 0.06352626\n",
      "Iteration 282, loss = 0.06322650\n",
      "Iteration 283, loss = 0.06293033\n",
      "Iteration 284, loss = 0.06263769\n",
      "Iteration 285, loss = 0.06234853\n",
      "Iteration 286, loss = 0.06206277\n",
      "Iteration 287, loss = 0.06178035\n",
      "Iteration 288, loss = 0.06150123\n",
      "Iteration 289, loss = 0.06122533\n",
      "Iteration 290, loss = 0.06095261\n",
      "Iteration 291, loss = 0.06068300\n",
      "Iteration 292, loss = 0.06041646\n",
      "Iteration 293, loss = 0.06015293\n",
      "Iteration 294, loss = 0.05989236\n",
      "Iteration 295, loss = 0.05963470\n",
      "Iteration 296, loss = 0.05937989\n",
      "Iteration 297, loss = 0.05912790\n",
      "Iteration 298, loss = 0.05887867\n",
      "Iteration 299, loss = 0.05863215\n",
      "Iteration 300, loss = 0.05838830\n",
      "Iteration 301, loss = 0.05814708\n",
      "Iteration 302, loss = 0.05790844\n",
      "Iteration 303, loss = 0.05767233\n",
      "Iteration 304, loss = 0.05743873\n",
      "Iteration 305, loss = 0.05720758\n",
      "Iteration 306, loss = 0.05697884\n",
      "Iteration 307, loss = 0.05675248\n",
      "Iteration 308, loss = 0.05652845\n",
      "Iteration 309, loss = 0.05630673\n",
      "Iteration 310, loss = 0.05608726\n",
      "Iteration 311, loss = 0.05587002\n",
      "Iteration 312, loss = 0.05565497\n",
      "Iteration 313, loss = 0.05544212\n",
      "Iteration 314, loss = 0.05523138\n",
      "Iteration 315, loss = 0.05502273\n",
      "Iteration 316, loss = 0.05481614\n",
      "Iteration 317, loss = 0.05461157\n",
      "Iteration 318, loss = 0.05440898\n",
      "Iteration 319, loss = 0.05420836\n",
      "Iteration 320, loss = 0.05400967\n",
      "Iteration 321, loss = 0.05381288\n",
      "Iteration 322, loss = 0.05361795\n",
      "Iteration 323, loss = 0.05342487\n",
      "Iteration 324, loss = 0.05323360\n",
      "Iteration 325, loss = 0.05304411\n",
      "Iteration 326, loss = 0.05285638\n",
      "Iteration 327, loss = 0.05267038\n",
      "Iteration 328, loss = 0.05248608\n",
      "Iteration 329, loss = 0.05230346\n",
      "Iteration 330, loss = 0.05212250\n",
      "Iteration 331, loss = 0.05194316\n",
      "Iteration 332, loss = 0.05176543\n",
      "Iteration 333, loss = 0.05158927\n",
      "Iteration 334, loss = 0.05141468\n",
      "Iteration 335, loss = 0.05124162\n",
      "Iteration 336, loss = 0.05107007\n",
      "Iteration 337, loss = 0.05090002\n",
      "Iteration 338, loss = 0.05073143\n",
      "Iteration 339, loss = 0.05056429\n",
      "Iteration 340, loss = 0.05039858\n",
      "Iteration 341, loss = 0.05023428\n",
      "Iteration 342, loss = 0.05007136\n",
      "Iteration 343, loss = 0.04990981\n",
      "Iteration 344, loss = 0.04974960\n",
      "Iteration 345, loss = 0.04959073\n",
      "Iteration 346, loss = 0.04943317\n",
      "Iteration 347, loss = 0.04927690\n",
      "Iteration 348, loss = 0.04912190\n",
      "Iteration 349, loss = 0.04896816\n",
      "Iteration 350, loss = 0.04881565\n",
      "Iteration 351, loss = 0.04866438\n",
      "Iteration 352, loss = 0.04851430\n",
      "Iteration 353, loss = 0.04836542\n",
      "Iteration 354, loss = 0.04821770\n",
      "Iteration 355, loss = 0.04807115\n",
      "Iteration 356, loss = 0.04792574\n",
      "Iteration 357, loss = 0.04778145\n",
      "Iteration 358, loss = 0.04763828\n",
      "Iteration 359, loss = 0.04749620\n",
      "Iteration 360, loss = 0.04735520\n",
      "Iteration 361, loss = 0.04721531\n",
      "Iteration 362, loss = 0.04707647\n",
      "Iteration 363, loss = 0.04693869\n",
      "Iteration 364, loss = 0.04680194\n",
      "Iteration 365, loss = 0.04666619\n",
      "Iteration 366, loss = 0.04653144\n",
      "Iteration 367, loss = 0.04639768\n",
      "Iteration 368, loss = 0.04626490\n",
      "Iteration 369, loss = 0.04613311\n",
      "Iteration 370, loss = 0.04600228\n",
      "Iteration 371, loss = 0.04587240\n",
      "Iteration 372, loss = 0.04574344\n",
      "Iteration 373, loss = 0.04561539\n",
      "Iteration 374, loss = 0.04548825\n",
      "Iteration 375, loss = 0.04536201\n",
      "Iteration 376, loss = 0.04523665\n",
      "Iteration 377, loss = 0.04511216\n",
      "Iteration 378, loss = 0.04498854\n",
      "Iteration 379, loss = 0.04486576\n",
      "Iteration 380, loss = 0.04474381\n",
      "Iteration 381, loss = 0.04462270\n",
      "Iteration 382, loss = 0.04450240\n",
      "Iteration 383, loss = 0.04438290\n",
      "Iteration 384, loss = 0.04426421\n",
      "Iteration 385, loss = 0.04414630\n",
      "Iteration 386, loss = 0.04402917\n",
      "Iteration 387, loss = 0.04391281\n",
      "Iteration 388, loss = 0.04379721\n",
      "Iteration 389, loss = 0.04368236\n",
      "Iteration 390, loss = 0.04356825\n",
      "Iteration 391, loss = 0.04345488\n",
      "Iteration 392, loss = 0.04334223\n",
      "Iteration 393, loss = 0.04323029\n",
      "Iteration 394, loss = 0.04311907\n",
      "Iteration 395, loss = 0.04300854\n",
      "Iteration 396, loss = 0.04289871\n",
      "Iteration 397, loss = 0.04278956\n",
      "Iteration 398, loss = 0.04268108\n",
      "Iteration 399, loss = 0.04257328\n",
      "Iteration 400, loss = 0.04246613\n",
      "Iteration 401, loss = 0.04235964\n",
      "Iteration 402, loss = 0.04225379\n",
      "Iteration 403, loss = 0.04214859\n",
      "Iteration 404, loss = 0.04204401\n",
      "Iteration 405, loss = 0.04194006\n",
      "Iteration 406, loss = 0.04183672\n",
      "Iteration 407, loss = 0.04173400\n",
      "Iteration 408, loss = 0.04163188\n",
      "Iteration 409, loss = 0.04153036\n",
      "Iteration 410, loss = 0.04142944\n",
      "Iteration 411, loss = 0.04132909\n",
      "Iteration 412, loss = 0.04122933\n",
      "Iteration 413, loss = 0.04113013\n",
      "Iteration 414, loss = 0.04103151\n",
      "Iteration 415, loss = 0.04093344\n",
      "Iteration 416, loss = 0.04083593\n",
      "Iteration 417, loss = 0.04073896\n",
      "Iteration 418, loss = 0.04064254\n",
      "Iteration 419, loss = 0.04054666\n",
      "Iteration 420, loss = 0.04045131\n",
      "Iteration 421, loss = 0.04035648\n",
      "Iteration 422, loss = 0.04026218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52120330\n",
      "Iteration 2, loss = 1.36097367\n",
      "Iteration 3, loss = 1.22846705\n",
      "Iteration 4, loss = 1.11941632\n",
      "Iteration 5, loss = 1.03186408\n",
      "Iteration 6, loss = 0.95722137\n",
      "Iteration 7, loss = 0.89528612\n",
      "Iteration 8, loss = 0.84818819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.81825825\n",
      "Iteration 10, loss = 0.80394310\n",
      "Iteration 11, loss = 0.80187866\n",
      "Iteration 12, loss = 0.80724692\n",
      "Iteration 13, loss = 0.81514803\n",
      "Iteration 14, loss = 0.82133498\n",
      "Iteration 15, loss = 0.82314817\n",
      "Iteration 16, loss = 0.81969489\n",
      "Iteration 17, loss = 0.81140585\n",
      "Iteration 18, loss = 0.79949948\n",
      "Iteration 19, loss = 0.78550419\n",
      "Iteration 20, loss = 0.77088832\n",
      "Iteration 21, loss = 0.75688086\n",
      "Iteration 22, loss = 0.74400487\n",
      "Iteration 23, loss = 0.73241779\n",
      "Iteration 24, loss = 0.72235852\n",
      "Iteration 25, loss = 0.71331133\n",
      "Iteration 26, loss = 0.70506572\n",
      "Iteration 27, loss = 0.69785113\n",
      "Iteration 28, loss = 0.69140469\n",
      "Iteration 29, loss = 0.68545207\n",
      "Iteration 30, loss = 0.67970445\n",
      "Iteration 31, loss = 0.67378195\n",
      "Iteration 32, loss = 0.66755684\n",
      "Iteration 33, loss = 0.66100278\n",
      "Iteration 34, loss = 0.65400856\n",
      "Iteration 35, loss = 0.64647518\n",
      "Iteration 36, loss = 0.63831533\n",
      "Iteration 37, loss = 0.62957152\n",
      "Iteration 38, loss = 0.61978492\n",
      "Iteration 39, loss = 0.60994757\n",
      "Iteration 40, loss = 0.60114555\n",
      "Iteration 41, loss = 0.59370581\n",
      "Iteration 42, loss = 0.58753752\n",
      "Iteration 43, loss = 0.58256716\n",
      "Iteration 44, loss = 0.57823715\n",
      "Iteration 45, loss = 0.57406705\n",
      "Iteration 46, loss = 0.56972214\n",
      "Iteration 47, loss = 0.56505924\n",
      "Iteration 48, loss = 0.56009039\n",
      "Iteration 49, loss = 0.55489112\n",
      "Iteration 50, loss = 0.54959342\n",
      "Iteration 51, loss = 0.54433805\n",
      "Iteration 52, loss = 0.53925020\n",
      "Iteration 53, loss = 0.53434466\n",
      "Iteration 54, loss = 0.52916803\n",
      "Iteration 55, loss = 0.52387757\n",
      "Iteration 56, loss = 0.51851882\n",
      "Iteration 57, loss = 0.51507946\n",
      "Iteration 58, loss = 0.51067509\n",
      "Iteration 59, loss = 0.50569205\n",
      "Iteration 60, loss = 0.50177306\n",
      "Iteration 61, loss = 0.49799604\n",
      "Iteration 62, loss = 0.49393541\n",
      "Iteration 63, loss = 0.48954180\n",
      "Iteration 64, loss = 0.48490805\n",
      "Iteration 65, loss = 0.48029631\n",
      "Iteration 66, loss = 0.47614874\n",
      "Iteration 67, loss = 0.47215095\n",
      "Iteration 68, loss = 0.46769785\n",
      "Iteration 69, loss = 0.46332761\n",
      "Iteration 70, loss = 0.45930663\n",
      "Iteration 71, loss = 0.45543288\n",
      "Iteration 72, loss = 0.45145868\n",
      "Iteration 73, loss = 0.44739526\n",
      "Iteration 74, loss = 0.44345807\n",
      "Iteration 75, loss = 0.43963273\n",
      "Iteration 76, loss = 0.43587998\n",
      "Iteration 77, loss = 0.43209282\n",
      "Iteration 78, loss = 0.42832126\n",
      "Iteration 79, loss = 0.42464329\n",
      "Iteration 80, loss = 0.42103488\n",
      "Iteration 81, loss = 0.41749881\n",
      "Iteration 82, loss = 0.41393806\n",
      "Iteration 83, loss = 0.41038846\n",
      "Iteration 84, loss = 0.40692474\n",
      "Iteration 85, loss = 0.40353871\n",
      "Iteration 86, loss = 0.40019314\n",
      "Iteration 87, loss = 0.39686968\n",
      "Iteration 88, loss = 0.39355938\n",
      "Iteration 89, loss = 0.39029668\n",
      "Iteration 90, loss = 0.38706926\n",
      "Iteration 91, loss = 0.38389152\n",
      "Iteration 92, loss = 0.38073396\n",
      "Iteration 93, loss = 0.37759069\n",
      "Iteration 94, loss = 0.37447218\n",
      "Iteration 95, loss = 0.37139424\n",
      "Iteration 96, loss = 0.36833248\n",
      "Iteration 97, loss = 0.36528135\n",
      "Iteration 98, loss = 0.36225282\n",
      "Iteration 99, loss = 0.35924738\n",
      "Iteration 100, loss = 0.35625583\n",
      "Iteration 101, loss = 0.35327756\n",
      "Iteration 102, loss = 0.35031201\n",
      "Iteration 103, loss = 0.34735870\n",
      "Iteration 104, loss = 0.34441902\n",
      "Iteration 105, loss = 0.34149127\n",
      "Iteration 106, loss = 0.33857471\n",
      "Iteration 107, loss = 0.33566836\n",
      "Iteration 108, loss = 0.33277095\n",
      "Iteration 109, loss = 0.32988139\n",
      "Iteration 110, loss = 0.32700087\n",
      "Iteration 111, loss = 0.32412982\n",
      "Iteration 112, loss = 0.32126861\n",
      "Iteration 113, loss = 0.31841561\n",
      "Iteration 114, loss = 0.31556880\n",
      "Iteration 115, loss = 0.31272815\n",
      "Iteration 116, loss = 0.30989519\n",
      "Iteration 117, loss = 0.30707130\n",
      "Iteration 118, loss = 0.30425451\n",
      "Iteration 119, loss = 0.30144674\n",
      "Iteration 120, loss = 0.29864636\n",
      "Iteration 121, loss = 0.29585281\n",
      "Iteration 122, loss = 0.29306629\n",
      "Iteration 123, loss = 0.29028731\n",
      "Iteration 124, loss = 0.28751757\n",
      "Iteration 125, loss = 0.28475698\n",
      "Iteration 126, loss = 0.28200661\n",
      "Iteration 127, loss = 0.27926379\n",
      "Iteration 128, loss = 0.27653161\n",
      "Iteration 129, loss = 0.27380963\n",
      "Iteration 130, loss = 0.27109847\n",
      "Iteration 131, loss = 0.26839870\n",
      "Iteration 132, loss = 0.26571076\n",
      "Iteration 133, loss = 0.26303512\n",
      "Iteration 134, loss = 0.26037235\n",
      "Iteration 135, loss = 0.25772313\n",
      "Iteration 136, loss = 0.25508815\n",
      "Iteration 137, loss = 0.25246804\n",
      "Iteration 138, loss = 0.24986337\n",
      "Iteration 139, loss = 0.24727475\n",
      "Iteration 140, loss = 0.24470341\n",
      "Iteration 141, loss = 0.24214976\n",
      "Iteration 142, loss = 0.23961415\n",
      "Iteration 143, loss = 0.23709715\n",
      "Iteration 144, loss = 0.23459934\n",
      "Iteration 145, loss = 0.23212130\n",
      "Iteration 146, loss = 0.22966366\n",
      "Iteration 147, loss = 0.22722700\n",
      "Iteration 148, loss = 0.22481186\n",
      "Iteration 149, loss = 0.22241875\n",
      "Iteration 150, loss = 0.22004819\n",
      "Iteration 151, loss = 0.21770068\n",
      "Iteration 152, loss = 0.21537670\n",
      "Iteration 153, loss = 0.21307669\n",
      "Iteration 154, loss = 0.21080105\n",
      "Iteration 155, loss = 0.20855020\n",
      "Iteration 156, loss = 0.20632449\n",
      "Iteration 157, loss = 0.20412430\n",
      "Iteration 158, loss = 0.20194994\n",
      "Iteration 159, loss = 0.19980170\n",
      "Iteration 160, loss = 0.19767983\n",
      "Iteration 161, loss = 0.19558457\n",
      "Iteration 162, loss = 0.19351613\n",
      "Iteration 163, loss = 0.19147469\n",
      "Iteration 164, loss = 0.18946045\n",
      "Iteration 165, loss = 0.18747353\n",
      "Iteration 166, loss = 0.18551415\n",
      "Iteration 167, loss = 0.18358230\n",
      "Iteration 168, loss = 0.18167794\n",
      "Iteration 169, loss = 0.17980105\n",
      "Iteration 170, loss = 0.17795158\n",
      "Iteration 171, loss = 0.17612973\n",
      "Iteration 172, loss = 0.17433555\n",
      "Iteration 173, loss = 0.17256861\n",
      "Iteration 174, loss = 0.17082893\n",
      "Iteration 175, loss = 0.16911675\n",
      "Iteration 176, loss = 0.16743145\n",
      "Iteration 177, loss = 0.16577286\n",
      "Iteration 178, loss = 0.16414123\n",
      "Iteration 179, loss = 0.16253609\n",
      "Iteration 180, loss = 0.16095725\n",
      "Iteration 181, loss = 0.15940450\n",
      "Iteration 182, loss = 0.15787760\n",
      "Iteration 183, loss = 0.15637629\n",
      "Iteration 184, loss = 0.15490031\n",
      "Iteration 185, loss = 0.15344946\n",
      "Iteration 186, loss = 0.15202338\n",
      "Iteration 187, loss = 0.15062187\n",
      "Iteration 188, loss = 0.14924450\n",
      "Iteration 189, loss = 0.14789093\n",
      "Iteration 190, loss = 0.14656109\n",
      "Iteration 191, loss = 0.14525435\n",
      "Iteration 192, loss = 0.14397071\n",
      "Iteration 193, loss = 0.14270966\n",
      "Iteration 194, loss = 0.14147067\n",
      "Iteration 195, loss = 0.14025368\n",
      "Iteration 196, loss = 0.13905838\n",
      "Iteration 197, loss = 0.13788417\n",
      "Iteration 198, loss = 0.13673077\n",
      "Iteration 199, loss = 0.13559847\n",
      "Iteration 200, loss = 0.13448599\n",
      "Iteration 201, loss = 0.13339296\n",
      "Iteration 202, loss = 0.13231996\n",
      "Iteration 203, loss = 0.13126609\n",
      "Iteration 204, loss = 0.13023097\n",
      "Iteration 205, loss = 0.12921428\n",
      "Iteration 206, loss = 0.12821568\n",
      "Iteration 207, loss = 0.12723487\n",
      "Iteration 208, loss = 0.12627153\n",
      "Iteration 209, loss = 0.12532533\n",
      "Iteration 210, loss = 0.12439597\n",
      "Iteration 211, loss = 0.12348310\n",
      "Iteration 212, loss = 0.12258643\n",
      "Iteration 213, loss = 0.12170561\n",
      "Iteration 214, loss = 0.12084034\n",
      "Iteration 215, loss = 0.11999029\n",
      "Iteration 216, loss = 0.11915516\n",
      "Iteration 217, loss = 0.11833465\n",
      "Iteration 218, loss = 0.11752846\n",
      "Iteration 219, loss = 0.11673630\n",
      "Iteration 220, loss = 0.11595788\n",
      "Iteration 221, loss = 0.11519293\n",
      "Iteration 222, loss = 0.11444118\n",
      "Iteration 223, loss = 0.11370236\n",
      "Iteration 224, loss = 0.11297621\n",
      "Iteration 225, loss = 0.11226246\n",
      "Iteration 226, loss = 0.11156087\n",
      "Iteration 227, loss = 0.11087119\n",
      "Iteration 228, loss = 0.11019316\n",
      "Iteration 229, loss = 0.10952686\n",
      "Iteration 230, loss = 0.10887180\n",
      "Iteration 231, loss = 0.10822745\n",
      "Iteration 232, loss = 0.10759367\n",
      "Iteration 233, loss = 0.10697050\n",
      "Iteration 234, loss = 0.10635797\n",
      "Iteration 235, loss = 0.10575548\n",
      "Iteration 236, loss = 0.10516279\n",
      "Iteration 237, loss = 0.10457967\n",
      "Iteration 238, loss = 0.10400637\n",
      "Iteration 239, loss = 0.10344231\n",
      "Iteration 240, loss = 0.10288732\n",
      "Iteration 241, loss = 0.10234122\n",
      "Iteration 242, loss = 0.10180385\n",
      "Iteration 243, loss = 0.10127503\n",
      "Iteration 244, loss = 0.10075461\n",
      "Iteration 245, loss = 0.10024241\n",
      "Iteration 246, loss = 0.09973826\n",
      "Iteration 247, loss = 0.09924200\n",
      "Iteration 248, loss = 0.09875346\n",
      "Iteration 249, loss = 0.09827247\n",
      "Iteration 250, loss = 0.09779888\n",
      "Iteration 251, loss = 0.09733253\n",
      "Iteration 252, loss = 0.09687327\n",
      "Iteration 253, loss = 0.09642097\n",
      "Iteration 254, loss = 0.09597547\n",
      "Iteration 255, loss = 0.09553665\n",
      "Iteration 256, loss = 0.09510437\n",
      "Iteration 257, loss = 0.09467866\n",
      "Iteration 258, loss = 0.09425939\n",
      "Iteration 259, loss = 0.09384613\n",
      "Iteration 260, loss = 0.09343882\n",
      "Iteration 261, loss = 0.09303756\n",
      "Iteration 262, loss = 0.09264227\n",
      "Iteration 263, loss = 0.09225263\n",
      "Iteration 264, loss = 0.09186851\n",
      "Iteration 265, loss = 0.09148983\n",
      "Iteration 266, loss = 0.09111671\n",
      "Iteration 267, loss = 0.09074885\n",
      "Iteration 268, loss = 0.09038615\n",
      "Iteration 269, loss = 0.09002853\n",
      "Iteration 270, loss = 0.08967605\n",
      "Iteration 271, loss = 0.08932833\n",
      "Iteration 272, loss = 0.08898539\n",
      "Iteration 273, loss = 0.08864724\n",
      "Iteration 274, loss = 0.08831371\n",
      "Iteration 275, loss = 0.08798472\n",
      "Iteration 276, loss = 0.08766017\n",
      "Iteration 277, loss = 0.08734000\n",
      "Iteration 278, loss = 0.08702412\n",
      "Iteration 279, loss = 0.08671246\n",
      "Iteration 280, loss = 0.08640493\n",
      "Iteration 281, loss = 0.08610147\n",
      "Iteration 282, loss = 0.08580199\n",
      "Iteration 283, loss = 0.08550643\n",
      "Iteration 284, loss = 0.08521471\n",
      "Iteration 285, loss = 0.08492677\n",
      "Iteration 286, loss = 0.08464253\n",
      "Iteration 287, loss = 0.08436193\n",
      "Iteration 288, loss = 0.08408491\n",
      "Iteration 289, loss = 0.08381140\n",
      "Iteration 290, loss = 0.08354133\n",
      "Iteration 291, loss = 0.08327466\n",
      "Iteration 292, loss = 0.08301132\n",
      "Iteration 293, loss = 0.08275125\n",
      "Iteration 294, loss = 0.08249440\n",
      "Iteration 295, loss = 0.08224071\n",
      "Iteration 296, loss = 0.08199013\n",
      "Iteration 297, loss = 0.08174261\n",
      "Iteration 298, loss = 0.08149809\n",
      "Iteration 299, loss = 0.08125652\n",
      "Iteration 300, loss = 0.08101786\n",
      "Iteration 301, loss = 0.08078205\n",
      "Iteration 302, loss = 0.08054904\n",
      "Iteration 303, loss = 0.08031880\n",
      "Iteration 304, loss = 0.08009127\n",
      "Iteration 305, loss = 0.07986642\n",
      "Iteration 306, loss = 0.07964419\n",
      "Iteration 307, loss = 0.07942454\n",
      "Iteration 308, loss = 0.07920744\n",
      "Iteration 309, loss = 0.07899284\n",
      "Iteration 310, loss = 0.07878080\n",
      "Iteration 311, loss = 0.07857117\n",
      "Iteration 312, loss = 0.07836390\n",
      "Iteration 313, loss = 0.07815897\n",
      "Iteration 314, loss = 0.07795635\n",
      "Iteration 315, loss = 0.07775601\n",
      "Iteration 316, loss = 0.07755792\n",
      "Iteration 317, loss = 0.07736205\n",
      "Iteration 318, loss = 0.07716837\n",
      "Iteration 319, loss = 0.07697684\n",
      "Iteration 320, loss = 0.07678743\n",
      "Iteration 321, loss = 0.07660010\n",
      "Iteration 322, loss = 0.07641482\n",
      "Iteration 323, loss = 0.07623154\n",
      "Iteration 324, loss = 0.07605025\n",
      "Iteration 325, loss = 0.07587090\n",
      "Iteration 326, loss = 0.07569347\n",
      "Iteration 327, loss = 0.07551792\n",
      "Iteration 328, loss = 0.07534422\n",
      "Iteration 329, loss = 0.07517236\n",
      "Iteration 330, loss = 0.07500230\n",
      "Iteration 331, loss = 0.07483402\n",
      "Iteration 332, loss = 0.07466749\n",
      "Iteration 333, loss = 0.07450270\n",
      "Iteration 334, loss = 0.07433960\n",
      "Iteration 335, loss = 0.07417819\n",
      "Iteration 336, loss = 0.07401843\n",
      "Iteration 337, loss = 0.07386031\n",
      "Iteration 338, loss = 0.07370380\n",
      "Iteration 339, loss = 0.07354887\n",
      "Iteration 340, loss = 0.07339551\n",
      "Iteration 341, loss = 0.07324369\n",
      "Iteration 342, loss = 0.07309339\n",
      "Iteration 343, loss = 0.07294459\n",
      "Iteration 344, loss = 0.07279726\n",
      "Iteration 345, loss = 0.07265140\n",
      "Iteration 346, loss = 0.07250696\n",
      "Iteration 347, loss = 0.07236395\n",
      "Iteration 348, loss = 0.07222233\n",
      "Iteration 349, loss = 0.07208209\n",
      "Iteration 350, loss = 0.07194321\n",
      "Iteration 351, loss = 0.07180568\n",
      "Iteration 352, loss = 0.07166947\n",
      "Iteration 353, loss = 0.07153456\n",
      "Iteration 354, loss = 0.07140094\n",
      "Iteration 355, loss = 0.07126860\n",
      "Iteration 356, loss = 0.07113751\n",
      "Iteration 357, loss = 0.07100766\n",
      "Iteration 358, loss = 0.07087903\n",
      "Iteration 359, loss = 0.07075161\n",
      "Iteration 360, loss = 0.07062538\n",
      "Iteration 361, loss = 0.07050032\n",
      "Iteration 362, loss = 0.07037643\n",
      "Iteration 363, loss = 0.07025368\n",
      "Iteration 364, loss = 0.07013206\n",
      "Iteration 365, loss = 0.07001156\n",
      "Iteration 366, loss = 0.06989216\n",
      "Iteration 367, loss = 0.06977385\n",
      "Iteration 368, loss = 0.06965662\n",
      "Iteration 369, loss = 0.06954044\n",
      "Iteration 370, loss = 0.06942532\n",
      "Iteration 371, loss = 0.06931123\n",
      "Iteration 372, loss = 0.06919816\n",
      "Iteration 373, loss = 0.06908610\n",
      "Iteration 374, loss = 0.06897504\n",
      "Iteration 375, loss = 0.06886496\n",
      "Iteration 376, loss = 0.06875589\n",
      "Iteration 377, loss = 0.06864778\n",
      "Iteration 378, loss = 0.06854062\n",
      "Iteration 379, loss = 0.06843439\n",
      "Iteration 380, loss = 0.06832911\n",
      "Iteration 381, loss = 0.06822475\n",
      "Iteration 382, loss = 0.06812128\n",
      "Iteration 383, loss = 0.06801871\n",
      "Iteration 384, loss = 0.06791702\n",
      "Iteration 385, loss = 0.06781622\n",
      "Iteration 386, loss = 0.06771628\n",
      "Iteration 387, loss = 0.06761720\n",
      "Iteration 388, loss = 0.06751896\n",
      "Iteration 389, loss = 0.06742157\n",
      "Iteration 390, loss = 0.06732500\n",
      "Iteration 391, loss = 0.06722925\n",
      "Iteration 392, loss = 0.06713431\n",
      "Iteration 393, loss = 0.06704016\n",
      "Iteration 394, loss = 0.06694680\n",
      "Iteration 395, loss = 0.06685421\n",
      "Iteration 396, loss = 0.06676238\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50048972\n",
      "Iteration 2, loss = 1.25000445\n",
      "Iteration 3, loss = 1.07160607\n",
      "Iteration 4, loss = 0.93867839\n",
      "Iteration 5, loss = 0.84646792\n",
      "Iteration 6, loss = 0.80541613\n",
      "Iteration 7, loss = 0.80146676\n",
      "Iteration 8, loss = 0.80850734\n",
      "Iteration 9, loss = 0.80955677\n",
      "Iteration 10, loss = 0.80006663\n",
      "Iteration 11, loss = 0.78093339\n",
      "Iteration 12, loss = 0.75620370\n",
      "Iteration 13, loss = 0.73076395\n",
      "Iteration 14, loss = 0.70980167\n",
      "Iteration 15, loss = 0.69313610\n",
      "Iteration 16, loss = 0.67896061\n",
      "Iteration 17, loss = 0.66597398\n",
      "Iteration 18, loss = 0.65342494\n",
      "Iteration 19, loss = 0.64074268\n",
      "Iteration 20, loss = 0.62758921\n",
      "Iteration 21, loss = 0.61422058\n",
      "Iteration 22, loss = 0.60128166\n",
      "Iteration 23, loss = 0.58927944\n",
      "Iteration 24, loss = 0.57840700\n",
      "Iteration 25, loss = 0.56853020\n",
      "Iteration 26, loss = 0.55955846\n",
      "Iteration 27, loss = 0.55112696\n",
      "Iteration 28, loss = 0.54293952\n",
      "Iteration 29, loss = 0.53495745\n",
      "Iteration 30, loss = 0.52720497\n",
      "Iteration 31, loss = 0.51971007\n",
      "Iteration 32, loss = 0.51249612\n",
      "Iteration 33, loss = 0.50557169\n",
      "Iteration 34, loss = 0.49891834\n",
      "Iteration 35, loss = 0.49254521\n",
      "Iteration 36, loss = 0.48640712\n",
      "Iteration 37, loss = 0.48046517\n",
      "Iteration 38, loss = 0.47469846\n",
      "Iteration 39, loss = 0.46909068\n",
      "Iteration 40, loss = 0.46364012\n",
      "Iteration 41, loss = 0.45833764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 42, loss = 0.45317436\n",
      "Iteration 43, loss = 0.44814186\n",
      "Iteration 44, loss = 0.44323404\n",
      "Iteration 45, loss = 0.43844445\n",
      "Iteration 46, loss = 0.43376775\n",
      "Iteration 47, loss = 0.42918478\n",
      "Iteration 48, loss = 0.42468631\n",
      "Iteration 49, loss = 0.42026458\n",
      "Iteration 50, loss = 0.41592335\n",
      "Iteration 51, loss = 0.41167139\n",
      "Iteration 52, loss = 0.40748521\n",
      "Iteration 53, loss = 0.40336072\n",
      "Iteration 54, loss = 0.39929753\n",
      "Iteration 55, loss = 0.39529264\n",
      "Iteration 56, loss = 0.39135023\n",
      "Iteration 57, loss = 0.38745868\n",
      "Iteration 58, loss = 0.38362155\n",
      "Iteration 59, loss = 0.37982849\n",
      "Iteration 60, loss = 0.37607760\n",
      "Iteration 61, loss = 0.37236851\n",
      "Iteration 62, loss = 0.36870304\n",
      "Iteration 63, loss = 0.36507758\n",
      "Iteration 64, loss = 0.36149628\n",
      "Iteration 65, loss = 0.35795454\n",
      "Iteration 66, loss = 0.35444764\n",
      "Iteration 67, loss = 0.35097470\n",
      "Iteration 68, loss = 0.34753493\n",
      "Iteration 69, loss = 0.34412768\n",
      "Iteration 70, loss = 0.34075348\n",
      "Iteration 71, loss = 0.33741218\n",
      "Iteration 72, loss = 0.33410192\n",
      "Iteration 73, loss = 0.33082234\n",
      "Iteration 74, loss = 0.32757509\n",
      "Iteration 75, loss = 0.32436165\n",
      "Iteration 76, loss = 0.32117846\n",
      "Iteration 77, loss = 0.31802582\n",
      "Iteration 78, loss = 0.31490425\n",
      "Iteration 79, loss = 0.31181261\n",
      "Iteration 80, loss = 0.30875088\n",
      "Iteration 81, loss = 0.30571906\n",
      "Iteration 82, loss = 0.30271720\n",
      "Iteration 83, loss = 0.29974534\n",
      "Iteration 84, loss = 0.29680357\n",
      "Iteration 85, loss = 0.29389199\n",
      "Iteration 86, loss = 0.29101069\n",
      "Iteration 87, loss = 0.28815981\n",
      "Iteration 88, loss = 0.28533945\n",
      "Iteration 89, loss = 0.28254974\n",
      "Iteration 90, loss = 0.27979080\n",
      "Iteration 91, loss = 0.27706276\n",
      "Iteration 92, loss = 0.27436572\n",
      "Iteration 93, loss = 0.27169979\n",
      "Iteration 94, loss = 0.26906509\n",
      "Iteration 95, loss = 0.26646168\n",
      "Iteration 96, loss = 0.26388966\n",
      "Iteration 97, loss = 0.26134908\n",
      "Iteration 98, loss = 0.25884001\n",
      "Iteration 99, loss = 0.25636561\n",
      "Iteration 100, loss = 0.25392370\n",
      "Iteration 101, loss = 0.25151352\n",
      "Iteration 102, loss = 0.24913497\n",
      "Iteration 103, loss = 0.24678799\n",
      "Iteration 104, loss = 0.24447257\n",
      "Iteration 105, loss = 0.24218861\n",
      "Iteration 106, loss = 0.23993646\n",
      "Iteration 107, loss = 0.23771561\n",
      "Iteration 108, loss = 0.23552589\n",
      "Iteration 109, loss = 0.23336715\n",
      "Iteration 110, loss = 0.23123921\n",
      "Iteration 111, loss = 0.22914192\n",
      "Iteration 112, loss = 0.22707510\n",
      "Iteration 113, loss = 0.22503854\n",
      "Iteration 114, loss = 0.22303203\n",
      "Iteration 115, loss = 0.22105537\n",
      "Iteration 116, loss = 0.21910831\n",
      "Iteration 117, loss = 0.21719061\n",
      "Iteration 118, loss = 0.21530203\n",
      "Iteration 119, loss = 0.21344231\n",
      "Iteration 120, loss = 0.21161117\n",
      "Iteration 121, loss = 0.20980833\n",
      "Iteration 122, loss = 0.20803350\n",
      "Iteration 123, loss = 0.20628638\n",
      "Iteration 124, loss = 0.20456667\n",
      "Iteration 125, loss = 0.20287404\n",
      "Iteration 126, loss = 0.20120818\n",
      "Iteration 127, loss = 0.19956877\n",
      "Iteration 128, loss = 0.19795546\n",
      "Iteration 129, loss = 0.19636791\n",
      "Iteration 130, loss = 0.19480580\n",
      "Iteration 131, loss = 0.19326877\n",
      "Iteration 132, loss = 0.19175648\n",
      "Iteration 133, loss = 0.19026857\n",
      "Iteration 134, loss = 0.18880471\n",
      "Iteration 135, loss = 0.18736452\n",
      "Iteration 136, loss = 0.18594767\n",
      "Iteration 137, loss = 0.18455380\n",
      "Iteration 138, loss = 0.18318255\n",
      "Iteration 139, loss = 0.18183357\n",
      "Iteration 140, loss = 0.18050651\n",
      "Iteration 141, loss = 0.17920102\n",
      "Iteration 142, loss = 0.17791675\n",
      "Iteration 143, loss = 0.17665335\n",
      "Iteration 144, loss = 0.17541058\n",
      "Iteration 145, loss = 0.17418799\n",
      "Iteration 146, loss = 0.17298524\n",
      "Iteration 147, loss = 0.17180199\n",
      "Iteration 148, loss = 0.17063789\n",
      "Iteration 149, loss = 0.16949263\n",
      "Iteration 150, loss = 0.16836588\n",
      "Iteration 151, loss = 0.16725730\n",
      "Iteration 152, loss = 0.16616658\n",
      "Iteration 153, loss = 0.16509340\n",
      "Iteration 154, loss = 0.16403745\n",
      "Iteration 155, loss = 0.16299841\n",
      "Iteration 156, loss = 0.16197598\n",
      "Iteration 157, loss = 0.16096986\n",
      "Iteration 158, loss = 0.15997974\n",
      "Iteration 159, loss = 0.15900534\n",
      "Iteration 160, loss = 0.15804637\n",
      "Iteration 161, loss = 0.15710253\n",
      "Iteration 162, loss = 0.15617355\n",
      "Iteration 163, loss = 0.15525916\n",
      "Iteration 164, loss = 0.15435908\n",
      "Iteration 165, loss = 0.15347304\n",
      "Iteration 166, loss = 0.15260079\n",
      "Iteration 167, loss = 0.15174206\n",
      "Iteration 168, loss = 0.15089660\n",
      "Iteration 169, loss = 0.15006416\n",
      "Iteration 170, loss = 0.14924449\n",
      "Iteration 171, loss = 0.14843737\n",
      "Iteration 172, loss = 0.14764254\n",
      "Iteration 173, loss = 0.14685978\n",
      "Iteration 174, loss = 0.14608886\n",
      "Iteration 175, loss = 0.14532956\n",
      "Iteration 176, loss = 0.14458166\n",
      "Iteration 177, loss = 0.14384494\n",
      "Iteration 178, loss = 0.14311920\n",
      "Iteration 179, loss = 0.14240422\n",
      "Iteration 180, loss = 0.14169981\n",
      "Iteration 181, loss = 0.14100576\n",
      "Iteration 182, loss = 0.14032189\n",
      "Iteration 183, loss = 0.13964799\n",
      "Iteration 184, loss = 0.13898389\n",
      "Iteration 185, loss = 0.13832940\n",
      "Iteration 186, loss = 0.13768435\n",
      "Iteration 187, loss = 0.13704854\n",
      "Iteration 188, loss = 0.13642182\n",
      "Iteration 189, loss = 0.13580402\n",
      "Iteration 190, loss = 0.13519497\n",
      "Iteration 191, loss = 0.13459450\n",
      "Iteration 192, loss = 0.13400246\n",
      "Iteration 193, loss = 0.13341870\n",
      "Iteration 194, loss = 0.13284305\n",
      "Iteration 195, loss = 0.13227538\n",
      "Iteration 196, loss = 0.13171554\n",
      "Iteration 197, loss = 0.13116338\n",
      "Iteration 198, loss = 0.13061876\n",
      "Iteration 199, loss = 0.13008155\n",
      "Iteration 200, loss = 0.12955162\n",
      "Iteration 201, loss = 0.12902882\n",
      "Iteration 202, loss = 0.12851304\n",
      "Iteration 203, loss = 0.12800414\n",
      "Iteration 204, loss = 0.12750201\n",
      "Iteration 205, loss = 0.12700652\n",
      "Iteration 206, loss = 0.12651756\n",
      "Iteration 207, loss = 0.12603500\n",
      "Iteration 208, loss = 0.12555874\n",
      "Iteration 209, loss = 0.12508866\n",
      "Iteration 210, loss = 0.12462466\n",
      "Iteration 211, loss = 0.12416662\n",
      "Iteration 212, loss = 0.12371445\n",
      "Iteration 213, loss = 0.12326804\n",
      "Iteration 214, loss = 0.12282730\n",
      "Iteration 215, loss = 0.12239211\n",
      "Iteration 216, loss = 0.12196240\n",
      "Iteration 217, loss = 0.12153806\n",
      "Iteration 218, loss = 0.12111900\n",
      "Iteration 219, loss = 0.12070513\n",
      "Iteration 220, loss = 0.12029637\n",
      "Iteration 221, loss = 0.11989262\n",
      "Iteration 222, loss = 0.11949380\n",
      "Iteration 223, loss = 0.11909983\n",
      "Iteration 224, loss = 0.11871063\n",
      "Iteration 225, loss = 0.11832612\n",
      "Iteration 226, loss = 0.11794622\n",
      "Iteration 227, loss = 0.11757085\n",
      "Iteration 228, loss = 0.11719994\n",
      "Iteration 229, loss = 0.11683341\n",
      "Iteration 230, loss = 0.11647120\n",
      "Iteration 231, loss = 0.11611323\n",
      "Iteration 232, loss = 0.11575943\n",
      "Iteration 233, loss = 0.11540974\n",
      "Iteration 234, loss = 0.11506408\n",
      "Iteration 235, loss = 0.11472240\n",
      "Iteration 236, loss = 0.11438463\n",
      "Iteration 237, loss = 0.11405070\n",
      "Iteration 238, loss = 0.11372056\n",
      "Iteration 239, loss = 0.11339415\n",
      "Iteration 240, loss = 0.11307140\n",
      "Iteration 241, loss = 0.11275227\n",
      "Iteration 242, loss = 0.11243668\n",
      "Iteration 243, loss = 0.11212459\n",
      "Iteration 244, loss = 0.11181594\n",
      "Iteration 245, loss = 0.11151068\n",
      "Iteration 246, loss = 0.11120876\n",
      "Iteration 247, loss = 0.11091012\n",
      "Iteration 248, loss = 0.11061472\n",
      "Iteration 249, loss = 0.11032250\n",
      "Iteration 250, loss = 0.11003342\n",
      "Iteration 251, loss = 0.10974743\n",
      "Iteration 252, loss = 0.10946447\n",
      "Iteration 253, loss = 0.10918452\n",
      "Iteration 254, loss = 0.10890751\n",
      "Iteration 255, loss = 0.10863341\n",
      "Iteration 256, loss = 0.10836218\n",
      "Iteration 257, loss = 0.10809377\n",
      "Iteration 258, loss = 0.10782813\n",
      "Iteration 259, loss = 0.10756524\n",
      "Iteration 260, loss = 0.10730504\n",
      "Iteration 261, loss = 0.10704750\n",
      "Iteration 262, loss = 0.10679259\n",
      "Iteration 263, loss = 0.10654025\n",
      "Iteration 264, loss = 0.10629046\n",
      "Iteration 265, loss = 0.10604318\n",
      "Iteration 266, loss = 0.10579837\n",
      "Iteration 267, loss = 0.10555599\n",
      "Iteration 268, loss = 0.10531602\n",
      "Iteration 269, loss = 0.10507842\n",
      "Iteration 270, loss = 0.10484315\n",
      "Iteration 271, loss = 0.10461018\n",
      "Iteration 272, loss = 0.10437948\n",
      "Iteration 273, loss = 0.10415102\n",
      "Iteration 274, loss = 0.10392477\n",
      "Iteration 275, loss = 0.10370069\n",
      "Iteration 276, loss = 0.10347876\n",
      "Iteration 277, loss = 0.10325895\n",
      "Iteration 278, loss = 0.10304122\n",
      "Iteration 279, loss = 0.10282555\n",
      "Iteration 280, loss = 0.10261191\n",
      "Iteration 281, loss = 0.10240028\n",
      "Iteration 282, loss = 0.10219062\n",
      "Iteration 283, loss = 0.10198291\n",
      "Iteration 284, loss = 0.10177713\n",
      "Iteration 285, loss = 0.10157324\n",
      "Iteration 286, loss = 0.10137122\n",
      "Iteration 287, loss = 0.10117106\n",
      "Iteration 288, loss = 0.10097271\n",
      "Iteration 289, loss = 0.10077617\n",
      "Iteration 290, loss = 0.10058140\n",
      "Iteration 291, loss = 0.10038838\n",
      "Iteration 292, loss = 0.10019710\n",
      "Iteration 293, loss = 0.10000752\n",
      "Iteration 294, loss = 0.09981962\n",
      "Iteration 295, loss = 0.09963339\n",
      "Iteration 296, loss = 0.09944880\n",
      "Iteration 297, loss = 0.09926583\n",
      "Iteration 298, loss = 0.09908447\n",
      "Iteration 299, loss = 0.09890468\n",
      "Iteration 300, loss = 0.09872645\n",
      "Iteration 301, loss = 0.09854976\n",
      "Iteration 302, loss = 0.09837460\n",
      "Iteration 303, loss = 0.09820093\n",
      "Iteration 304, loss = 0.09802875\n",
      "Iteration 305, loss = 0.09785804\n",
      "Iteration 306, loss = 0.09768877\n",
      "Iteration 307, loss = 0.09752095\n",
      "Iteration 308, loss = 0.09735456\n",
      "Iteration 309, loss = 0.09718956\n",
      "Iteration 310, loss = 0.09702593\n",
      "Iteration 311, loss = 0.09686367\n",
      "Iteration 312, loss = 0.09670276\n",
      "Iteration 313, loss = 0.09654317\n",
      "Iteration 314, loss = 0.09638489\n",
      "Iteration 315, loss = 0.09622791\n",
      "Iteration 316, loss = 0.09607221\n",
      "Iteration 317, loss = 0.09591778\n",
      "Iteration 318, loss = 0.09576460\n",
      "Iteration 319, loss = 0.09561265\n",
      "Iteration 320, loss = 0.09546192\n",
      "Iteration 321, loss = 0.09531240\n",
      "Iteration 322, loss = 0.09516408\n",
      "Iteration 323, loss = 0.09501693\n",
      "Iteration 324, loss = 0.09487094\n",
      "Iteration 325, loss = 0.09472611\n",
      "Iteration 326, loss = 0.09458241\n",
      "Iteration 327, loss = 0.09443984\n",
      "Iteration 328, loss = 0.09429838\n",
      "Iteration 329, loss = 0.09415802\n",
      "Iteration 330, loss = 0.09401874\n",
      "Iteration 331, loss = 0.09388054\n",
      "Iteration 332, loss = 0.09374340\n",
      "Iteration 333, loss = 0.09360731\n",
      "Iteration 334, loss = 0.09347226\n",
      "Iteration 335, loss = 0.09333823\n",
      "Iteration 336, loss = 0.09320522\n",
      "Iteration 337, loss = 0.09307321\n",
      "Iteration 338, loss = 0.09294220\n",
      "Iteration 339, loss = 0.09281216\n",
      "Iteration 340, loss = 0.09268310\n",
      "Iteration 341, loss = 0.09255499\n",
      "Iteration 342, loss = 0.09242784\n",
      "Iteration 343, loss = 0.09230162\n",
      "Iteration 344, loss = 0.09217633\n",
      "Iteration 345, loss = 0.09205196\n",
      "Iteration 346, loss = 0.09192850\n",
      "Iteration 347, loss = 0.09180594\n",
      "Iteration 348, loss = 0.09168427\n",
      "Iteration 349, loss = 0.09156347\n",
      "Iteration 350, loss = 0.09144355\n",
      "Iteration 351, loss = 0.09132449\n",
      "Iteration 352, loss = 0.09120628\n",
      "Iteration 353, loss = 0.09108891\n",
      "Iteration 354, loss = 0.09097237\n",
      "Iteration 355, loss = 0.09085666\n",
      "Iteration 356, loss = 0.09074177\n",
      "Iteration 357, loss = 0.09062768\n",
      "Iteration 358, loss = 0.09051440\n",
      "Iteration 359, loss = 0.09040190\n",
      "Iteration 360, loss = 0.09029019\n",
      "Iteration 361, loss = 0.09017926\n",
      "Iteration 362, loss = 0.09006909\n",
      "Iteration 363, loss = 0.08995967\n",
      "Iteration 364, loss = 0.08985102\n",
      "Iteration 365, loss = 0.08974310\n",
      "Iteration 366, loss = 0.08963592\n",
      "Iteration 367, loss = 0.08952947\n",
      "Iteration 368, loss = 0.08942374\n",
      "Iteration 369, loss = 0.08931872\n",
      "Iteration 370, loss = 0.08921441\n",
      "Iteration 371, loss = 0.08911080\n",
      "Iteration 372, loss = 0.08900788\n",
      "Iteration 373, loss = 0.08890565\n",
      "Iteration 374, loss = 0.08880410\n",
      "Iteration 375, loss = 0.08870322\n",
      "Iteration 376, loss = 0.08860301\n",
      "Iteration 377, loss = 0.08850345\n",
      "Iteration 378, loss = 0.08840455\n",
      "Iteration 379, loss = 0.08830629\n",
      "Iteration 380, loss = 0.08820867\n",
      "Iteration 381, loss = 0.08811169\n",
      "Iteration 382, loss = 0.08801534\n",
      "Iteration 383, loss = 0.08791960\n",
      "Iteration 384, loss = 0.08782449\n",
      "Iteration 385, loss = 0.08772998\n",
      "Iteration 386, loss = 0.08763607\n",
      "Iteration 387, loss = 0.08754277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51234105\n",
      "Iteration 2, loss = 1.25524836\n",
      "Iteration 3, loss = 1.07633224\n",
      "Iteration 4, loss = 0.94282267\n",
      "Iteration 5, loss = 0.85211586\n",
      "Iteration 6, loss = 0.81548507\n",
      "Iteration 7, loss = 0.81806661\n",
      "Iteration 8, loss = 0.82081222\n",
      "Iteration 9, loss = 0.81786650\n",
      "Iteration 10, loss = 0.80741081\n",
      "Iteration 11, loss = 0.78890568\n",
      "Iteration 12, loss = 0.76508026\n",
      "Iteration 13, loss = 0.73946058\n",
      "Iteration 14, loss = 0.71891145\n",
      "Iteration 15, loss = 0.70285483\n",
      "Iteration 16, loss = 0.68908052\n",
      "Iteration 17, loss = 0.67624460\n",
      "Iteration 18, loss = 0.66377599\n",
      "Iteration 19, loss = 0.65114523\n",
      "Iteration 20, loss = 0.63787105\n",
      "Iteration 21, loss = 0.62418721\n",
      "Iteration 22, loss = 0.61080944\n",
      "Iteration 23, loss = 0.59837856\n",
      "Iteration 24, loss = 0.58750776\n",
      "Iteration 25, loss = 0.57768789\n",
      "Iteration 26, loss = 0.56867488\n",
      "Iteration 27, loss = 0.56008709\n",
      "Iteration 28, loss = 0.55171567\n",
      "Iteration 29, loss = 0.54350346\n",
      "Iteration 30, loss = 0.53550853\n",
      "Iteration 31, loss = 0.52777767\n",
      "Iteration 32, loss = 0.52035534\n",
      "Iteration 33, loss = 0.51326201\n",
      "Iteration 34, loss = 0.50645160\n",
      "Iteration 35, loss = 0.49989157\n",
      "Iteration 36, loss = 0.49354208\n",
      "Iteration 37, loss = 0.48737307\n",
      "Iteration 38, loss = 0.48137421\n",
      "Iteration 39, loss = 0.47552968\n",
      "Iteration 40, loss = 0.46984180\n",
      "Iteration 41, loss = 0.46430373\n",
      "Iteration 42, loss = 0.45891059\n",
      "Iteration 43, loss = 0.45364887\n",
      "Iteration 44, loss = 0.44850767\n",
      "Iteration 45, loss = 0.44347537\n",
      "Iteration 46, loss = 0.43854076\n",
      "Iteration 47, loss = 0.43369369\n",
      "Iteration 48, loss = 0.42896960\n",
      "Iteration 49, loss = 0.42435845\n",
      "Iteration 50, loss = 0.41983153\n",
      "Iteration 51, loss = 0.41537443\n",
      "Iteration 52, loss = 0.41098257\n",
      "Iteration 53, loss = 0.40666150\n",
      "Iteration 54, loss = 0.40240125\n",
      "Iteration 55, loss = 0.39819553\n",
      "Iteration 56, loss = 0.39404450\n",
      "Iteration 57, loss = 0.38993983\n",
      "Iteration 58, loss = 0.38587914\n",
      "Iteration 59, loss = 0.38186243\n",
      "Iteration 60, loss = 0.37788973\n",
      "Iteration 61, loss = 0.37395756\n",
      "Iteration 62, loss = 0.37007040\n",
      "Iteration 63, loss = 0.36622444\n",
      "Iteration 64, loss = 0.36241416\n",
      "Iteration 65, loss = 0.35863857\n",
      "Iteration 66, loss = 0.35489686\n",
      "Iteration 67, loss = 0.35118835\n",
      "Iteration 68, loss = 0.34751487\n",
      "Iteration 69, loss = 0.34387378\n",
      "Iteration 70, loss = 0.34026447\n",
      "Iteration 71, loss = 0.33668664\n",
      "Iteration 72, loss = 0.33314094\n",
      "Iteration 73, loss = 0.32963072\n",
      "Iteration 74, loss = 0.32615173\n",
      "Iteration 75, loss = 0.32270472\n",
      "Iteration 76, loss = 0.31928965\n",
      "Iteration 77, loss = 0.31590583\n",
      "Iteration 78, loss = 0.31255337\n",
      "Iteration 79, loss = 0.30923239\n",
      "Iteration 80, loss = 0.30594304\n",
      "Iteration 81, loss = 0.30268551\n",
      "Iteration 82, loss = 0.29945998\n",
      "Iteration 83, loss = 0.29626667\n",
      "Iteration 84, loss = 0.29310582\n",
      "Iteration 85, loss = 0.28997768\n",
      "Iteration 86, loss = 0.28688249\n",
      "Iteration 87, loss = 0.28382053\n",
      "Iteration 88, loss = 0.28079206\n",
      "Iteration 89, loss = 0.27779732\n",
      "Iteration 90, loss = 0.27483658\n",
      "Iteration 91, loss = 0.27191008\n",
      "Iteration 92, loss = 0.26901804\n",
      "Iteration 93, loss = 0.26616068\n",
      "Iteration 94, loss = 0.26333818\n",
      "Iteration 95, loss = 0.26055073\n",
      "Iteration 96, loss = 0.25779919\n",
      "Iteration 97, loss = 0.25508651\n",
      "Iteration 98, loss = 0.25240943\n",
      "Iteration 99, loss = 0.24976794\n",
      "Iteration 100, loss = 0.24716208\n",
      "Iteration 101, loss = 0.24459189\n",
      "Iteration 102, loss = 0.24205785\n",
      "Iteration 103, loss = 0.23955967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 104, loss = 0.23709708\n",
      "Iteration 105, loss = 0.23466998\n",
      "Iteration 106, loss = 0.23227828\n",
      "Iteration 107, loss = 0.22992185\n",
      "Iteration 108, loss = 0.22760056\n",
      "Iteration 109, loss = 0.22531424\n",
      "Iteration 110, loss = 0.22306271\n",
      "Iteration 111, loss = 0.22084576\n",
      "Iteration 112, loss = 0.21866318\n",
      "Iteration 113, loss = 0.21651473\n",
      "Iteration 114, loss = 0.21440016\n",
      "Iteration 115, loss = 0.21231921\n",
      "Iteration 116, loss = 0.21027159\n",
      "Iteration 117, loss = 0.20825700\n",
      "Iteration 118, loss = 0.20627513\n",
      "Iteration 119, loss = 0.20432566\n",
      "Iteration 120, loss = 0.20240825\n",
      "Iteration 121, loss = 0.20052254\n",
      "Iteration 122, loss = 0.19866818\n",
      "Iteration 123, loss = 0.19684480\n",
      "Iteration 124, loss = 0.19505200\n",
      "Iteration 125, loss = 0.19328941\n",
      "Iteration 126, loss = 0.19155663\n",
      "Iteration 127, loss = 0.18985325\n",
      "Iteration 128, loss = 0.18817888\n",
      "Iteration 129, loss = 0.18653309\n",
      "Iteration 130, loss = 0.18491547\n",
      "Iteration 131, loss = 0.18332562\n",
      "Iteration 132, loss = 0.18176309\n",
      "Iteration 133, loss = 0.18022748\n",
      "Iteration 134, loss = 0.17871836\n",
      "Iteration 135, loss = 0.17723530\n",
      "Iteration 136, loss = 0.17577789\n",
      "Iteration 137, loss = 0.17434569\n",
      "Iteration 138, loss = 0.17293828\n",
      "Iteration 139, loss = 0.17155525\n",
      "Iteration 140, loss = 0.17019617\n",
      "Iteration 141, loss = 0.16886063\n",
      "Iteration 142, loss = 0.16754822\n",
      "Iteration 143, loss = 0.16625851\n",
      "Iteration 144, loss = 0.16499111\n",
      "Iteration 145, loss = 0.16374560\n",
      "Iteration 146, loss = 0.16252159\n",
      "Iteration 147, loss = 0.16131869\n",
      "Iteration 148, loss = 0.16013649\n",
      "Iteration 149, loss = 0.15897462\n",
      "Iteration 150, loss = 0.15783269\n",
      "Iteration 151, loss = 0.15671031\n",
      "Iteration 152, loss = 0.15560713\n",
      "Iteration 153, loss = 0.15452278\n",
      "Iteration 154, loss = 0.15345688\n",
      "Iteration 155, loss = 0.15240909\n",
      "Iteration 156, loss = 0.15137905\n",
      "Iteration 157, loss = 0.15036642\n",
      "Iteration 158, loss = 0.14937086\n",
      "Iteration 159, loss = 0.14839203\n",
      "Iteration 160, loss = 0.14742961\n",
      "Iteration 161, loss = 0.14648326\n",
      "Iteration 162, loss = 0.14555269\n",
      "Iteration 163, loss = 0.14463756\n",
      "Iteration 164, loss = 0.14373758\n",
      "Iteration 165, loss = 0.14285245\n",
      "Iteration 166, loss = 0.14198188\n",
      "Iteration 167, loss = 0.14112556\n",
      "Iteration 168, loss = 0.14028323\n",
      "Iteration 169, loss = 0.13945468\n",
      "Iteration 170, loss = 0.13863961\n",
      "Iteration 171, loss = 0.13783771\n",
      "Iteration 172, loss = 0.13704871\n",
      "Iteration 173, loss = 0.13627235\n",
      "Iteration 174, loss = 0.13550840\n",
      "Iteration 175, loss = 0.13475659\n",
      "Iteration 176, loss = 0.13401669\n",
      "Iteration 177, loss = 0.13328847\n",
      "Iteration 178, loss = 0.13257168\n",
      "Iteration 179, loss = 0.13186611\n",
      "Iteration 180, loss = 0.13117154\n",
      "Iteration 181, loss = 0.13048774\n",
      "Iteration 182, loss = 0.12981450\n",
      "Iteration 183, loss = 0.12915162\n",
      "Iteration 184, loss = 0.12849889\n",
      "Iteration 185, loss = 0.12785611\n",
      "Iteration 186, loss = 0.12722308\n",
      "Iteration 187, loss = 0.12659963\n",
      "Iteration 188, loss = 0.12598555\n",
      "Iteration 189, loss = 0.12538067\n",
      "Iteration 190, loss = 0.12478481\n",
      "Iteration 191, loss = 0.12419779\n",
      "Iteration 192, loss = 0.12361944\n",
      "Iteration 193, loss = 0.12304960\n",
      "Iteration 194, loss = 0.12248810\n",
      "Iteration 195, loss = 0.12193479\n",
      "Iteration 196, loss = 0.12138949\n",
      "Iteration 197, loss = 0.12085208\n",
      "Iteration 198, loss = 0.12032238\n",
      "Iteration 199, loss = 0.11980026\n",
      "Iteration 200, loss = 0.11928557\n",
      "Iteration 201, loss = 0.11877818\n",
      "Iteration 202, loss = 0.11827794\n",
      "Iteration 203, loss = 0.11778472\n",
      "Iteration 204, loss = 0.11729839\n",
      "Iteration 205, loss = 0.11681883\n",
      "Iteration 206, loss = 0.11634590\n",
      "Iteration 207, loss = 0.11587949\n",
      "Iteration 208, loss = 0.11541947\n",
      "Iteration 209, loss = 0.11496573\n",
      "Iteration 210, loss = 0.11451814\n",
      "Iteration 211, loss = 0.11407661\n",
      "Iteration 212, loss = 0.11364102\n",
      "Iteration 213, loss = 0.11321126\n",
      "Iteration 214, loss = 0.11278722\n",
      "Iteration 215, loss = 0.11236881\n",
      "Iteration 216, loss = 0.11195592\n",
      "Iteration 217, loss = 0.11154846\n",
      "Iteration 218, loss = 0.11114632\n",
      "Iteration 219, loss = 0.11074941\n",
      "Iteration 220, loss = 0.11035765\n",
      "Iteration 221, loss = 0.10997093\n",
      "Iteration 222, loss = 0.10958918\n",
      "Iteration 223, loss = 0.10921230\n",
      "Iteration 224, loss = 0.10884022\n",
      "Iteration 225, loss = 0.10847284\n",
      "Iteration 226, loss = 0.10811008\n",
      "Iteration 227, loss = 0.10775187\n",
      "Iteration 228, loss = 0.10739813\n",
      "Iteration 229, loss = 0.10704878\n",
      "Iteration 230, loss = 0.10670375\n",
      "Iteration 231, loss = 0.10636296\n",
      "Iteration 232, loss = 0.10602634\n",
      "Iteration 233, loss = 0.10569383\n",
      "Iteration 234, loss = 0.10536535\n",
      "Iteration 235, loss = 0.10504083\n",
      "Iteration 236, loss = 0.10472022\n",
      "Iteration 237, loss = 0.10440344\n",
      "Iteration 238, loss = 0.10409043\n",
      "Iteration 239, loss = 0.10378113\n",
      "Iteration 240, loss = 0.10347548\n",
      "Iteration 241, loss = 0.10317342\n",
      "Iteration 242, loss = 0.10287490\n",
      "Iteration 243, loss = 0.10257985\n",
      "Iteration 244, loss = 0.10228821\n",
      "Iteration 245, loss = 0.10199994\n",
      "Iteration 246, loss = 0.10171498\n",
      "Iteration 247, loss = 0.10143328\n",
      "Iteration 248, loss = 0.10115478\n",
      "Iteration 249, loss = 0.10087943\n",
      "Iteration 250, loss = 0.10060719\n",
      "Iteration 251, loss = 0.10033801\n",
      "Iteration 252, loss = 0.10007183\n",
      "Iteration 253, loss = 0.09980862\n",
      "Iteration 254, loss = 0.09954832\n",
      "Iteration 255, loss = 0.09929089\n",
      "Iteration 256, loss = 0.09903629\n",
      "Iteration 257, loss = 0.09878447\n",
      "Iteration 258, loss = 0.09853540\n",
      "Iteration 259, loss = 0.09828902\n",
      "Iteration 260, loss = 0.09804529\n",
      "Iteration 261, loss = 0.09780419\n",
      "Iteration 262, loss = 0.09756567\n",
      "Iteration 263, loss = 0.09732968\n",
      "Iteration 264, loss = 0.09709620\n",
      "Iteration 265, loss = 0.09686518\n",
      "Iteration 266, loss = 0.09663659\n",
      "Iteration 267, loss = 0.09641039\n",
      "Iteration 268, loss = 0.09618655\n",
      "Iteration 269, loss = 0.09596503\n",
      "Iteration 270, loss = 0.09574580\n",
      "Iteration 271, loss = 0.09552883\n",
      "Iteration 272, loss = 0.09531407\n",
      "Iteration 273, loss = 0.09510151\n",
      "Iteration 274, loss = 0.09489111\n",
      "Iteration 275, loss = 0.09468284\n",
      "Iteration 276, loss = 0.09447666\n",
      "Iteration 277, loss = 0.09427256\n",
      "Iteration 278, loss = 0.09407049\n",
      "Iteration 279, loss = 0.09387043\n",
      "Iteration 280, loss = 0.09367235\n",
      "Iteration 281, loss = 0.09347623\n",
      "Iteration 282, loss = 0.09328204\n",
      "Iteration 283, loss = 0.09308974\n",
      "Iteration 284, loss = 0.09289932\n",
      "Iteration 285, loss = 0.09271075\n",
      "Iteration 286, loss = 0.09252400\n",
      "Iteration 287, loss = 0.09233905\n",
      "Iteration 288, loss = 0.09215587\n",
      "Iteration 289, loss = 0.09197444\n",
      "Iteration 290, loss = 0.09179473\n",
      "Iteration 291, loss = 0.09161673\n",
      "Iteration 292, loss = 0.09144041\n",
      "Iteration 293, loss = 0.09126574\n",
      "Iteration 294, loss = 0.09109271\n",
      "Iteration 295, loss = 0.09092129\n",
      "Iteration 296, loss = 0.09075146\n",
      "Iteration 297, loss = 0.09058320\n",
      "Iteration 298, loss = 0.09041649\n",
      "Iteration 299, loss = 0.09025131\n",
      "Iteration 300, loss = 0.09008764\n",
      "Iteration 301, loss = 0.08992546\n",
      "Iteration 302, loss = 0.08976476\n",
      "Iteration 303, loss = 0.08960550\n",
      "Iteration 304, loss = 0.08944767\n",
      "Iteration 305, loss = 0.08929126\n",
      "Iteration 306, loss = 0.08913625\n",
      "Iteration 307, loss = 0.08898261\n",
      "Iteration 308, loss = 0.08883034\n",
      "Iteration 309, loss = 0.08867941\n",
      "Iteration 310, loss = 0.08852981\n",
      "Iteration 311, loss = 0.08838151\n",
      "Iteration 312, loss = 0.08823451\n",
      "Iteration 313, loss = 0.08808879\n",
      "Iteration 314, loss = 0.08794437\n",
      "Iteration 315, loss = 0.08780119\n",
      "Iteration 316, loss = 0.08765925\n",
      "Iteration 317, loss = 0.08751852\n",
      "Iteration 318, loss = 0.08737899\n",
      "Iteration 319, loss = 0.08724066\n",
      "Iteration 320, loss = 0.08710349\n",
      "Iteration 321, loss = 0.08696748\n",
      "Iteration 322, loss = 0.08683262\n",
      "Iteration 323, loss = 0.08669888\n",
      "Iteration 324, loss = 0.08656627\n",
      "Iteration 325, loss = 0.08643475\n",
      "Iteration 326, loss = 0.08630433\n",
      "Iteration 327, loss = 0.08617498\n",
      "Iteration 328, loss = 0.08604669\n",
      "Iteration 329, loss = 0.08591946\n",
      "Iteration 330, loss = 0.08579326\n",
      "Iteration 331, loss = 0.08566810\n",
      "Iteration 332, loss = 0.08554394\n",
      "Iteration 333, loss = 0.08542079\n",
      "Iteration 334, loss = 0.08529863\n",
      "Iteration 335, loss = 0.08517745\n",
      "Iteration 336, loss = 0.08505724\n",
      "Iteration 337, loss = 0.08493798\n",
      "Iteration 338, loss = 0.08481967\n",
      "Iteration 339, loss = 0.08470230\n",
      "Iteration 340, loss = 0.08458584\n",
      "Iteration 341, loss = 0.08447031\n",
      "Iteration 342, loss = 0.08435567\n",
      "Iteration 343, loss = 0.08424193\n",
      "Iteration 344, loss = 0.08412907\n",
      "Iteration 345, loss = 0.08401709\n",
      "Iteration 346, loss = 0.08390597\n",
      "Iteration 347, loss = 0.08379570\n",
      "Iteration 348, loss = 0.08368628\n",
      "Iteration 349, loss = 0.08357769\n",
      "Iteration 350, loss = 0.08346993\n",
      "Iteration 351, loss = 0.08336298\n",
      "Iteration 352, loss = 0.08325684\n",
      "Iteration 353, loss = 0.08315150\n",
      "Iteration 354, loss = 0.08304695\n",
      "Iteration 355, loss = 0.08294319\n",
      "Iteration 356, loss = 0.08284019\n",
      "Iteration 357, loss = 0.08273796\n",
      "Iteration 358, loss = 0.08263648\n",
      "Iteration 359, loss = 0.08253576\n",
      "Iteration 360, loss = 0.08243577\n",
      "Iteration 361, loss = 0.08233652\n",
      "Iteration 362, loss = 0.08223799\n",
      "Iteration 363, loss = 0.08214018\n",
      "Iteration 364, loss = 0.08204307\n",
      "Iteration 365, loss = 0.08194667\n",
      "Iteration 366, loss = 0.08185096\n",
      "Iteration 367, loss = 0.08175594\n",
      "Iteration 368, loss = 0.08166160\n",
      "Iteration 369, loss = 0.08156793\n",
      "Iteration 370, loss = 0.08147493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50852130\n",
      "Iteration 2, loss = 1.25534402\n",
      "Iteration 3, loss = 1.07504527\n",
      "Iteration 4, loss = 0.94083969\n",
      "Iteration 5, loss = 0.84940305\n",
      "Iteration 6, loss = 0.80866502\n",
      "Iteration 7, loss = 0.80636543\n",
      "Iteration 8, loss = 0.81128803\n",
      "Iteration 9, loss = 0.81041569\n",
      "Iteration 10, loss = 0.79944692\n",
      "Iteration 11, loss = 0.77971899\n",
      "Iteration 12, loss = 0.75498725\n",
      "Iteration 13, loss = 0.73024916\n",
      "Iteration 14, loss = 0.71123492\n",
      "Iteration 15, loss = 0.69592228\n",
      "Iteration 16, loss = 0.68143585\n",
      "Iteration 17, loss = 0.66741663\n",
      "Iteration 18, loss = 0.65388613\n",
      "Iteration 19, loss = 0.64050937\n",
      "Iteration 20, loss = 0.62689592\n",
      "Iteration 21, loss = 0.61325272\n",
      "Iteration 22, loss = 0.59983949\n",
      "Iteration 23, loss = 0.58786792\n",
      "Iteration 24, loss = 0.57711704\n",
      "Iteration 25, loss = 0.56726222\n",
      "Iteration 26, loss = 0.55820624\n",
      "Iteration 27, loss = 0.54952926\n",
      "Iteration 28, loss = 0.54102447\n",
      "Iteration 29, loss = 0.53271788\n",
      "Iteration 30, loss = 0.52464044\n",
      "Iteration 31, loss = 0.51681725\n",
      "Iteration 32, loss = 0.50926627\n",
      "Iteration 33, loss = 0.50200036\n",
      "Iteration 34, loss = 0.49500547\n",
      "Iteration 35, loss = 0.48822873\n",
      "Iteration 36, loss = 0.48166150\n",
      "Iteration 37, loss = 0.47528633\n",
      "Iteration 38, loss = 0.46907188\n",
      "Iteration 39, loss = 0.46301769\n",
      "Iteration 40, loss = 0.45712448\n",
      "Iteration 41, loss = 0.45137806\n",
      "Iteration 42, loss = 0.44576941\n",
      "Iteration 43, loss = 0.44028794\n",
      "Iteration 44, loss = 0.43492722\n",
      "Iteration 45, loss = 0.42968258\n",
      "Iteration 46, loss = 0.42454780\n",
      "Iteration 47, loss = 0.41951022\n",
      "Iteration 48, loss = 0.41455363\n",
      "Iteration 49, loss = 0.40967059\n",
      "Iteration 50, loss = 0.40486922\n",
      "Iteration 51, loss = 0.40019355\n",
      "Iteration 52, loss = 0.39559566\n",
      "Iteration 53, loss = 0.39106726\n",
      "Iteration 54, loss = 0.38660066\n",
      "Iteration 55, loss = 0.38218725\n",
      "Iteration 56, loss = 0.37782919\n",
      "Iteration 57, loss = 0.37352546\n",
      "Iteration 58, loss = 0.36926928\n",
      "Iteration 59, loss = 0.36505727\n",
      "Iteration 60, loss = 0.36088784\n",
      "Iteration 61, loss = 0.35675961\n",
      "Iteration 62, loss = 0.35267139\n",
      "Iteration 63, loss = 0.34862224\n",
      "Iteration 64, loss = 0.34461135\n",
      "Iteration 65, loss = 0.34063811\n",
      "Iteration 66, loss = 0.33670199\n",
      "Iteration 67, loss = 0.33280257\n",
      "Iteration 68, loss = 0.32893949\n",
      "Iteration 69, loss = 0.32511246\n",
      "Iteration 70, loss = 0.32132122\n",
      "Iteration 71, loss = 0.31756606\n",
      "Iteration 72, loss = 0.31385023\n",
      "Iteration 73, loss = 0.31016993\n",
      "Iteration 74, loss = 0.30652509\n",
      "Iteration 75, loss = 0.30291575\n",
      "Iteration 76, loss = 0.29934199\n",
      "Iteration 77, loss = 0.29580398\n",
      "Iteration 78, loss = 0.29230188\n",
      "Iteration 79, loss = 0.28883592\n",
      "Iteration 80, loss = 0.28540634\n",
      "Iteration 81, loss = 0.28201340\n",
      "Iteration 82, loss = 0.27865736\n",
      "Iteration 83, loss = 0.27533847\n",
      "Iteration 84, loss = 0.27205708\n",
      "Iteration 85, loss = 0.26881458\n",
      "Iteration 86, loss = 0.26561010\n",
      "Iteration 87, loss = 0.26244387\n",
      "Iteration 88, loss = 0.25931610\n",
      "Iteration 89, loss = 0.25622701\n",
      "Iteration 90, loss = 0.25317679\n",
      "Iteration 91, loss = 0.25016563\n",
      "Iteration 92, loss = 0.24719368\n",
      "Iteration 93, loss = 0.24426110\n",
      "Iteration 94, loss = 0.24136799\n",
      "Iteration 95, loss = 0.23851446\n",
      "Iteration 96, loss = 0.23570057\n",
      "Iteration 97, loss = 0.23292638\n",
      "Iteration 98, loss = 0.23019190\n",
      "Iteration 99, loss = 0.22749711\n",
      "Iteration 100, loss = 0.22484199\n",
      "Iteration 101, loss = 0.22222694\n",
      "Iteration 102, loss = 0.21965160\n",
      "Iteration 103, loss = 0.21711564\n",
      "Iteration 104, loss = 0.21461890\n",
      "Iteration 105, loss = 0.21216120\n",
      "Iteration 106, loss = 0.20974234\n",
      "Iteration 107, loss = 0.20736208\n",
      "Iteration 108, loss = 0.20502018\n",
      "Iteration 109, loss = 0.20271638\n",
      "Iteration 110, loss = 0.20045036\n",
      "Iteration 111, loss = 0.19822184\n",
      "Iteration 112, loss = 0.19603046\n",
      "Iteration 113, loss = 0.19387589\n",
      "Iteration 114, loss = 0.19175776\n",
      "Iteration 115, loss = 0.18967568\n",
      "Iteration 116, loss = 0.18762926\n",
      "Iteration 117, loss = 0.18561959\n",
      "Iteration 118, loss = 0.18364551\n",
      "Iteration 119, loss = 0.18170586\n",
      "Iteration 120, loss = 0.17980012\n",
      "Iteration 121, loss = 0.17792783\n",
      "Iteration 122, loss = 0.17608853\n",
      "Iteration 123, loss = 0.17428176\n",
      "Iteration 124, loss = 0.17250704\n",
      "Iteration 125, loss = 0.17076388\n",
      "Iteration 126, loss = 0.16905180\n",
      "Iteration 127, loss = 0.16737031\n",
      "Iteration 128, loss = 0.16571892\n",
      "Iteration 129, loss = 0.16409713\n",
      "Iteration 130, loss = 0.16250446\n",
      "Iteration 131, loss = 0.16094042\n",
      "Iteration 132, loss = 0.15940451\n",
      "Iteration 133, loss = 0.15789624\n",
      "Iteration 134, loss = 0.15641514\n",
      "Iteration 135, loss = 0.15496072\n",
      "Iteration 136, loss = 0.15353250\n",
      "Iteration 137, loss = 0.15212999\n",
      "Iteration 138, loss = 0.15075273\n",
      "Iteration 139, loss = 0.14940024\n",
      "Iteration 140, loss = 0.14807206\n",
      "Iteration 141, loss = 0.14676772\n",
      "Iteration 142, loss = 0.14548677\n",
      "Iteration 143, loss = 0.14422876\n",
      "Iteration 144, loss = 0.14299323\n",
      "Iteration 145, loss = 0.14177976\n",
      "Iteration 146, loss = 0.14058790\n",
      "Iteration 147, loss = 0.13941722\n",
      "Iteration 148, loss = 0.13826731\n",
      "Iteration 149, loss = 0.13713774\n",
      "Iteration 150, loss = 0.13602812\n",
      "Iteration 151, loss = 0.13493803\n",
      "Iteration 152, loss = 0.13386707\n",
      "Iteration 153, loss = 0.13281487\n",
      "Iteration 154, loss = 0.13178103\n",
      "Iteration 155, loss = 0.13076518\n",
      "Iteration 156, loss = 0.12976696\n",
      "Iteration 157, loss = 0.12878600\n",
      "Iteration 158, loss = 0.12782193\n",
      "Iteration 159, loss = 0.12687443\n",
      "Iteration 160, loss = 0.12594313\n",
      "Iteration 161, loss = 0.12502772\n",
      "Iteration 162, loss = 0.12412789\n",
      "Iteration 163, loss = 0.12324341\n",
      "Iteration 164, loss = 0.12237385\n",
      "Iteration 165, loss = 0.12151890\n",
      "Iteration 166, loss = 0.12067825\n",
      "Iteration 167, loss = 0.11985160\n",
      "Iteration 168, loss = 0.11903868\n",
      "Iteration 169, loss = 0.11823918\n",
      "Iteration 170, loss = 0.11745284\n",
      "Iteration 171, loss = 0.11667938\n",
      "Iteration 172, loss = 0.11591853\n",
      "Iteration 173, loss = 0.11517003\n",
      "Iteration 174, loss = 0.11443363\n",
      "Iteration 175, loss = 0.11370909\n",
      "Iteration 176, loss = 0.11299614\n",
      "Iteration 177, loss = 0.11229456\n",
      "Iteration 178, loss = 0.11160412\n",
      "Iteration 179, loss = 0.11092458\n",
      "Iteration 180, loss = 0.11025572\n",
      "Iteration 181, loss = 0.10959733\n",
      "Iteration 182, loss = 0.10894919\n",
      "Iteration 183, loss = 0.10831110\n",
      "Iteration 184, loss = 0.10768285\n",
      "Iteration 185, loss = 0.10706423\n",
      "Iteration 186, loss = 0.10645507\n",
      "Iteration 187, loss = 0.10585517\n",
      "Iteration 188, loss = 0.10526434\n",
      "Iteration 189, loss = 0.10468240\n",
      "Iteration 190, loss = 0.10410917\n",
      "Iteration 191, loss = 0.10354449\n",
      "Iteration 192, loss = 0.10298818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 193, loss = 0.10244008\n",
      "Iteration 194, loss = 0.10190003\n",
      "Iteration 195, loss = 0.10136786\n",
      "Iteration 196, loss = 0.10084343\n",
      "Iteration 197, loss = 0.10032659\n",
      "Iteration 198, loss = 0.09981718\n",
      "Iteration 199, loss = 0.09931506\n",
      "Iteration 200, loss = 0.09882010\n",
      "Iteration 201, loss = 0.09833215\n",
      "Iteration 202, loss = 0.09785108\n",
      "Iteration 203, loss = 0.09737676\n",
      "Iteration 204, loss = 0.09690907\n",
      "Iteration 205, loss = 0.09644786\n",
      "Iteration 206, loss = 0.09599304\n",
      "Iteration 207, loss = 0.09554446\n",
      "Iteration 208, loss = 0.09510203\n",
      "Iteration 209, loss = 0.09466561\n",
      "Iteration 210, loss = 0.09423511\n",
      "Iteration 211, loss = 0.09381041\n",
      "Iteration 212, loss = 0.09339140\n",
      "Iteration 213, loss = 0.09297798\n",
      "Iteration 214, loss = 0.09257004\n",
      "Iteration 215, loss = 0.09216750\n",
      "Iteration 216, loss = 0.09177024\n",
      "Iteration 217, loss = 0.09137818\n",
      "Iteration 218, loss = 0.09099121\n",
      "Iteration 219, loss = 0.09060926\n",
      "Iteration 220, loss = 0.09023222\n",
      "Iteration 221, loss = 0.08986002\n",
      "Iteration 222, loss = 0.08949256\n",
      "Iteration 223, loss = 0.08912976\n",
      "Iteration 224, loss = 0.08877155\n",
      "Iteration 225, loss = 0.08841783\n",
      "Iteration 226, loss = 0.08806854\n",
      "Iteration 227, loss = 0.08772359\n",
      "Iteration 228, loss = 0.08738290\n",
      "Iteration 229, loss = 0.08704642\n",
      "Iteration 230, loss = 0.08671406\n",
      "Iteration 231, loss = 0.08638575\n",
      "Iteration 232, loss = 0.08606143\n",
      "Iteration 233, loss = 0.08574102\n",
      "Iteration 234, loss = 0.08542446\n",
      "Iteration 235, loss = 0.08511169\n",
      "Iteration 236, loss = 0.08480264\n",
      "Iteration 237, loss = 0.08449726\n",
      "Iteration 238, loss = 0.08419547\n",
      "Iteration 239, loss = 0.08389722\n",
      "Iteration 240, loss = 0.08360246\n",
      "Iteration 241, loss = 0.08331112\n",
      "Iteration 242, loss = 0.08302322\n",
      "Iteration 243, loss = 0.08273864\n",
      "Iteration 244, loss = 0.08245732\n",
      "Iteration 245, loss = 0.08217922\n",
      "Iteration 246, loss = 0.08190427\n",
      "Iteration 247, loss = 0.08163243\n",
      "Iteration 248, loss = 0.08136365\n",
      "Iteration 249, loss = 0.08109788\n",
      "Iteration 250, loss = 0.08083507\n",
      "Iteration 251, loss = 0.08057517\n",
      "Iteration 252, loss = 0.08031815\n",
      "Iteration 253, loss = 0.08006395\n",
      "Iteration 254, loss = 0.07981252\n",
      "Iteration 255, loss = 0.07956384\n",
      "Iteration 256, loss = 0.07931784\n",
      "Iteration 257, loss = 0.07907450\n",
      "Iteration 258, loss = 0.07883378\n",
      "Iteration 259, loss = 0.07859562\n",
      "Iteration 260, loss = 0.07836000\n",
      "Iteration 261, loss = 0.07812687\n",
      "Iteration 262, loss = 0.07789620\n",
      "Iteration 263, loss = 0.07766795\n",
      "Iteration 264, loss = 0.07744208\n",
      "Iteration 265, loss = 0.07721856\n",
      "Iteration 266, loss = 0.07699736\n",
      "Iteration 267, loss = 0.07677843\n",
      "Iteration 268, loss = 0.07656175\n",
      "Iteration 269, loss = 0.07634728\n",
      "Iteration 270, loss = 0.07613499\n",
      "Iteration 271, loss = 0.07592486\n",
      "Iteration 272, loss = 0.07571684\n",
      "Iteration 273, loss = 0.07551090\n",
      "Iteration 274, loss = 0.07530703\n",
      "Iteration 275, loss = 0.07510518\n",
      "Iteration 276, loss = 0.07490533\n",
      "Iteration 277, loss = 0.07470745\n",
      "Iteration 278, loss = 0.07451152\n",
      "Iteration 279, loss = 0.07431750\n",
      "Iteration 280, loss = 0.07412537\n",
      "Iteration 281, loss = 0.07393510\n",
      "Iteration 282, loss = 0.07374666\n",
      "Iteration 283, loss = 0.07356004\n",
      "Iteration 284, loss = 0.07337520\n",
      "Iteration 285, loss = 0.07319213\n",
      "Iteration 286, loss = 0.07301079\n",
      "Iteration 287, loss = 0.07283117\n",
      "Iteration 288, loss = 0.07265323\n",
      "Iteration 289, loss = 0.07247697\n",
      "Iteration 290, loss = 0.07230234\n",
      "Iteration 291, loss = 0.07212934\n",
      "Iteration 292, loss = 0.07195795\n",
      "Iteration 293, loss = 0.07178813\n",
      "Iteration 294, loss = 0.07161987\n",
      "Iteration 295, loss = 0.07145314\n",
      "Iteration 296, loss = 0.07128794\n",
      "Iteration 297, loss = 0.07112423\n",
      "Iteration 298, loss = 0.07096201\n",
      "Iteration 299, loss = 0.07080124\n",
      "Iteration 300, loss = 0.07064191\n",
      "Iteration 301, loss = 0.07048400\n",
      "Iteration 302, loss = 0.07032750\n",
      "Iteration 303, loss = 0.07017238\n",
      "Iteration 304, loss = 0.07001863\n",
      "Iteration 305, loss = 0.06986622\n",
      "Iteration 306, loss = 0.06971515\n",
      "Iteration 307, loss = 0.06956540\n",
      "Iteration 308, loss = 0.06941695\n",
      "Iteration 309, loss = 0.06926978\n",
      "Iteration 310, loss = 0.06912388\n",
      "Iteration 311, loss = 0.06897922\n",
      "Iteration 312, loss = 0.06883581\n",
      "Iteration 313, loss = 0.06869361\n",
      "Iteration 314, loss = 0.06855262\n",
      "Iteration 315, loss = 0.06841282\n",
      "Iteration 316, loss = 0.06827420\n",
      "Iteration 317, loss = 0.06813674\n",
      "Iteration 318, loss = 0.06800042\n",
      "Iteration 319, loss = 0.06786524\n",
      "Iteration 320, loss = 0.06773118\n",
      "Iteration 321, loss = 0.06759823\n",
      "Iteration 322, loss = 0.06746637\n",
      "Iteration 323, loss = 0.06733558\n",
      "Iteration 324, loss = 0.06720587\n",
      "Iteration 325, loss = 0.06707721\n",
      "Iteration 326, loss = 0.06694960\n",
      "Iteration 327, loss = 0.06682301\n",
      "Iteration 328, loss = 0.06669744\n",
      "Iteration 329, loss = 0.06657288\n",
      "Iteration 330, loss = 0.06644931\n",
      "Iteration 331, loss = 0.06632673\n",
      "Iteration 332, loss = 0.06620511\n",
      "Iteration 333, loss = 0.06608446\n",
      "Iteration 334, loss = 0.06596476\n",
      "Iteration 335, loss = 0.06584599\n",
      "Iteration 336, loss = 0.06572815\n",
      "Iteration 337, loss = 0.06561123\n",
      "Iteration 338, loss = 0.06549522\n",
      "Iteration 339, loss = 0.06538010\n",
      "Iteration 340, loss = 0.06526587\n",
      "Iteration 341, loss = 0.06515251\n",
      "Iteration 342, loss = 0.06504002\n",
      "Iteration 343, loss = 0.06492839\n",
      "Iteration 344, loss = 0.06481761\n",
      "Iteration 345, loss = 0.06470766\n",
      "Iteration 346, loss = 0.06459855\n",
      "Iteration 347, loss = 0.06449025\n",
      "Iteration 348, loss = 0.06438276\n",
      "Iteration 349, loss = 0.06427608\n",
      "Iteration 350, loss = 0.06417019\n",
      "Iteration 351, loss = 0.06406508\n",
      "Iteration 352, loss = 0.06396075\n",
      "Iteration 353, loss = 0.06385718\n",
      "Iteration 354, loss = 0.06375438\n",
      "Iteration 355, loss = 0.06365233\n",
      "Iteration 356, loss = 0.06355102\n",
      "Iteration 357, loss = 0.06345045\n",
      "Iteration 358, loss = 0.06335060\n",
      "Iteration 359, loss = 0.06325148\n",
      "Iteration 360, loss = 0.06315306\n",
      "Iteration 361, loss = 0.06305535\n",
      "Iteration 362, loss = 0.06295834\n",
      "Iteration 363, loss = 0.06286202\n",
      "Iteration 364, loss = 0.06276638\n",
      "Iteration 365, loss = 0.06267142\n",
      "Iteration 366, loss = 0.06257713\n",
      "Iteration 367, loss = 0.06248350\n",
      "Iteration 368, loss = 0.06239052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50041490\n",
      "Iteration 2, loss = 1.24864642\n",
      "Iteration 3, loss = 1.07020606\n",
      "Iteration 4, loss = 0.93725820\n",
      "Iteration 5, loss = 0.84667646\n",
      "Iteration 6, loss = 0.80885830\n",
      "Iteration 7, loss = 0.80798883\n",
      "Iteration 8, loss = 0.81166745\n",
      "Iteration 9, loss = 0.81048102\n",
      "Iteration 10, loss = 0.79998960\n",
      "Iteration 11, loss = 0.78121435\n",
      "Iteration 12, loss = 0.75715925\n",
      "Iteration 13, loss = 0.73194824\n",
      "Iteration 14, loss = 0.71225169\n",
      "Iteration 15, loss = 0.69646687\n",
      "Iteration 16, loss = 0.68222666\n",
      "Iteration 17, loss = 0.66906075\n",
      "Iteration 18, loss = 0.65635517\n",
      "Iteration 19, loss = 0.64362293\n",
      "Iteration 20, loss = 0.63040921\n",
      "Iteration 21, loss = 0.61697441\n",
      "Iteration 22, loss = 0.60378146\n",
      "Iteration 23, loss = 0.59126204\n",
      "Iteration 24, loss = 0.58018246\n",
      "Iteration 25, loss = 0.57017457\n",
      "Iteration 26, loss = 0.56102912\n",
      "Iteration 27, loss = 0.55240690\n",
      "Iteration 28, loss = 0.54401008\n",
      "Iteration 29, loss = 0.53574997\n",
      "Iteration 30, loss = 0.52766126\n",
      "Iteration 31, loss = 0.51984886\n",
      "Iteration 32, loss = 0.51231441\n",
      "Iteration 33, loss = 0.50509379\n",
      "Iteration 34, loss = 0.49815481\n",
      "Iteration 35, loss = 0.49145309\n",
      "Iteration 36, loss = 0.48495976\n",
      "Iteration 37, loss = 0.47864092\n",
      "Iteration 38, loss = 0.47248547\n",
      "Iteration 39, loss = 0.46648263\n",
      "Iteration 40, loss = 0.46062426\n",
      "Iteration 41, loss = 0.45491199\n",
      "Iteration 42, loss = 0.44933223\n",
      "Iteration 43, loss = 0.44387887\n",
      "Iteration 44, loss = 0.43855014\n",
      "Iteration 45, loss = 0.43332646\n",
      "Iteration 46, loss = 0.42819718\n",
      "Iteration 47, loss = 0.42315557\n",
      "Iteration 48, loss = 0.41820429\n",
      "Iteration 49, loss = 0.41336365\n",
      "Iteration 50, loss = 0.40860861\n",
      "Iteration 51, loss = 0.40392998\n",
      "Iteration 52, loss = 0.39931691\n",
      "Iteration 53, loss = 0.39476785\n",
      "Iteration 54, loss = 0.39027789\n",
      "Iteration 55, loss = 0.38583965\n",
      "Iteration 56, loss = 0.38145919\n",
      "Iteration 57, loss = 0.37712538\n",
      "Iteration 58, loss = 0.37283622\n",
      "Iteration 59, loss = 0.36859085\n",
      "Iteration 60, loss = 0.36439255\n",
      "Iteration 61, loss = 0.36023676\n",
      "Iteration 62, loss = 0.35612589\n",
      "Iteration 63, loss = 0.35205359\n",
      "Iteration 64, loss = 0.34801901\n",
      "Iteration 65, loss = 0.34402142\n",
      "Iteration 66, loss = 0.34006025\n",
      "Iteration 67, loss = 0.33613531\n",
      "Iteration 68, loss = 0.33224898\n",
      "Iteration 69, loss = 0.32839797\n",
      "Iteration 70, loss = 0.32458207\n",
      "Iteration 71, loss = 0.32080454\n",
      "Iteration 72, loss = 0.31706533\n",
      "Iteration 73, loss = 0.31336167\n",
      "Iteration 74, loss = 0.30969493\n",
      "Iteration 75, loss = 0.30606351\n",
      "Iteration 76, loss = 0.30246754\n",
      "Iteration 77, loss = 0.29890720\n",
      "Iteration 78, loss = 0.29538268\n",
      "Iteration 79, loss = 0.29189417\n",
      "Iteration 80, loss = 0.28844189\n",
      "Iteration 81, loss = 0.28502608\n",
      "Iteration 82, loss = 0.28164698\n",
      "Iteration 83, loss = 0.27830487\n",
      "Iteration 84, loss = 0.27500000\n",
      "Iteration 85, loss = 0.27173265\n",
      "Iteration 86, loss = 0.26850307\n",
      "Iteration 87, loss = 0.26531154\n",
      "Iteration 88, loss = 0.26215831\n",
      "Iteration 89, loss = 0.25904359\n",
      "Iteration 90, loss = 0.25596762\n",
      "Iteration 91, loss = 0.25293110\n",
      "Iteration 92, loss = 0.24993875\n",
      "Iteration 93, loss = 0.24698584\n",
      "Iteration 94, loss = 0.24407281\n",
      "Iteration 95, loss = 0.24120000\n",
      "Iteration 96, loss = 0.23836662\n",
      "Iteration 97, loss = 0.23557272\n",
      "Iteration 98, loss = 0.23281832\n",
      "Iteration 99, loss = 0.23010338\n",
      "Iteration 100, loss = 0.22742784\n",
      "Iteration 101, loss = 0.22479161\n",
      "Iteration 102, loss = 0.22219456\n",
      "Iteration 103, loss = 0.21963656\n",
      "Iteration 104, loss = 0.21711743\n",
      "Iteration 105, loss = 0.21463697\n",
      "Iteration 106, loss = 0.21219499\n",
      "Iteration 107, loss = 0.20979124\n",
      "Iteration 108, loss = 0.20742548\n",
      "Iteration 109, loss = 0.20509743\n",
      "Iteration 110, loss = 0.20280679\n",
      "Iteration 111, loss = 0.20055327\n",
      "Iteration 112, loss = 0.19833652\n",
      "Iteration 113, loss = 0.19615619\n",
      "Iteration 114, loss = 0.19401193\n",
      "Iteration 115, loss = 0.19190335\n",
      "Iteration 116, loss = 0.18983005\n",
      "Iteration 117, loss = 0.18779162\n",
      "Iteration 118, loss = 0.18578764\n",
      "Iteration 119, loss = 0.18381769\n",
      "Iteration 120, loss = 0.18188131\n",
      "Iteration 121, loss = 0.17997807\n",
      "Iteration 122, loss = 0.17810750\n",
      "Iteration 123, loss = 0.17626914\n",
      "Iteration 124, loss = 0.17446253\n",
      "Iteration 125, loss = 0.17268719\n",
      "Iteration 126, loss = 0.17094264\n",
      "Iteration 127, loss = 0.16922841\n",
      "Iteration 128, loss = 0.16754402\n",
      "Iteration 129, loss = 0.16588897\n",
      "Iteration 130, loss = 0.16426280\n",
      "Iteration 131, loss = 0.16266501\n",
      "Iteration 132, loss = 0.16109512\n",
      "Iteration 133, loss = 0.15955265\n",
      "Iteration 134, loss = 0.15803712\n",
      "Iteration 135, loss = 0.15654805\n",
      "Iteration 136, loss = 0.15508496\n",
      "Iteration 137, loss = 0.15364739\n",
      "Iteration 138, loss = 0.15223487\n",
      "Iteration 139, loss = 0.15084693\n",
      "Iteration 140, loss = 0.14948311\n",
      "Iteration 141, loss = 0.14814296\n",
      "Iteration 142, loss = 0.14682603\n",
      "Iteration 143, loss = 0.14553187\n",
      "Iteration 144, loss = 0.14426005\n",
      "Iteration 145, loss = 0.14301013\n",
      "Iteration 146, loss = 0.14178168\n",
      "Iteration 147, loss = 0.14057429\n",
      "Iteration 148, loss = 0.13938753\n",
      "Iteration 149, loss = 0.13822100\n",
      "Iteration 150, loss = 0.13707430\n",
      "Iteration 151, loss = 0.13594703\n",
      "Iteration 152, loss = 0.13483880\n",
      "Iteration 153, loss = 0.13374922\n",
      "Iteration 154, loss = 0.13267793\n",
      "Iteration 155, loss = 0.13162456\n",
      "Iteration 156, loss = 0.13058873\n",
      "Iteration 157, loss = 0.12957010\n",
      "Iteration 158, loss = 0.12856831\n",
      "Iteration 159, loss = 0.12758303\n",
      "Iteration 160, loss = 0.12661391\n",
      "Iteration 161, loss = 0.12566063\n",
      "Iteration 162, loss = 0.12472286\n",
      "Iteration 163, loss = 0.12380030\n",
      "Iteration 164, loss = 0.12289261\n",
      "Iteration 165, loss = 0.12199952\n",
      "Iteration 166, loss = 0.12112071\n",
      "Iteration 167, loss = 0.12025589\n",
      "Iteration 168, loss = 0.11940479\n",
      "Iteration 169, loss = 0.11856712\n",
      "Iteration 170, loss = 0.11774260\n",
      "Iteration 171, loss = 0.11693098\n",
      "Iteration 172, loss = 0.11613199\n",
      "Iteration 173, loss = 0.11534537\n",
      "Iteration 174, loss = 0.11457087\n",
      "Iteration 175, loss = 0.11380825\n",
      "Iteration 176, loss = 0.11305727\n",
      "Iteration 177, loss = 0.11231770\n",
      "Iteration 178, loss = 0.11158930\n",
      "Iteration 179, loss = 0.11087185\n",
      "Iteration 180, loss = 0.11016514\n",
      "Iteration 181, loss = 0.10946895\n",
      "Iteration 182, loss = 0.10878307\n",
      "Iteration 183, loss = 0.10810729\n",
      "Iteration 184, loss = 0.10744142\n",
      "Iteration 185, loss = 0.10678525\n",
      "Iteration 186, loss = 0.10613861\n",
      "Iteration 187, loss = 0.10550130\n",
      "Iteration 188, loss = 0.10487314\n",
      "Iteration 189, loss = 0.10425395\n",
      "Iteration 190, loss = 0.10364355\n",
      "Iteration 191, loss = 0.10304179\n",
      "Iteration 192, loss = 0.10244848\n",
      "Iteration 193, loss = 0.10186347\n",
      "Iteration 194, loss = 0.10128659\n",
      "Iteration 195, loss = 0.10071770\n",
      "Iteration 196, loss = 0.10015664\n",
      "Iteration 197, loss = 0.09960326\n",
      "Iteration 198, loss = 0.09905741\n",
      "Iteration 199, loss = 0.09851896\n",
      "Iteration 200, loss = 0.09798776\n",
      "Iteration 201, loss = 0.09746368\n",
      "Iteration 202, loss = 0.09694658\n",
      "Iteration 203, loss = 0.09643634\n",
      "Iteration 204, loss = 0.09593283\n",
      "Iteration 205, loss = 0.09543593\n",
      "Iteration 206, loss = 0.09494553\n",
      "Iteration 207, loss = 0.09446155\n",
      "Iteration 208, loss = 0.09398381\n",
      "Iteration 209, loss = 0.09351221\n",
      "Iteration 210, loss = 0.09304664\n",
      "Iteration 211, loss = 0.09258698\n",
      "Iteration 212, loss = 0.09213313\n",
      "Iteration 213, loss = 0.09168499\n",
      "Iteration 214, loss = 0.09124245\n",
      "Iteration 215, loss = 0.09080541\n",
      "Iteration 216, loss = 0.09037379\n",
      "Iteration 217, loss = 0.08994747\n",
      "Iteration 218, loss = 0.08952638\n",
      "Iteration 219, loss = 0.08911042\n",
      "Iteration 220, loss = 0.08869949\n",
      "Iteration 221, loss = 0.08829352\n",
      "Iteration 222, loss = 0.08789241\n",
      "Iteration 223, loss = 0.08749609\n",
      "Iteration 224, loss = 0.08710447\n",
      "Iteration 225, loss = 0.08671747\n",
      "Iteration 226, loss = 0.08633501\n",
      "Iteration 227, loss = 0.08595703\n",
      "Iteration 228, loss = 0.08558343\n",
      "Iteration 229, loss = 0.08521415\n",
      "Iteration 230, loss = 0.08484912\n",
      "Iteration 231, loss = 0.08448826\n",
      "Iteration 232, loss = 0.08413151\n",
      "Iteration 233, loss = 0.08377880\n",
      "Iteration 234, loss = 0.08343006\n",
      "Iteration 235, loss = 0.08308523\n",
      "Iteration 236, loss = 0.08274424\n",
      "Iteration 237, loss = 0.08240704\n",
      "Iteration 238, loss = 0.08207356\n",
      "Iteration 239, loss = 0.08174374\n",
      "Iteration 240, loss = 0.08141753\n",
      "Iteration 241, loss = 0.08109486\n",
      "Iteration 242, loss = 0.08077568\n",
      "Iteration 243, loss = 0.08045993\n",
      "Iteration 244, loss = 0.08014757\n",
      "Iteration 245, loss = 0.07983854\n",
      "Iteration 246, loss = 0.07953278\n",
      "Iteration 247, loss = 0.07923024\n",
      "Iteration 248, loss = 0.07893089\n",
      "Iteration 249, loss = 0.07863466\n",
      "Iteration 250, loss = 0.07834151\n",
      "Iteration 251, loss = 0.07805139\n",
      "Iteration 252, loss = 0.07776426\n",
      "Iteration 253, loss = 0.07748007\n",
      "Iteration 254, loss = 0.07719877\n",
      "Iteration 255, loss = 0.07692033\n",
      "Iteration 256, loss = 0.07664470\n",
      "Iteration 257, loss = 0.07637184\n",
      "Iteration 258, loss = 0.07610171\n",
      "Iteration 259, loss = 0.07583426\n",
      "Iteration 260, loss = 0.07556947\n",
      "Iteration 261, loss = 0.07530728\n",
      "Iteration 262, loss = 0.07504767\n",
      "Iteration 263, loss = 0.07479059\n",
      "Iteration 264, loss = 0.07453600\n",
      "Iteration 265, loss = 0.07428388\n",
      "Iteration 266, loss = 0.07403419\n",
      "Iteration 267, loss = 0.07378689\n",
      "Iteration 268, loss = 0.07354195\n",
      "Iteration 269, loss = 0.07329933\n",
      "Iteration 270, loss = 0.07305900\n",
      "Iteration 271, loss = 0.07282094\n",
      "Iteration 272, loss = 0.07258510\n",
      "Iteration 273, loss = 0.07235146\n",
      "Iteration 274, loss = 0.07211999\n",
      "Iteration 275, loss = 0.07189065\n",
      "Iteration 276, loss = 0.07166343\n",
      "Iteration 277, loss = 0.07143828\n",
      "Iteration 278, loss = 0.07121518\n",
      "Iteration 279, loss = 0.07099410\n",
      "Iteration 280, loss = 0.07077502\n",
      "Iteration 281, loss = 0.07055791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 282, loss = 0.07034274\n",
      "Iteration 283, loss = 0.07012949\n",
      "Iteration 284, loss = 0.06991813\n",
      "Iteration 285, loss = 0.06970863\n",
      "Iteration 286, loss = 0.06950098\n",
      "Iteration 287, loss = 0.06929514\n",
      "Iteration 288, loss = 0.06909110\n",
      "Iteration 289, loss = 0.06888882\n",
      "Iteration 290, loss = 0.06868829\n",
      "Iteration 291, loss = 0.06848949\n",
      "Iteration 292, loss = 0.06829238\n",
      "Iteration 293, loss = 0.06809696\n",
      "Iteration 294, loss = 0.06790320\n",
      "Iteration 295, loss = 0.06771107\n",
      "Iteration 296, loss = 0.06752056\n",
      "Iteration 297, loss = 0.06733165\n",
      "Iteration 298, loss = 0.06714431\n",
      "Iteration 299, loss = 0.06695853\n",
      "Iteration 300, loss = 0.06677429\n",
      "Iteration 301, loss = 0.06659156\n",
      "Iteration 302, loss = 0.06641033\n",
      "Iteration 303, loss = 0.06623059\n",
      "Iteration 304, loss = 0.06605230\n",
      "Iteration 305, loss = 0.06587546\n",
      "Iteration 306, loss = 0.06570005\n",
      "Iteration 307, loss = 0.06552604\n",
      "Iteration 308, loss = 0.06535343\n",
      "Iteration 309, loss = 0.06518220\n",
      "Iteration 310, loss = 0.06501232\n",
      "Iteration 311, loss = 0.06484378\n",
      "Iteration 312, loss = 0.06467657\n",
      "Iteration 313, loss = 0.06451068\n",
      "Iteration 314, loss = 0.06434607\n",
      "Iteration 315, loss = 0.06418275\n",
      "Iteration 316, loss = 0.06402069\n",
      "Iteration 317, loss = 0.06385989\n",
      "Iteration 318, loss = 0.06370031\n",
      "Iteration 319, loss = 0.06354196\n",
      "Iteration 320, loss = 0.06338482\n",
      "Iteration 321, loss = 0.06322887\n",
      "Iteration 322, loss = 0.06307410\n",
      "Iteration 323, loss = 0.06292049\n",
      "Iteration 324, loss = 0.06276804\n",
      "Iteration 325, loss = 0.06261672\n",
      "Iteration 326, loss = 0.06246653\n",
      "Iteration 327, loss = 0.06231746\n",
      "Iteration 328, loss = 0.06216948\n",
      "Iteration 329, loss = 0.06202260\n",
      "Iteration 330, loss = 0.06187679\n",
      "Iteration 331, loss = 0.06173205\n",
      "Iteration 332, loss = 0.06158836\n",
      "Iteration 333, loss = 0.06144571\n",
      "Iteration 334, loss = 0.06130409\n",
      "Iteration 335, loss = 0.06116349\n",
      "Iteration 336, loss = 0.06102389\n",
      "Iteration 337, loss = 0.06088529\n",
      "Iteration 338, loss = 0.06074768\n",
      "Iteration 339, loss = 0.06061104\n",
      "Iteration 340, loss = 0.06047536\n",
      "Iteration 341, loss = 0.06034064\n",
      "Iteration 342, loss = 0.06020687\n",
      "Iteration 343, loss = 0.06007402\n",
      "Iteration 344, loss = 0.05994210\n",
      "Iteration 345, loss = 0.05981109\n",
      "Iteration 346, loss = 0.05968099\n",
      "Iteration 347, loss = 0.05955178\n",
      "Iteration 348, loss = 0.05942346\n",
      "Iteration 349, loss = 0.05929601\n",
      "Iteration 350, loss = 0.05916943\n",
      "Iteration 351, loss = 0.05904371\n",
      "Iteration 352, loss = 0.05891883\n",
      "Iteration 353, loss = 0.05879480\n",
      "Iteration 354, loss = 0.05867160\n",
      "Iteration 355, loss = 0.05854921\n",
      "Iteration 356, loss = 0.05842765\n",
      "Iteration 357, loss = 0.05830689\n",
      "Iteration 358, loss = 0.05818693\n",
      "Iteration 359, loss = 0.05806775\n",
      "Iteration 360, loss = 0.05794936\n",
      "Iteration 361, loss = 0.05783175\n",
      "Iteration 362, loss = 0.05771490\n",
      "Iteration 363, loss = 0.05759880\n",
      "Iteration 364, loss = 0.05748346\n",
      "Iteration 365, loss = 0.05736887\n",
      "Iteration 366, loss = 0.05725501\n",
      "Iteration 367, loss = 0.05714187\n",
      "Iteration 368, loss = 0.05702946\n",
      "Iteration 369, loss = 0.05691777\n",
      "Iteration 370, loss = 0.05680678\n",
      "Iteration 371, loss = 0.05669649\n",
      "Iteration 372, loss = 0.05658690\n",
      "Iteration 373, loss = 0.05647799\n",
      "Iteration 374, loss = 0.05636977\n",
      "Iteration 375, loss = 0.05626221\n",
      "Iteration 376, loss = 0.05615533\n",
      "Iteration 377, loss = 0.05604911\n",
      "Iteration 378, loss = 0.05594354\n",
      "Iteration 379, loss = 0.05583862\n",
      "Iteration 380, loss = 0.05573434\n",
      "Iteration 381, loss = 0.05563070\n",
      "Iteration 382, loss = 0.05552769\n",
      "Iteration 383, loss = 0.05542531\n",
      "Iteration 384, loss = 0.05532354\n",
      "Iteration 385, loss = 0.05522239\n",
      "Iteration 386, loss = 0.05512184\n",
      "Iteration 387, loss = 0.05502190\n",
      "Iteration 388, loss = 0.05492255\n",
      "Iteration 389, loss = 0.05482379\n",
      "Iteration 390, loss = 0.05472562\n",
      "Iteration 391, loss = 0.05462803\n",
      "Iteration 392, loss = 0.05453101\n",
      "Iteration 393, loss = 0.05443456\n",
      "Iteration 394, loss = 0.05433867\n",
      "Iteration 395, loss = 0.05424335\n",
      "Iteration 396, loss = 0.05414858\n",
      "Iteration 397, loss = 0.05405435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52120330\n",
      "Iteration 2, loss = 1.25876057\n",
      "Iteration 3, loss = 1.07438653\n",
      "Iteration 4, loss = 0.94009858\n",
      "Iteration 5, loss = 0.85127810\n",
      "Iteration 6, loss = 0.81726426\n",
      "Iteration 7, loss = 0.82697664\n",
      "Iteration 8, loss = 0.83829536\n",
      "Iteration 9, loss = 0.82485587\n",
      "Iteration 10, loss = 0.80258125\n",
      "Iteration 11, loss = 0.78090832\n",
      "Iteration 12, loss = 0.75870748\n",
      "Iteration 13, loss = 0.73613263\n",
      "Iteration 14, loss = 0.71591907\n",
      "Iteration 15, loss = 0.69968262\n",
      "Iteration 16, loss = 0.68528664\n",
      "Iteration 17, loss = 0.67193233\n",
      "Iteration 18, loss = 0.65947462\n",
      "Iteration 19, loss = 0.64700583\n",
      "Iteration 20, loss = 0.63425128\n",
      "Iteration 21, loss = 0.62103283\n",
      "Iteration 22, loss = 0.60791596\n",
      "Iteration 23, loss = 0.59569361\n",
      "Iteration 24, loss = 0.58510904\n",
      "Iteration 25, loss = 0.57550745\n",
      "Iteration 26, loss = 0.56656903\n",
      "Iteration 27, loss = 0.55810944\n",
      "Iteration 28, loss = 0.54991226\n",
      "Iteration 29, loss = 0.54192178\n",
      "Iteration 30, loss = 0.53415019\n",
      "Iteration 31, loss = 0.52666131\n",
      "Iteration 32, loss = 0.51946850\n",
      "Iteration 33, loss = 0.51256277\n",
      "Iteration 34, loss = 0.50592603\n",
      "Iteration 35, loss = 0.49954795\n",
      "Iteration 36, loss = 0.49338613\n",
      "Iteration 37, loss = 0.48741327\n",
      "Iteration 38, loss = 0.48160554\n",
      "Iteration 39, loss = 0.47595913\n",
      "Iteration 40, loss = 0.47046362\n",
      "Iteration 41, loss = 0.46510860\n",
      "Iteration 42, loss = 0.45989251\n",
      "Iteration 43, loss = 0.45480767\n",
      "Iteration 44, loss = 0.44984625\n",
      "Iteration 45, loss = 0.44499974\n",
      "Iteration 46, loss = 0.44025220\n",
      "Iteration 47, loss = 0.43559397\n",
      "Iteration 48, loss = 0.43102202\n",
      "Iteration 49, loss = 0.42655022\n",
      "Iteration 50, loss = 0.42219263\n",
      "Iteration 51, loss = 0.41791078\n",
      "Iteration 52, loss = 0.41369859\n",
      "Iteration 53, loss = 0.40954787\n",
      "Iteration 54, loss = 0.40545104\n",
      "Iteration 55, loss = 0.40140429\n",
      "Iteration 56, loss = 0.39741012\n",
      "Iteration 57, loss = 0.39345967\n",
      "Iteration 58, loss = 0.38955238\n",
      "Iteration 59, loss = 0.38568858\n",
      "Iteration 60, loss = 0.38186219\n",
      "Iteration 61, loss = 0.37807174\n",
      "Iteration 62, loss = 0.37431597\n",
      "Iteration 63, loss = 0.37059552\n",
      "Iteration 64, loss = 0.36691051\n",
      "Iteration 65, loss = 0.36325753\n",
      "Iteration 66, loss = 0.35963865\n",
      "Iteration 67, loss = 0.35605342\n",
      "Iteration 68, loss = 0.35250225\n",
      "Iteration 69, loss = 0.34898080\n",
      "Iteration 70, loss = 0.34548866\n",
      "Iteration 71, loss = 0.34202551\n",
      "Iteration 72, loss = 0.33859113\n",
      "Iteration 73, loss = 0.33518538\n",
      "Iteration 74, loss = 0.33180816\n",
      "Iteration 75, loss = 0.32845942\n",
      "Iteration 76, loss = 0.32513920\n",
      "Iteration 77, loss = 0.32184756\n",
      "Iteration 78, loss = 0.31858462\n",
      "Iteration 79, loss = 0.31535051\n",
      "Iteration 80, loss = 0.31214543\n",
      "Iteration 81, loss = 0.30896957\n",
      "Iteration 82, loss = 0.30582650\n",
      "Iteration 83, loss = 0.30271364\n",
      "Iteration 84, loss = 0.29963102\n",
      "Iteration 85, loss = 0.29657889\n",
      "Iteration 86, loss = 0.29355749\n",
      "Iteration 87, loss = 0.29056710\n",
      "Iteration 88, loss = 0.28760799\n",
      "Iteration 89, loss = 0.28468042\n",
      "Iteration 90, loss = 0.28178467\n",
      "Iteration 91, loss = 0.27892098\n",
      "Iteration 92, loss = 0.27608961\n",
      "Iteration 93, loss = 0.27329081\n",
      "Iteration 94, loss = 0.27052479\n",
      "Iteration 95, loss = 0.26779177\n",
      "Iteration 96, loss = 0.26509193\n",
      "Iteration 97, loss = 0.26242545\n",
      "Iteration 98, loss = 0.25979248\n",
      "Iteration 99, loss = 0.25719315\n",
      "Iteration 100, loss = 0.25462756\n",
      "Iteration 101, loss = 0.25209581\n",
      "Iteration 102, loss = 0.24959795\n",
      "Iteration 103, loss = 0.24713401\n",
      "Iteration 104, loss = 0.24470402\n",
      "Iteration 105, loss = 0.24230794\n",
      "Iteration 106, loss = 0.23994576\n",
      "Iteration 107, loss = 0.23761742\n",
      "Iteration 108, loss = 0.23532282\n",
      "Iteration 109, loss = 0.23306186\n",
      "Iteration 110, loss = 0.23083442\n",
      "Iteration 111, loss = 0.22864036\n",
      "Iteration 112, loss = 0.22647949\n",
      "Iteration 113, loss = 0.22435164\n",
      "Iteration 114, loss = 0.22225659\n",
      "Iteration 115, loss = 0.22019412\n",
      "Iteration 116, loss = 0.21816398\n",
      "Iteration 117, loss = 0.21616592\n",
      "Iteration 118, loss = 0.21419966\n",
      "Iteration 119, loss = 0.21226490\n",
      "Iteration 120, loss = 0.21036136\n",
      "Iteration 121, loss = 0.20848870\n",
      "Iteration 122, loss = 0.20664660\n",
      "Iteration 123, loss = 0.20483472\n",
      "Iteration 124, loss = 0.20305271\n",
      "Iteration 125, loss = 0.20130022\n",
      "Iteration 126, loss = 0.19957710\n",
      "Iteration 127, loss = 0.19788311\n",
      "Iteration 128, loss = 0.19621752\n",
      "Iteration 129, loss = 0.19457992\n",
      "Iteration 130, loss = 0.19296992\n",
      "Iteration 131, loss = 0.19138712\n",
      "Iteration 132, loss = 0.18983112\n",
      "Iteration 133, loss = 0.18830153\n",
      "Iteration 134, loss = 0.18679793\n",
      "Iteration 135, loss = 0.18531992\n",
      "Iteration 136, loss = 0.18386710\n",
      "Iteration 137, loss = 0.18243906\n",
      "Iteration 138, loss = 0.18103539\n",
      "Iteration 139, loss = 0.17965569\n",
      "Iteration 140, loss = 0.17829956\n",
      "Iteration 141, loss = 0.17696659\n",
      "Iteration 142, loss = 0.17565639\n",
      "Iteration 143, loss = 0.17436871\n",
      "Iteration 144, loss = 0.17310305\n",
      "Iteration 145, loss = 0.17185896\n",
      "Iteration 146, loss = 0.17063606\n",
      "Iteration 147, loss = 0.16943395\n",
      "Iteration 148, loss = 0.16825227\n",
      "Iteration 149, loss = 0.16709063\n",
      "Iteration 150, loss = 0.16594866\n",
      "Iteration 151, loss = 0.16482600\n",
      "Iteration 152, loss = 0.16372228\n",
      "Iteration 153, loss = 0.16263716\n",
      "Iteration 154, loss = 0.16157027\n",
      "Iteration 155, loss = 0.16052126\n",
      "Iteration 156, loss = 0.15948980\n",
      "Iteration 157, loss = 0.15847555\n",
      "Iteration 158, loss = 0.15747818\n",
      "Iteration 159, loss = 0.15649736\n",
      "Iteration 160, loss = 0.15553277\n",
      "Iteration 161, loss = 0.15458410\n",
      "Iteration 162, loss = 0.15365102\n",
      "Iteration 163, loss = 0.15273325\n",
      "Iteration 164, loss = 0.15183047\n",
      "Iteration 165, loss = 0.15094240\n",
      "Iteration 166, loss = 0.15006875\n",
      "Iteration 167, loss = 0.14920922\n",
      "Iteration 168, loss = 0.14836355\n",
      "Iteration 169, loss = 0.14753147\n",
      "Iteration 170, loss = 0.14671269\n",
      "Iteration 171, loss = 0.14590697\n",
      "Iteration 172, loss = 0.14511403\n",
      "Iteration 173, loss = 0.14433364\n",
      "Iteration 174, loss = 0.14356555\n",
      "Iteration 175, loss = 0.14280950\n",
      "Iteration 176, loss = 0.14206527\n",
      "Iteration 177, loss = 0.14133261\n",
      "Iteration 178, loss = 0.14061131\n",
      "Iteration 179, loss = 0.13990114\n",
      "Iteration 180, loss = 0.13920188\n",
      "Iteration 181, loss = 0.13851332\n",
      "Iteration 182, loss = 0.13783524\n",
      "Iteration 183, loss = 0.13716745\n",
      "Iteration 184, loss = 0.13650974\n",
      "Iteration 185, loss = 0.13586191\n",
      "Iteration 186, loss = 0.13522377\n",
      "Iteration 187, loss = 0.13459514\n",
      "Iteration 188, loss = 0.13397583\n",
      "Iteration 189, loss = 0.13336566\n",
      "Iteration 190, loss = 0.13276446\n",
      "Iteration 191, loss = 0.13217207\n",
      "Iteration 192, loss = 0.13158832\n",
      "Iteration 193, loss = 0.13101303\n",
      "Iteration 194, loss = 0.13044603\n",
      "Iteration 195, loss = 0.12988717\n",
      "Iteration 196, loss = 0.12933630\n",
      "Iteration 197, loss = 0.12879326\n",
      "Iteration 198, loss = 0.12825790\n",
      "Iteration 199, loss = 0.12773009\n",
      "Iteration 200, loss = 0.12720967\n",
      "Iteration 201, loss = 0.12669651\n",
      "Iteration 202, loss = 0.12619047\n",
      "Iteration 203, loss = 0.12569147\n",
      "Iteration 204, loss = 0.12519933\n",
      "Iteration 205, loss = 0.12471392\n",
      "Iteration 206, loss = 0.12423511\n",
      "Iteration 207, loss = 0.12376280\n",
      "Iteration 208, loss = 0.12329685\n",
      "Iteration 209, loss = 0.12283716\n",
      "Iteration 210, loss = 0.12238361\n",
      "Iteration 211, loss = 0.12193608\n",
      "Iteration 212, loss = 0.12149448\n",
      "Iteration 213, loss = 0.12105869\n",
      "Iteration 214, loss = 0.12062861\n",
      "Iteration 215, loss = 0.12020415\n",
      "Iteration 216, loss = 0.11978521\n",
      "Iteration 217, loss = 0.11937167\n",
      "Iteration 218, loss = 0.11896345\n",
      "Iteration 219, loss = 0.11856045\n",
      "Iteration 220, loss = 0.11816257\n",
      "Iteration 221, loss = 0.11776972\n",
      "Iteration 222, loss = 0.11738183\n",
      "Iteration 223, loss = 0.11699880\n",
      "Iteration 224, loss = 0.11662055\n",
      "Iteration 225, loss = 0.11624699\n",
      "Iteration 226, loss = 0.11587805\n",
      "Iteration 227, loss = 0.11551364\n",
      "Iteration 228, loss = 0.11515369\n",
      "Iteration 229, loss = 0.11479814\n",
      "Iteration 230, loss = 0.11444689\n",
      "Iteration 231, loss = 0.11409988\n",
      "Iteration 232, loss = 0.11375703\n",
      "Iteration 233, loss = 0.11341828\n",
      "Iteration 234, loss = 0.11308356\n",
      "Iteration 235, loss = 0.11275279\n",
      "Iteration 236, loss = 0.11242593\n",
      "Iteration 237, loss = 0.11210289\n",
      "Iteration 238, loss = 0.11178362\n",
      "Iteration 239, loss = 0.11146806\n",
      "Iteration 240, loss = 0.11115614\n",
      "Iteration 241, loss = 0.11084781\n",
      "Iteration 242, loss = 0.11054301\n",
      "Iteration 243, loss = 0.11024169\n",
      "Iteration 244, loss = 0.10994378\n",
      "Iteration 245, loss = 0.10964923\n",
      "Iteration 246, loss = 0.10935799\n",
      "Iteration 247, loss = 0.10907001\n",
      "Iteration 248, loss = 0.10878523\n",
      "Iteration 249, loss = 0.10850361\n",
      "Iteration 250, loss = 0.10822509\n",
      "Iteration 251, loss = 0.10794963\n",
      "Iteration 252, loss = 0.10767718\n",
      "Iteration 253, loss = 0.10740769\n",
      "Iteration 254, loss = 0.10714112\n",
      "Iteration 255, loss = 0.10687742\n",
      "Iteration 256, loss = 0.10661655\n",
      "Iteration 257, loss = 0.10635847\n",
      "Iteration 258, loss = 0.10610313\n",
      "Iteration 259, loss = 0.10585049\n",
      "Iteration 260, loss = 0.10560051\n",
      "Iteration 261, loss = 0.10535315\n",
      "Iteration 262, loss = 0.10510837\n",
      "Iteration 263, loss = 0.10486614\n",
      "Iteration 264, loss = 0.10462641\n",
      "Iteration 265, loss = 0.10438915\n",
      "Iteration 266, loss = 0.10415433\n",
      "Iteration 267, loss = 0.10392190\n",
      "Iteration 268, loss = 0.10369183\n",
      "Iteration 269, loss = 0.10346409\n",
      "Iteration 270, loss = 0.10323864\n",
      "Iteration 271, loss = 0.10301545\n",
      "Iteration 272, loss = 0.10279450\n",
      "Iteration 273, loss = 0.10257573\n",
      "Iteration 274, loss = 0.10235914\n",
      "Iteration 275, loss = 0.10214467\n",
      "Iteration 276, loss = 0.10193231\n",
      "Iteration 277, loss = 0.10172203\n",
      "Iteration 278, loss = 0.10151379\n",
      "Iteration 279, loss = 0.10130756\n",
      "Iteration 280, loss = 0.10110333\n",
      "Iteration 281, loss = 0.10090105\n",
      "Iteration 282, loss = 0.10070071\n",
      "Iteration 283, loss = 0.10050228\n",
      "Iteration 284, loss = 0.10030573\n",
      "Iteration 285, loss = 0.10011103\n",
      "Iteration 286, loss = 0.09991816\n",
      "Iteration 287, loss = 0.09972710\n",
      "Iteration 288, loss = 0.09953782\n",
      "Iteration 289, loss = 0.09935029\n",
      "Iteration 290, loss = 0.09916450\n",
      "Iteration 291, loss = 0.09898041\n",
      "Iteration 292, loss = 0.09879802\n",
      "Iteration 293, loss = 0.09861729\n",
      "Iteration 294, loss = 0.09843820\n",
      "Iteration 295, loss = 0.09826073\n",
      "Iteration 296, loss = 0.09808486\n",
      "Iteration 297, loss = 0.09791057\n",
      "Iteration 298, loss = 0.09773784\n",
      "Iteration 299, loss = 0.09756665\n",
      "Iteration 300, loss = 0.09739698\n",
      "Iteration 301, loss = 0.09722880\n",
      "Iteration 302, loss = 0.09706210\n",
      "Iteration 303, loss = 0.09689687\n",
      "Iteration 304, loss = 0.09673307\n",
      "Iteration 305, loss = 0.09657070\n",
      "Iteration 306, loss = 0.09640974\n",
      "Iteration 307, loss = 0.09625016\n",
      "Iteration 308, loss = 0.09609195\n",
      "Iteration 309, loss = 0.09593510\n",
      "Iteration 310, loss = 0.09577958\n",
      "Iteration 311, loss = 0.09562539\n",
      "Iteration 312, loss = 0.09547249\n",
      "Iteration 313, loss = 0.09532089\n",
      "Iteration 314, loss = 0.09517055\n",
      "Iteration 315, loss = 0.09502147\n",
      "Iteration 316, loss = 0.09487363\n",
      "Iteration 317, loss = 0.09472702\n",
      "Iteration 318, loss = 0.09458162\n",
      "Iteration 319, loss = 0.09443741\n",
      "Iteration 320, loss = 0.09429438\n",
      "Iteration 321, loss = 0.09415253\n",
      "Iteration 322, loss = 0.09401182\n",
      "Iteration 323, loss = 0.09387226\n",
      "Iteration 324, loss = 0.09373382\n",
      "Iteration 325, loss = 0.09359649\n",
      "Iteration 326, loss = 0.09346027\n",
      "Iteration 327, loss = 0.09332513\n",
      "Iteration 328, loss = 0.09319107\n",
      "Iteration 329, loss = 0.09305806\n",
      "Iteration 330, loss = 0.09292611\n",
      "Iteration 331, loss = 0.09279519\n",
      "Iteration 332, loss = 0.09266530\n",
      "Iteration 333, loss = 0.09253642\n",
      "Iteration 334, loss = 0.09240854\n",
      "Iteration 335, loss = 0.09228166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 336, loss = 0.09215575\n",
      "Iteration 337, loss = 0.09203081\n",
      "Iteration 338, loss = 0.09190683\n",
      "Iteration 339, loss = 0.09178379\n",
      "Iteration 340, loss = 0.09166169\n",
      "Iteration 341, loss = 0.09154051\n",
      "Iteration 342, loss = 0.09142025\n",
      "Iteration 343, loss = 0.09130089\n",
      "Iteration 344, loss = 0.09118243\n",
      "Iteration 345, loss = 0.09106485\n",
      "Iteration 346, loss = 0.09094814\n",
      "Iteration 347, loss = 0.09083230\n",
      "Iteration 348, loss = 0.09071732\n",
      "Iteration 349, loss = 0.09060318\n",
      "Iteration 350, loss = 0.09048988\n",
      "Iteration 351, loss = 0.09037741\n",
      "Iteration 352, loss = 0.09026575\n",
      "Iteration 353, loss = 0.09015491\n",
      "Iteration 354, loss = 0.09004487\n",
      "Iteration 355, loss = 0.08993562\n",
      "Iteration 356, loss = 0.08982715\n",
      "Iteration 357, loss = 0.08971947\n",
      "Iteration 358, loss = 0.08961254\n",
      "Iteration 359, loss = 0.08950638\n",
      "Iteration 360, loss = 0.08940097\n",
      "Iteration 361, loss = 0.08929631\n",
      "Iteration 362, loss = 0.08919238\n",
      "Iteration 363, loss = 0.08908918\n",
      "Iteration 364, loss = 0.08898670\n",
      "Iteration 365, loss = 0.08888493\n",
      "Iteration 366, loss = 0.08878387\n",
      "Iteration 367, loss = 0.08868350\n",
      "Iteration 368, loss = 0.08858383\n",
      "Iteration 369, loss = 0.08848484\n",
      "Iteration 370, loss = 0.08838653\n",
      "Iteration 371, loss = 0.08828889\n",
      "Iteration 372, loss = 0.08819192\n",
      "Iteration 373, loss = 0.08809560\n",
      "Iteration 374, loss = 0.08799993\n",
      "Iteration 375, loss = 0.08790490\n",
      "Iteration 376, loss = 0.08781052\n",
      "Iteration 377, loss = 0.08771676\n",
      "Iteration 378, loss = 0.08762362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50048972\n",
      "Iteration 2, loss = 1.48355398\n",
      "Iteration 3, loss = 1.46689210\n",
      "Iteration 4, loss = 1.45050551\n",
      "Iteration 5, loss = 1.43441096\n",
      "Iteration 6, loss = 1.41860439\n",
      "Iteration 7, loss = 1.40303736\n",
      "Iteration 8, loss = 1.38773124\n",
      "Iteration 9, loss = 1.37272443\n",
      "Iteration 10, loss = 1.35794509\n",
      "Iteration 11, loss = 1.34338762\n",
      "Iteration 12, loss = 1.32910968\n",
      "Iteration 13, loss = 1.31507103\n",
      "Iteration 14, loss = 1.30128492\n",
      "Iteration 15, loss = 1.28775872\n",
      "Iteration 16, loss = 1.27449084\n",
      "Iteration 17, loss = 1.26146594\n",
      "Iteration 18, loss = 1.24865694\n",
      "Iteration 19, loss = 1.23605455\n",
      "Iteration 20, loss = 1.22366408\n",
      "Iteration 21, loss = 1.21152881\n",
      "Iteration 22, loss = 1.19962433\n",
      "Iteration 23, loss = 1.18794430\n",
      "Iteration 24, loss = 1.17644404\n",
      "Iteration 25, loss = 1.16511882\n",
      "Iteration 26, loss = 1.15397796\n",
      "Iteration 27, loss = 1.14302333\n",
      "Iteration 28, loss = 1.13225214\n",
      "Iteration 29, loss = 1.12167991\n",
      "Iteration 30, loss = 1.11127171\n",
      "Iteration 31, loss = 1.10104357\n",
      "Iteration 32, loss = 1.09100722\n",
      "Iteration 33, loss = 1.08114377\n",
      "Iteration 34, loss = 1.07145123\n",
      "Iteration 35, loss = 1.06190306\n",
      "Iteration 36, loss = 1.05251305\n",
      "Iteration 37, loss = 1.04328602\n",
      "Iteration 38, loss = 1.03422326\n",
      "Iteration 39, loss = 1.02529602\n",
      "Iteration 40, loss = 1.01650350\n",
      "Iteration 41, loss = 1.00784827\n",
      "Iteration 42, loss = 0.99933457\n",
      "Iteration 43, loss = 0.99097341\n",
      "Iteration 44, loss = 0.98277649\n",
      "Iteration 45, loss = 0.97472455\n",
      "Iteration 46, loss = 0.96680304\n",
      "Iteration 47, loss = 0.95901081\n",
      "Iteration 48, loss = 0.95135315\n",
      "Iteration 49, loss = 0.94386582\n",
      "Iteration 50, loss = 0.93652474\n",
      "Iteration 51, loss = 0.92936345\n",
      "Iteration 52, loss = 0.92235677\n",
      "Iteration 53, loss = 0.91551327\n",
      "Iteration 54, loss = 0.90886065\n",
      "Iteration 55, loss = 0.90238233\n",
      "Iteration 56, loss = 0.89605827\n",
      "Iteration 57, loss = 0.88992911\n",
      "Iteration 58, loss = 0.88399470\n",
      "Iteration 59, loss = 0.87821588\n",
      "Iteration 60, loss = 0.87258781\n",
      "Iteration 61, loss = 0.86713358\n",
      "Iteration 62, loss = 0.86185245\n",
      "Iteration 63, loss = 0.85674258\n",
      "Iteration 64, loss = 0.85178651\n",
      "Iteration 65, loss = 0.84702527\n",
      "Iteration 66, loss = 0.84241683\n",
      "Iteration 67, loss = 0.83799252\n",
      "Iteration 68, loss = 0.83372554\n",
      "Iteration 69, loss = 0.82962197\n",
      "Iteration 70, loss = 0.82569619\n",
      "Iteration 71, loss = 0.82193645\n",
      "Iteration 72, loss = 0.81833078\n",
      "Iteration 73, loss = 0.81489317\n",
      "Iteration 74, loss = 0.81159226\n",
      "Iteration 75, loss = 0.80843295\n",
      "Iteration 76, loss = 0.80541490\n",
      "Iteration 77, loss = 0.80251296\n",
      "Iteration 78, loss = 0.79972023\n",
      "Iteration 79, loss = 0.79704469\n",
      "Iteration 80, loss = 0.79449350\n",
      "Iteration 81, loss = 0.79204156\n",
      "Iteration 82, loss = 0.78968178\n",
      "Iteration 83, loss = 0.78740992\n",
      "Iteration 84, loss = 0.78523508\n",
      "Iteration 85, loss = 0.78314440\n",
      "Iteration 86, loss = 0.78114201\n",
      "Iteration 87, loss = 0.77922413\n",
      "Iteration 88, loss = 0.77737512\n",
      "Iteration 89, loss = 0.77558918\n",
      "Iteration 90, loss = 0.77386232\n",
      "Iteration 91, loss = 0.77219067\n",
      "Iteration 92, loss = 0.77057049\n",
      "Iteration 93, loss = 0.76899816\n",
      "Iteration 94, loss = 0.76747020\n",
      "Iteration 95, loss = 0.76598325\n",
      "Iteration 96, loss = 0.76453413\n",
      "Iteration 97, loss = 0.76311979\n",
      "Iteration 98, loss = 0.76174267\n",
      "Iteration 99, loss = 0.76040092\n",
      "Iteration 100, loss = 0.75909361\n",
      "Iteration 101, loss = 0.75782321\n",
      "Iteration 102, loss = 0.75659890\n",
      "Iteration 103, loss = 0.75539894\n",
      "Iteration 104, loss = 0.75422081\n",
      "Iteration 105, loss = 0.75306370\n",
      "Iteration 106, loss = 0.75193149\n",
      "Iteration 107, loss = 0.75081608\n",
      "Iteration 108, loss = 0.74971583\n",
      "Iteration 109, loss = 0.74862925\n",
      "Iteration 110, loss = 0.74755496\n",
      "Iteration 111, loss = 0.74649166\n",
      "Iteration 112, loss = 0.74544347\n",
      "Iteration 113, loss = 0.74440495\n",
      "Iteration 114, loss = 0.74337901\n",
      "Iteration 115, loss = 0.74236310\n",
      "Iteration 116, loss = 0.74135404\n",
      "Iteration 117, loss = 0.74035325\n",
      "Iteration 118, loss = 0.73936722\n",
      "Iteration 119, loss = 0.73838825\n",
      "Iteration 120, loss = 0.73741407\n",
      "Iteration 121, loss = 0.73644413\n",
      "Iteration 122, loss = 0.73547838\n",
      "Iteration 123, loss = 0.73452258\n",
      "Iteration 124, loss = 0.73357518\n",
      "Iteration 125, loss = 0.73263210\n",
      "Iteration 126, loss = 0.73169482\n",
      "Iteration 127, loss = 0.73075988\n",
      "Iteration 128, loss = 0.72982923\n",
      "Iteration 129, loss = 0.72890408\n",
      "Iteration 130, loss = 0.72798617\n",
      "Iteration 131, loss = 0.72707067\n",
      "Iteration 132, loss = 0.72615732\n",
      "Iteration 133, loss = 0.72524905\n",
      "Iteration 134, loss = 0.72434415\n",
      "Iteration 135, loss = 0.72344234\n",
      "Iteration 136, loss = 0.72254494\n",
      "Iteration 137, loss = 0.72164921\n",
      "Iteration 138, loss = 0.72075502\n",
      "Iteration 139, loss = 0.71986223\n",
      "Iteration 140, loss = 0.71897074\n",
      "Iteration 141, loss = 0.71808171\n",
      "Iteration 142, loss = 0.71719480\n",
      "Iteration 143, loss = 0.71631036\n",
      "Iteration 144, loss = 0.71543031\n",
      "Iteration 145, loss = 0.71455288\n",
      "Iteration 146, loss = 0.71367652\n",
      "Iteration 147, loss = 0.71280308\n",
      "Iteration 148, loss = 0.71193260\n",
      "Iteration 149, loss = 0.71106320\n",
      "Iteration 150, loss = 0.71019481\n",
      "Iteration 151, loss = 0.70932739\n",
      "Iteration 152, loss = 0.70846090\n",
      "Iteration 153, loss = 0.70759529\n",
      "Iteration 154, loss = 0.70673055\n",
      "Iteration 155, loss = 0.70586664\n",
      "Iteration 156, loss = 0.70500353\n",
      "Iteration 157, loss = 0.70414121\n",
      "Iteration 158, loss = 0.70327965\n",
      "Iteration 159, loss = 0.70241884\n",
      "Iteration 160, loss = 0.70155877\n",
      "Iteration 161, loss = 0.70070183\n",
      "Iteration 162, loss = 0.69984645\n",
      "Iteration 163, loss = 0.69899194\n",
      "Iteration 164, loss = 0.69813830\n",
      "Iteration 165, loss = 0.69728550\n",
      "Iteration 166, loss = 0.69643355\n",
      "Iteration 167, loss = 0.69558243\n",
      "Iteration 168, loss = 0.69473215\n",
      "Iteration 169, loss = 0.69388271\n",
      "Iteration 170, loss = 0.69303409\n",
      "Iteration 171, loss = 0.69218630\n",
      "Iteration 172, loss = 0.69133934\n",
      "Iteration 173, loss = 0.69049434\n",
      "Iteration 174, loss = 0.68965124\n",
      "Iteration 175, loss = 0.68880907\n",
      "Iteration 176, loss = 0.68796784\n",
      "Iteration 177, loss = 0.68712755\n",
      "Iteration 178, loss = 0.68628821\n",
      "Iteration 179, loss = 0.68544981\n",
      "Iteration 180, loss = 0.68461236\n",
      "Iteration 181, loss = 0.68377587\n",
      "Iteration 182, loss = 0.68294034\n",
      "Iteration 183, loss = 0.68210577\n",
      "Iteration 184, loss = 0.68127216\n",
      "Iteration 185, loss = 0.68043953\n",
      "Iteration 186, loss = 0.67960788\n",
      "Iteration 187, loss = 0.67877721\n",
      "Iteration 188, loss = 0.67794752\n",
      "Iteration 189, loss = 0.67711962\n",
      "Iteration 190, loss = 0.67629369\n",
      "Iteration 191, loss = 0.67547015\n",
      "Iteration 192, loss = 0.67464783\n",
      "Iteration 193, loss = 0.67382655\n",
      "Iteration 194, loss = 0.67300636\n",
      "Iteration 195, loss = 0.67218727\n",
      "Iteration 196, loss = 0.67136931\n",
      "Iteration 197, loss = 0.67055248\n",
      "Iteration 198, loss = 0.66973682\n",
      "Iteration 199, loss = 0.66892233\n",
      "Iteration 200, loss = 0.66810903\n",
      "Iteration 201, loss = 0.66729722\n",
      "Iteration 202, loss = 0.66648677\n",
      "Iteration 203, loss = 0.66567747\n",
      "Iteration 204, loss = 0.66486937\n",
      "Iteration 205, loss = 0.66406248\n",
      "Iteration 206, loss = 0.66325683\n",
      "Iteration 207, loss = 0.66245244\n",
      "Iteration 208, loss = 0.66164934\n",
      "Iteration 209, loss = 0.66084754\n",
      "Iteration 210, loss = 0.66004706\n",
      "Iteration 211, loss = 0.65924791\n",
      "Iteration 212, loss = 0.65845010\n",
      "Iteration 213, loss = 0.65765364\n",
      "Iteration 214, loss = 0.65685855\n",
      "Iteration 215, loss = 0.65606482\n",
      "Iteration 216, loss = 0.65527261\n",
      "Iteration 217, loss = 0.65448236\n",
      "Iteration 218, loss = 0.65369347\n",
      "Iteration 219, loss = 0.65290594\n",
      "Iteration 220, loss = 0.65211979\n",
      "Iteration 221, loss = 0.65133503\n",
      "Iteration 222, loss = 0.65055166\n",
      "Iteration 223, loss = 0.64976970\n",
      "Iteration 224, loss = 0.64898917\n",
      "Iteration 225, loss = 0.64821006\n",
      "Iteration 226, loss = 0.64743239\n",
      "Iteration 227, loss = 0.64665617\n",
      "Iteration 228, loss = 0.64588140\n",
      "Iteration 229, loss = 0.64510809\n",
      "Iteration 230, loss = 0.64433625\n",
      "Iteration 231, loss = 0.64356589\n",
      "Iteration 232, loss = 0.64279700\n",
      "Iteration 233, loss = 0.64202960\n",
      "Iteration 234, loss = 0.64126368\n",
      "Iteration 235, loss = 0.64049927\n",
      "Iteration 236, loss = 0.63973635\n",
      "Iteration 237, loss = 0.63897494\n",
      "Iteration 238, loss = 0.63821503\n",
      "Iteration 239, loss = 0.63745665\n",
      "Iteration 240, loss = 0.63669978\n",
      "Iteration 241, loss = 0.63594444\n",
      "Iteration 242, loss = 0.63519062\n",
      "Iteration 243, loss = 0.63443835\n",
      "Iteration 244, loss = 0.63368761\n",
      "Iteration 245, loss = 0.63293842\n",
      "Iteration 246, loss = 0.63219078\n",
      "Iteration 247, loss = 0.63144470\n",
      "Iteration 248, loss = 0.63070018\n",
      "Iteration 249, loss = 0.62995723\n",
      "Iteration 250, loss = 0.62921585\n",
      "Iteration 251, loss = 0.62847605\n",
      "Iteration 252, loss = 0.62773783\n",
      "Iteration 253, loss = 0.62700120\n",
      "Iteration 254, loss = 0.62626616\n",
      "Iteration 255, loss = 0.62553285\n",
      "Iteration 256, loss = 0.62480159\n",
      "Iteration 257, loss = 0.62407195\n",
      "Iteration 258, loss = 0.62334393\n",
      "Iteration 259, loss = 0.62261754\n",
      "Iteration 260, loss = 0.62189279\n",
      "Iteration 261, loss = 0.62116969\n",
      "Iteration 262, loss = 0.62044823\n",
      "Iteration 263, loss = 0.61972843\n",
      "Iteration 264, loss = 0.61901030\n",
      "Iteration 265, loss = 0.61829383\n",
      "Iteration 266, loss = 0.61757904\n",
      "Iteration 267, loss = 0.61686592\n",
      "Iteration 268, loss = 0.61615448\n",
      "Iteration 269, loss = 0.61544473\n",
      "Iteration 270, loss = 0.61473666\n",
      "Iteration 271, loss = 0.61403029\n",
      "Iteration 272, loss = 0.61332560\n",
      "Iteration 273, loss = 0.61262261\n",
      "Iteration 274, loss = 0.61192131\n",
      "Iteration 275, loss = 0.61122170\n",
      "Iteration 276, loss = 0.61052380\n",
      "Iteration 277, loss = 0.60982759\n",
      "Iteration 278, loss = 0.60913308\n",
      "Iteration 279, loss = 0.60844026\n",
      "Iteration 280, loss = 0.60774915\n",
      "Iteration 281, loss = 0.60705974\n",
      "Iteration 282, loss = 0.60637202\n",
      "Iteration 283, loss = 0.60568601\n",
      "Iteration 284, loss = 0.60500169\n",
      "Iteration 285, loss = 0.60431907\n",
      "Iteration 286, loss = 0.60363815\n",
      "Iteration 287, loss = 0.60295893\n",
      "Iteration 288, loss = 0.60228140\n",
      "Iteration 289, loss = 0.60160556\n",
      "Iteration 290, loss = 0.60093143\n",
      "Iteration 291, loss = 0.60025898\n",
      "Iteration 292, loss = 0.59958823\n",
      "Iteration 293, loss = 0.59891917\n",
      "Iteration 294, loss = 0.59825180\n",
      "Iteration 295, loss = 0.59758611\n",
      "Iteration 296, loss = 0.59692212\n",
      "Iteration 297, loss = 0.59625981\n",
      "Iteration 298, loss = 0.59559918\n",
      "Iteration 299, loss = 0.59494024\n",
      "Iteration 300, loss = 0.59428297\n",
      "Iteration 301, loss = 0.59362738\n",
      "Iteration 302, loss = 0.59297347\n",
      "Iteration 303, loss = 0.59232124\n",
      "Iteration 304, loss = 0.59167067\n",
      "Iteration 305, loss = 0.59102177\n",
      "Iteration 306, loss = 0.59037455\n",
      "Iteration 307, loss = 0.58972898\n",
      "Iteration 308, loss = 0.58908508\n",
      "Iteration 309, loss = 0.58844284\n",
      "Iteration 310, loss = 0.58780226\n",
      "Iteration 311, loss = 0.58716333\n",
      "Iteration 312, loss = 0.58652606\n",
      "Iteration 313, loss = 0.58589043\n",
      "Iteration 314, loss = 0.58525645\n",
      "Iteration 315, loss = 0.58462411\n",
      "Iteration 316, loss = 0.58399341\n",
      "Iteration 317, loss = 0.58336435\n",
      "Iteration 318, loss = 0.58273692\n",
      "Iteration 319, loss = 0.58211112\n",
      "Iteration 320, loss = 0.58148695\n",
      "Iteration 321, loss = 0.58086440\n",
      "Iteration 322, loss = 0.58024347\n",
      "Iteration 323, loss = 0.57962416\n",
      "Iteration 324, loss = 0.57900646\n",
      "Iteration 325, loss = 0.57839083\n",
      "Iteration 326, loss = 0.57777717\n",
      "Iteration 327, loss = 0.57716514\n",
      "Iteration 328, loss = 0.57655475\n",
      "Iteration 329, loss = 0.57594598\n",
      "Iteration 330, loss = 0.57533885\n",
      "Iteration 331, loss = 0.57473334\n",
      "Iteration 332, loss = 0.57412975\n",
      "Iteration 333, loss = 0.57352798\n",
      "Iteration 334, loss = 0.57292779\n",
      "Iteration 335, loss = 0.57232920\n",
      "Iteration 336, loss = 0.57173221\n",
      "Iteration 337, loss = 0.57113682\n",
      "Iteration 338, loss = 0.57054304\n",
      "Iteration 339, loss = 0.56995088\n",
      "Iteration 340, loss = 0.56936034\n",
      "Iteration 341, loss = 0.56877142\n",
      "Iteration 342, loss = 0.56818411\n",
      "Iteration 343, loss = 0.56759843\n",
      "Iteration 344, loss = 0.56701436\n",
      "Iteration 345, loss = 0.56643190\n",
      "Iteration 346, loss = 0.56585105\n",
      "Iteration 347, loss = 0.56527180\n",
      "Iteration 348, loss = 0.56469415\n",
      "Iteration 349, loss = 0.56411810\n",
      "Iteration 350, loss = 0.56354363\n",
      "Iteration 351, loss = 0.56297073\n",
      "Iteration 352, loss = 0.56239942\n",
      "Iteration 353, loss = 0.56182966\n",
      "Iteration 354, loss = 0.56126147\n",
      "Iteration 355, loss = 0.56069482\n",
      "Iteration 356, loss = 0.56012972\n",
      "Iteration 357, loss = 0.55956615\n",
      "Iteration 358, loss = 0.55900411\n",
      "Iteration 359, loss = 0.55844358\n",
      "Iteration 360, loss = 0.55788457\n",
      "Iteration 361, loss = 0.55732707\n",
      "Iteration 362, loss = 0.55677106\n",
      "Iteration 363, loss = 0.55621654\n",
      "Iteration 364, loss = 0.55566350\n",
      "Iteration 365, loss = 0.55511193\n",
      "Iteration 366, loss = 0.55456232\n",
      "Iteration 367, loss = 0.55401448\n",
      "Iteration 368, loss = 0.55346810\n",
      "Iteration 369, loss = 0.55292317\n",
      "Iteration 370, loss = 0.55237971\n",
      "Iteration 371, loss = 0.55183771\n",
      "Iteration 372, loss = 0.55129717\n",
      "Iteration 373, loss = 0.55075809\n",
      "Iteration 374, loss = 0.55022047\n",
      "Iteration 375, loss = 0.54968431\n",
      "Iteration 376, loss = 0.54914959\n",
      "Iteration 377, loss = 0.54861632\n",
      "Iteration 378, loss = 0.54808449\n",
      "Iteration 379, loss = 0.54755410\n",
      "Iteration 380, loss = 0.54702513\n",
      "Iteration 381, loss = 0.54649758\n",
      "Iteration 382, loss = 0.54597144\n",
      "Iteration 383, loss = 0.54544671\n",
      "Iteration 384, loss = 0.54492337\n",
      "Iteration 385, loss = 0.54440142\n",
      "Iteration 386, loss = 0.54388085\n",
      "Iteration 387, loss = 0.54336166\n",
      "Iteration 388, loss = 0.54284383\n",
      "Iteration 389, loss = 0.54232735\n",
      "Iteration 390, loss = 0.54181223\n",
      "Iteration 391, loss = 0.54129845\n",
      "Iteration 392, loss = 0.54078601\n",
      "Iteration 393, loss = 0.54027489\n",
      "Iteration 394, loss = 0.53976510\n",
      "Iteration 395, loss = 0.53925662\n",
      "Iteration 396, loss = 0.53874945\n",
      "Iteration 397, loss = 0.53824358\n",
      "Iteration 398, loss = 0.53773900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 399, loss = 0.53723571\n",
      "Iteration 400, loss = 0.53673371\n",
      "Iteration 401, loss = 0.53623297\n",
      "Iteration 402, loss = 0.53573350\n",
      "Iteration 403, loss = 0.53523530\n",
      "Iteration 404, loss = 0.53473834\n",
      "Iteration 405, loss = 0.53424263\n",
      "Iteration 406, loss = 0.53374815\n",
      "Iteration 407, loss = 0.53325491\n",
      "Iteration 408, loss = 0.53276293\n",
      "Iteration 409, loss = 0.53227264\n",
      "Iteration 410, loss = 0.53178358\n",
      "Iteration 411, loss = 0.53129575\n",
      "Iteration 412, loss = 0.53080914\n",
      "Iteration 413, loss = 0.53032375\n",
      "Iteration 414, loss = 0.52983957\n",
      "Iteration 415, loss = 0.52935660\n",
      "Iteration 416, loss = 0.52887483\n",
      "Iteration 417, loss = 0.52839426\n",
      "Iteration 418, loss = 0.52791488\n",
      "Iteration 419, loss = 0.52743669\n",
      "Iteration 420, loss = 0.52695967\n",
      "Iteration 421, loss = 0.52648383\n",
      "Iteration 422, loss = 0.52600915\n",
      "Iteration 423, loss = 0.52553563\n",
      "Iteration 424, loss = 0.52506326\n",
      "Iteration 425, loss = 0.52459203\n",
      "Iteration 426, loss = 0.52412195\n",
      "Iteration 427, loss = 0.52365299\n",
      "Iteration 428, loss = 0.52318515\n",
      "Iteration 429, loss = 0.52271842\n",
      "Iteration 430, loss = 0.52225280\n",
      "Iteration 431, loss = 0.52178828\n",
      "Iteration 432, loss = 0.52132486\n",
      "Iteration 433, loss = 0.52086251\n",
      "Iteration 434, loss = 0.52040124\n",
      "Iteration 435, loss = 0.51994104\n",
      "Iteration 436, loss = 0.51948190\n",
      "Iteration 437, loss = 0.51902381\n",
      "Iteration 438, loss = 0.51856677\n",
      "Iteration 439, loss = 0.51811076\n",
      "Iteration 440, loss = 0.51765579\n",
      "Iteration 441, loss = 0.51720184\n",
      "Iteration 442, loss = 0.51674891\n",
      "Iteration 443, loss = 0.51629699\n",
      "Iteration 444, loss = 0.51584607\n",
      "Iteration 445, loss = 0.51539615\n",
      "Iteration 446, loss = 0.51494721\n",
      "Iteration 447, loss = 0.51449926\n",
      "Iteration 448, loss = 0.51405229\n",
      "Iteration 449, loss = 0.51360628\n",
      "Iteration 450, loss = 0.51316123\n",
      "Iteration 451, loss = 0.51271714\n",
      "Iteration 452, loss = 0.51227399\n",
      "Iteration 453, loss = 0.51183179\n",
      "Iteration 454, loss = 0.51139053\n",
      "Iteration 455, loss = 0.51095019\n",
      "Iteration 456, loss = 0.51051077\n",
      "Iteration 457, loss = 0.51007227\n",
      "Iteration 458, loss = 0.50963474\n",
      "Iteration 459, loss = 0.50919873\n",
      "Iteration 460, loss = 0.50876366\n",
      "Iteration 461, loss = 0.50832952\n",
      "Iteration 462, loss = 0.50789630\n",
      "Iteration 463, loss = 0.50746401\n",
      "Iteration 464, loss = 0.50703263\n",
      "Iteration 465, loss = 0.50660215\n",
      "Iteration 466, loss = 0.50617257\n",
      "Iteration 467, loss = 0.50574389\n",
      "Iteration 468, loss = 0.50531609\n",
      "Iteration 469, loss = 0.50488918\n",
      "Iteration 470, loss = 0.50446313\n",
      "Iteration 471, loss = 0.50403795\n",
      "Iteration 472, loss = 0.50361363\n",
      "Iteration 473, loss = 0.50319017\n",
      "Iteration 474, loss = 0.50276755\n",
      "Iteration 475, loss = 0.50234576\n",
      "Iteration 476, loss = 0.50192481\n",
      "Iteration 477, loss = 0.50150469\n",
      "Iteration 478, loss = 0.50108538\n",
      "Iteration 479, loss = 0.50066688\n",
      "Iteration 480, loss = 0.50024919\n",
      "Iteration 481, loss = 0.49983230\n",
      "Iteration 482, loss = 0.49941620\n",
      "Iteration 483, loss = 0.49900089\n",
      "Iteration 484, loss = 0.49858635\n",
      "Iteration 485, loss = 0.49817259\n",
      "Iteration 486, loss = 0.49775960\n",
      "Iteration 487, loss = 0.49734736\n",
      "Iteration 488, loss = 0.49693588\n",
      "Iteration 489, loss = 0.49652515\n",
      "Iteration 490, loss = 0.49611516\n",
      "Iteration 491, loss = 0.49570591\n",
      "Iteration 492, loss = 0.49529739\n",
      "Iteration 493, loss = 0.49488959\n",
      "Iteration 494, loss = 0.49448251\n",
      "Iteration 495, loss = 0.49407614\n",
      "Iteration 496, loss = 0.49367049\n",
      "Iteration 497, loss = 0.49326570\n",
      "Iteration 498, loss = 0.49286203\n",
      "Iteration 499, loss = 0.49245968\n",
      "Iteration 500, loss = 0.49205808\n",
      "Iteration 501, loss = 0.49165722\n",
      "Iteration 502, loss = 0.49125710\n",
      "Iteration 503, loss = 0.49085772\n",
      "Iteration 504, loss = 0.49045907\n",
      "Iteration 505, loss = 0.49006114\n",
      "Iteration 506, loss = 0.48966393\n",
      "Iteration 507, loss = 0.48926743\n",
      "Iteration 508, loss = 0.48887164\n",
      "Iteration 509, loss = 0.48847655\n",
      "Iteration 510, loss = 0.48808215\n",
      "Iteration 511, loss = 0.48768844\n",
      "Iteration 512, loss = 0.48729541\n",
      "Iteration 513, loss = 0.48690332\n",
      "Iteration 514, loss = 0.48651254\n",
      "Iteration 515, loss = 0.48612244\n",
      "Iteration 516, loss = 0.48573302\n",
      "Iteration 517, loss = 0.48534427\n",
      "Iteration 518, loss = 0.48495620\n",
      "Iteration 519, loss = 0.48456881\n",
      "Iteration 520, loss = 0.48418209\n",
      "Iteration 521, loss = 0.48379604\n",
      "Iteration 522, loss = 0.48341065\n",
      "Iteration 523, loss = 0.48302592\n",
      "Iteration 524, loss = 0.48263757\n",
      "Iteration 525, loss = 0.48224557\n",
      "Iteration 526, loss = 0.48183493\n",
      "Iteration 527, loss = 0.48139938\n",
      "Iteration 528, loss = 0.48087500\n",
      "Iteration 529, loss = 0.48029068\n",
      "Iteration 530, loss = 0.47964859\n",
      "Iteration 531, loss = 0.47896739\n",
      "Iteration 532, loss = 0.47824939\n",
      "Iteration 533, loss = 0.47752070\n",
      "Iteration 534, loss = 0.47681534\n",
      "Iteration 535, loss = 0.47623350\n",
      "Iteration 536, loss = 0.47601197\n",
      "Iteration 537, loss = 0.47561575\n",
      "Iteration 538, loss = 0.47497535\n",
      "Iteration 539, loss = 0.47424731\n",
      "Iteration 540, loss = 0.47362892\n",
      "Iteration 541, loss = 0.47313834\n",
      "Iteration 542, loss = 0.47265605\n",
      "Iteration 543, loss = 0.47215994\n",
      "Iteration 544, loss = 0.47165543\n",
      "Iteration 545, loss = 0.47112161\n",
      "Iteration 546, loss = 0.47055930\n",
      "Iteration 547, loss = 0.46997513\n",
      "Iteration 548, loss = 0.46937893\n",
      "Iteration 549, loss = 0.46877083\n",
      "Iteration 550, loss = 0.46815460\n",
      "Iteration 551, loss = 0.46754668\n",
      "Iteration 552, loss = 0.46697351\n",
      "Iteration 553, loss = 0.46644727\n",
      "Iteration 554, loss = 0.46589854\n",
      "Iteration 555, loss = 0.46530236\n",
      "Iteration 556, loss = 0.46467347\n",
      "Iteration 557, loss = 0.46406927\n",
      "Iteration 558, loss = 0.46349238\n",
      "Iteration 559, loss = 0.46292088\n",
      "Iteration 560, loss = 0.46235172\n",
      "Iteration 561, loss = 0.46176532\n",
      "Iteration 562, loss = 0.46116271\n",
      "Iteration 563, loss = 0.46055366\n",
      "Iteration 564, loss = 0.45994971\n",
      "Iteration 565, loss = 0.45934877\n",
      "Iteration 566, loss = 0.45875666\n",
      "Iteration 567, loss = 0.45816666\n",
      "Iteration 568, loss = 0.45757540\n",
      "Iteration 569, loss = 0.45697436\n",
      "Iteration 570, loss = 0.45636885\n",
      "Iteration 571, loss = 0.45576752\n",
      "Iteration 572, loss = 0.45516611\n",
      "Iteration 573, loss = 0.45456753\n",
      "Iteration 574, loss = 0.45397163\n",
      "Iteration 575, loss = 0.45337495\n",
      "Iteration 576, loss = 0.45277425\n",
      "Iteration 577, loss = 0.45217135\n",
      "Iteration 578, loss = 0.45157128\n",
      "Iteration 579, loss = 0.45097332\n",
      "Iteration 580, loss = 0.45037613\n",
      "Iteration 581, loss = 0.44977913\n",
      "Iteration 582, loss = 0.44918247\n",
      "Iteration 583, loss = 0.44858628\n",
      "Iteration 584, loss = 0.44799068\n",
      "Iteration 585, loss = 0.44739578\n",
      "Iteration 586, loss = 0.44680169\n",
      "Iteration 587, loss = 0.44620851\n",
      "Iteration 588, loss = 0.44561632\n",
      "Iteration 589, loss = 0.44502522\n",
      "Iteration 590, loss = 0.44443526\n",
      "Iteration 591, loss = 0.44384651\n",
      "Iteration 592, loss = 0.44325904\n",
      "Iteration 593, loss = 0.44267413\n",
      "Iteration 594, loss = 0.44209033\n",
      "Iteration 595, loss = 0.44150721\n",
      "Iteration 596, loss = 0.44092533\n",
      "Iteration 597, loss = 0.44034477\n",
      "Iteration 598, loss = 0.43976593\n",
      "Iteration 599, loss = 0.43918924\n",
      "Iteration 600, loss = 0.43861444\n",
      "Iteration 601, loss = 0.43804070\n",
      "Iteration 602, loss = 0.43746805\n",
      "Iteration 603, loss = 0.43689823\n",
      "Iteration 604, loss = 0.43632991\n",
      "Iteration 605, loss = 0.43576315\n",
      "Iteration 606, loss = 0.43519844\n",
      "Iteration 607, loss = 0.43463557\n",
      "Iteration 608, loss = 0.43407438\n",
      "Iteration 609, loss = 0.43351491\n",
      "Iteration 610, loss = 0.43295715\n",
      "Iteration 611, loss = 0.43240141\n",
      "Iteration 612, loss = 0.43184761\n",
      "Iteration 613, loss = 0.43129550\n",
      "Iteration 614, loss = 0.43074512\n",
      "Iteration 615, loss = 0.43019652\n",
      "Iteration 616, loss = 0.42965000\n",
      "Iteration 617, loss = 0.42910553\n",
      "Iteration 618, loss = 0.42856282\n",
      "Iteration 619, loss = 0.42802212\n",
      "Iteration 620, loss = 0.42748349\n",
      "Iteration 621, loss = 0.42694648\n",
      "Iteration 622, loss = 0.42641160\n",
      "Iteration 623, loss = 0.42587856\n",
      "Iteration 624, loss = 0.42534735\n",
      "Iteration 625, loss = 0.42481799\n",
      "Iteration 626, loss = 0.42429087\n",
      "Iteration 627, loss = 0.42376706\n",
      "Iteration 628, loss = 0.42324518\n",
      "Iteration 629, loss = 0.42272523\n",
      "Iteration 630, loss = 0.42220732\n",
      "Iteration 631, loss = 0.42169120\n",
      "Iteration 632, loss = 0.42117710\n",
      "Iteration 633, loss = 0.42066488\n",
      "Iteration 634, loss = 0.42015451\n",
      "Iteration 635, loss = 0.41964615\n",
      "Iteration 636, loss = 0.41913975\n",
      "Iteration 637, loss = 0.41863573\n",
      "Iteration 638, loss = 0.41813354\n",
      "Iteration 639, loss = 0.41763332\n",
      "Iteration 640, loss = 0.41713489\n",
      "Iteration 641, loss = 0.41663814\n",
      "Iteration 642, loss = 0.41614310\n",
      "Iteration 643, loss = 0.41564992\n",
      "Iteration 644, loss = 0.41515851\n",
      "Iteration 645, loss = 0.41466872\n",
      "Iteration 646, loss = 0.41418064\n",
      "Iteration 647, loss = 0.41369418\n",
      "Iteration 648, loss = 0.41320947\n",
      "Iteration 649, loss = 0.41272619\n",
      "Iteration 650, loss = 0.41224461\n",
      "Iteration 651, loss = 0.41176459\n",
      "Iteration 652, loss = 0.41128612\n",
      "Iteration 653, loss = 0.41080935\n",
      "Iteration 654, loss = 0.41033441\n",
      "Iteration 655, loss = 0.40986104\n",
      "Iteration 656, loss = 0.40938915\n",
      "Iteration 657, loss = 0.40891874\n",
      "Iteration 658, loss = 0.40844978\n",
      "Iteration 659, loss = 0.40798224\n",
      "Iteration 660, loss = 0.40751612\n",
      "Iteration 661, loss = 0.40705138\n",
      "Iteration 662, loss = 0.40658842\n",
      "Iteration 663, loss = 0.40612711\n",
      "Iteration 664, loss = 0.40566717\n",
      "Iteration 665, loss = 0.40520856\n",
      "Iteration 666, loss = 0.40475125\n",
      "Iteration 667, loss = 0.40429522\n",
      "Iteration 668, loss = 0.40384045\n",
      "Iteration 669, loss = 0.40338690\n",
      "Iteration 670, loss = 0.40293455\n",
      "Iteration 671, loss = 0.40248338\n",
      "Iteration 672, loss = 0.40203336\n",
      "Iteration 673, loss = 0.40158447\n",
      "Iteration 674, loss = 0.40113669\n",
      "Iteration 675, loss = 0.40068999\n",
      "Iteration 676, loss = 0.40024436\n",
      "Iteration 677, loss = 0.39979977\n",
      "Iteration 678, loss = 0.39935621\n",
      "Iteration 679, loss = 0.39891365\n",
      "Iteration 680, loss = 0.39847208\n",
      "Iteration 681, loss = 0.39803147\n",
      "Iteration 682, loss = 0.39759239\n",
      "Iteration 683, loss = 0.39715467\n",
      "Iteration 684, loss = 0.39671758\n",
      "Iteration 685, loss = 0.39628119\n",
      "Iteration 686, loss = 0.39584556\n",
      "Iteration 687, loss = 0.39541074\n",
      "Iteration 688, loss = 0.39497726\n",
      "Iteration 689, loss = 0.39454504\n",
      "Iteration 690, loss = 0.39411355\n",
      "Iteration 691, loss = 0.39368275\n",
      "Iteration 692, loss = 0.39325261\n",
      "Iteration 693, loss = 0.39282361\n",
      "Iteration 694, loss = 0.39239572\n",
      "Iteration 695, loss = 0.39196856\n",
      "Iteration 696, loss = 0.39154217\n",
      "Iteration 697, loss = 0.39111655\n",
      "Iteration 698, loss = 0.39069171\n",
      "Iteration 699, loss = 0.39026767\n",
      "Iteration 700, loss = 0.38984443\n",
      "Iteration 701, loss = 0.38942199\n",
      "Iteration 702, loss = 0.38900059\n",
      "Iteration 703, loss = 0.38857962\n",
      "Iteration 704, loss = 0.38815947\n",
      "Iteration 705, loss = 0.38774013\n",
      "Iteration 706, loss = 0.38732149\n",
      "Iteration 707, loss = 0.38690354\n",
      "Iteration 708, loss = 0.38648627\n",
      "Iteration 709, loss = 0.38606968\n",
      "Iteration 710, loss = 0.38565375\n",
      "Iteration 711, loss = 0.38523848\n",
      "Iteration 712, loss = 0.38482399\n",
      "Iteration 713, loss = 0.38441095\n",
      "Iteration 714, loss = 0.38399848\n",
      "Iteration 715, loss = 0.38358658\n",
      "Iteration 716, loss = 0.38317565\n",
      "Iteration 717, loss = 0.38276537\n",
      "Iteration 718, loss = 0.38235570\n",
      "Iteration 719, loss = 0.38194663\n",
      "Iteration 720, loss = 0.38153812\n",
      "Iteration 721, loss = 0.38113021\n",
      "Iteration 722, loss = 0.38072304\n",
      "Iteration 723, loss = 0.38031653\n",
      "Iteration 724, loss = 0.37991059\n",
      "Iteration 725, loss = 0.37950511\n",
      "Iteration 726, loss = 0.37910040\n",
      "Iteration 727, loss = 0.37869662\n",
      "Iteration 728, loss = 0.37829332\n",
      "Iteration 729, loss = 0.37789067\n",
      "Iteration 730, loss = 0.37748876\n",
      "Iteration 731, loss = 0.37708726\n",
      "Iteration 732, loss = 0.37668615\n",
      "Iteration 733, loss = 0.37628594\n",
      "Iteration 734, loss = 0.37588608\n",
      "Iteration 735, loss = 0.37548647\n",
      "Iteration 736, loss = 0.37508761\n",
      "Iteration 737, loss = 0.37468932\n",
      "Iteration 738, loss = 0.37429142\n",
      "Iteration 739, loss = 0.37389390\n",
      "Iteration 740, loss = 0.37349679\n",
      "Iteration 741, loss = 0.37310011\n",
      "Iteration 742, loss = 0.37270428\n",
      "Iteration 743, loss = 0.37230866\n",
      "Iteration 744, loss = 0.37191313\n",
      "Iteration 745, loss = 0.37151841\n",
      "Iteration 746, loss = 0.37112407\n",
      "Iteration 747, loss = 0.37073011\n",
      "Iteration 748, loss = 0.37033655\n",
      "Iteration 749, loss = 0.36994338\n",
      "Iteration 750, loss = 0.36955066\n",
      "Iteration 751, loss = 0.36915829\n",
      "Iteration 752, loss = 0.36876634\n",
      "Iteration 753, loss = 0.36837476\n",
      "Iteration 754, loss = 0.36798367\n",
      "Iteration 755, loss = 0.36759367\n",
      "Iteration 756, loss = 0.36720410\n",
      "Iteration 757, loss = 0.36681493\n",
      "Iteration 758, loss = 0.36642616\n",
      "Iteration 759, loss = 0.36603778\n",
      "Iteration 760, loss = 0.36564978\n",
      "Iteration 761, loss = 0.36526220\n",
      "Iteration 762, loss = 0.36487536\n",
      "Iteration 763, loss = 0.36448890\n",
      "Iteration 764, loss = 0.36410280\n",
      "Iteration 765, loss = 0.36371706\n",
      "Iteration 766, loss = 0.36333167\n",
      "Iteration 767, loss = 0.36294661\n",
      "Iteration 768, loss = 0.36256187\n",
      "Iteration 769, loss = 0.36217745\n",
      "Iteration 770, loss = 0.36179334\n",
      "Iteration 771, loss = 0.36140952\n",
      "Iteration 772, loss = 0.36102598\n",
      "Iteration 773, loss = 0.36064273\n",
      "Iteration 774, loss = 0.36025975\n",
      "Iteration 775, loss = 0.35987704\n",
      "Iteration 776, loss = 0.35949478\n",
      "Iteration 777, loss = 0.35911288\n",
      "Iteration 778, loss = 0.35873125\n",
      "Iteration 779, loss = 0.35834989\n",
      "Iteration 780, loss = 0.35796878\n",
      "Iteration 781, loss = 0.35758792\n",
      "Iteration 782, loss = 0.35720730\n",
      "Iteration 783, loss = 0.35682692\n",
      "Iteration 784, loss = 0.35644686\n",
      "Iteration 785, loss = 0.35606730\n",
      "Iteration 786, loss = 0.35568774\n",
      "Iteration 787, loss = 0.35530823\n",
      "Iteration 788, loss = 0.35492915\n",
      "Iteration 789, loss = 0.35455040\n",
      "Iteration 790, loss = 0.35417182\n",
      "Iteration 791, loss = 0.35379342\n",
      "Iteration 792, loss = 0.35341517\n",
      "Iteration 793, loss = 0.35303707\n",
      "Iteration 794, loss = 0.35265911\n",
      "Iteration 795, loss = 0.35228167\n",
      "Iteration 796, loss = 0.35190439\n",
      "Iteration 797, loss = 0.35152718\n",
      "Iteration 798, loss = 0.35115007\n",
      "Iteration 799, loss = 0.35077309\n",
      "Iteration 800, loss = 0.35039650\n",
      "Iteration 801, loss = 0.35002067\n",
      "Iteration 802, loss = 0.34964501\n",
      "Iteration 803, loss = 0.34926944\n",
      "Iteration 804, loss = 0.34889419\n",
      "Iteration 805, loss = 0.34851927\n",
      "Iteration 806, loss = 0.34814450\n",
      "Iteration 807, loss = 0.34776988\n",
      "Iteration 808, loss = 0.34739542\n",
      "Iteration 809, loss = 0.34702113\n",
      "Iteration 810, loss = 0.34664711\n",
      "Iteration 811, loss = 0.34627325\n",
      "Iteration 812, loss = 0.34589954\n",
      "Iteration 813, loss = 0.34552606\n",
      "Iteration 814, loss = 0.34515277\n",
      "Iteration 815, loss = 0.34477982\n",
      "Iteration 816, loss = 0.34440703\n",
      "Iteration 817, loss = 0.34403442\n",
      "Iteration 818, loss = 0.34366209\n",
      "Iteration 819, loss = 0.34329029\n",
      "Iteration 820, loss = 0.34291881\n",
      "Iteration 821, loss = 0.34254728\n",
      "Iteration 822, loss = 0.34217606\n",
      "Iteration 823, loss = 0.34180500\n",
      "Iteration 824, loss = 0.34143412\n",
      "Iteration 825, loss = 0.34106339\n",
      "Iteration 826, loss = 0.34069309\n",
      "Iteration 827, loss = 0.34032261\n",
      "Iteration 828, loss = 0.33995215\n",
      "Iteration 829, loss = 0.33958246\n",
      "Iteration 830, loss = 0.33921245\n",
      "Iteration 831, loss = 0.33884224\n",
      "Iteration 832, loss = 0.33847286\n",
      "Iteration 833, loss = 0.33810325\n",
      "Iteration 834, loss = 0.33773362\n",
      "Iteration 835, loss = 0.33736435\n",
      "Iteration 836, loss = 0.33699532\n",
      "Iteration 837, loss = 0.33662621\n",
      "Iteration 838, loss = 0.33625742\n",
      "Iteration 839, loss = 0.33588869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 840, loss = 0.33551970\n",
      "Iteration 841, loss = 0.33515192\n",
      "Iteration 842, loss = 0.33478385\n",
      "Iteration 843, loss = 0.33441536\n",
      "Iteration 844, loss = 0.33404714\n",
      "Iteration 845, loss = 0.33367957\n",
      "Iteration 846, loss = 0.33331176\n",
      "Iteration 847, loss = 0.33294376\n",
      "Iteration 848, loss = 0.33257671\n",
      "Iteration 849, loss = 0.33220948\n",
      "Iteration 850, loss = 0.33184173\n",
      "Iteration 851, loss = 0.33147464\n",
      "Iteration 852, loss = 0.33110790\n",
      "Iteration 853, loss = 0.33074096\n",
      "Iteration 854, loss = 0.33037387\n",
      "Iteration 855, loss = 0.33000667\n",
      "Iteration 856, loss = 0.32964060\n",
      "Iteration 857, loss = 0.32927429\n",
      "Iteration 858, loss = 0.32890732\n",
      "Iteration 859, loss = 0.32854079\n",
      "Iteration 860, loss = 0.32817488\n",
      "Iteration 861, loss = 0.32780881\n",
      "Iteration 862, loss = 0.32744264\n",
      "Iteration 863, loss = 0.32707638\n",
      "Iteration 864, loss = 0.32671033\n",
      "Iteration 865, loss = 0.32634488\n",
      "Iteration 866, loss = 0.32597895\n",
      "Iteration 867, loss = 0.32561326\n",
      "Iteration 868, loss = 0.32524799\n",
      "Iteration 869, loss = 0.32488264\n",
      "Iteration 870, loss = 0.32451722\n",
      "Iteration 871, loss = 0.32415178\n",
      "Iteration 872, loss = 0.32378673\n",
      "Iteration 873, loss = 0.32342189\n",
      "Iteration 874, loss = 0.32305676\n",
      "Iteration 875, loss = 0.32269176\n",
      "Iteration 876, loss = 0.32232712\n",
      "Iteration 877, loss = 0.32196322\n",
      "Iteration 878, loss = 0.32159953\n",
      "Iteration 879, loss = 0.32123588\n",
      "Iteration 880, loss = 0.32087231\n",
      "Iteration 881, loss = 0.32050922\n",
      "Iteration 882, loss = 0.32014624\n",
      "Iteration 883, loss = 0.31978330\n",
      "Iteration 884, loss = 0.31942041\n",
      "Iteration 885, loss = 0.31905755\n",
      "Iteration 886, loss = 0.31869474\n",
      "Iteration 887, loss = 0.31833247\n",
      "Iteration 888, loss = 0.31797023\n",
      "Iteration 889, loss = 0.31760801\n",
      "Iteration 890, loss = 0.31724582\n",
      "Iteration 891, loss = 0.31688368\n",
      "Iteration 892, loss = 0.31652160\n",
      "Iteration 893, loss = 0.31615959\n",
      "Iteration 894, loss = 0.31579802\n",
      "Iteration 895, loss = 0.31543651\n",
      "Iteration 896, loss = 0.31507496\n",
      "Iteration 897, loss = 0.31471339\n",
      "Iteration 898, loss = 0.31435180\n",
      "Iteration 899, loss = 0.31399069\n",
      "Iteration 900, loss = 0.31362965\n",
      "Iteration 901, loss = 0.31326865\n",
      "Iteration 902, loss = 0.31290770\n",
      "Iteration 903, loss = 0.31254680\n",
      "Iteration 904, loss = 0.31218596\n",
      "Iteration 905, loss = 0.31182520\n",
      "Iteration 906, loss = 0.31146464\n",
      "Iteration 907, loss = 0.31110432\n",
      "Iteration 908, loss = 0.31074429\n",
      "Iteration 909, loss = 0.31038440\n",
      "Iteration 910, loss = 0.31002443\n",
      "Iteration 911, loss = 0.30966458\n",
      "Iteration 912, loss = 0.30930498\n",
      "Iteration 913, loss = 0.30894546\n",
      "Iteration 914, loss = 0.30858603\n",
      "Iteration 915, loss = 0.30822669\n",
      "Iteration 916, loss = 0.30786744\n",
      "Iteration 917, loss = 0.30750829\n",
      "Iteration 918, loss = 0.30714924\n",
      "Iteration 919, loss = 0.30679029\n",
      "Iteration 920, loss = 0.30643161\n",
      "Iteration 921, loss = 0.30607286\n",
      "Iteration 922, loss = 0.30571419\n",
      "Iteration 923, loss = 0.30535572\n",
      "Iteration 924, loss = 0.30499734\n",
      "Iteration 925, loss = 0.30463906\n",
      "Iteration 926, loss = 0.30428087\n",
      "Iteration 927, loss = 0.30392279\n",
      "Iteration 928, loss = 0.30356481\n",
      "Iteration 929, loss = 0.30320693\n",
      "Iteration 930, loss = 0.30284915\n",
      "Iteration 931, loss = 0.30249148\n",
      "Iteration 932, loss = 0.30213392\n",
      "Iteration 933, loss = 0.30177647\n",
      "Iteration 934, loss = 0.30141913\n",
      "Iteration 935, loss = 0.30106191\n",
      "Iteration 936, loss = 0.30070479\n",
      "Iteration 937, loss = 0.30034779\n",
      "Iteration 938, loss = 0.29999090\n",
      "Iteration 939, loss = 0.29963413\n",
      "Iteration 940, loss = 0.29927750\n",
      "Iteration 941, loss = 0.29892092\n",
      "Iteration 942, loss = 0.29856448\n",
      "Iteration 943, loss = 0.29820816\n",
      "Iteration 944, loss = 0.29785195\n",
      "Iteration 945, loss = 0.29749585\n",
      "Iteration 946, loss = 0.29713988\n",
      "Iteration 947, loss = 0.29678402\n",
      "Iteration 948, loss = 0.29642828\n",
      "Iteration 949, loss = 0.29607266\n",
      "Iteration 950, loss = 0.29571717\n",
      "Iteration 951, loss = 0.29536179\n",
      "Iteration 952, loss = 0.29500654\n",
      "Iteration 953, loss = 0.29465140\n",
      "Iteration 954, loss = 0.29429640\n",
      "Iteration 955, loss = 0.29394152\n",
      "Iteration 956, loss = 0.29358677\n",
      "Iteration 957, loss = 0.29323214\n",
      "Iteration 958, loss = 0.29287764\n",
      "Iteration 959, loss = 0.29252327\n",
      "Iteration 960, loss = 0.29216904\n",
      "Iteration 961, loss = 0.29181493\n",
      "Iteration 962, loss = 0.29146096\n",
      "Iteration 963, loss = 0.29110712\n",
      "Iteration 964, loss = 0.29075341\n",
      "Iteration 965, loss = 0.29039985\n",
      "Iteration 966, loss = 0.29004641\n",
      "Iteration 967, loss = 0.28969312\n",
      "Iteration 968, loss = 0.28933997\n",
      "Iteration 969, loss = 0.28898695\n",
      "Iteration 970, loss = 0.28863408\n",
      "Iteration 971, loss = 0.28828135\n",
      "Iteration 972, loss = 0.28792877\n",
      "Iteration 973, loss = 0.28757633\n",
      "Iteration 974, loss = 0.28722404\n",
      "Iteration 975, loss = 0.28687189\n",
      "Iteration 976, loss = 0.28651989\n",
      "Iteration 977, loss = 0.28616804\n",
      "Iteration 978, loss = 0.28581635\n",
      "Iteration 979, loss = 0.28546481\n",
      "Iteration 980, loss = 0.28511342\n",
      "Iteration 981, loss = 0.28476218\n",
      "Iteration 982, loss = 0.28441110\n",
      "Iteration 983, loss = 0.28406018\n",
      "Iteration 984, loss = 0.28370942\n",
      "Iteration 985, loss = 0.28335881\n",
      "Iteration 986, loss = 0.28300837\n",
      "Iteration 987, loss = 0.28265809\n",
      "Iteration 988, loss = 0.28230798\n",
      "Iteration 989, loss = 0.28195803\n",
      "Iteration 990, loss = 0.28160824\n",
      "Iteration 991, loss = 0.28125863\n",
      "Iteration 992, loss = 0.28090918\n",
      "Iteration 993, loss = 0.28056035\n",
      "Iteration 994, loss = 0.28021132\n",
      "Iteration 995, loss = 0.27986214\n",
      "Iteration 996, loss = 0.27951346\n",
      "Iteration 997, loss = 0.27916508\n",
      "Iteration 998, loss = 0.27881687\n",
      "Iteration 999, loss = 0.27846882\n",
      "Iteration 1000, loss = 0.27812095\n",
      "Iteration 1, loss = 1.51234105\n",
      "Iteration 2, loss = 1.49516013\n",
      "Iteration 3, loss = 1.47825519\n",
      "Iteration 4, loss = 1.46162858\n",
      "Iteration 5, loss = 1.44529917\n",
      "Iteration 6, loss = 1.42926369\n",
      "Iteration 7, loss = 1.41347201\n",
      "Iteration 8, loss = 1.39794426\n",
      "Iteration 9, loss = 1.38271807\n",
      "Iteration 10, loss = 1.36775081\n",
      "Iteration 11, loss = 1.35300592\n",
      "Iteration 12, loss = 1.33847673\n",
      "Iteration 13, loss = 1.32416052\n",
      "Iteration 14, loss = 1.31009180\n",
      "Iteration 15, loss = 1.29625839\n",
      "Iteration 16, loss = 1.28269076\n",
      "Iteration 17, loss = 1.26942282\n",
      "Iteration 18, loss = 1.25637513\n",
      "Iteration 19, loss = 1.24356540\n",
      "Iteration 20, loss = 1.23097624\n",
      "Iteration 21, loss = 1.21864898\n",
      "Iteration 22, loss = 1.20657693\n",
      "Iteration 23, loss = 1.19474701\n",
      "Iteration 24, loss = 1.18311341\n",
      "Iteration 25, loss = 1.17165936\n",
      "Iteration 26, loss = 1.16040093\n",
      "Iteration 27, loss = 1.14935072\n",
      "Iteration 28, loss = 1.13849124\n",
      "Iteration 29, loss = 1.12782896\n",
      "Iteration 30, loss = 1.11732890\n",
      "Iteration 31, loss = 1.10700915\n",
      "Iteration 32, loss = 1.09686602\n",
      "Iteration 33, loss = 1.08687650\n",
      "Iteration 34, loss = 1.07703863\n",
      "Iteration 35, loss = 1.06737160\n",
      "Iteration 36, loss = 1.05788376\n",
      "Iteration 37, loss = 1.04857036\n",
      "Iteration 38, loss = 1.03941878\n",
      "Iteration 39, loss = 1.03040859\n",
      "Iteration 40, loss = 1.02153698\n",
      "Iteration 41, loss = 1.01281541\n",
      "Iteration 42, loss = 1.00422870\n",
      "Iteration 43, loss = 0.99582008\n",
      "Iteration 44, loss = 0.98757938\n",
      "Iteration 45, loss = 0.97947322\n",
      "Iteration 46, loss = 0.97150094\n",
      "Iteration 47, loss = 0.96366113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 48, loss = 0.95595849\n",
      "Iteration 49, loss = 0.94843112\n",
      "Iteration 50, loss = 0.94105484\n",
      "Iteration 51, loss = 0.93386309\n",
      "Iteration 52, loss = 0.92683027\n",
      "Iteration 53, loss = 0.91995735\n",
      "Iteration 54, loss = 0.91327614\n",
      "Iteration 55, loss = 0.90677142\n",
      "Iteration 56, loss = 0.90041780\n",
      "Iteration 57, loss = 0.89424538\n",
      "Iteration 58, loss = 0.88825398\n",
      "Iteration 59, loss = 0.88243039\n",
      "Iteration 60, loss = 0.87680598\n",
      "Iteration 61, loss = 0.87140502\n",
      "Iteration 62, loss = 0.86620246\n",
      "Iteration 63, loss = 0.86115947\n",
      "Iteration 64, loss = 0.85627554\n",
      "Iteration 65, loss = 0.85156241\n",
      "Iteration 66, loss = 0.84702838\n",
      "Iteration 67, loss = 0.84267984\n",
      "Iteration 68, loss = 0.83848972\n",
      "Iteration 69, loss = 0.83446302\n",
      "Iteration 70, loss = 0.83059654\n",
      "Iteration 71, loss = 0.82688357\n",
      "Iteration 72, loss = 0.82331206\n",
      "Iteration 73, loss = 0.81991643\n",
      "Iteration 74, loss = 0.81666327\n",
      "Iteration 75, loss = 0.81355078\n",
      "Iteration 76, loss = 0.81057311\n",
      "Iteration 77, loss = 0.80772356\n",
      "Iteration 78, loss = 0.80498680\n",
      "Iteration 79, loss = 0.80235518\n",
      "Iteration 80, loss = 0.79983806\n",
      "Iteration 81, loss = 0.79742091\n",
      "Iteration 82, loss = 0.79509286\n",
      "Iteration 83, loss = 0.79286274\n",
      "Iteration 84, loss = 0.79072273\n",
      "Iteration 85, loss = 0.78866085\n",
      "Iteration 86, loss = 0.78667622\n",
      "Iteration 87, loss = 0.78476149\n",
      "Iteration 88, loss = 0.78291707\n",
      "Iteration 89, loss = 0.78113879\n",
      "Iteration 90, loss = 0.77942820\n",
      "Iteration 91, loss = 0.77777724\n",
      "Iteration 92, loss = 0.77617716\n",
      "Iteration 93, loss = 0.77462441\n",
      "Iteration 94, loss = 0.77311562\n",
      "Iteration 95, loss = 0.77164750\n",
      "Iteration 96, loss = 0.77021693\n",
      "Iteration 97, loss = 0.76882381\n",
      "Iteration 98, loss = 0.76746781\n",
      "Iteration 99, loss = 0.76614167\n",
      "Iteration 100, loss = 0.76484282\n",
      "Iteration 101, loss = 0.76357796\n",
      "Iteration 102, loss = 0.76234302\n",
      "Iteration 103, loss = 0.76114190\n",
      "Iteration 104, loss = 0.75996545\n",
      "Iteration 105, loss = 0.75882315\n",
      "Iteration 106, loss = 0.75770637\n",
      "Iteration 107, loss = 0.75660634\n",
      "Iteration 108, loss = 0.75552150\n",
      "Iteration 109, loss = 0.75445144\n",
      "Iteration 110, loss = 0.75339945\n",
      "Iteration 111, loss = 0.75235887\n",
      "Iteration 112, loss = 0.75133361\n",
      "Iteration 113, loss = 0.75031786\n",
      "Iteration 114, loss = 0.74931052\n",
      "Iteration 115, loss = 0.74831067\n",
      "Iteration 116, loss = 0.74731748\n",
      "Iteration 117, loss = 0.74633132\n",
      "Iteration 118, loss = 0.74535379\n",
      "Iteration 119, loss = 0.74438105\n",
      "Iteration 120, loss = 0.74341980\n",
      "Iteration 121, loss = 0.74247408\n",
      "Iteration 122, loss = 0.74153247\n",
      "Iteration 123, loss = 0.74059456\n",
      "Iteration 124, loss = 0.73966215\n",
      "Iteration 125, loss = 0.73874043\n",
      "Iteration 126, loss = 0.73782230\n",
      "Iteration 127, loss = 0.73690969\n",
      "Iteration 128, loss = 0.73599953\n",
      "Iteration 129, loss = 0.73509217\n",
      "Iteration 130, loss = 0.73419193\n",
      "Iteration 131, loss = 0.73329390\n",
      "Iteration 132, loss = 0.73240177\n",
      "Iteration 133, loss = 0.73151286\n",
      "Iteration 134, loss = 0.73062588\n",
      "Iteration 135, loss = 0.72974065\n",
      "Iteration 136, loss = 0.72885877\n",
      "Iteration 137, loss = 0.72797935\n",
      "Iteration 138, loss = 0.72710127\n",
      "Iteration 139, loss = 0.72622441\n",
      "Iteration 140, loss = 0.72535204\n",
      "Iteration 141, loss = 0.72448230\n",
      "Iteration 142, loss = 0.72361853\n",
      "Iteration 143, loss = 0.72275606\n",
      "Iteration 144, loss = 0.72189480\n",
      "Iteration 145, loss = 0.72103467\n",
      "Iteration 146, loss = 0.72017674\n",
      "Iteration 147, loss = 0.71932253\n",
      "Iteration 148, loss = 0.71846936\n",
      "Iteration 149, loss = 0.71761718\n",
      "Iteration 150, loss = 0.71676596\n",
      "Iteration 151, loss = 0.71591565\n",
      "Iteration 152, loss = 0.71506622\n",
      "Iteration 153, loss = 0.71421765\n",
      "Iteration 154, loss = 0.71337013\n",
      "Iteration 155, loss = 0.71252374\n",
      "Iteration 156, loss = 0.71167808\n",
      "Iteration 157, loss = 0.71083312\n",
      "Iteration 158, loss = 0.70998947\n",
      "Iteration 159, loss = 0.70914836\n",
      "Iteration 160, loss = 0.70830928\n",
      "Iteration 161, loss = 0.70747344\n",
      "Iteration 162, loss = 0.70663922\n",
      "Iteration 163, loss = 0.70580565\n",
      "Iteration 164, loss = 0.70497396\n",
      "Iteration 165, loss = 0.70414305\n",
      "Iteration 166, loss = 0.70331267\n",
      "Iteration 167, loss = 0.70248289\n",
      "Iteration 168, loss = 0.70165376\n",
      "Iteration 169, loss = 0.70082533\n",
      "Iteration 170, loss = 0.69999763\n",
      "Iteration 171, loss = 0.69917070\n",
      "Iteration 172, loss = 0.69834458\n",
      "Iteration 173, loss = 0.69751928\n",
      "Iteration 174, loss = 0.69669482\n",
      "Iteration 175, loss = 0.69587124\n",
      "Iteration 176, loss = 0.69504855\n",
      "Iteration 177, loss = 0.69422707\n",
      "Iteration 178, loss = 0.69340650\n",
      "Iteration 179, loss = 0.69258684\n",
      "Iteration 180, loss = 0.69176811\n",
      "Iteration 181, loss = 0.69095030\n",
      "Iteration 182, loss = 0.69013344\n",
      "Iteration 183, loss = 0.68931751\n",
      "Iteration 184, loss = 0.68850252\n",
      "Iteration 185, loss = 0.68768848\n",
      "Iteration 186, loss = 0.68687538\n",
      "Iteration 187, loss = 0.68606323\n",
      "Iteration 188, loss = 0.68525203\n",
      "Iteration 189, loss = 0.68444177\n",
      "Iteration 190, loss = 0.68363246\n",
      "Iteration 191, loss = 0.68282411\n",
      "Iteration 192, loss = 0.68201670\n",
      "Iteration 193, loss = 0.68121024\n",
      "Iteration 194, loss = 0.68040475\n",
      "Iteration 195, loss = 0.67960021\n",
      "Iteration 196, loss = 0.67879663\n",
      "Iteration 197, loss = 0.67799402\n",
      "Iteration 198, loss = 0.67719238\n",
      "Iteration 199, loss = 0.67639172\n",
      "Iteration 200, loss = 0.67559204\n",
      "Iteration 201, loss = 0.67479335\n",
      "Iteration 202, loss = 0.67399566\n",
      "Iteration 203, loss = 0.67319897\n",
      "Iteration 204, loss = 0.67240329\n",
      "Iteration 205, loss = 0.67160864\n",
      "Iteration 206, loss = 0.67081500\n",
      "Iteration 207, loss = 0.67002241\n",
      "Iteration 208, loss = 0.66923086\n",
      "Iteration 209, loss = 0.66844036\n",
      "Iteration 210, loss = 0.66765093\n",
      "Iteration 211, loss = 0.66686257\n",
      "Iteration 212, loss = 0.66607529\n",
      "Iteration 213, loss = 0.66528910\n",
      "Iteration 214, loss = 0.66450401\n",
      "Iteration 215, loss = 0.66372003\n",
      "Iteration 216, loss = 0.66293717\n",
      "Iteration 217, loss = 0.66215544\n",
      "Iteration 218, loss = 0.66137485\n",
      "Iteration 219, loss = 0.66059540\n",
      "Iteration 220, loss = 0.65981711\n",
      "Iteration 221, loss = 0.65903998\n",
      "Iteration 222, loss = 0.65826403\n",
      "Iteration 223, loss = 0.65748926\n",
      "Iteration 224, loss = 0.65671569\n",
      "Iteration 225, loss = 0.65594331\n",
      "Iteration 226, loss = 0.65517215\n",
      "Iteration 227, loss = 0.65440220\n",
      "Iteration 228, loss = 0.65363348\n",
      "Iteration 229, loss = 0.65286600\n",
      "Iteration 230, loss = 0.65209976\n",
      "Iteration 231, loss = 0.65133477\n",
      "Iteration 232, loss = 0.65057105\n",
      "Iteration 233, loss = 0.64980859\n",
      "Iteration 234, loss = 0.64904741\n",
      "Iteration 235, loss = 0.64828751\n",
      "Iteration 236, loss = 0.64752891\n",
      "Iteration 237, loss = 0.64677160\n",
      "Iteration 238, loss = 0.64601560\n",
      "Iteration 239, loss = 0.64526092\n",
      "Iteration 240, loss = 0.64450756\n",
      "Iteration 241, loss = 0.64375638\n",
      "Iteration 242, loss = 0.64300679\n",
      "Iteration 243, loss = 0.64225859\n",
      "Iteration 244, loss = 0.64151178\n",
      "Iteration 245, loss = 0.64076637\n",
      "Iteration 246, loss = 0.64002236\n",
      "Iteration 247, loss = 0.63927977\n",
      "Iteration 248, loss = 0.63853859\n",
      "Iteration 249, loss = 0.63779885\n",
      "Iteration 250, loss = 0.63706053\n",
      "Iteration 251, loss = 0.63632366\n",
      "Iteration 252, loss = 0.63558884\n",
      "Iteration 253, loss = 0.63485551\n",
      "Iteration 254, loss = 0.63412361\n",
      "Iteration 255, loss = 0.63339315\n",
      "Iteration 256, loss = 0.63266416\n",
      "Iteration 257, loss = 0.63193677\n",
      "Iteration 258, loss = 0.63121082\n",
      "Iteration 259, loss = 0.63048632\n",
      "Iteration 260, loss = 0.62976328\n",
      "Iteration 261, loss = 0.62904178\n",
      "Iteration 262, loss = 0.62832189\n",
      "Iteration 263, loss = 0.62760354\n",
      "Iteration 264, loss = 0.62688673\n",
      "Iteration 265, loss = 0.62617147\n",
      "Iteration 266, loss = 0.62545777\n",
      "Iteration 267, loss = 0.62474563\n",
      "Iteration 268, loss = 0.62403505\n",
      "Iteration 269, loss = 0.62332604\n",
      "Iteration 270, loss = 0.62261861\n",
      "Iteration 271, loss = 0.62191274\n",
      "Iteration 272, loss = 0.62120846\n",
      "Iteration 273, loss = 0.62050574\n",
      "Iteration 274, loss = 0.61980464\n",
      "Iteration 275, loss = 0.61910511\n",
      "Iteration 276, loss = 0.61840713\n",
      "Iteration 277, loss = 0.61771076\n",
      "Iteration 278, loss = 0.61701597\n",
      "Iteration 279, loss = 0.61632276\n",
      "Iteration 280, loss = 0.61563114\n",
      "Iteration 281, loss = 0.61494110\n",
      "Iteration 282, loss = 0.61425265\n",
      "Iteration 283, loss = 0.61356578\n",
      "Iteration 284, loss = 0.61288048\n",
      "Iteration 285, loss = 0.61219680\n",
      "Iteration 286, loss = 0.61151467\n",
      "Iteration 287, loss = 0.61083414\n",
      "Iteration 288, loss = 0.61015520\n",
      "Iteration 289, loss = 0.60947784\n",
      "Iteration 290, loss = 0.60880206\n",
      "Iteration 291, loss = 0.60812786\n",
      "Iteration 292, loss = 0.60745524\n",
      "Iteration 293, loss = 0.60678422\n",
      "Iteration 294, loss = 0.60611479\n",
      "Iteration 295, loss = 0.60544690\n",
      "Iteration 296, loss = 0.60478063\n",
      "Iteration 297, loss = 0.60411594\n",
      "Iteration 298, loss = 0.60345284\n",
      "Iteration 299, loss = 0.60279132\n",
      "Iteration 300, loss = 0.60213137\n",
      "Iteration 301, loss = 0.60147303\n",
      "Iteration 302, loss = 0.60081625\n",
      "Iteration 303, loss = 0.60016103\n",
      "Iteration 304, loss = 0.59950742\n",
      "Iteration 305, loss = 0.59885537\n",
      "Iteration 306, loss = 0.59820492\n",
      "Iteration 307, loss = 0.59755603\n",
      "Iteration 308, loss = 0.59690873\n",
      "Iteration 309, loss = 0.59626300\n",
      "Iteration 310, loss = 0.59561882\n",
      "Iteration 311, loss = 0.59497668\n",
      "Iteration 312, loss = 0.59433643\n",
      "Iteration 313, loss = 0.59369775\n",
      "Iteration 314, loss = 0.59306065\n",
      "Iteration 315, loss = 0.59242519\n",
      "Iteration 316, loss = 0.59179132\n",
      "Iteration 317, loss = 0.59115903\n",
      "Iteration 318, loss = 0.59052833\n",
      "Iteration 319, loss = 0.58989920\n",
      "Iteration 320, loss = 0.58927173\n",
      "Iteration 321, loss = 0.58864584\n",
      "Iteration 322, loss = 0.58802152\n",
      "Iteration 323, loss = 0.58739876\n",
      "Iteration 324, loss = 0.58677762\n",
      "Iteration 325, loss = 0.58615809\n",
      "Iteration 326, loss = 0.58554013\n",
      "Iteration 327, loss = 0.58492374\n",
      "Iteration 328, loss = 0.58430891\n",
      "Iteration 329, loss = 0.58369566\n",
      "Iteration 330, loss = 0.58308403\n",
      "Iteration 331, loss = 0.58247395\n",
      "Iteration 332, loss = 0.58186541\n",
      "Iteration 333, loss = 0.58125840\n",
      "Iteration 334, loss = 0.58065300\n",
      "Iteration 335, loss = 0.58004915\n",
      "Iteration 336, loss = 0.57944684\n",
      "Iteration 337, loss = 0.57884607\n",
      "Iteration 338, loss = 0.57824682\n",
      "Iteration 339, loss = 0.57764911\n",
      "Iteration 340, loss = 0.57705291\n",
      "Iteration 341, loss = 0.57645824\n",
      "Iteration 342, loss = 0.57586513\n",
      "Iteration 343, loss = 0.57527353\n",
      "Iteration 344, loss = 0.57468341\n",
      "Iteration 345, loss = 0.57409480\n",
      "Iteration 346, loss = 0.57350772\n",
      "Iteration 347, loss = 0.57292213\n",
      "Iteration 348, loss = 0.57233802\n",
      "Iteration 349, loss = 0.57175540\n",
      "Iteration 350, loss = 0.57117426\n",
      "Iteration 351, loss = 0.57059460\n",
      "Iteration 352, loss = 0.57001641\n",
      "Iteration 353, loss = 0.56943969\n",
      "Iteration 354, loss = 0.56886447\n",
      "Iteration 355, loss = 0.56829068\n",
      "Iteration 356, loss = 0.56771832\n",
      "Iteration 357, loss = 0.56714744\n",
      "Iteration 358, loss = 0.56657800\n",
      "Iteration 359, loss = 0.56601000\n",
      "Iteration 360, loss = 0.56544344\n",
      "Iteration 361, loss = 0.56487830\n",
      "Iteration 362, loss = 0.56431459\n",
      "Iteration 363, loss = 0.56375263\n",
      "Iteration 364, loss = 0.56319255\n",
      "Iteration 365, loss = 0.56263391\n",
      "Iteration 366, loss = 0.56207671\n",
      "Iteration 367, loss = 0.56152096\n",
      "Iteration 368, loss = 0.56096664\n",
      "Iteration 369, loss = 0.56041375\n",
      "Iteration 370, loss = 0.55986229\n",
      "Iteration 371, loss = 0.55931225\n",
      "Iteration 372, loss = 0.55876362\n",
      "Iteration 373, loss = 0.55821640\n",
      "Iteration 374, loss = 0.55767059\n",
      "Iteration 375, loss = 0.55712617\n",
      "Iteration 376, loss = 0.55658315\n",
      "Iteration 377, loss = 0.55604163\n",
      "Iteration 378, loss = 0.55550150\n",
      "Iteration 379, loss = 0.55496273\n",
      "Iteration 380, loss = 0.55442530\n",
      "Iteration 381, loss = 0.55388922\n",
      "Iteration 382, loss = 0.55335448\n",
      "Iteration 383, loss = 0.55282107\n",
      "Iteration 384, loss = 0.55228899\n",
      "Iteration 385, loss = 0.55175824\n",
      "Iteration 386, loss = 0.55122881\n",
      "Iteration 387, loss = 0.55070069\n",
      "Iteration 388, loss = 0.55017388\n",
      "Iteration 389, loss = 0.54964837\n",
      "Iteration 390, loss = 0.54912425\n",
      "Iteration 391, loss = 0.54860141\n",
      "Iteration 392, loss = 0.54807985\n",
      "Iteration 393, loss = 0.54755955\n",
      "Iteration 394, loss = 0.54704051\n",
      "Iteration 395, loss = 0.54652273\n",
      "Iteration 396, loss = 0.54600619\n",
      "Iteration 397, loss = 0.54549090\n",
      "Iteration 398, loss = 0.54497690\n",
      "Iteration 399, loss = 0.54446482\n",
      "Iteration 400, loss = 0.54395402\n",
      "Iteration 401, loss = 0.54344453\n",
      "Iteration 402, loss = 0.54293631\n",
      "Iteration 403, loss = 0.54242935\n",
      "Iteration 404, loss = 0.54192362\n",
      "Iteration 405, loss = 0.54141913\n",
      "Iteration 406, loss = 0.54091586\n",
      "Iteration 407, loss = 0.54041384\n",
      "Iteration 408, loss = 0.53991308\n",
      "Iteration 409, loss = 0.53941353\n",
      "Iteration 410, loss = 0.53891518\n",
      "Iteration 411, loss = 0.53841804\n",
      "Iteration 412, loss = 0.53792210\n",
      "Iteration 413, loss = 0.53742734\n",
      "Iteration 414, loss = 0.53693377\n",
      "Iteration 415, loss = 0.53644137\n",
      "Iteration 416, loss = 0.53595015\n",
      "Iteration 417, loss = 0.53546010\n",
      "Iteration 418, loss = 0.53497119\n",
      "Iteration 419, loss = 0.53448344\n",
      "Iteration 420, loss = 0.53399683\n",
      "Iteration 421, loss = 0.53351135\n",
      "Iteration 422, loss = 0.53302699\n",
      "Iteration 423, loss = 0.53254390\n",
      "Iteration 424, loss = 0.53206226\n",
      "Iteration 425, loss = 0.53158175\n",
      "Iteration 426, loss = 0.53110238\n",
      "Iteration 427, loss = 0.53062414\n",
      "Iteration 428, loss = 0.53014701\n",
      "Iteration 429, loss = 0.52967100\n",
      "Iteration 430, loss = 0.52919612\n",
      "Iteration 431, loss = 0.52872237\n",
      "Iteration 432, loss = 0.52825042\n",
      "Iteration 433, loss = 0.52777959\n",
      "Iteration 434, loss = 0.52730988\n",
      "Iteration 435, loss = 0.52684127\n",
      "Iteration 436, loss = 0.52637376\n",
      "Iteration 437, loss = 0.52590735\n",
      "Iteration 438, loss = 0.52544206\n",
      "Iteration 439, loss = 0.52497815\n",
      "Iteration 440, loss = 0.52451529\n",
      "Iteration 441, loss = 0.52405348\n",
      "Iteration 442, loss = 0.52359271\n",
      "Iteration 443, loss = 0.52313300\n",
      "Iteration 444, loss = 0.52267433\n",
      "Iteration 445, loss = 0.52221670\n",
      "Iteration 446, loss = 0.52176011\n",
      "Iteration 447, loss = 0.52130455\n",
      "Iteration 448, loss = 0.52085002\n",
      "Iteration 449, loss = 0.52039651\n",
      "Iteration 450, loss = 0.51994401\n",
      "Iteration 451, loss = 0.51949252\n",
      "Iteration 452, loss = 0.51904202\n",
      "Iteration 453, loss = 0.51859252\n",
      "Iteration 454, loss = 0.51814400\n",
      "Iteration 455, loss = 0.51769645\n",
      "Iteration 456, loss = 0.51724987\n",
      "Iteration 457, loss = 0.51680426\n",
      "Iteration 458, loss = 0.51635959\n",
      "Iteration 459, loss = 0.51591594\n",
      "Iteration 460, loss = 0.51547321\n",
      "Iteration 461, loss = 0.51503131\n",
      "Iteration 462, loss = 0.51459037\n",
      "Iteration 463, loss = 0.51415040\n",
      "Iteration 464, loss = 0.51371133\n",
      "Iteration 465, loss = 0.51327317\n",
      "Iteration 466, loss = 0.51283591\n",
      "Iteration 467, loss = 0.51239955\n",
      "Iteration 468, loss = 0.51196406\n",
      "Iteration 469, loss = 0.51152946\n",
      "Iteration 470, loss = 0.51109573\n",
      "Iteration 471, loss = 0.51066289\n",
      "Iteration 472, loss = 0.51023092\n",
      "Iteration 473, loss = 0.50979979\n",
      "Iteration 474, loss = 0.50936949\n",
      "Iteration 475, loss = 0.50894005\n",
      "Iteration 476, loss = 0.50851146\n",
      "Iteration 477, loss = 0.50808369\n",
      "Iteration 478, loss = 0.50765674\n",
      "Iteration 479, loss = 0.50723061\n",
      "Iteration 480, loss = 0.50680528\n",
      "Iteration 481, loss = 0.50638075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 482, loss = 0.50595704\n",
      "Iteration 483, loss = 0.50553412\n",
      "Iteration 484, loss = 0.50511197\n",
      "Iteration 485, loss = 0.50469061\n",
      "Iteration 486, loss = 0.50427003\n",
      "Iteration 487, loss = 0.50385022\n",
      "Iteration 488, loss = 0.50343116\n",
      "Iteration 489, loss = 0.50301287\n",
      "Iteration 490, loss = 0.50259532\n",
      "Iteration 491, loss = 0.50217852\n",
      "Iteration 492, loss = 0.50176246\n",
      "Iteration 493, loss = 0.50134714\n",
      "Iteration 494, loss = 0.50093255\n",
      "Iteration 495, loss = 0.50051868\n",
      "Iteration 496, loss = 0.50010553\n",
      "Iteration 497, loss = 0.49969310\n",
      "Iteration 498, loss = 0.49928137\n",
      "Iteration 499, loss = 0.49887035\n",
      "Iteration 500, loss = 0.49846002\n",
      "Iteration 501, loss = 0.49805038\n",
      "Iteration 502, loss = 0.49764142\n",
      "Iteration 503, loss = 0.49723315\n",
      "Iteration 504, loss = 0.49682555\n",
      "Iteration 505, loss = 0.49641862\n",
      "Iteration 506, loss = 0.49601236\n",
      "Iteration 507, loss = 0.49560675\n",
      "Iteration 508, loss = 0.49520180\n",
      "Iteration 509, loss = 0.49479750\n",
      "Iteration 510, loss = 0.49439384\n",
      "Iteration 511, loss = 0.49399082\n",
      "Iteration 512, loss = 0.49358843\n",
      "Iteration 513, loss = 0.49318801\n",
      "Iteration 514, loss = 0.49279000\n",
      "Iteration 515, loss = 0.49239258\n",
      "Iteration 516, loss = 0.49199577\n",
      "Iteration 517, loss = 0.49159959\n",
      "Iteration 518, loss = 0.49120405\n",
      "Iteration 519, loss = 0.49080917\n",
      "Iteration 520, loss = 0.49041495\n",
      "Iteration 521, loss = 0.49002141\n",
      "Iteration 522, loss = 0.48962856\n",
      "Iteration 523, loss = 0.48923641\n",
      "Iteration 524, loss = 0.48884495\n",
      "Iteration 525, loss = 0.48845419\n",
      "Iteration 526, loss = 0.48806413\n",
      "Iteration 527, loss = 0.48767458\n",
      "Iteration 528, loss = 0.48728393\n",
      "Iteration 529, loss = 0.48688661\n",
      "Iteration 530, loss = 0.48646842\n",
      "Iteration 531, loss = 0.48601736\n",
      "Iteration 532, loss = 0.48546998\n",
      "Iteration 533, loss = 0.48486439\n",
      "Iteration 534, loss = 0.48420734\n",
      "Iteration 535, loss = 0.48351201\n",
      "Iteration 536, loss = 0.48278024\n",
      "Iteration 537, loss = 0.48204665\n",
      "Iteration 538, loss = 0.48137764\n",
      "Iteration 539, loss = 0.48081132\n",
      "Iteration 540, loss = 0.48052717\n",
      "Iteration 541, loss = 0.48010688\n",
      "Iteration 542, loss = 0.47952149\n",
      "Iteration 543, loss = 0.47883437\n",
      "Iteration 544, loss = 0.47814352\n",
      "Iteration 545, loss = 0.47758706\n",
      "Iteration 546, loss = 0.47708281\n",
      "Iteration 547, loss = 0.47657999\n",
      "Iteration 548, loss = 0.47605449\n",
      "Iteration 549, loss = 0.47550301\n",
      "Iteration 550, loss = 0.47492642\n",
      "Iteration 551, loss = 0.47432586\n",
      "Iteration 552, loss = 0.47370273\n",
      "Iteration 553, loss = 0.47307199\n",
      "Iteration 554, loss = 0.47244692\n",
      "Iteration 555, loss = 0.47185024\n",
      "Iteration 556, loss = 0.47129009\n",
      "Iteration 557, loss = 0.47072311\n",
      "Iteration 558, loss = 0.47012836\n",
      "Iteration 559, loss = 0.46951116\n",
      "Iteration 560, loss = 0.46887727\n",
      "Iteration 561, loss = 0.46825258\n",
      "Iteration 562, loss = 0.46765009\n",
      "Iteration 563, loss = 0.46704919\n",
      "Iteration 564, loss = 0.46644436\n",
      "Iteration 565, loss = 0.46583606\n",
      "Iteration 566, loss = 0.46522129\n",
      "Iteration 567, loss = 0.46460174\n",
      "Iteration 568, loss = 0.46397877\n",
      "Iteration 569, loss = 0.46335252\n",
      "Iteration 570, loss = 0.46272858\n",
      "Iteration 571, loss = 0.46210958\n",
      "Iteration 572, loss = 0.46149306\n",
      "Iteration 573, loss = 0.46087648\n",
      "Iteration 574, loss = 0.46025569\n",
      "Iteration 575, loss = 0.45963158\n",
      "Iteration 576, loss = 0.45900495\n",
      "Iteration 577, loss = 0.45838294\n",
      "Iteration 578, loss = 0.45776304\n",
      "Iteration 579, loss = 0.45714366\n",
      "Iteration 580, loss = 0.45652482\n",
      "Iteration 581, loss = 0.45590760\n",
      "Iteration 582, loss = 0.45529024\n",
      "Iteration 583, loss = 0.45467223\n",
      "Iteration 584, loss = 0.45405442\n",
      "Iteration 585, loss = 0.45343847\n",
      "Iteration 586, loss = 0.45282416\n",
      "Iteration 587, loss = 0.45221104\n",
      "Iteration 588, loss = 0.45159919\n",
      "Iteration 589, loss = 0.45098868\n",
      "Iteration 590, loss = 0.45037960\n",
      "Iteration 591, loss = 0.44977202\n",
      "Iteration 592, loss = 0.44916602\n",
      "Iteration 593, loss = 0.44856166\n",
      "Iteration 594, loss = 0.44795900\n",
      "Iteration 595, loss = 0.44735849\n",
      "Iteration 596, loss = 0.44676009\n",
      "Iteration 597, loss = 0.44616307\n",
      "Iteration 598, loss = 0.44556750\n",
      "Iteration 599, loss = 0.44497426\n",
      "Iteration 600, loss = 0.44438325\n",
      "Iteration 601, loss = 0.44379473\n",
      "Iteration 602, loss = 0.44320841\n",
      "Iteration 603, loss = 0.44262417\n",
      "Iteration 604, loss = 0.44204202\n",
      "Iteration 605, loss = 0.44146260\n",
      "Iteration 606, loss = 0.44088557\n",
      "Iteration 607, loss = 0.44031078\n",
      "Iteration 608, loss = 0.43973822\n",
      "Iteration 609, loss = 0.43916790\n",
      "Iteration 610, loss = 0.43859985\n",
      "Iteration 611, loss = 0.43803407\n",
      "Iteration 612, loss = 0.43747055\n",
      "Iteration 613, loss = 0.43690931\n",
      "Iteration 614, loss = 0.43635033\n",
      "Iteration 615, loss = 0.43579362\n",
      "Iteration 616, loss = 0.43523917\n",
      "Iteration 617, loss = 0.43468697\n",
      "Iteration 618, loss = 0.43413757\n",
      "Iteration 619, loss = 0.43359008\n",
      "Iteration 620, loss = 0.43304439\n",
      "Iteration 621, loss = 0.43250157\n",
      "Iteration 622, loss = 0.43196143\n",
      "Iteration 623, loss = 0.43142257\n",
      "Iteration 624, loss = 0.43088692\n",
      "Iteration 625, loss = 0.43035329\n",
      "Iteration 626, loss = 0.42982202\n",
      "Iteration 627, loss = 0.42929294\n",
      "Iteration 628, loss = 0.42876625\n",
      "Iteration 629, loss = 0.42824173\n",
      "Iteration 630, loss = 0.42771929\n",
      "Iteration 631, loss = 0.42719892\n",
      "Iteration 632, loss = 0.42668112\n",
      "Iteration 633, loss = 0.42616524\n",
      "Iteration 634, loss = 0.42565120\n",
      "Iteration 635, loss = 0.42513905\n",
      "Iteration 636, loss = 0.42462931\n",
      "Iteration 637, loss = 0.42412172\n",
      "Iteration 638, loss = 0.42361629\n",
      "Iteration 639, loss = 0.42311271\n",
      "Iteration 640, loss = 0.42261113\n",
      "Iteration 641, loss = 0.42211181\n",
      "Iteration 642, loss = 0.42161433\n",
      "Iteration 643, loss = 0.42111868\n",
      "Iteration 644, loss = 0.42062489\n",
      "Iteration 645, loss = 0.42013326\n",
      "Iteration 646, loss = 0.41964335\n",
      "Iteration 647, loss = 0.41915508\n",
      "Iteration 648, loss = 0.41866889\n",
      "Iteration 649, loss = 0.41818445\n",
      "Iteration 650, loss = 0.41770176\n",
      "Iteration 651, loss = 0.41722082\n",
      "Iteration 652, loss = 0.41674169\n",
      "Iteration 653, loss = 0.41626419\n",
      "Iteration 654, loss = 0.41578845\n",
      "Iteration 655, loss = 0.41531437\n",
      "Iteration 656, loss = 0.41484194\n",
      "Iteration 657, loss = 0.41437113\n",
      "Iteration 658, loss = 0.41390193\n",
      "Iteration 659, loss = 0.41343432\n",
      "Iteration 660, loss = 0.41296828\n",
      "Iteration 661, loss = 0.41250380\n",
      "Iteration 662, loss = 0.41204084\n",
      "Iteration 663, loss = 0.41157960\n",
      "Iteration 664, loss = 0.41111994\n",
      "Iteration 665, loss = 0.41066176\n",
      "Iteration 666, loss = 0.41020503\n",
      "Iteration 667, loss = 0.40974974\n",
      "Iteration 668, loss = 0.40929585\n",
      "Iteration 669, loss = 0.40884334\n",
      "Iteration 670, loss = 0.40839218\n",
      "Iteration 671, loss = 0.40794237\n",
      "Iteration 672, loss = 0.40749386\n",
      "Iteration 673, loss = 0.40704671\n",
      "Iteration 674, loss = 0.40660098\n",
      "Iteration 675, loss = 0.40615652\n",
      "Iteration 676, loss = 0.40571352\n",
      "Iteration 677, loss = 0.40527185\n",
      "Iteration 678, loss = 0.40483139\n",
      "Iteration 679, loss = 0.40439214\n",
      "Iteration 680, loss = 0.40395405\n",
      "Iteration 681, loss = 0.40351711\n",
      "Iteration 682, loss = 0.40308130\n",
      "Iteration 683, loss = 0.40264682\n",
      "Iteration 684, loss = 0.40221372\n",
      "Iteration 685, loss = 0.40178173\n",
      "Iteration 686, loss = 0.40135082\n",
      "Iteration 687, loss = 0.40092098\n",
      "Iteration 688, loss = 0.40049217\n",
      "Iteration 689, loss = 0.40006439\n",
      "Iteration 690, loss = 0.39963761\n",
      "Iteration 691, loss = 0.39921257\n",
      "Iteration 692, loss = 0.39878883\n",
      "Iteration 693, loss = 0.39836582\n",
      "Iteration 694, loss = 0.39794360\n",
      "Iteration 695, loss = 0.39752219\n",
      "Iteration 696, loss = 0.39710165\n",
      "Iteration 697, loss = 0.39668221\n",
      "Iteration 698, loss = 0.39626402\n",
      "Iteration 699, loss = 0.39584681\n",
      "Iteration 700, loss = 0.39543059\n",
      "Iteration 701, loss = 0.39501583\n",
      "Iteration 702, loss = 0.39460198\n",
      "Iteration 703, loss = 0.39418872\n",
      "Iteration 704, loss = 0.39377602\n",
      "Iteration 705, loss = 0.39336448\n",
      "Iteration 706, loss = 0.39295393\n",
      "Iteration 707, loss = 0.39254419\n",
      "Iteration 708, loss = 0.39213524\n",
      "Iteration 709, loss = 0.39172708\n",
      "Iteration 710, loss = 0.39131969\n",
      "Iteration 711, loss = 0.39091307\n",
      "Iteration 712, loss = 0.39050720\n",
      "Iteration 713, loss = 0.39010206\n",
      "Iteration 714, loss = 0.38969766\n",
      "Iteration 715, loss = 0.38929396\n",
      "Iteration 716, loss = 0.38889097\n",
      "Iteration 717, loss = 0.38848865\n",
      "Iteration 718, loss = 0.38808700\n",
      "Iteration 719, loss = 0.38768601\n",
      "Iteration 720, loss = 0.38728565\n",
      "Iteration 721, loss = 0.38688591\n",
      "Iteration 722, loss = 0.38648678\n",
      "Iteration 723, loss = 0.38608847\n",
      "Iteration 724, loss = 0.38569088\n",
      "Iteration 725, loss = 0.38529416\n",
      "Iteration 726, loss = 0.38489835\n",
      "Iteration 727, loss = 0.38450316\n",
      "Iteration 728, loss = 0.38410856\n",
      "Iteration 729, loss = 0.38371454\n",
      "Iteration 730, loss = 0.38332108\n",
      "Iteration 731, loss = 0.38292836\n",
      "Iteration 732, loss = 0.38253646\n",
      "Iteration 733, loss = 0.38214556\n",
      "Iteration 734, loss = 0.38175584\n",
      "Iteration 735, loss = 0.38136638\n",
      "Iteration 736, loss = 0.38097725\n",
      "Iteration 737, loss = 0.38058849\n",
      "Iteration 738, loss = 0.38020016\n",
      "Iteration 739, loss = 0.37981232\n",
      "Iteration 740, loss = 0.37942569\n",
      "Iteration 741, loss = 0.37903967\n",
      "Iteration 742, loss = 0.37865395\n",
      "Iteration 743, loss = 0.37826881\n",
      "Iteration 744, loss = 0.37788397\n",
      "Iteration 745, loss = 0.37749973\n",
      "Iteration 746, loss = 0.37711646\n",
      "Iteration 747, loss = 0.37673359\n",
      "Iteration 748, loss = 0.37635115\n",
      "Iteration 749, loss = 0.37596913\n",
      "Iteration 750, loss = 0.37558755\n",
      "Iteration 751, loss = 0.37520640\n",
      "Iteration 752, loss = 0.37482571\n",
      "Iteration 753, loss = 0.37444546\n",
      "Iteration 754, loss = 0.37406565\n",
      "Iteration 755, loss = 0.37368628\n",
      "Iteration 756, loss = 0.37330734\n",
      "Iteration 757, loss = 0.37292883\n",
      "Iteration 758, loss = 0.37255073\n",
      "Iteration 759, loss = 0.37217305\n",
      "Iteration 760, loss = 0.37179565\n",
      "Iteration 761, loss = 0.37141864\n",
      "Iteration 762, loss = 0.37104198\n",
      "Iteration 763, loss = 0.37066608\n",
      "Iteration 764, loss = 0.37029025\n",
      "Iteration 765, loss = 0.36991438\n",
      "Iteration 766, loss = 0.36953930\n",
      "Iteration 767, loss = 0.36916498\n",
      "Iteration 768, loss = 0.36879120\n",
      "Iteration 769, loss = 0.36841775\n",
      "Iteration 770, loss = 0.36804489\n",
      "Iteration 771, loss = 0.36767220\n",
      "Iteration 772, loss = 0.36729987\n",
      "Iteration 773, loss = 0.36692801\n",
      "Iteration 774, loss = 0.36655640\n",
      "Iteration 775, loss = 0.36618501\n",
      "Iteration 776, loss = 0.36581438\n",
      "Iteration 777, loss = 0.36544414\n",
      "Iteration 778, loss = 0.36507405\n",
      "Iteration 779, loss = 0.36470413\n",
      "Iteration 780, loss = 0.36433520\n",
      "Iteration 781, loss = 0.36396642\n",
      "Iteration 782, loss = 0.36359760\n",
      "Iteration 783, loss = 0.36322967\n",
      "Iteration 784, loss = 0.36286234\n",
      "Iteration 785, loss = 0.36249520\n",
      "Iteration 786, loss = 0.36212825\n",
      "Iteration 787, loss = 0.36176151\n",
      "Iteration 788, loss = 0.36139500\n",
      "Iteration 789, loss = 0.36102875\n",
      "Iteration 790, loss = 0.36066306\n",
      "Iteration 791, loss = 0.36029753\n",
      "Iteration 792, loss = 0.35993232\n",
      "Iteration 793, loss = 0.35956731\n",
      "Iteration 794, loss = 0.35920252\n",
      "Iteration 795, loss = 0.35883815\n",
      "Iteration 796, loss = 0.35847375\n",
      "Iteration 797, loss = 0.35810974\n",
      "Iteration 798, loss = 0.35774609\n",
      "Iteration 799, loss = 0.35738285\n",
      "Iteration 800, loss = 0.35701974\n",
      "Iteration 801, loss = 0.35665694\n",
      "Iteration 802, loss = 0.35629434\n",
      "Iteration 803, loss = 0.35593195\n",
      "Iteration 804, loss = 0.35556977\n",
      "Iteration 805, loss = 0.35520779\n",
      "Iteration 806, loss = 0.35484601\n",
      "Iteration 807, loss = 0.35448444\n",
      "Iteration 808, loss = 0.35412307\n",
      "Iteration 809, loss = 0.35376189\n",
      "Iteration 810, loss = 0.35340091\n",
      "Iteration 811, loss = 0.35304012\n",
      "Iteration 812, loss = 0.35267954\n",
      "Iteration 813, loss = 0.35231906\n",
      "Iteration 814, loss = 0.35195877\n",
      "Iteration 815, loss = 0.35159865\n",
      "Iteration 816, loss = 0.35123868\n",
      "Iteration 817, loss = 0.35087887\n",
      "Iteration 818, loss = 0.35051922\n",
      "Iteration 819, loss = 0.35015971\n",
      "Iteration 820, loss = 0.34980035\n",
      "Iteration 821, loss = 0.34944113\n",
      "Iteration 822, loss = 0.34908205\n",
      "Iteration 823, loss = 0.34872311\n",
      "Iteration 824, loss = 0.34836430\n",
      "Iteration 825, loss = 0.34800562\n",
      "Iteration 826, loss = 0.34764707\n",
      "Iteration 827, loss = 0.34728864\n",
      "Iteration 828, loss = 0.34693034\n",
      "Iteration 829, loss = 0.34657215\n",
      "Iteration 830, loss = 0.34621408\n",
      "Iteration 831, loss = 0.34585613\n",
      "Iteration 832, loss = 0.34549832\n",
      "Iteration 833, loss = 0.34514167\n",
      "Iteration 834, loss = 0.34478520\n",
      "Iteration 835, loss = 0.34442891\n",
      "Iteration 836, loss = 0.34407279\n",
      "Iteration 837, loss = 0.34371683\n",
      "Iteration 838, loss = 0.34336102\n",
      "Iteration 839, loss = 0.34300536\n",
      "Iteration 840, loss = 0.34264985\n",
      "Iteration 841, loss = 0.34229446\n",
      "Iteration 842, loss = 0.34193920\n",
      "Iteration 843, loss = 0.34158407\n",
      "Iteration 844, loss = 0.34122905\n",
      "Iteration 845, loss = 0.34087414\n",
      "Iteration 846, loss = 0.34051973\n",
      "Iteration 847, loss = 0.34016548\n",
      "Iteration 848, loss = 0.33981096\n",
      "Iteration 849, loss = 0.33945625\n",
      "Iteration 850, loss = 0.33910227\n",
      "Iteration 851, loss = 0.33874844\n",
      "Iteration 852, loss = 0.33839478\n",
      "Iteration 853, loss = 0.33804148\n",
      "Iteration 854, loss = 0.33768825\n",
      "Iteration 855, loss = 0.33733508\n",
      "Iteration 856, loss = 0.33698197\n",
      "Iteration 857, loss = 0.33662890\n",
      "Iteration 858, loss = 0.33627588\n",
      "Iteration 859, loss = 0.33592365\n",
      "Iteration 860, loss = 0.33557144\n",
      "Iteration 861, loss = 0.33521910\n",
      "Iteration 862, loss = 0.33486670\n",
      "Iteration 863, loss = 0.33451426\n",
      "Iteration 864, loss = 0.33416194\n",
      "Iteration 865, loss = 0.33381020\n",
      "Iteration 866, loss = 0.33345840\n",
      "Iteration 867, loss = 0.33310654\n",
      "Iteration 868, loss = 0.33275487\n",
      "Iteration 869, loss = 0.33240344\n",
      "Iteration 870, loss = 0.33205200\n",
      "Iteration 871, loss = 0.33170057\n",
      "Iteration 872, loss = 0.33134936\n",
      "Iteration 873, loss = 0.33099825\n",
      "Iteration 874, loss = 0.33064709\n",
      "Iteration 875, loss = 0.33029618\n",
      "Iteration 876, loss = 0.32994529\n",
      "Iteration 877, loss = 0.32959456\n",
      "Iteration 878, loss = 0.32924382\n",
      "Iteration 879, loss = 0.32889324\n",
      "Iteration 880, loss = 0.32854279\n",
      "Iteration 881, loss = 0.32819237\n",
      "Iteration 882, loss = 0.32784208\n",
      "Iteration 883, loss = 0.32749185\n",
      "Iteration 884, loss = 0.32714168\n",
      "Iteration 885, loss = 0.32679159\n",
      "Iteration 886, loss = 0.32644166\n",
      "Iteration 887, loss = 0.32609180\n",
      "Iteration 888, loss = 0.32574201\n",
      "Iteration 889, loss = 0.32539229\n",
      "Iteration 890, loss = 0.32504265\n",
      "Iteration 891, loss = 0.32469309\n",
      "Iteration 892, loss = 0.32434370\n",
      "Iteration 893, loss = 0.32399429\n",
      "Iteration 894, loss = 0.32364502\n",
      "Iteration 895, loss = 0.32329584\n",
      "Iteration 896, loss = 0.32294673\n",
      "Iteration 897, loss = 0.32259770\n",
      "Iteration 898, loss = 0.32224875\n",
      "Iteration 899, loss = 0.32189989\n",
      "Iteration 900, loss = 0.32155111\n",
      "Iteration 901, loss = 0.32120241\n",
      "Iteration 902, loss = 0.32085380\n",
      "Iteration 903, loss = 0.32050528\n",
      "Iteration 904, loss = 0.32015684\n",
      "Iteration 905, loss = 0.31980848\n",
      "Iteration 906, loss = 0.31946020\n",
      "Iteration 907, loss = 0.31911201\n",
      "Iteration 908, loss = 0.31876390\n",
      "Iteration 909, loss = 0.31841587\n",
      "Iteration 910, loss = 0.31806793\n",
      "Iteration 911, loss = 0.31772006\n",
      "Iteration 912, loss = 0.31737227\n",
      "Iteration 913, loss = 0.31702456\n",
      "Iteration 914, loss = 0.31667693\n",
      "Iteration 915, loss = 0.31632938\n",
      "Iteration 916, loss = 0.31598190\n",
      "Iteration 917, loss = 0.31563451\n",
      "Iteration 918, loss = 0.31528719\n",
      "Iteration 919, loss = 0.31493995\n",
      "Iteration 920, loss = 0.31459279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 921, loss = 0.31424571\n",
      "Iteration 922, loss = 0.31389871\n",
      "Iteration 923, loss = 0.31355179\n",
      "Iteration 924, loss = 0.31320495\n",
      "Iteration 925, loss = 0.31285819\n",
      "Iteration 926, loss = 0.31251151\n",
      "Iteration 927, loss = 0.31216491\n",
      "Iteration 928, loss = 0.31181840\n",
      "Iteration 929, loss = 0.31147196\n",
      "Iteration 930, loss = 0.31112561\n",
      "Iteration 931, loss = 0.31077935\n",
      "Iteration 932, loss = 0.31043317\n",
      "Iteration 933, loss = 0.31008707\n",
      "Iteration 934, loss = 0.30974106\n",
      "Iteration 935, loss = 0.30939513\n",
      "Iteration 936, loss = 0.30904929\n",
      "Iteration 937, loss = 0.30870353\n",
      "Iteration 938, loss = 0.30835786\n",
      "Iteration 939, loss = 0.30801228\n",
      "Iteration 940, loss = 0.30766742\n",
      "Iteration 941, loss = 0.30732232\n",
      "Iteration 942, loss = 0.30697695\n",
      "Iteration 943, loss = 0.30663140\n",
      "Iteration 944, loss = 0.30628659\n",
      "Iteration 945, loss = 0.30594194\n",
      "Iteration 946, loss = 0.30559736\n",
      "Iteration 947, loss = 0.30525286\n",
      "Iteration 948, loss = 0.30490842\n",
      "Iteration 949, loss = 0.30456403\n",
      "Iteration 950, loss = 0.30421969\n",
      "Iteration 951, loss = 0.30387541\n",
      "Iteration 952, loss = 0.30353119\n",
      "Iteration 953, loss = 0.30318702\n",
      "Iteration 954, loss = 0.30284313\n",
      "Iteration 955, loss = 0.30249968\n",
      "Iteration 956, loss = 0.30215613\n",
      "Iteration 957, loss = 0.30181251\n",
      "Iteration 958, loss = 0.30146899\n",
      "Iteration 959, loss = 0.30112589\n",
      "Iteration 960, loss = 0.30078280\n",
      "Iteration 961, loss = 0.30043974\n",
      "Iteration 962, loss = 0.30009700\n",
      "Iteration 963, loss = 0.29975436\n",
      "Iteration 964, loss = 0.29941171\n",
      "Iteration 965, loss = 0.29906908\n",
      "Iteration 966, loss = 0.29872678\n",
      "Iteration 967, loss = 0.29838463\n",
      "Iteration 968, loss = 0.29804255\n",
      "Iteration 969, loss = 0.29770060\n",
      "Iteration 970, loss = 0.29735875\n",
      "Iteration 971, loss = 0.29701722\n",
      "Iteration 972, loss = 0.29667573\n",
      "Iteration 973, loss = 0.29633429\n",
      "Iteration 974, loss = 0.29599293\n",
      "Iteration 975, loss = 0.29565191\n",
      "Iteration 976, loss = 0.29531095\n",
      "Iteration 977, loss = 0.29496991\n",
      "Iteration 978, loss = 0.29462924\n",
      "Iteration 979, loss = 0.29428865\n",
      "Iteration 980, loss = 0.29394829\n",
      "Iteration 981, loss = 0.29360793\n",
      "Iteration 982, loss = 0.29326779\n",
      "Iteration 983, loss = 0.29292781\n",
      "Iteration 984, loss = 0.29258798\n",
      "Iteration 985, loss = 0.29224830\n",
      "Iteration 986, loss = 0.29190874\n",
      "Iteration 987, loss = 0.29156930\n",
      "Iteration 988, loss = 0.29123000\n",
      "Iteration 989, loss = 0.29089092\n",
      "Iteration 990, loss = 0.29055197\n",
      "Iteration 991, loss = 0.29021316\n",
      "Iteration 992, loss = 0.28987448\n",
      "Iteration 993, loss = 0.28953600\n",
      "Iteration 994, loss = 0.28919765\n",
      "Iteration 995, loss = 0.28885948\n",
      "Iteration 996, loss = 0.28852146\n",
      "Iteration 997, loss = 0.28818358\n",
      "Iteration 998, loss = 0.28784586\n",
      "Iteration 999, loss = 0.28750829\n",
      "Iteration 1000, loss = 0.28717088\n",
      "Iteration 1, loss = 1.50852130\n",
      "Iteration 2, loss = 1.49152146\n",
      "Iteration 3, loss = 1.47479352\n",
      "Iteration 4, loss = 1.45833687\n",
      "Iteration 5, loss = 1.44216377\n",
      "Iteration 6, loss = 1.42625923\n",
      "Iteration 7, loss = 1.41057029\n",
      "Iteration 8, loss = 1.39512091\n",
      "Iteration 9, loss = 1.37995412\n",
      "Iteration 10, loss = 1.36504278\n",
      "Iteration 11, loss = 1.35035746\n",
      "Iteration 12, loss = 1.33595199\n",
      "Iteration 13, loss = 1.32178425\n",
      "Iteration 14, loss = 1.30785897\n",
      "Iteration 15, loss = 1.29417132\n",
      "Iteration 16, loss = 1.28071905\n",
      "Iteration 17, loss = 1.26753058\n",
      "Iteration 18, loss = 1.25455784\n",
      "Iteration 19, loss = 1.24180471\n",
      "Iteration 20, loss = 1.22925408\n",
      "Iteration 21, loss = 1.21696149\n",
      "Iteration 22, loss = 1.20491996\n",
      "Iteration 23, loss = 1.19311928\n",
      "Iteration 24, loss = 1.18151334\n",
      "Iteration 25, loss = 1.17008472\n",
      "Iteration 26, loss = 1.15883093\n",
      "Iteration 27, loss = 1.14777854\n",
      "Iteration 28, loss = 1.13691490\n",
      "Iteration 29, loss = 1.12624922\n",
      "Iteration 30, loss = 1.11575017\n",
      "Iteration 31, loss = 1.10543915\n",
      "Iteration 32, loss = 1.09531611\n",
      "Iteration 33, loss = 1.08537425\n",
      "Iteration 34, loss = 1.07559484\n",
      "Iteration 35, loss = 1.06598168\n",
      "Iteration 36, loss = 1.05654088\n",
      "Iteration 37, loss = 1.04725753\n",
      "Iteration 38, loss = 1.03812791\n",
      "Iteration 39, loss = 1.02913675\n",
      "Iteration 40, loss = 1.02028320\n",
      "Iteration 41, loss = 1.01156562\n",
      "Iteration 42, loss = 1.00298136\n",
      "Iteration 43, loss = 0.99453990\n",
      "Iteration 44, loss = 0.98624960\n",
      "Iteration 45, loss = 0.97809312\n",
      "Iteration 46, loss = 0.97006571\n",
      "Iteration 47, loss = 0.96217058\n",
      "Iteration 48, loss = 0.95440443\n",
      "Iteration 49, loss = 0.94676815\n",
      "Iteration 50, loss = 0.93926684\n",
      "Iteration 51, loss = 0.93194408\n",
      "Iteration 52, loss = 0.92476073\n",
      "Iteration 53, loss = 0.91772538\n",
      "Iteration 54, loss = 0.91086120\n",
      "Iteration 55, loss = 0.90416783\n",
      "Iteration 56, loss = 0.89762163\n",
      "Iteration 57, loss = 0.89126747\n",
      "Iteration 58, loss = 0.88507618\n",
      "Iteration 59, loss = 0.87906522\n",
      "Iteration 60, loss = 0.87323763\n",
      "Iteration 61, loss = 0.86764990\n",
      "Iteration 62, loss = 0.86225957\n",
      "Iteration 63, loss = 0.85703468\n",
      "Iteration 64, loss = 0.85199022\n",
      "Iteration 65, loss = 0.84713318\n",
      "Iteration 66, loss = 0.84243732\n",
      "Iteration 67, loss = 0.83789417\n",
      "Iteration 68, loss = 0.83352232\n",
      "Iteration 69, loss = 0.82931872\n",
      "Iteration 70, loss = 0.82528805\n",
      "Iteration 71, loss = 0.82142884\n",
      "Iteration 72, loss = 0.81775801\n",
      "Iteration 73, loss = 0.81425390\n",
      "Iteration 74, loss = 0.81091365\n",
      "Iteration 75, loss = 0.80772441\n",
      "Iteration 76, loss = 0.80466910\n",
      "Iteration 77, loss = 0.80173913\n",
      "Iteration 78, loss = 0.79894217\n",
      "Iteration 79, loss = 0.79625608\n",
      "Iteration 80, loss = 0.79369371\n",
      "Iteration 81, loss = 0.79123244\n",
      "Iteration 82, loss = 0.78887457\n",
      "Iteration 83, loss = 0.78662066\n",
      "Iteration 84, loss = 0.78446471\n",
      "Iteration 85, loss = 0.78239273\n",
      "Iteration 86, loss = 0.78039596\n",
      "Iteration 87, loss = 0.77847462\n",
      "Iteration 88, loss = 0.77662259\n",
      "Iteration 89, loss = 0.77483370\n",
      "Iteration 90, loss = 0.77310394\n",
      "Iteration 91, loss = 0.77142943\n",
      "Iteration 92, loss = 0.76980637\n",
      "Iteration 93, loss = 0.76823112\n",
      "Iteration 94, loss = 0.76670744\n",
      "Iteration 95, loss = 0.76522565\n",
      "Iteration 96, loss = 0.76378245\n",
      "Iteration 97, loss = 0.76237793\n",
      "Iteration 98, loss = 0.76101780\n",
      "Iteration 99, loss = 0.75969313\n",
      "Iteration 100, loss = 0.75840240\n",
      "Iteration 101, loss = 0.75714439\n",
      "Iteration 102, loss = 0.75593654\n",
      "Iteration 103, loss = 0.75475297\n",
      "Iteration 104, loss = 0.75359160\n",
      "Iteration 105, loss = 0.75245048\n",
      "Iteration 106, loss = 0.75132777\n",
      "Iteration 107, loss = 0.75022172\n",
      "Iteration 108, loss = 0.74913485\n",
      "Iteration 109, loss = 0.74806348\n",
      "Iteration 110, loss = 0.74700464\n",
      "Iteration 111, loss = 0.74595703\n",
      "Iteration 112, loss = 0.74491946\n",
      "Iteration 113, loss = 0.74389592\n",
      "Iteration 114, loss = 0.74288068\n",
      "Iteration 115, loss = 0.74187575\n",
      "Iteration 116, loss = 0.74088583\n",
      "Iteration 117, loss = 0.73990221\n",
      "Iteration 118, loss = 0.73892417\n",
      "Iteration 119, loss = 0.73795106\n",
      "Iteration 120, loss = 0.73698549\n",
      "Iteration 121, loss = 0.73602574\n",
      "Iteration 122, loss = 0.73506956\n",
      "Iteration 123, loss = 0.73411650\n",
      "Iteration 124, loss = 0.73316618\n",
      "Iteration 125, loss = 0.73221825\n",
      "Iteration 126, loss = 0.73127649\n",
      "Iteration 127, loss = 0.73033703\n",
      "Iteration 128, loss = 0.72940225\n",
      "Iteration 129, loss = 0.72847587\n",
      "Iteration 130, loss = 0.72755752\n",
      "Iteration 131, loss = 0.72664147\n",
      "Iteration 132, loss = 0.72572879\n",
      "Iteration 133, loss = 0.72482190\n",
      "Iteration 134, loss = 0.72391702\n",
      "Iteration 135, loss = 0.72301397\n",
      "Iteration 136, loss = 0.72211402\n",
      "Iteration 137, loss = 0.72121926\n",
      "Iteration 138, loss = 0.72032616\n",
      "Iteration 139, loss = 0.71943458\n",
      "Iteration 140, loss = 0.71854574\n",
      "Iteration 141, loss = 0.71766051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 142, loss = 0.71677693\n",
      "Iteration 143, loss = 0.71589882\n",
      "Iteration 144, loss = 0.71502209\n",
      "Iteration 145, loss = 0.71414664\n",
      "Iteration 146, loss = 0.71327543\n",
      "Iteration 147, loss = 0.71240818\n",
      "Iteration 148, loss = 0.71154218\n",
      "Iteration 149, loss = 0.71067924\n",
      "Iteration 150, loss = 0.70981742\n",
      "Iteration 151, loss = 0.70895656\n",
      "Iteration 152, loss = 0.70809663\n",
      "Iteration 153, loss = 0.70723759\n",
      "Iteration 154, loss = 0.70637942\n",
      "Iteration 155, loss = 0.70552208\n",
      "Iteration 156, loss = 0.70466555\n",
      "Iteration 157, loss = 0.70380981\n",
      "Iteration 158, loss = 0.70295676\n",
      "Iteration 159, loss = 0.70210545\n",
      "Iteration 160, loss = 0.70125705\n",
      "Iteration 161, loss = 0.70041032\n",
      "Iteration 162, loss = 0.69956445\n",
      "Iteration 163, loss = 0.69871944\n",
      "Iteration 164, loss = 0.69787529\n",
      "Iteration 165, loss = 0.69703200\n",
      "Iteration 166, loss = 0.69618955\n",
      "Iteration 167, loss = 0.69534796\n",
      "Iteration 168, loss = 0.69450721\n",
      "Iteration 169, loss = 0.69366731\n",
      "Iteration 170, loss = 0.69282945\n",
      "Iteration 171, loss = 0.69199286\n",
      "Iteration 172, loss = 0.69115711\n",
      "Iteration 173, loss = 0.69032223\n",
      "Iteration 174, loss = 0.68948821\n",
      "Iteration 175, loss = 0.68865507\n",
      "Iteration 176, loss = 0.68782282\n",
      "Iteration 177, loss = 0.68699147\n",
      "Iteration 178, loss = 0.68616101\n",
      "Iteration 179, loss = 0.68533146\n",
      "Iteration 180, loss = 0.68450363\n",
      "Iteration 181, loss = 0.68367669\n",
      "Iteration 182, loss = 0.68285065\n",
      "Iteration 183, loss = 0.68202553\n",
      "Iteration 184, loss = 0.68120136\n",
      "Iteration 185, loss = 0.68037813\n",
      "Iteration 186, loss = 0.67955588\n",
      "Iteration 187, loss = 0.67873462\n",
      "Iteration 188, loss = 0.67791434\n",
      "Iteration 189, loss = 0.67709507\n",
      "Iteration 190, loss = 0.67627681\n",
      "Iteration 191, loss = 0.67545957\n",
      "Iteration 192, loss = 0.67464336\n",
      "Iteration 193, loss = 0.67382818\n",
      "Iteration 194, loss = 0.67301403\n",
      "Iteration 195, loss = 0.67220094\n",
      "Iteration 196, loss = 0.67138889\n",
      "Iteration 197, loss = 0.67057790\n",
      "Iteration 198, loss = 0.66976797\n",
      "Iteration 199, loss = 0.66895928\n",
      "Iteration 200, loss = 0.66815185\n",
      "Iteration 201, loss = 0.66734544\n",
      "Iteration 202, loss = 0.66654008\n",
      "Iteration 203, loss = 0.66573577\n",
      "Iteration 204, loss = 0.66493252\n",
      "Iteration 205, loss = 0.66413035\n",
      "Iteration 206, loss = 0.66332926\n",
      "Iteration 207, loss = 0.66252928\n",
      "Iteration 208, loss = 0.66173041\n",
      "Iteration 209, loss = 0.66093267\n",
      "Iteration 210, loss = 0.66013606\n",
      "Iteration 211, loss = 0.65934060\n",
      "Iteration 212, loss = 0.65854630\n",
      "Iteration 213, loss = 0.65775317\n",
      "Iteration 214, loss = 0.65696139\n",
      "Iteration 215, loss = 0.65617074\n",
      "Iteration 216, loss = 0.65538124\n",
      "Iteration 217, loss = 0.65459291\n",
      "Iteration 218, loss = 0.65380575\n",
      "Iteration 219, loss = 0.65301979\n",
      "Iteration 220, loss = 0.65223515\n",
      "Iteration 221, loss = 0.65145172\n",
      "Iteration 222, loss = 0.65066953\n",
      "Iteration 223, loss = 0.64988857\n",
      "Iteration 224, loss = 0.64910886\n",
      "Iteration 225, loss = 0.64833040\n",
      "Iteration 226, loss = 0.64755321\n",
      "Iteration 227, loss = 0.64677730\n",
      "Iteration 228, loss = 0.64600332\n",
      "Iteration 229, loss = 0.64523093\n",
      "Iteration 230, loss = 0.64445982\n",
      "Iteration 231, loss = 0.64369008\n",
      "Iteration 232, loss = 0.64292171\n",
      "Iteration 233, loss = 0.64215501\n",
      "Iteration 234, loss = 0.64138983\n",
      "Iteration 235, loss = 0.64062595\n",
      "Iteration 236, loss = 0.63986340\n",
      "Iteration 237, loss = 0.63910220\n",
      "Iteration 238, loss = 0.63834236\n",
      "Iteration 239, loss = 0.63758391\n",
      "Iteration 240, loss = 0.63682687\n",
      "Iteration 241, loss = 0.63607129\n",
      "Iteration 242, loss = 0.63531714\n",
      "Iteration 243, loss = 0.63456441\n",
      "Iteration 244, loss = 0.63381318\n",
      "Iteration 245, loss = 0.63306343\n",
      "Iteration 246, loss = 0.63231516\n",
      "Iteration 247, loss = 0.63156839\n",
      "Iteration 248, loss = 0.63082314\n",
      "Iteration 249, loss = 0.63007938\n",
      "Iteration 250, loss = 0.62933716\n",
      "Iteration 251, loss = 0.62859647\n",
      "Iteration 252, loss = 0.62785730\n",
      "Iteration 253, loss = 0.62711966\n",
      "Iteration 254, loss = 0.62638360\n",
      "Iteration 255, loss = 0.62564905\n",
      "Iteration 256, loss = 0.62491596\n",
      "Iteration 257, loss = 0.62418440\n",
      "Iteration 258, loss = 0.62345444\n",
      "Iteration 259, loss = 0.62272602\n",
      "Iteration 260, loss = 0.62199914\n",
      "Iteration 261, loss = 0.62127381\n",
      "Iteration 262, loss = 0.62055003\n",
      "Iteration 263, loss = 0.61982781\n",
      "Iteration 264, loss = 0.61910714\n",
      "Iteration 265, loss = 0.61838803\n",
      "Iteration 266, loss = 0.61767048\n",
      "Iteration 267, loss = 0.61695450\n",
      "Iteration 268, loss = 0.61624008\n",
      "Iteration 269, loss = 0.61552723\n",
      "Iteration 270, loss = 0.61481595\n",
      "Iteration 271, loss = 0.61410624\n",
      "Iteration 272, loss = 0.61339810\n",
      "Iteration 273, loss = 0.61269153\n",
      "Iteration 274, loss = 0.61198654\n",
      "Iteration 275, loss = 0.61128311\n",
      "Iteration 276, loss = 0.61058127\n",
      "Iteration 277, loss = 0.60988100\n",
      "Iteration 278, loss = 0.60918230\n",
      "Iteration 279, loss = 0.60848519\n",
      "Iteration 280, loss = 0.60778967\n",
      "Iteration 281, loss = 0.60709570\n",
      "Iteration 282, loss = 0.60640332\n",
      "Iteration 283, loss = 0.60571253\n",
      "Iteration 284, loss = 0.60502331\n",
      "Iteration 285, loss = 0.60433574\n",
      "Iteration 286, loss = 0.60364999\n",
      "Iteration 287, loss = 0.60296583\n",
      "Iteration 288, loss = 0.60228325\n",
      "Iteration 289, loss = 0.60160227\n",
      "Iteration 290, loss = 0.60092289\n",
      "Iteration 291, loss = 0.60024510\n",
      "Iteration 292, loss = 0.59956892\n",
      "Iteration 293, loss = 0.59889434\n",
      "Iteration 294, loss = 0.59822138\n",
      "Iteration 295, loss = 0.59755002\n",
      "Iteration 296, loss = 0.59688028\n",
      "Iteration 297, loss = 0.59621214\n",
      "Iteration 298, loss = 0.59554562\n",
      "Iteration 299, loss = 0.59488072\n",
      "Iteration 300, loss = 0.59421742\n",
      "Iteration 301, loss = 0.59355573\n",
      "Iteration 302, loss = 0.59289566\n",
      "Iteration 303, loss = 0.59223719\n",
      "Iteration 304, loss = 0.59158032\n",
      "Iteration 305, loss = 0.59092506\n",
      "Iteration 306, loss = 0.59027141\n",
      "Iteration 307, loss = 0.58961935\n",
      "Iteration 308, loss = 0.58896889\n",
      "Iteration 309, loss = 0.58832002\n",
      "Iteration 310, loss = 0.58767274\n",
      "Iteration 311, loss = 0.58702705\n",
      "Iteration 312, loss = 0.58638295\n",
      "Iteration 313, loss = 0.58574043\n",
      "Iteration 314, loss = 0.58509948\n",
      "Iteration 315, loss = 0.58446011\n",
      "Iteration 316, loss = 0.58382231\n",
      "Iteration 317, loss = 0.58318608\n",
      "Iteration 318, loss = 0.58255142\n",
      "Iteration 319, loss = 0.58191831\n",
      "Iteration 320, loss = 0.58128676\n",
      "Iteration 321, loss = 0.58065677\n",
      "Iteration 322, loss = 0.58002859\n",
      "Iteration 323, loss = 0.57940208\n",
      "Iteration 324, loss = 0.57877713\n",
      "Iteration 325, loss = 0.57815372\n",
      "Iteration 326, loss = 0.57753187\n",
      "Iteration 327, loss = 0.57691157\n",
      "Iteration 328, loss = 0.57629284\n",
      "Iteration 329, loss = 0.57567565\n",
      "Iteration 330, loss = 0.57506003\n",
      "Iteration 331, loss = 0.57444595\n",
      "Iteration 332, loss = 0.57383343\n",
      "Iteration 333, loss = 0.57322246\n",
      "Iteration 334, loss = 0.57261304\n",
      "Iteration 335, loss = 0.57200516\n",
      "Iteration 336, loss = 0.57139882\n",
      "Iteration 337, loss = 0.57079402\n",
      "Iteration 338, loss = 0.57019090\n",
      "Iteration 339, loss = 0.56958929\n",
      "Iteration 340, loss = 0.56898919\n",
      "Iteration 341, loss = 0.56839061\n",
      "Iteration 342, loss = 0.56779355\n",
      "Iteration 343, loss = 0.56719801\n",
      "Iteration 344, loss = 0.56660400\n",
      "Iteration 345, loss = 0.56601151\n",
      "Iteration 346, loss = 0.56542054\n",
      "Iteration 347, loss = 0.56483110\n",
      "Iteration 348, loss = 0.56424318\n",
      "Iteration 349, loss = 0.56365680\n",
      "Iteration 350, loss = 0.56307192\n",
      "Iteration 351, loss = 0.56248851\n",
      "Iteration 352, loss = 0.56190679\n",
      "Iteration 353, loss = 0.56132676\n",
      "Iteration 354, loss = 0.56074817\n",
      "Iteration 355, loss = 0.56017103\n",
      "Iteration 356, loss = 0.55959535\n",
      "Iteration 357, loss = 0.55902112\n",
      "Iteration 358, loss = 0.55844834\n",
      "Iteration 359, loss = 0.55787703\n",
      "Iteration 360, loss = 0.55730716\n",
      "Iteration 361, loss = 0.55673875\n",
      "Iteration 362, loss = 0.55617179\n",
      "Iteration 363, loss = 0.55560628\n",
      "Iteration 364, loss = 0.55504221\n",
      "Iteration 365, loss = 0.55447957\n",
      "Iteration 366, loss = 0.55391837\n",
      "Iteration 367, loss = 0.55335860\n",
      "Iteration 368, loss = 0.55280033\n",
      "Iteration 369, loss = 0.55224339\n",
      "Iteration 370, loss = 0.55168779\n",
      "Iteration 371, loss = 0.55113366\n",
      "Iteration 372, loss = 0.55058092\n",
      "Iteration 373, loss = 0.55002966\n",
      "Iteration 374, loss = 0.54947979\n",
      "Iteration 375, loss = 0.54893128\n",
      "Iteration 376, loss = 0.54838412\n",
      "Iteration 377, loss = 0.54783831\n",
      "Iteration 378, loss = 0.54729385\n",
      "Iteration 379, loss = 0.54675071\n",
      "Iteration 380, loss = 0.54620891\n",
      "Iteration 381, loss = 0.54566842\n",
      "Iteration 382, loss = 0.54512925\n",
      "Iteration 383, loss = 0.54459139\n",
      "Iteration 384, loss = 0.54405483\n",
      "Iteration 385, loss = 0.54351956\n",
      "Iteration 386, loss = 0.54298558\n",
      "Iteration 387, loss = 0.54245288\n",
      "Iteration 388, loss = 0.54192144\n",
      "Iteration 389, loss = 0.54139129\n",
      "Iteration 390, loss = 0.54086243\n",
      "Iteration 391, loss = 0.54033479\n",
      "Iteration 392, loss = 0.53980838\n",
      "Iteration 393, loss = 0.53928318\n",
      "Iteration 394, loss = 0.53875927\n",
      "Iteration 395, loss = 0.53823658\n",
      "Iteration 396, loss = 0.53771510\n",
      "Iteration 397, loss = 0.53719482\n",
      "Iteration 398, loss = 0.53667574\n",
      "Iteration 399, loss = 0.53615785\n",
      "Iteration 400, loss = 0.53564114\n",
      "Iteration 401, loss = 0.53512561\n",
      "Iteration 402, loss = 0.53461129\n",
      "Iteration 403, loss = 0.53409815\n",
      "Iteration 404, loss = 0.53358617\n",
      "Iteration 405, loss = 0.53307533\n",
      "Iteration 406, loss = 0.53256564\n",
      "Iteration 407, loss = 0.53205709\n",
      "Iteration 408, loss = 0.53154968\n",
      "Iteration 409, loss = 0.53104343\n",
      "Iteration 410, loss = 0.53053831\n",
      "Iteration 411, loss = 0.53003429\n",
      "Iteration 412, loss = 0.52953136\n",
      "Iteration 413, loss = 0.52902958\n",
      "Iteration 414, loss = 0.52852890\n",
      "Iteration 415, loss = 0.52802931\n",
      "Iteration 416, loss = 0.52753081\n",
      "Iteration 417, loss = 0.52703338\n",
      "Iteration 418, loss = 0.52653703\n",
      "Iteration 419, loss = 0.52604175\n",
      "Iteration 420, loss = 0.52554753\n",
      "Iteration 421, loss = 0.52505437\n",
      "Iteration 422, loss = 0.52456296\n",
      "Iteration 423, loss = 0.52407274\n",
      "Iteration 424, loss = 0.52358362\n",
      "Iteration 425, loss = 0.52309557\n",
      "Iteration 426, loss = 0.52260860\n",
      "Iteration 427, loss = 0.52212270\n",
      "Iteration 428, loss = 0.52163786\n",
      "Iteration 429, loss = 0.52115407\n",
      "Iteration 430, loss = 0.52067133\n",
      "Iteration 431, loss = 0.52018963\n",
      "Iteration 432, loss = 0.51970897\n",
      "Iteration 433, loss = 0.51922933\n",
      "Iteration 434, loss = 0.51875071\n",
      "Iteration 435, loss = 0.51827311\n",
      "Iteration 436, loss = 0.51779651\n",
      "Iteration 437, loss = 0.51732141\n",
      "Iteration 438, loss = 0.51684733\n",
      "Iteration 439, loss = 0.51637426\n",
      "Iteration 440, loss = 0.51590220\n",
      "Iteration 441, loss = 0.51543114\n",
      "Iteration 442, loss = 0.51496109\n",
      "Iteration 443, loss = 0.51449203\n",
      "Iteration 444, loss = 0.51402396\n",
      "Iteration 445, loss = 0.51355687\n",
      "Iteration 446, loss = 0.51309077\n",
      "Iteration 447, loss = 0.51262563\n",
      "Iteration 448, loss = 0.51216146\n",
      "Iteration 449, loss = 0.51169825\n",
      "Iteration 450, loss = 0.51123599\n",
      "Iteration 451, loss = 0.51077467\n",
      "Iteration 452, loss = 0.51031429\n",
      "Iteration 453, loss = 0.50985484\n",
      "Iteration 454, loss = 0.50939631\n",
      "Iteration 455, loss = 0.50893869\n",
      "Iteration 456, loss = 0.50848221\n",
      "Iteration 457, loss = 0.50802693\n",
      "Iteration 458, loss = 0.50757257\n",
      "Iteration 459, loss = 0.50711914\n",
      "Iteration 460, loss = 0.50666666\n",
      "Iteration 461, loss = 0.50621539\n",
      "Iteration 462, loss = 0.50576501\n",
      "Iteration 463, loss = 0.50531551\n",
      "Iteration 464, loss = 0.50486689\n",
      "Iteration 465, loss = 0.50441916\n",
      "Iteration 466, loss = 0.50397231\n",
      "Iteration 467, loss = 0.50352634\n",
      "Iteration 468, loss = 0.50308123\n",
      "Iteration 469, loss = 0.50263699\n",
      "Iteration 470, loss = 0.50219362\n",
      "Iteration 471, loss = 0.50175110\n",
      "Iteration 472, loss = 0.50130943\n",
      "Iteration 473, loss = 0.50086860\n",
      "Iteration 474, loss = 0.50042861\n",
      "Iteration 475, loss = 0.49998944\n",
      "Iteration 476, loss = 0.49955113\n",
      "Iteration 477, loss = 0.49911376\n",
      "Iteration 478, loss = 0.49867717\n",
      "Iteration 479, loss = 0.49824137\n",
      "Iteration 480, loss = 0.49780634\n",
      "Iteration 481, loss = 0.49737208\n",
      "Iteration 482, loss = 0.49693858\n",
      "Iteration 483, loss = 0.49650585\n",
      "Iteration 484, loss = 0.49607387\n",
      "Iteration 485, loss = 0.49564264\n",
      "Iteration 486, loss = 0.49521216\n",
      "Iteration 487, loss = 0.49478242\n",
      "Iteration 488, loss = 0.49435341\n",
      "Iteration 489, loss = 0.49392514\n",
      "Iteration 490, loss = 0.49349758\n",
      "Iteration 491, loss = 0.49307075\n",
      "Iteration 492, loss = 0.49264463\n",
      "Iteration 493, loss = 0.49221922\n",
      "Iteration 494, loss = 0.49179450\n",
      "Iteration 495, loss = 0.49137047\n",
      "Iteration 496, loss = 0.49094717\n",
      "Iteration 497, loss = 0.49052456\n",
      "Iteration 498, loss = 0.49010262\n",
      "Iteration 499, loss = 0.48968132\n",
      "Iteration 500, loss = 0.48926068\n",
      "Iteration 501, loss = 0.48884069\n",
      "Iteration 502, loss = 0.48842135\n",
      "Iteration 503, loss = 0.48800267\n",
      "Iteration 504, loss = 0.48758466\n",
      "Iteration 505, loss = 0.48716728\n",
      "Iteration 506, loss = 0.48675053\n",
      "Iteration 507, loss = 0.48633440\n",
      "Iteration 508, loss = 0.48591888\n",
      "Iteration 509, loss = 0.48550398\n",
      "Iteration 510, loss = 0.48508971\n",
      "Iteration 511, loss = 0.48467605\n",
      "Iteration 512, loss = 0.48426299\n",
      "Iteration 513, loss = 0.48385053\n",
      "Iteration 514, loss = 0.48343865\n",
      "Iteration 515, loss = 0.48302737\n",
      "Iteration 516, loss = 0.48261667\n",
      "Iteration 517, loss = 0.48220655\n",
      "Iteration 518, loss = 0.48179701\n",
      "Iteration 519, loss = 0.48138802\n",
      "Iteration 520, loss = 0.48097961\n",
      "Iteration 521, loss = 0.48057203\n",
      "Iteration 522, loss = 0.48016561\n",
      "Iteration 523, loss = 0.47975978\n",
      "Iteration 524, loss = 0.47935145\n",
      "Iteration 525, loss = 0.47894032\n",
      "Iteration 526, loss = 0.47850375\n",
      "Iteration 527, loss = 0.47803399\n",
      "Iteration 528, loss = 0.47747828\n",
      "Iteration 529, loss = 0.47685443\n",
      "Iteration 530, loss = 0.47617921\n",
      "Iteration 531, loss = 0.47546473\n",
      "Iteration 532, loss = 0.47471402\n",
      "Iteration 533, loss = 0.47395131\n",
      "Iteration 534, loss = 0.47324446\n",
      "Iteration 535, loss = 0.47266349\n",
      "Iteration 536, loss = 0.47235827\n",
      "Iteration 537, loss = 0.47193512\n",
      "Iteration 538, loss = 0.47129184\n",
      "Iteration 539, loss = 0.47055538\n",
      "Iteration 540, loss = 0.46989982\n",
      "Iteration 541, loss = 0.46935606\n",
      "Iteration 542, loss = 0.46883140\n",
      "Iteration 543, loss = 0.46829924\n",
      "Iteration 544, loss = 0.46775428\n",
      "Iteration 545, loss = 0.46718575\n",
      "Iteration 546, loss = 0.46659399\n",
      "Iteration 547, loss = 0.46597975\n",
      "Iteration 548, loss = 0.46534725\n",
      "Iteration 549, loss = 0.46470133\n",
      "Iteration 550, loss = 0.46405261\n",
      "Iteration 551, loss = 0.46341174\n",
      "Iteration 552, loss = 0.46280224\n",
      "Iteration 553, loss = 0.46220864\n",
      "Iteration 554, loss = 0.46161522\n",
      "Iteration 555, loss = 0.46098429\n",
      "Iteration 556, loss = 0.46032712\n",
      "Iteration 557, loss = 0.45968604\n",
      "Iteration 558, loss = 0.45905390\n",
      "Iteration 559, loss = 0.45842542\n",
      "Iteration 560, loss = 0.45779816\n",
      "Iteration 561, loss = 0.45717187\n",
      "Iteration 562, loss = 0.45653450\n",
      "Iteration 563, loss = 0.45588725\n",
      "Iteration 564, loss = 0.45523951\n",
      "Iteration 565, loss = 0.45459234\n",
      "Iteration 566, loss = 0.45394593\n",
      "Iteration 567, loss = 0.45330057\n",
      "Iteration 568, loss = 0.45265724\n",
      "Iteration 569, loss = 0.45201221\n",
      "Iteration 570, loss = 0.45136971\n",
      "Iteration 571, loss = 0.45072107\n",
      "Iteration 572, loss = 0.45007198\n",
      "Iteration 573, loss = 0.44942528\n",
      "Iteration 574, loss = 0.44877918\n",
      "Iteration 575, loss = 0.44813387\n",
      "Iteration 576, loss = 0.44749089\n",
      "Iteration 577, loss = 0.44684780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 578, loss = 0.44620501\n",
      "Iteration 579, loss = 0.44556109\n",
      "Iteration 580, loss = 0.44491774\n",
      "Iteration 581, loss = 0.44427511\n",
      "Iteration 582, loss = 0.44363482\n",
      "Iteration 583, loss = 0.44299547\n",
      "Iteration 584, loss = 0.44235715\n",
      "Iteration 585, loss = 0.44171994\n",
      "Iteration 586, loss = 0.44108400\n",
      "Iteration 587, loss = 0.44044927\n",
      "Iteration 588, loss = 0.43981582\n",
      "Iteration 589, loss = 0.43918388\n",
      "Iteration 590, loss = 0.43855342\n",
      "Iteration 591, loss = 0.43792450\n",
      "Iteration 592, loss = 0.43729816\n",
      "Iteration 593, loss = 0.43667403\n",
      "Iteration 594, loss = 0.43604999\n",
      "Iteration 595, loss = 0.43542760\n",
      "Iteration 596, loss = 0.43480865\n",
      "Iteration 597, loss = 0.43419158\n",
      "Iteration 598, loss = 0.43357642\n",
      "Iteration 599, loss = 0.43296319\n",
      "Iteration 600, loss = 0.43235189\n",
      "Iteration 601, loss = 0.43174255\n",
      "Iteration 602, loss = 0.43113516\n",
      "Iteration 603, loss = 0.43052975\n",
      "Iteration 604, loss = 0.42992632\n",
      "Iteration 605, loss = 0.42932488\n",
      "Iteration 606, loss = 0.42872544\n",
      "Iteration 607, loss = 0.42812800\n",
      "Iteration 608, loss = 0.42753257\n",
      "Iteration 609, loss = 0.42693915\n",
      "Iteration 610, loss = 0.42634775\n",
      "Iteration 611, loss = 0.42575836\n",
      "Iteration 612, loss = 0.42517100\n",
      "Iteration 613, loss = 0.42458564\n",
      "Iteration 614, loss = 0.42400249\n",
      "Iteration 615, loss = 0.42342195\n",
      "Iteration 616, loss = 0.42284320\n",
      "Iteration 617, loss = 0.42226629\n",
      "Iteration 618, loss = 0.42169128\n",
      "Iteration 619, loss = 0.42111824\n",
      "Iteration 620, loss = 0.42054771\n",
      "Iteration 621, loss = 0.41997909\n",
      "Iteration 622, loss = 0.41941245\n",
      "Iteration 623, loss = 0.41884795\n",
      "Iteration 624, loss = 0.41828539\n",
      "Iteration 625, loss = 0.41772495\n",
      "Iteration 626, loss = 0.41716636\n",
      "Iteration 627, loss = 0.41660997\n",
      "Iteration 628, loss = 0.41605551\n",
      "Iteration 629, loss = 0.41550296\n",
      "Iteration 630, loss = 0.41495232\n",
      "Iteration 631, loss = 0.41440359\n",
      "Iteration 632, loss = 0.41385712\n",
      "Iteration 633, loss = 0.41331273\n",
      "Iteration 634, loss = 0.41277027\n",
      "Iteration 635, loss = 0.41222973\n",
      "Iteration 636, loss = 0.41169107\n",
      "Iteration 637, loss = 0.41115428\n",
      "Iteration 638, loss = 0.41061968\n",
      "Iteration 639, loss = 0.41008842\n",
      "Iteration 640, loss = 0.40955928\n",
      "Iteration 641, loss = 0.40903245\n",
      "Iteration 642, loss = 0.40850733\n",
      "Iteration 643, loss = 0.40798472\n",
      "Iteration 644, loss = 0.40746360\n",
      "Iteration 645, loss = 0.40694432\n",
      "Iteration 646, loss = 0.40642673\n",
      "Iteration 647, loss = 0.40591145\n",
      "Iteration 648, loss = 0.40539778\n",
      "Iteration 649, loss = 0.40488573\n",
      "Iteration 650, loss = 0.40437529\n",
      "Iteration 651, loss = 0.40386650\n",
      "Iteration 652, loss = 0.40335935\n",
      "Iteration 653, loss = 0.40285421\n",
      "Iteration 654, loss = 0.40235064\n",
      "Iteration 655, loss = 0.40184842\n",
      "Iteration 656, loss = 0.40134778\n",
      "Iteration 657, loss = 0.40084909\n",
      "Iteration 658, loss = 0.40035227\n",
      "Iteration 659, loss = 0.39985697\n",
      "Iteration 660, loss = 0.39936320\n",
      "Iteration 661, loss = 0.39887136\n",
      "Iteration 662, loss = 0.39838104\n",
      "Iteration 663, loss = 0.39789222\n",
      "Iteration 664, loss = 0.39740487\n",
      "Iteration 665, loss = 0.39691907\n",
      "Iteration 666, loss = 0.39643525\n",
      "Iteration 667, loss = 0.39595297\n",
      "Iteration 668, loss = 0.39547212\n",
      "Iteration 669, loss = 0.39499265\n",
      "Iteration 670, loss = 0.39451454\n",
      "Iteration 671, loss = 0.39403776\n",
      "Iteration 672, loss = 0.39356227\n",
      "Iteration 673, loss = 0.39308805\n",
      "Iteration 674, loss = 0.39261507\n",
      "Iteration 675, loss = 0.39214330\n",
      "Iteration 676, loss = 0.39167286\n",
      "Iteration 677, loss = 0.39120361\n",
      "Iteration 678, loss = 0.39073535\n",
      "Iteration 679, loss = 0.39026882\n",
      "Iteration 680, loss = 0.38980344\n",
      "Iteration 681, loss = 0.38933934\n",
      "Iteration 682, loss = 0.38887624\n",
      "Iteration 683, loss = 0.38841457\n",
      "Iteration 684, loss = 0.38795425\n",
      "Iteration 685, loss = 0.38749494\n",
      "Iteration 686, loss = 0.38703687\n",
      "Iteration 687, loss = 0.38657984\n",
      "Iteration 688, loss = 0.38612376\n",
      "Iteration 689, loss = 0.38566865\n",
      "Iteration 690, loss = 0.38521491\n",
      "Iteration 691, loss = 0.38476199\n",
      "Iteration 692, loss = 0.38430985\n",
      "Iteration 693, loss = 0.38385870\n",
      "Iteration 694, loss = 0.38340869\n",
      "Iteration 695, loss = 0.38295954\n",
      "Iteration 696, loss = 0.38251127\n",
      "Iteration 697, loss = 0.38206386\n",
      "Iteration 698, loss = 0.38161731\n",
      "Iteration 699, loss = 0.38117162\n",
      "Iteration 700, loss = 0.38072679\n",
      "Iteration 701, loss = 0.38028298\n",
      "Iteration 702, loss = 0.37983971\n",
      "Iteration 703, loss = 0.37939739\n",
      "Iteration 704, loss = 0.37895587\n",
      "Iteration 705, loss = 0.37851512\n",
      "Iteration 706, loss = 0.37807511\n",
      "Iteration 707, loss = 0.37763584\n",
      "Iteration 708, loss = 0.37719730\n",
      "Iteration 709, loss = 0.37675947\n",
      "Iteration 710, loss = 0.37632236\n",
      "Iteration 711, loss = 0.37588594\n",
      "Iteration 712, loss = 0.37545020\n",
      "Iteration 713, loss = 0.37501513\n",
      "Iteration 714, loss = 0.37458072\n",
      "Iteration 715, loss = 0.37414707\n",
      "Iteration 716, loss = 0.37371416\n",
      "Iteration 717, loss = 0.37328188\n",
      "Iteration 718, loss = 0.37285023\n",
      "Iteration 719, loss = 0.37241919\n",
      "Iteration 720, loss = 0.37198875\n",
      "Iteration 721, loss = 0.37155893\n",
      "Iteration 722, loss = 0.37112992\n",
      "Iteration 723, loss = 0.37070149\n",
      "Iteration 724, loss = 0.37027362\n",
      "Iteration 725, loss = 0.36984631\n",
      "Iteration 726, loss = 0.36941955\n",
      "Iteration 727, loss = 0.36899331\n",
      "Iteration 728, loss = 0.36856759\n",
      "Iteration 729, loss = 0.36814238\n",
      "Iteration 730, loss = 0.36771766\n",
      "Iteration 731, loss = 0.36729343\n",
      "Iteration 732, loss = 0.36686967\n",
      "Iteration 733, loss = 0.36644640\n",
      "Iteration 734, loss = 0.36602407\n",
      "Iteration 735, loss = 0.36560223\n",
      "Iteration 736, loss = 0.36518086\n",
      "Iteration 737, loss = 0.36475996\n",
      "Iteration 738, loss = 0.36433950\n",
      "Iteration 739, loss = 0.36392040\n",
      "Iteration 740, loss = 0.36350163\n",
      "Iteration 741, loss = 0.36308295\n",
      "Iteration 742, loss = 0.36266445\n",
      "Iteration 743, loss = 0.36224617\n",
      "Iteration 744, loss = 0.36182819\n",
      "Iteration 745, loss = 0.36141123\n",
      "Iteration 746, loss = 0.36099495\n",
      "Iteration 747, loss = 0.36057897\n",
      "Iteration 748, loss = 0.36016324\n",
      "Iteration 749, loss = 0.35974776\n",
      "Iteration 750, loss = 0.35933250\n",
      "Iteration 751, loss = 0.35891746\n",
      "Iteration 752, loss = 0.35850348\n",
      "Iteration 753, loss = 0.35808989\n",
      "Iteration 754, loss = 0.35767677\n",
      "Iteration 755, loss = 0.35726325\n",
      "Iteration 756, loss = 0.35685029\n",
      "Iteration 757, loss = 0.35643768\n",
      "Iteration 758, loss = 0.35602572\n",
      "Iteration 759, loss = 0.35561437\n",
      "Iteration 760, loss = 0.35520361\n",
      "Iteration 761, loss = 0.35479332\n",
      "Iteration 762, loss = 0.35438371\n",
      "Iteration 763, loss = 0.35397443\n",
      "Iteration 764, loss = 0.35356549\n",
      "Iteration 765, loss = 0.35315705\n",
      "Iteration 766, loss = 0.35274858\n",
      "Iteration 767, loss = 0.35234064\n",
      "Iteration 768, loss = 0.35193316\n",
      "Iteration 769, loss = 0.35152583\n",
      "Iteration 770, loss = 0.35111887\n",
      "Iteration 771, loss = 0.35071237\n",
      "Iteration 772, loss = 0.35030601\n",
      "Iteration 773, loss = 0.34989990\n",
      "Iteration 774, loss = 0.34949428\n",
      "Iteration 775, loss = 0.34908874\n",
      "Iteration 776, loss = 0.34868354\n",
      "Iteration 777, loss = 0.34827859\n",
      "Iteration 778, loss = 0.34787393\n",
      "Iteration 779, loss = 0.34746954\n",
      "Iteration 780, loss = 0.34706542\n",
      "Iteration 781, loss = 0.34666153\n",
      "Iteration 782, loss = 0.34625785\n",
      "Iteration 783, loss = 0.34585465\n",
      "Iteration 784, loss = 0.34545134\n",
      "Iteration 785, loss = 0.34504864\n",
      "Iteration 786, loss = 0.34464579\n",
      "Iteration 787, loss = 0.34424356\n",
      "Iteration 788, loss = 0.34384158\n",
      "Iteration 789, loss = 0.34343950\n",
      "Iteration 790, loss = 0.34303736\n",
      "Iteration 791, loss = 0.34263605\n",
      "Iteration 792, loss = 0.34223466\n",
      "Iteration 793, loss = 0.34183326\n",
      "Iteration 794, loss = 0.34143253\n",
      "Iteration 795, loss = 0.34103175\n",
      "Iteration 796, loss = 0.34063097\n",
      "Iteration 797, loss = 0.34023075\n",
      "Iteration 798, loss = 0.33983031\n",
      "Iteration 799, loss = 0.33943044\n",
      "Iteration 800, loss = 0.33903069\n",
      "Iteration 801, loss = 0.33863096\n",
      "Iteration 802, loss = 0.33823170\n",
      "Iteration 803, loss = 0.33783224\n",
      "Iteration 804, loss = 0.33743321\n",
      "Iteration 805, loss = 0.33703429\n",
      "Iteration 806, loss = 0.33663557\n",
      "Iteration 807, loss = 0.33623700\n",
      "Iteration 808, loss = 0.33583863\n",
      "Iteration 809, loss = 0.33544037\n",
      "Iteration 810, loss = 0.33504239\n",
      "Iteration 811, loss = 0.33464443\n",
      "Iteration 812, loss = 0.33424671\n",
      "Iteration 813, loss = 0.33384916\n",
      "Iteration 814, loss = 0.33345191\n",
      "Iteration 815, loss = 0.33305482\n",
      "Iteration 816, loss = 0.33265805\n",
      "Iteration 817, loss = 0.33226136\n",
      "Iteration 818, loss = 0.33186480\n",
      "Iteration 819, loss = 0.33146851\n",
      "Iteration 820, loss = 0.33107232\n",
      "Iteration 821, loss = 0.33067624\n",
      "Iteration 822, loss = 0.33028030\n",
      "Iteration 823, loss = 0.32988464\n",
      "Iteration 824, loss = 0.32948895\n",
      "Iteration 825, loss = 0.32909356\n",
      "Iteration 826, loss = 0.32869830\n",
      "Iteration 827, loss = 0.32830316\n",
      "Iteration 828, loss = 0.32790813\n",
      "Iteration 829, loss = 0.32751321\n",
      "Iteration 830, loss = 0.32711841\n",
      "Iteration 831, loss = 0.32672374\n",
      "Iteration 832, loss = 0.32632919\n",
      "Iteration 833, loss = 0.32593477\n",
      "Iteration 834, loss = 0.32554049\n",
      "Iteration 835, loss = 0.32514642\n",
      "Iteration 836, loss = 0.32475266\n",
      "Iteration 837, loss = 0.32435905\n",
      "Iteration 838, loss = 0.32396557\n",
      "Iteration 839, loss = 0.32357223\n",
      "Iteration 840, loss = 0.32317901\n",
      "Iteration 841, loss = 0.32278593\n",
      "Iteration 842, loss = 0.32239297\n",
      "Iteration 843, loss = 0.32200013\n",
      "Iteration 844, loss = 0.32160742\n",
      "Iteration 845, loss = 0.32121483\n",
      "Iteration 846, loss = 0.32082237\n",
      "Iteration 847, loss = 0.32043001\n",
      "Iteration 848, loss = 0.32003778\n",
      "Iteration 849, loss = 0.31964566\n",
      "Iteration 850, loss = 0.31925365\n",
      "Iteration 851, loss = 0.31886175\n",
      "Iteration 852, loss = 0.31846995\n",
      "Iteration 853, loss = 0.31807827\n",
      "Iteration 854, loss = 0.31768669\n",
      "Iteration 855, loss = 0.31729521\n",
      "Iteration 856, loss = 0.31690384\n",
      "Iteration 857, loss = 0.31651257\n",
      "Iteration 858, loss = 0.31612140\n",
      "Iteration 859, loss = 0.31573033\n",
      "Iteration 860, loss = 0.31533936\n",
      "Iteration 861, loss = 0.31494849\n",
      "Iteration 862, loss = 0.31455771\n",
      "Iteration 863, loss = 0.31416703\n",
      "Iteration 864, loss = 0.31377661\n",
      "Iteration 865, loss = 0.31338718\n",
      "Iteration 866, loss = 0.31299793\n",
      "Iteration 867, loss = 0.31260885\n",
      "Iteration 868, loss = 0.31221993\n",
      "Iteration 869, loss = 0.31183116\n",
      "Iteration 870, loss = 0.31144253\n",
      "Iteration 871, loss = 0.31105405\n",
      "Iteration 872, loss = 0.31066570\n",
      "Iteration 873, loss = 0.31027749\n",
      "Iteration 874, loss = 0.30988940\n",
      "Iteration 875, loss = 0.30950143\n",
      "Iteration 876, loss = 0.30911359\n",
      "Iteration 877, loss = 0.30872586\n",
      "Iteration 878, loss = 0.30833825\n",
      "Iteration 879, loss = 0.30795074\n",
      "Iteration 880, loss = 0.30756335\n",
      "Iteration 881, loss = 0.30717606\n",
      "Iteration 882, loss = 0.30678888\n",
      "Iteration 883, loss = 0.30640180\n",
      "Iteration 884, loss = 0.30601482\n",
      "Iteration 885, loss = 0.30562794\n",
      "Iteration 886, loss = 0.30524117\n",
      "Iteration 887, loss = 0.30485449\n",
      "Iteration 888, loss = 0.30446791\n",
      "Iteration 889, loss = 0.30408142\n",
      "Iteration 890, loss = 0.30369504\n",
      "Iteration 891, loss = 0.30330895\n",
      "Iteration 892, loss = 0.30292299\n",
      "Iteration 893, loss = 0.30253691\n",
      "Iteration 894, loss = 0.30215079\n",
      "Iteration 895, loss = 0.30176506\n",
      "Iteration 896, loss = 0.30137943\n",
      "Iteration 897, loss = 0.30099388\n",
      "Iteration 898, loss = 0.30060842\n",
      "Iteration 899, loss = 0.30022303\n",
      "Iteration 900, loss = 0.29983782\n",
      "Iteration 901, loss = 0.29945272\n",
      "Iteration 902, loss = 0.29906769\n",
      "Iteration 903, loss = 0.29868283\n",
      "Iteration 904, loss = 0.29829803\n",
      "Iteration 905, loss = 0.29791347\n",
      "Iteration 906, loss = 0.29752894\n",
      "Iteration 907, loss = 0.29714444\n",
      "Iteration 908, loss = 0.29676028\n",
      "Iteration 909, loss = 0.29637617\n",
      "Iteration 910, loss = 0.29599210\n",
      "Iteration 911, loss = 0.29560806\n",
      "Iteration 912, loss = 0.29522427\n",
      "Iteration 913, loss = 0.29484066\n",
      "Iteration 914, loss = 0.29445710\n",
      "Iteration 915, loss = 0.29407361\n",
      "Iteration 916, loss = 0.29369021\n",
      "Iteration 917, loss = 0.29330707\n",
      "Iteration 918, loss = 0.29292405\n",
      "Iteration 919, loss = 0.29254102\n",
      "Iteration 920, loss = 0.29215822\n",
      "Iteration 921, loss = 0.29177559\n",
      "Iteration 922, loss = 0.29139305\n",
      "Iteration 923, loss = 0.29101061\n",
      "Iteration 924, loss = 0.29062828\n",
      "Iteration 925, loss = 0.29024607\n",
      "Iteration 926, loss = 0.28986399\n",
      "Iteration 927, loss = 0.28948218\n",
      "Iteration 928, loss = 0.28910039\n",
      "Iteration 929, loss = 0.28871869\n",
      "Iteration 930, loss = 0.28833722\n",
      "Iteration 931, loss = 0.28795587\n",
      "Iteration 932, loss = 0.28757465\n",
      "Iteration 933, loss = 0.28719356\n",
      "Iteration 934, loss = 0.28681260\n",
      "Iteration 935, loss = 0.28643178\n",
      "Iteration 936, loss = 0.28605110\n",
      "Iteration 937, loss = 0.28567057\n",
      "Iteration 938, loss = 0.28529018\n",
      "Iteration 939, loss = 0.28490994\n",
      "Iteration 940, loss = 0.28452985\n",
      "Iteration 941, loss = 0.28414996\n",
      "Iteration 942, loss = 0.28377011\n",
      "Iteration 943, loss = 0.28339045\n",
      "Iteration 944, loss = 0.28301095\n",
      "Iteration 945, loss = 0.28263159\n",
      "Iteration 946, loss = 0.28225238\n",
      "Iteration 947, loss = 0.28187332\n",
      "Iteration 948, loss = 0.28149441\n",
      "Iteration 949, loss = 0.28111565\n",
      "Iteration 950, loss = 0.28073705\n",
      "Iteration 951, loss = 0.28035860\n",
      "Iteration 952, loss = 0.27998030\n",
      "Iteration 953, loss = 0.27960216\n",
      "Iteration 954, loss = 0.27922418\n",
      "Iteration 955, loss = 0.27884635\n",
      "Iteration 956, loss = 0.27846868\n",
      "Iteration 957, loss = 0.27809117\n",
      "Iteration 958, loss = 0.27771382\n",
      "Iteration 959, loss = 0.27733663\n",
      "Iteration 960, loss = 0.27695961\n",
      "Iteration 961, loss = 0.27658275\n",
      "Iteration 962, loss = 0.27620605\n",
      "Iteration 963, loss = 0.27582952\n",
      "Iteration 964, loss = 0.27545315\n",
      "Iteration 965, loss = 0.27507696\n",
      "Iteration 966, loss = 0.27470093\n",
      "Iteration 967, loss = 0.27432507\n",
      "Iteration 968, loss = 0.27394939\n",
      "Iteration 969, loss = 0.27357387\n",
      "Iteration 970, loss = 0.27319853\n",
      "Iteration 971, loss = 0.27282337\n",
      "Iteration 972, loss = 0.27244838\n",
      "Iteration 973, loss = 0.27207357\n",
      "Iteration 974, loss = 0.27169894\n",
      "Iteration 975, loss = 0.27132449\n",
      "Iteration 976, loss = 0.27095022\n",
      "Iteration 977, loss = 0.27057614\n",
      "Iteration 978, loss = 0.27020223\n",
      "Iteration 979, loss = 0.26982852\n",
      "Iteration 980, loss = 0.26945499\n",
      "Iteration 981, loss = 0.26908165\n",
      "Iteration 982, loss = 0.26870849\n",
      "Iteration 983, loss = 0.26833553\n",
      "Iteration 984, loss = 0.26796276\n",
      "Iteration 985, loss = 0.26759019\n",
      "Iteration 986, loss = 0.26721780\n",
      "Iteration 987, loss = 0.26684562\n",
      "Iteration 988, loss = 0.26647363\n",
      "Iteration 989, loss = 0.26610184\n",
      "Iteration 990, loss = 0.26573025\n",
      "Iteration 991, loss = 0.26535886\n",
      "Iteration 992, loss = 0.26498816\n",
      "Iteration 993, loss = 0.26461732\n",
      "Iteration 994, loss = 0.26424636\n",
      "Iteration 995, loss = 0.26387572\n",
      "Iteration 996, loss = 0.26350557\n",
      "Iteration 997, loss = 0.26313562\n",
      "Iteration 998, loss = 0.26276588\n",
      "Iteration 999, loss = 0.26239633\n",
      "Iteration 1000, loss = 0.26202697\n",
      "Iteration 1, loss = 1.50041490\n",
      "Iteration 2, loss = 1.48344226\n",
      "Iteration 3, loss = 1.46674228\n",
      "Iteration 4, loss = 1.45031680\n",
      "Iteration 5, loss = 1.43416969\n",
      "Iteration 6, loss = 1.41828908\n",
      "Iteration 7, loss = 1.40264830\n",
      "Iteration 8, loss = 1.38724231\n",
      "Iteration 9, loss = 1.37208827\n",
      "Iteration 10, loss = 1.35719141\n",
      "Iteration 11, loss = 1.34252303\n",
      "Iteration 12, loss = 1.32813793\n",
      "Iteration 13, loss = 1.31399362\n",
      "Iteration 14, loss = 1.30008179\n",
      "Iteration 15, loss = 1.28642830\n",
      "Iteration 16, loss = 1.27304367\n",
      "Iteration 17, loss = 1.25994541\n",
      "Iteration 18, loss = 1.24704988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19, loss = 1.23439036\n",
      "Iteration 20, loss = 1.22194911\n",
      "Iteration 21, loss = 1.20976662\n",
      "Iteration 22, loss = 1.19781388\n",
      "Iteration 23, loss = 1.18609731\n",
      "Iteration 24, loss = 1.17457443\n",
      "Iteration 25, loss = 1.16322827\n",
      "Iteration 26, loss = 1.15207256\n",
      "Iteration 27, loss = 1.14112319\n",
      "Iteration 28, loss = 1.13035975\n",
      "Iteration 29, loss = 1.11979356\n",
      "Iteration 30, loss = 1.10939125\n",
      "Iteration 31, loss = 1.09916633\n",
      "Iteration 32, loss = 1.08912009\n",
      "Iteration 33, loss = 1.07924897\n",
      "Iteration 34, loss = 1.06954347\n",
      "Iteration 35, loss = 1.05999684\n",
      "Iteration 36, loss = 1.05061895\n",
      "Iteration 37, loss = 1.04140049\n",
      "Iteration 38, loss = 1.03233015\n",
      "Iteration 39, loss = 1.02340039\n",
      "Iteration 40, loss = 1.01460342\n",
      "Iteration 41, loss = 1.00594234\n",
      "Iteration 42, loss = 0.99741455\n",
      "Iteration 43, loss = 0.98904083\n",
      "Iteration 44, loss = 0.98083051\n",
      "Iteration 45, loss = 0.97275708\n",
      "Iteration 46, loss = 0.96482076\n",
      "Iteration 47, loss = 0.95701578\n",
      "Iteration 48, loss = 0.94934612\n",
      "Iteration 49, loss = 0.94185047\n",
      "Iteration 50, loss = 0.93450469\n",
      "Iteration 51, loss = 0.92734248\n",
      "Iteration 52, loss = 0.92034076\n",
      "Iteration 53, loss = 0.91349739\n",
      "Iteration 54, loss = 0.90681873\n",
      "Iteration 55, loss = 0.90028980\n",
      "Iteration 56, loss = 0.89391122\n",
      "Iteration 57, loss = 0.88772060\n",
      "Iteration 58, loss = 0.88172177\n",
      "Iteration 59, loss = 0.87588923\n",
      "Iteration 60, loss = 0.87024401\n",
      "Iteration 61, loss = 0.86479115\n",
      "Iteration 62, loss = 0.85951928\n",
      "Iteration 63, loss = 0.85442749\n",
      "Iteration 64, loss = 0.84949189\n",
      "Iteration 65, loss = 0.84472851\n",
      "Iteration 66, loss = 0.84014518\n",
      "Iteration 67, loss = 0.83573798\n",
      "Iteration 68, loss = 0.83148866\n",
      "Iteration 69, loss = 0.82740506\n",
      "Iteration 70, loss = 0.82349686\n",
      "Iteration 71, loss = 0.81975415\n",
      "Iteration 72, loss = 0.81617700\n",
      "Iteration 73, loss = 0.81275619\n",
      "Iteration 74, loss = 0.80948110\n",
      "Iteration 75, loss = 0.80635519\n",
      "Iteration 76, loss = 0.80336051\n",
      "Iteration 77, loss = 0.80048749\n",
      "Iteration 78, loss = 0.79772232\n",
      "Iteration 79, loss = 0.79507594\n",
      "Iteration 80, loss = 0.79254019\n",
      "Iteration 81, loss = 0.79009956\n",
      "Iteration 82, loss = 0.78775914\n",
      "Iteration 83, loss = 0.78551410\n",
      "Iteration 84, loss = 0.78336006\n",
      "Iteration 85, loss = 0.78129218\n",
      "Iteration 86, loss = 0.77931350\n",
      "Iteration 87, loss = 0.77741789\n",
      "Iteration 88, loss = 0.77558760\n",
      "Iteration 89, loss = 0.77381856\n",
      "Iteration 90, loss = 0.77210680\n",
      "Iteration 91, loss = 0.77044850\n",
      "Iteration 92, loss = 0.76884201\n",
      "Iteration 93, loss = 0.76728775\n",
      "Iteration 94, loss = 0.76577701\n",
      "Iteration 95, loss = 0.76430649\n",
      "Iteration 96, loss = 0.76287815\n",
      "Iteration 97, loss = 0.76148644\n",
      "Iteration 98, loss = 0.76013231\n",
      "Iteration 99, loss = 0.75881046\n",
      "Iteration 100, loss = 0.75752874\n",
      "Iteration 101, loss = 0.75629139\n",
      "Iteration 102, loss = 0.75507897\n",
      "Iteration 103, loss = 0.75388934\n",
      "Iteration 104, loss = 0.75272326\n",
      "Iteration 105, loss = 0.75158072\n",
      "Iteration 106, loss = 0.75045718\n",
      "Iteration 107, loss = 0.74935414\n",
      "Iteration 108, loss = 0.74826566\n",
      "Iteration 109, loss = 0.74719027\n",
      "Iteration 110, loss = 0.74612662\n",
      "Iteration 111, loss = 0.74507346\n",
      "Iteration 112, loss = 0.74402963\n",
      "Iteration 113, loss = 0.74299745\n",
      "Iteration 114, loss = 0.74198752\n",
      "Iteration 115, loss = 0.74098497\n",
      "Iteration 116, loss = 0.73998897\n",
      "Iteration 117, loss = 0.73899961\n",
      "Iteration 118, loss = 0.73802865\n",
      "Iteration 119, loss = 0.73706772\n",
      "Iteration 120, loss = 0.73611413\n",
      "Iteration 121, loss = 0.73516548\n",
      "Iteration 122, loss = 0.73422244\n",
      "Iteration 123, loss = 0.73328699\n",
      "Iteration 124, loss = 0.73235887\n",
      "Iteration 125, loss = 0.73143699\n",
      "Iteration 126, loss = 0.73052129\n",
      "Iteration 127, loss = 0.72961253\n",
      "Iteration 128, loss = 0.72870741\n",
      "Iteration 129, loss = 0.72780506\n",
      "Iteration 130, loss = 0.72690519\n",
      "Iteration 131, loss = 0.72600758\n",
      "Iteration 132, loss = 0.72511200\n",
      "Iteration 133, loss = 0.72421824\n",
      "Iteration 134, loss = 0.72332614\n",
      "Iteration 135, loss = 0.72243553\n",
      "Iteration 136, loss = 0.72154913\n",
      "Iteration 137, loss = 0.72066553\n",
      "Iteration 138, loss = 0.71978332\n",
      "Iteration 139, loss = 0.71890240\n",
      "Iteration 140, loss = 0.71802266\n",
      "Iteration 141, loss = 0.71714399\n",
      "Iteration 142, loss = 0.71626821\n",
      "Iteration 143, loss = 0.71539417\n",
      "Iteration 144, loss = 0.71452109\n",
      "Iteration 145, loss = 0.71364890\n",
      "Iteration 146, loss = 0.71278122\n",
      "Iteration 147, loss = 0.71191471\n",
      "Iteration 148, loss = 0.71104921\n",
      "Iteration 149, loss = 0.71018464\n",
      "Iteration 150, loss = 0.70932098\n",
      "Iteration 151, loss = 0.70845817\n",
      "Iteration 152, loss = 0.70759635\n",
      "Iteration 153, loss = 0.70673944\n",
      "Iteration 154, loss = 0.70588395\n",
      "Iteration 155, loss = 0.70503012\n",
      "Iteration 156, loss = 0.70418060\n",
      "Iteration 157, loss = 0.70333189\n",
      "Iteration 158, loss = 0.70248396\n",
      "Iteration 159, loss = 0.70163681\n",
      "Iteration 160, loss = 0.70079045\n",
      "Iteration 161, loss = 0.69994484\n",
      "Iteration 162, loss = 0.69910000\n",
      "Iteration 163, loss = 0.69825592\n",
      "Iteration 164, loss = 0.69741260\n",
      "Iteration 165, loss = 0.69657002\n",
      "Iteration 166, loss = 0.69572909\n",
      "Iteration 167, loss = 0.69489005\n",
      "Iteration 168, loss = 0.69405392\n",
      "Iteration 169, loss = 0.69322024\n",
      "Iteration 170, loss = 0.69238712\n",
      "Iteration 171, loss = 0.69155460\n",
      "Iteration 172, loss = 0.69072274\n",
      "Iteration 173, loss = 0.68989159\n",
      "Iteration 174, loss = 0.68906119\n",
      "Iteration 175, loss = 0.68823158\n",
      "Iteration 176, loss = 0.68740280\n",
      "Iteration 177, loss = 0.68657486\n",
      "Iteration 178, loss = 0.68574781\n",
      "Iteration 179, loss = 0.68492167\n",
      "Iteration 180, loss = 0.68409645\n",
      "Iteration 181, loss = 0.68327217\n",
      "Iteration 182, loss = 0.68244886\n",
      "Iteration 183, loss = 0.68162651\n",
      "Iteration 184, loss = 0.68080515\n",
      "Iteration 185, loss = 0.67998477\n",
      "Iteration 186, loss = 0.67916540\n",
      "Iteration 187, loss = 0.67834704\n",
      "Iteration 188, loss = 0.67752969\n",
      "Iteration 189, loss = 0.67671335\n",
      "Iteration 190, loss = 0.67589804\n",
      "Iteration 191, loss = 0.67508376\n",
      "Iteration 192, loss = 0.67427051\n",
      "Iteration 193, loss = 0.67345830\n",
      "Iteration 194, loss = 0.67264713\n",
      "Iteration 195, loss = 0.67183721\n",
      "Iteration 196, loss = 0.67102878\n",
      "Iteration 197, loss = 0.67022125\n",
      "Iteration 198, loss = 0.66941463\n",
      "Iteration 199, loss = 0.66860895\n",
      "Iteration 200, loss = 0.66780422\n",
      "Iteration 201, loss = 0.66700055\n",
      "Iteration 202, loss = 0.66619817\n",
      "Iteration 203, loss = 0.66539689\n",
      "Iteration 204, loss = 0.66459673\n",
      "Iteration 205, loss = 0.66379768\n",
      "Iteration 206, loss = 0.66299983\n",
      "Iteration 207, loss = 0.66220310\n",
      "Iteration 208, loss = 0.66140744\n",
      "Iteration 209, loss = 0.66061285\n",
      "Iteration 210, loss = 0.65981938\n",
      "Iteration 211, loss = 0.65902714\n",
      "Iteration 212, loss = 0.65823606\n",
      "Iteration 213, loss = 0.65744613\n",
      "Iteration 214, loss = 0.65665737\n",
      "Iteration 215, loss = 0.65586979\n",
      "Iteration 216, loss = 0.65508339\n",
      "Iteration 217, loss = 0.65429818\n",
      "Iteration 218, loss = 0.65351417\n",
      "Iteration 219, loss = 0.65273202\n",
      "Iteration 220, loss = 0.65195130\n",
      "Iteration 221, loss = 0.65117185\n",
      "Iteration 222, loss = 0.65039366\n",
      "Iteration 223, loss = 0.64961676\n",
      "Iteration 224, loss = 0.64884115\n",
      "Iteration 225, loss = 0.64806684\n",
      "Iteration 226, loss = 0.64729385\n",
      "Iteration 227, loss = 0.64652217\n",
      "Iteration 228, loss = 0.64575183\n",
      "Iteration 229, loss = 0.64498282\n",
      "Iteration 230, loss = 0.64421516\n",
      "Iteration 231, loss = 0.64344885\n",
      "Iteration 232, loss = 0.64268390\n",
      "Iteration 233, loss = 0.64192032\n",
      "Iteration 234, loss = 0.64115811\n",
      "Iteration 235, loss = 0.64039732\n",
      "Iteration 236, loss = 0.63963787\n",
      "Iteration 237, loss = 0.63887981\n",
      "Iteration 238, loss = 0.63812317\n",
      "Iteration 239, loss = 0.63736794\n",
      "Iteration 240, loss = 0.63661411\n",
      "Iteration 241, loss = 0.63586170\n",
      "Iteration 242, loss = 0.63511071\n",
      "Iteration 243, loss = 0.63436115\n",
      "Iteration 244, loss = 0.63361301\n",
      "Iteration 245, loss = 0.63286631\n",
      "Iteration 246, loss = 0.63212105\n",
      "Iteration 247, loss = 0.63137723\n",
      "Iteration 248, loss = 0.63063486\n",
      "Iteration 249, loss = 0.62989398\n",
      "Iteration 250, loss = 0.62915452\n",
      "Iteration 251, loss = 0.62841651\n",
      "Iteration 252, loss = 0.62768000\n",
      "Iteration 253, loss = 0.62694495\n",
      "Iteration 254, loss = 0.62621138\n",
      "Iteration 255, loss = 0.62547928\n",
      "Iteration 256, loss = 0.62474866\n",
      "Iteration 257, loss = 0.62401953\n",
      "Iteration 258, loss = 0.62329189\n",
      "Iteration 259, loss = 0.62256575\n",
      "Iteration 260, loss = 0.62184111\n",
      "Iteration 261, loss = 0.62111797\n",
      "Iteration 262, loss = 0.62039633\n",
      "Iteration 263, loss = 0.61967619\n",
      "Iteration 264, loss = 0.61895755\n",
      "Iteration 265, loss = 0.61824044\n",
      "Iteration 266, loss = 0.61752484\n",
      "Iteration 267, loss = 0.61681076\n",
      "Iteration 268, loss = 0.61609820\n",
      "Iteration 269, loss = 0.61538715\n",
      "Iteration 270, loss = 0.61467766\n",
      "Iteration 271, loss = 0.61396977\n",
      "Iteration 272, loss = 0.61326413\n",
      "Iteration 273, loss = 0.61256008\n",
      "Iteration 274, loss = 0.61185758\n",
      "Iteration 275, loss = 0.61115666\n",
      "Iteration 276, loss = 0.61045731\n",
      "Iteration 277, loss = 0.60975955\n",
      "Iteration 278, loss = 0.60906336\n",
      "Iteration 279, loss = 0.60836875\n",
      "Iteration 280, loss = 0.60767573\n",
      "Iteration 281, loss = 0.60698430\n",
      "Iteration 282, loss = 0.60629448\n",
      "Iteration 283, loss = 0.60560624\n",
      "Iteration 284, loss = 0.60491971\n",
      "Iteration 285, loss = 0.60423507\n",
      "Iteration 286, loss = 0.60355202\n",
      "Iteration 287, loss = 0.60287054\n",
      "Iteration 288, loss = 0.60219065\n",
      "Iteration 289, loss = 0.60151239\n",
      "Iteration 290, loss = 0.60083573\n",
      "Iteration 291, loss = 0.60016069\n",
      "Iteration 292, loss = 0.59948730\n",
      "Iteration 293, loss = 0.59881553\n",
      "Iteration 294, loss = 0.59814538\n",
      "Iteration 295, loss = 0.59747689\n",
      "Iteration 296, loss = 0.59681005\n",
      "Iteration 297, loss = 0.59614483\n",
      "Iteration 298, loss = 0.59548126\n",
      "Iteration 299, loss = 0.59481933\n",
      "Iteration 300, loss = 0.59415904\n",
      "Iteration 301, loss = 0.59350039\n",
      "Iteration 302, loss = 0.59284339\n",
      "Iteration 303, loss = 0.59218801\n",
      "Iteration 304, loss = 0.59153425\n",
      "Iteration 305, loss = 0.59088214\n",
      "Iteration 306, loss = 0.59023164\n",
      "Iteration 307, loss = 0.58958276\n",
      "Iteration 308, loss = 0.58893549\n",
      "Iteration 309, loss = 0.58828984\n",
      "Iteration 310, loss = 0.58764580\n",
      "Iteration 311, loss = 0.58700337\n",
      "Iteration 312, loss = 0.58636254\n",
      "Iteration 313, loss = 0.58572330\n",
      "Iteration 314, loss = 0.58508567\n",
      "Iteration 315, loss = 0.58444962\n",
      "Iteration 316, loss = 0.58381515\n",
      "Iteration 317, loss = 0.58318227\n",
      "Iteration 318, loss = 0.58255096\n",
      "Iteration 319, loss = 0.58192123\n",
      "Iteration 320, loss = 0.58129308\n",
      "Iteration 321, loss = 0.58066678\n",
      "Iteration 322, loss = 0.58004208\n",
      "Iteration 323, loss = 0.57941895\n",
      "Iteration 324, loss = 0.57879739\n",
      "Iteration 325, loss = 0.57817741\n",
      "Iteration 326, loss = 0.57755900\n",
      "Iteration 327, loss = 0.57694217\n",
      "Iteration 328, loss = 0.57632690\n",
      "Iteration 329, loss = 0.57571322\n",
      "Iteration 330, loss = 0.57510126\n",
      "Iteration 331, loss = 0.57449088\n",
      "Iteration 332, loss = 0.57388206\n",
      "Iteration 333, loss = 0.57327481\n",
      "Iteration 334, loss = 0.57266913\n",
      "Iteration 335, loss = 0.57206500\n",
      "Iteration 336, loss = 0.57146243\n",
      "Iteration 337, loss = 0.57086142\n",
      "Iteration 338, loss = 0.57026195\n",
      "Iteration 339, loss = 0.56966402\n",
      "Iteration 340, loss = 0.56906762\n",
      "Iteration 341, loss = 0.56847276\n",
      "Iteration 342, loss = 0.56787942\n",
      "Iteration 343, loss = 0.56728761\n",
      "Iteration 344, loss = 0.56669730\n",
      "Iteration 345, loss = 0.56610851\n",
      "Iteration 346, loss = 0.56552122\n",
      "Iteration 347, loss = 0.56493542\n",
      "Iteration 348, loss = 0.56435111\n",
      "Iteration 349, loss = 0.56376829\n",
      "Iteration 350, loss = 0.56318694\n",
      "Iteration 351, loss = 0.56260707\n",
      "Iteration 352, loss = 0.56202866\n",
      "Iteration 353, loss = 0.56145172\n",
      "Iteration 354, loss = 0.56087623\n",
      "Iteration 355, loss = 0.56030219\n",
      "Iteration 356, loss = 0.55972959\n",
      "Iteration 357, loss = 0.55915842\n",
      "Iteration 358, loss = 0.55858869\n",
      "Iteration 359, loss = 0.55802038\n",
      "Iteration 360, loss = 0.55745349\n",
      "Iteration 361, loss = 0.55688802\n",
      "Iteration 362, loss = 0.55632395\n",
      "Iteration 363, loss = 0.55576128\n",
      "Iteration 364, loss = 0.55520000\n",
      "Iteration 365, loss = 0.55464011\n",
      "Iteration 366, loss = 0.55408161\n",
      "Iteration 367, loss = 0.55352448\n",
      "Iteration 368, loss = 0.55296872\n",
      "Iteration 369, loss = 0.55241453\n",
      "Iteration 370, loss = 0.55186212\n",
      "Iteration 371, loss = 0.55131109\n",
      "Iteration 372, loss = 0.55076143\n",
      "Iteration 373, loss = 0.55021315\n",
      "Iteration 374, loss = 0.54966624\n",
      "Iteration 375, loss = 0.54912069\n",
      "Iteration 376, loss = 0.54857650\n",
      "Iteration 377, loss = 0.54803367\n",
      "Iteration 378, loss = 0.54749219\n",
      "Iteration 379, loss = 0.54695206\n",
      "Iteration 380, loss = 0.54641326\n",
      "Iteration 381, loss = 0.54587581\n",
      "Iteration 382, loss = 0.54533968\n",
      "Iteration 383, loss = 0.54480488\n",
      "Iteration 384, loss = 0.54427139\n",
      "Iteration 385, loss = 0.54373921\n",
      "Iteration 386, loss = 0.54320834\n",
      "Iteration 387, loss = 0.54267876\n",
      "Iteration 388, loss = 0.54215047\n",
      "Iteration 389, loss = 0.54162347\n",
      "Iteration 390, loss = 0.54109774\n",
      "Iteration 391, loss = 0.54057328\n",
      "Iteration 392, loss = 0.54005008\n",
      "Iteration 393, loss = 0.53952813\n",
      "Iteration 394, loss = 0.53900742\n",
      "Iteration 395, loss = 0.53848796\n",
      "Iteration 396, loss = 0.53796973\n",
      "Iteration 397, loss = 0.53745272\n",
      "Iteration 398, loss = 0.53693693\n",
      "Iteration 399, loss = 0.53642234\n",
      "Iteration 400, loss = 0.53590896\n",
      "Iteration 401, loss = 0.53539677\n",
      "Iteration 402, loss = 0.53488578\n",
      "Iteration 403, loss = 0.53437596\n",
      "Iteration 404, loss = 0.53386731\n",
      "Iteration 405, loss = 0.53335984\n",
      "Iteration 406, loss = 0.53285352\n",
      "Iteration 407, loss = 0.53234835\n",
      "Iteration 408, loss = 0.53184433\n",
      "Iteration 409, loss = 0.53134145\n",
      "Iteration 410, loss = 0.53083970\n",
      "Iteration 411, loss = 0.53033908\n",
      "Iteration 412, loss = 0.52983957\n",
      "Iteration 413, loss = 0.52934118\n",
      "Iteration 414, loss = 0.52884389\n",
      "Iteration 415, loss = 0.52834770\n",
      "Iteration 416, loss = 0.52785260\n",
      "Iteration 417, loss = 0.52735896\n",
      "Iteration 418, loss = 0.52686687\n",
      "Iteration 419, loss = 0.52637595\n",
      "Iteration 420, loss = 0.52588613\n",
      "Iteration 421, loss = 0.52539740\n",
      "Iteration 422, loss = 0.52490977\n",
      "Iteration 423, loss = 0.52442323\n",
      "Iteration 424, loss = 0.52393778\n",
      "Iteration 425, loss = 0.52345342\n",
      "Iteration 426, loss = 0.52297012\n",
      "Iteration 427, loss = 0.52248790\n",
      "Iteration 428, loss = 0.52200675\n",
      "Iteration 429, loss = 0.52152666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 430, loss = 0.52104762\n",
      "Iteration 431, loss = 0.52056963\n",
      "Iteration 432, loss = 0.52009270\n",
      "Iteration 433, loss = 0.51961677\n",
      "Iteration 434, loss = 0.51914186\n",
      "Iteration 435, loss = 0.51866798\n",
      "Iteration 436, loss = 0.51819510\n",
      "Iteration 437, loss = 0.51772322\n",
      "Iteration 438, loss = 0.51725234\n",
      "Iteration 439, loss = 0.51678244\n",
      "Iteration 440, loss = 0.51631352\n",
      "Iteration 441, loss = 0.51584557\n",
      "Iteration 442, loss = 0.51537858\n",
      "Iteration 443, loss = 0.51491255\n",
      "Iteration 444, loss = 0.51444748\n",
      "Iteration 445, loss = 0.51398335\n",
      "Iteration 446, loss = 0.51352015\n",
      "Iteration 447, loss = 0.51305790\n",
      "Iteration 448, loss = 0.51259686\n",
      "Iteration 449, loss = 0.51213719\n",
      "Iteration 450, loss = 0.51167847\n",
      "Iteration 451, loss = 0.51122071\n",
      "Iteration 452, loss = 0.51076390\n",
      "Iteration 453, loss = 0.51030803\n",
      "Iteration 454, loss = 0.50985309\n",
      "Iteration 455, loss = 0.50939908\n",
      "Iteration 456, loss = 0.50894599\n",
      "Iteration 457, loss = 0.50849382\n",
      "Iteration 458, loss = 0.50804255\n",
      "Iteration 459, loss = 0.50759219\n",
      "Iteration 460, loss = 0.50714272\n",
      "Iteration 461, loss = 0.50669413\n",
      "Iteration 462, loss = 0.50624651\n",
      "Iteration 463, loss = 0.50580041\n",
      "Iteration 464, loss = 0.50535520\n",
      "Iteration 465, loss = 0.50491090\n",
      "Iteration 466, loss = 0.50446748\n",
      "Iteration 467, loss = 0.50402496\n",
      "Iteration 468, loss = 0.50358332\n",
      "Iteration 469, loss = 0.50314257\n",
      "Iteration 470, loss = 0.50270269\n",
      "Iteration 471, loss = 0.50226380\n",
      "Iteration 472, loss = 0.50182592\n",
      "Iteration 473, loss = 0.50138887\n",
      "Iteration 474, loss = 0.50095266\n",
      "Iteration 475, loss = 0.50051729\n",
      "Iteration 476, loss = 0.50008275\n",
      "Iteration 477, loss = 0.49964904\n",
      "Iteration 478, loss = 0.49921617\n",
      "Iteration 479, loss = 0.49878425\n",
      "Iteration 480, loss = 0.49835325\n",
      "Iteration 481, loss = 0.49792324\n",
      "Iteration 482, loss = 0.49749405\n",
      "Iteration 483, loss = 0.49706567\n",
      "Iteration 484, loss = 0.49663811\n",
      "Iteration 485, loss = 0.49621135\n",
      "Iteration 486, loss = 0.49578540\n",
      "Iteration 487, loss = 0.49536023\n",
      "Iteration 488, loss = 0.49493585\n",
      "Iteration 489, loss = 0.49451225\n",
      "Iteration 490, loss = 0.49408943\n",
      "Iteration 491, loss = 0.49366737\n",
      "Iteration 492, loss = 0.49324607\n",
      "Iteration 493, loss = 0.49282552\n",
      "Iteration 494, loss = 0.49240572\n",
      "Iteration 495, loss = 0.49198666\n",
      "Iteration 496, loss = 0.49156834\n",
      "Iteration 497, loss = 0.49115074\n",
      "Iteration 498, loss = 0.49073385\n",
      "Iteration 499, loss = 0.49031769\n",
      "Iteration 500, loss = 0.48990222\n",
      "Iteration 501, loss = 0.48948746\n",
      "Iteration 502, loss = 0.48907338\n",
      "Iteration 503, loss = 0.48866000\n",
      "Iteration 504, loss = 0.48824729\n",
      "Iteration 505, loss = 0.48783525\n",
      "Iteration 506, loss = 0.48742387\n",
      "Iteration 507, loss = 0.48701316\n",
      "Iteration 508, loss = 0.48660310\n",
      "Iteration 509, loss = 0.48619368\n",
      "Iteration 510, loss = 0.48578491\n",
      "Iteration 511, loss = 0.48537677\n",
      "Iteration 512, loss = 0.48496926\n",
      "Iteration 513, loss = 0.48456237\n",
      "Iteration 514, loss = 0.48415609\n",
      "Iteration 515, loss = 0.48375043\n",
      "Iteration 516, loss = 0.48334537\n",
      "Iteration 517, loss = 0.48294092\n",
      "Iteration 518, loss = 0.48253706\n",
      "Iteration 519, loss = 0.48213378\n",
      "Iteration 520, loss = 0.48173109\n",
      "Iteration 521, loss = 0.48132898\n",
      "Iteration 522, loss = 0.48092744\n",
      "Iteration 523, loss = 0.48052645\n",
      "Iteration 524, loss = 0.48012154\n",
      "Iteration 525, loss = 0.47971371\n",
      "Iteration 526, loss = 0.47928272\n",
      "Iteration 527, loss = 0.47881759\n",
      "Iteration 528, loss = 0.47827431\n",
      "Iteration 529, loss = 0.47766305\n",
      "Iteration 530, loss = 0.47698931\n",
      "Iteration 531, loss = 0.47627615\n",
      "Iteration 532, loss = 0.47552597\n",
      "Iteration 533, loss = 0.47476549\n",
      "Iteration 534, loss = 0.47406193\n",
      "Iteration 535, loss = 0.47351229\n",
      "Iteration 536, loss = 0.47315365\n",
      "Iteration 537, loss = 0.47276593\n",
      "Iteration 538, loss = 0.47219126\n",
      "Iteration 539, loss = 0.47150379\n",
      "Iteration 540, loss = 0.47079737\n",
      "Iteration 541, loss = 0.47020021\n",
      "Iteration 542, loss = 0.46966028\n",
      "Iteration 543, loss = 0.46911935\n",
      "Iteration 544, loss = 0.46857710\n",
      "Iteration 545, loss = 0.46802541\n",
      "Iteration 546, loss = 0.46745667\n",
      "Iteration 547, loss = 0.46686512\n",
      "Iteration 548, loss = 0.46625600\n",
      "Iteration 549, loss = 0.46563139\n",
      "Iteration 550, loss = 0.46499735\n",
      "Iteration 551, loss = 0.46435496\n",
      "Iteration 552, loss = 0.46371167\n",
      "Iteration 553, loss = 0.46306073\n",
      "Iteration 554, loss = 0.46242939\n",
      "Iteration 555, loss = 0.46184433\n",
      "Iteration 556, loss = 0.46124268\n",
      "Iteration 557, loss = 0.46061677\n",
      "Iteration 558, loss = 0.45997108\n",
      "Iteration 559, loss = 0.45930971\n",
      "Iteration 560, loss = 0.45865411\n",
      "Iteration 561, loss = 0.45802779\n",
      "Iteration 562, loss = 0.45740877\n",
      "Iteration 563, loss = 0.45678190\n",
      "Iteration 564, loss = 0.45614263\n",
      "Iteration 565, loss = 0.45549206\n",
      "Iteration 566, loss = 0.45483818\n",
      "Iteration 567, loss = 0.45419016\n",
      "Iteration 568, loss = 0.45354969\n",
      "Iteration 569, loss = 0.45291482\n",
      "Iteration 570, loss = 0.45227962\n",
      "Iteration 571, loss = 0.45163291\n",
      "Iteration 572, loss = 0.45098314\n",
      "Iteration 573, loss = 0.45033588\n",
      "Iteration 574, loss = 0.44969652\n",
      "Iteration 575, loss = 0.44905954\n",
      "Iteration 576, loss = 0.44842141\n",
      "Iteration 577, loss = 0.44778219\n",
      "Iteration 578, loss = 0.44714203\n",
      "Iteration 579, loss = 0.44650115\n",
      "Iteration 580, loss = 0.44586131\n",
      "Iteration 581, loss = 0.44522305\n",
      "Iteration 582, loss = 0.44458563\n",
      "Iteration 583, loss = 0.44394932\n",
      "Iteration 584, loss = 0.44331756\n",
      "Iteration 585, loss = 0.44268548\n",
      "Iteration 586, loss = 0.44205321\n",
      "Iteration 587, loss = 0.44142105\n",
      "Iteration 588, loss = 0.44079284\n",
      "Iteration 589, loss = 0.44016664\n",
      "Iteration 590, loss = 0.43954161\n",
      "Iteration 591, loss = 0.43891783\n",
      "Iteration 592, loss = 0.43829536\n",
      "Iteration 593, loss = 0.43767429\n",
      "Iteration 594, loss = 0.43705471\n",
      "Iteration 595, loss = 0.43643671\n",
      "Iteration 596, loss = 0.43582038\n",
      "Iteration 597, loss = 0.43520611\n",
      "Iteration 598, loss = 0.43459465\n",
      "Iteration 599, loss = 0.43398496\n",
      "Iteration 600, loss = 0.43337705\n",
      "Iteration 601, loss = 0.43277096\n",
      "Iteration 602, loss = 0.43216673\n",
      "Iteration 603, loss = 0.43156437\n",
      "Iteration 604, loss = 0.43096391\n",
      "Iteration 605, loss = 0.43036630\n",
      "Iteration 606, loss = 0.42977070\n",
      "Iteration 607, loss = 0.42917697\n",
      "Iteration 608, loss = 0.42858515\n",
      "Iteration 609, loss = 0.42799528\n",
      "Iteration 610, loss = 0.42740742\n",
      "Iteration 611, loss = 0.42682158\n",
      "Iteration 612, loss = 0.42623782\n",
      "Iteration 613, loss = 0.42565690\n",
      "Iteration 614, loss = 0.42507809\n",
      "Iteration 615, loss = 0.42450134\n",
      "Iteration 616, loss = 0.42392644\n",
      "Iteration 617, loss = 0.42335385\n",
      "Iteration 618, loss = 0.42278376\n",
      "Iteration 619, loss = 0.42221617\n",
      "Iteration 620, loss = 0.42165035\n",
      "Iteration 621, loss = 0.42108578\n",
      "Iteration 622, loss = 0.42052522\n",
      "Iteration 623, loss = 0.41996658\n",
      "Iteration 624, loss = 0.41940946\n",
      "Iteration 625, loss = 0.41885386\n",
      "Iteration 626, loss = 0.41830071\n",
      "Iteration 627, loss = 0.41775016\n",
      "Iteration 628, loss = 0.41720163\n",
      "Iteration 629, loss = 0.41665514\n",
      "Iteration 630, loss = 0.41611048\n",
      "Iteration 631, loss = 0.41556789\n",
      "Iteration 632, loss = 0.41502728\n",
      "Iteration 633, loss = 0.41448863\n",
      "Iteration 634, loss = 0.41395194\n",
      "Iteration 635, loss = 0.41341717\n",
      "Iteration 636, loss = 0.41288430\n",
      "Iteration 637, loss = 0.41235366\n",
      "Iteration 638, loss = 0.41182501\n",
      "Iteration 639, loss = 0.41129823\n",
      "Iteration 640, loss = 0.41077330\n",
      "Iteration 641, loss = 0.41025018\n",
      "Iteration 642, loss = 0.40972887\n",
      "Iteration 643, loss = 0.40920932\n",
      "Iteration 644, loss = 0.40869223\n",
      "Iteration 645, loss = 0.40817732\n",
      "Iteration 646, loss = 0.40766397\n",
      "Iteration 647, loss = 0.40715267\n",
      "Iteration 648, loss = 0.40664326\n",
      "Iteration 649, loss = 0.40613553\n",
      "Iteration 650, loss = 0.40562943\n",
      "Iteration 651, loss = 0.40512492\n",
      "Iteration 652, loss = 0.40462199\n",
      "Iteration 653, loss = 0.40412092\n",
      "Iteration 654, loss = 0.40362156\n",
      "Iteration 655, loss = 0.40312362\n",
      "Iteration 656, loss = 0.40262712\n",
      "Iteration 657, loss = 0.40213216\n",
      "Iteration 658, loss = 0.40163933\n",
      "Iteration 659, loss = 0.40114784\n",
      "Iteration 660, loss = 0.40065824\n",
      "Iteration 661, loss = 0.40016999\n",
      "Iteration 662, loss = 0.39968310\n",
      "Iteration 663, loss = 0.39919782\n",
      "Iteration 664, loss = 0.39871384\n",
      "Iteration 665, loss = 0.39823118\n",
      "Iteration 666, loss = 0.39775032\n",
      "Iteration 667, loss = 0.39727104\n",
      "Iteration 668, loss = 0.39679310\n",
      "Iteration 669, loss = 0.39631644\n",
      "Iteration 670, loss = 0.39584133\n",
      "Iteration 671, loss = 0.39536718\n",
      "Iteration 672, loss = 0.39489444\n",
      "Iteration 673, loss = 0.39442296\n",
      "Iteration 674, loss = 0.39395263\n",
      "Iteration 675, loss = 0.39348357\n",
      "Iteration 676, loss = 0.39301576\n",
      "Iteration 677, loss = 0.39254909\n",
      "Iteration 678, loss = 0.39208354\n",
      "Iteration 679, loss = 0.39161910\n",
      "Iteration 680, loss = 0.39115575\n",
      "Iteration 681, loss = 0.39069347\n",
      "Iteration 682, loss = 0.39023226\n",
      "Iteration 683, loss = 0.38977208\n",
      "Iteration 684, loss = 0.38931332\n",
      "Iteration 685, loss = 0.38885598\n",
      "Iteration 686, loss = 0.38840006\n",
      "Iteration 687, loss = 0.38794522\n",
      "Iteration 688, loss = 0.38749142\n",
      "Iteration 689, loss = 0.38703883\n",
      "Iteration 690, loss = 0.38658713\n",
      "Iteration 691, loss = 0.38613621\n",
      "Iteration 692, loss = 0.38568661\n",
      "Iteration 693, loss = 0.38523784\n",
      "Iteration 694, loss = 0.38478981\n",
      "Iteration 695, loss = 0.38434250\n",
      "Iteration 696, loss = 0.38389648\n",
      "Iteration 697, loss = 0.38345129\n",
      "Iteration 698, loss = 0.38300679\n",
      "Iteration 699, loss = 0.38256302\n",
      "Iteration 700, loss = 0.38212044\n",
      "Iteration 701, loss = 0.38167872\n",
      "Iteration 702, loss = 0.38123807\n",
      "Iteration 703, loss = 0.38079809\n",
      "Iteration 704, loss = 0.38035898\n",
      "Iteration 705, loss = 0.37992059\n",
      "Iteration 706, loss = 0.37948309\n",
      "Iteration 707, loss = 0.37904613\n",
      "Iteration 708, loss = 0.37861001\n",
      "Iteration 709, loss = 0.37817463\n",
      "Iteration 710, loss = 0.37773996\n",
      "Iteration 711, loss = 0.37730598\n",
      "Iteration 712, loss = 0.37687265\n",
      "Iteration 713, loss = 0.37643997\n",
      "Iteration 714, loss = 0.37600792\n",
      "Iteration 715, loss = 0.37557651\n",
      "Iteration 716, loss = 0.37514576\n",
      "Iteration 717, loss = 0.37471564\n",
      "Iteration 718, loss = 0.37428614\n",
      "Iteration 719, loss = 0.37385723\n",
      "Iteration 720, loss = 0.37342890\n",
      "Iteration 721, loss = 0.37300114\n",
      "Iteration 722, loss = 0.37257395\n",
      "Iteration 723, loss = 0.37214732\n",
      "Iteration 724, loss = 0.37172124\n",
      "Iteration 725, loss = 0.37129570\n",
      "Iteration 726, loss = 0.37087069\n",
      "Iteration 727, loss = 0.37044620\n",
      "Iteration 728, loss = 0.37002223\n",
      "Iteration 729, loss = 0.36959877\n",
      "Iteration 730, loss = 0.36917579\n",
      "Iteration 731, loss = 0.36875330\n",
      "Iteration 732, loss = 0.36833129\n",
      "Iteration 733, loss = 0.36790985\n",
      "Iteration 734, loss = 0.36748929\n",
      "Iteration 735, loss = 0.36706922\n",
      "Iteration 736, loss = 0.36664962\n",
      "Iteration 737, loss = 0.36623049\n",
      "Iteration 738, loss = 0.36581181\n",
      "Iteration 739, loss = 0.36539356\n",
      "Iteration 740, loss = 0.36497576\n",
      "Iteration 741, loss = 0.36455837\n",
      "Iteration 742, loss = 0.36414139\n",
      "Iteration 743, loss = 0.36372481\n",
      "Iteration 744, loss = 0.36330884\n",
      "Iteration 745, loss = 0.36289352\n",
      "Iteration 746, loss = 0.36247851\n",
      "Iteration 747, loss = 0.36206400\n",
      "Iteration 748, loss = 0.36164977\n",
      "Iteration 749, loss = 0.36123628\n",
      "Iteration 750, loss = 0.36082328\n",
      "Iteration 751, loss = 0.36041062\n",
      "Iteration 752, loss = 0.35999825\n",
      "Iteration 753, loss = 0.35958618\n",
      "Iteration 754, loss = 0.35917439\n",
      "Iteration 755, loss = 0.35876310\n",
      "Iteration 756, loss = 0.35835235\n",
      "Iteration 757, loss = 0.35794185\n",
      "Iteration 758, loss = 0.35753162\n",
      "Iteration 759, loss = 0.35712168\n",
      "Iteration 760, loss = 0.35671205\n",
      "Iteration 761, loss = 0.35630288\n",
      "Iteration 762, loss = 0.35589406\n",
      "Iteration 763, loss = 0.35548537\n",
      "Iteration 764, loss = 0.35507716\n",
      "Iteration 765, loss = 0.35466926\n",
      "Iteration 766, loss = 0.35426163\n",
      "Iteration 767, loss = 0.35385427\n",
      "Iteration 768, loss = 0.35344718\n",
      "Iteration 769, loss = 0.35304060\n",
      "Iteration 770, loss = 0.35263482\n",
      "Iteration 771, loss = 0.35222936\n",
      "Iteration 772, loss = 0.35182424\n",
      "Iteration 773, loss = 0.35141943\n",
      "Iteration 774, loss = 0.35101493\n",
      "Iteration 775, loss = 0.35061082\n",
      "Iteration 776, loss = 0.35020683\n",
      "Iteration 777, loss = 0.34980320\n",
      "Iteration 778, loss = 0.34939984\n",
      "Iteration 779, loss = 0.34899674\n",
      "Iteration 780, loss = 0.34859410\n",
      "Iteration 781, loss = 0.34819180\n",
      "Iteration 782, loss = 0.34778976\n",
      "Iteration 783, loss = 0.34738799\n",
      "Iteration 784, loss = 0.34698646\n",
      "Iteration 785, loss = 0.34658555\n",
      "Iteration 786, loss = 0.34618508\n",
      "Iteration 787, loss = 0.34578487\n",
      "Iteration 788, loss = 0.34538493\n",
      "Iteration 789, loss = 0.34498523\n",
      "Iteration 790, loss = 0.34458578\n",
      "Iteration 791, loss = 0.34418656\n",
      "Iteration 792, loss = 0.34378757\n",
      "Iteration 793, loss = 0.34338879\n",
      "Iteration 794, loss = 0.34299023\n",
      "Iteration 795, loss = 0.34259187\n",
      "Iteration 796, loss = 0.34219371\n",
      "Iteration 797, loss = 0.34179574\n",
      "Iteration 798, loss = 0.34139796\n",
      "Iteration 799, loss = 0.34100036\n",
      "Iteration 800, loss = 0.34060294\n",
      "Iteration 801, loss = 0.34020569\n",
      "Iteration 802, loss = 0.33980862\n",
      "Iteration 803, loss = 0.33941183\n",
      "Iteration 804, loss = 0.33901533\n",
      "Iteration 805, loss = 0.33861902\n",
      "Iteration 806, loss = 0.33822287\n",
      "Iteration 807, loss = 0.33782689\n",
      "Iteration 808, loss = 0.33743107\n",
      "Iteration 809, loss = 0.33703540\n",
      "Iteration 810, loss = 0.33663989\n",
      "Iteration 811, loss = 0.33624452\n",
      "Iteration 812, loss = 0.33584930\n",
      "Iteration 813, loss = 0.33545421\n",
      "Iteration 814, loss = 0.33505926\n",
      "Iteration 815, loss = 0.33466444\n",
      "Iteration 816, loss = 0.33426976\n",
      "Iteration 817, loss = 0.33387519\n",
      "Iteration 818, loss = 0.33348075\n",
      "Iteration 819, loss = 0.33308643\n",
      "Iteration 820, loss = 0.33269222\n",
      "Iteration 821, loss = 0.33229813\n",
      "Iteration 822, loss = 0.33190415\n",
      "Iteration 823, loss = 0.33151028\n",
      "Iteration 824, loss = 0.33111651\n",
      "Iteration 825, loss = 0.33072285\n",
      "Iteration 826, loss = 0.33032929\n",
      "Iteration 827, loss = 0.32993583\n",
      "Iteration 828, loss = 0.32954247\n",
      "Iteration 829, loss = 0.32914920\n",
      "Iteration 830, loss = 0.32875602\n",
      "Iteration 831, loss = 0.32836294\n",
      "Iteration 832, loss = 0.32796995\n",
      "Iteration 833, loss = 0.32757705\n",
      "Iteration 834, loss = 0.32718438\n",
      "Iteration 835, loss = 0.32679213\n",
      "Iteration 836, loss = 0.32639973\n",
      "Iteration 837, loss = 0.32600732\n",
      "Iteration 838, loss = 0.32561610\n",
      "Iteration 839, loss = 0.32522537\n",
      "Iteration 840, loss = 0.32483478\n",
      "Iteration 841, loss = 0.32444432\n",
      "Iteration 842, loss = 0.32405395\n",
      "Iteration 843, loss = 0.32366367\n",
      "Iteration 844, loss = 0.32327397\n",
      "Iteration 845, loss = 0.32288427\n",
      "Iteration 846, loss = 0.32249460\n",
      "Iteration 847, loss = 0.32210498\n",
      "Iteration 848, loss = 0.32171543\n",
      "Iteration 849, loss = 0.32132629\n",
      "Iteration 850, loss = 0.32093732\n",
      "Iteration 851, loss = 0.32054831\n",
      "Iteration 852, loss = 0.32015926\n",
      "Iteration 853, loss = 0.31977059\n",
      "Iteration 854, loss = 0.31938210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 855, loss = 0.31899365\n",
      "Iteration 856, loss = 0.31860527\n",
      "Iteration 857, loss = 0.31821697\n",
      "Iteration 858, loss = 0.31782876\n",
      "Iteration 859, loss = 0.31744097\n",
      "Iteration 860, loss = 0.31705335\n",
      "Iteration 861, loss = 0.31666601\n",
      "Iteration 862, loss = 0.31627873\n",
      "Iteration 863, loss = 0.31589153\n",
      "Iteration 864, loss = 0.31550454\n",
      "Iteration 865, loss = 0.31511765\n",
      "Iteration 866, loss = 0.31473087\n",
      "Iteration 867, loss = 0.31434420\n",
      "Iteration 868, loss = 0.31395763\n",
      "Iteration 869, loss = 0.31357117\n",
      "Iteration 870, loss = 0.31318481\n",
      "Iteration 871, loss = 0.31279856\n",
      "Iteration 872, loss = 0.31241242\n",
      "Iteration 873, loss = 0.31202638\n",
      "Iteration 874, loss = 0.31164044\n",
      "Iteration 875, loss = 0.31125461\n",
      "Iteration 876, loss = 0.31086888\n",
      "Iteration 877, loss = 0.31048324\n",
      "Iteration 878, loss = 0.31009770\n",
      "Iteration 879, loss = 0.30971226\n",
      "Iteration 880, loss = 0.30932690\n",
      "Iteration 881, loss = 0.30894164\n",
      "Iteration 882, loss = 0.30855646\n",
      "Iteration 883, loss = 0.30817138\n",
      "Iteration 884, loss = 0.30778638\n",
      "Iteration 885, loss = 0.30740147\n",
      "Iteration 886, loss = 0.30701664\n",
      "Iteration 887, loss = 0.30663190\n",
      "Iteration 888, loss = 0.30624724\n",
      "Iteration 889, loss = 0.30586266\n",
      "Iteration 890, loss = 0.30547817\n",
      "Iteration 891, loss = 0.30509377\n",
      "Iteration 892, loss = 0.30470945\n",
      "Iteration 893, loss = 0.30432521\n",
      "Iteration 894, loss = 0.30394106\n",
      "Iteration 895, loss = 0.30355700\n",
      "Iteration 896, loss = 0.30317302\n",
      "Iteration 897, loss = 0.30278912\n",
      "Iteration 898, loss = 0.30240531\n",
      "Iteration 899, loss = 0.30202159\n",
      "Iteration 900, loss = 0.30163796\n",
      "Iteration 901, loss = 0.30125441\n",
      "Iteration 902, loss = 0.30087096\n",
      "Iteration 903, loss = 0.30048758\n",
      "Iteration 904, loss = 0.30010430\n",
      "Iteration 905, loss = 0.29972111\n",
      "Iteration 906, loss = 0.29933801\n",
      "Iteration 907, loss = 0.29895499\n",
      "Iteration 908, loss = 0.29857207\n",
      "Iteration 909, loss = 0.29818924\n",
      "Iteration 910, loss = 0.29780650\n",
      "Iteration 911, loss = 0.29742385\n",
      "Iteration 912, loss = 0.29704129\n",
      "Iteration 913, loss = 0.29665882\n",
      "Iteration 914, loss = 0.29627645\n",
      "Iteration 915, loss = 0.29589417\n",
      "Iteration 916, loss = 0.29551199\n",
      "Iteration 917, loss = 0.29513040\n",
      "Iteration 918, loss = 0.29474833\n",
      "Iteration 919, loss = 0.29436643\n",
      "Iteration 920, loss = 0.29398494\n",
      "Iteration 921, loss = 0.29360356\n",
      "Iteration 922, loss = 0.29322226\n",
      "Iteration 923, loss = 0.29284104\n",
      "Iteration 924, loss = 0.29245990\n",
      "Iteration 925, loss = 0.29207883\n",
      "Iteration 926, loss = 0.29169783\n",
      "Iteration 927, loss = 0.29131691\n",
      "Iteration 928, loss = 0.29093605\n",
      "Iteration 929, loss = 0.29055549\n",
      "Iteration 930, loss = 0.29017509\n",
      "Iteration 931, loss = 0.28979483\n",
      "Iteration 932, loss = 0.28941485\n",
      "Iteration 933, loss = 0.28903490\n",
      "Iteration 934, loss = 0.28865500\n",
      "Iteration 935, loss = 0.28827512\n",
      "Iteration 936, loss = 0.28789528\n",
      "Iteration 937, loss = 0.28751653\n",
      "Iteration 938, loss = 0.28713723\n",
      "Iteration 939, loss = 0.28675740\n",
      "Iteration 940, loss = 0.28637869\n",
      "Iteration 941, loss = 0.28600016\n",
      "Iteration 942, loss = 0.28562161\n",
      "Iteration 943, loss = 0.28524301\n",
      "Iteration 944, loss = 0.28486437\n",
      "Iteration 945, loss = 0.28448570\n",
      "Iteration 946, loss = 0.28410702\n",
      "Iteration 947, loss = 0.28372969\n",
      "Iteration 948, loss = 0.28335222\n",
      "Iteration 949, loss = 0.28297432\n",
      "Iteration 950, loss = 0.28259608\n",
      "Iteration 951, loss = 0.28221856\n",
      "Iteration 952, loss = 0.28184170\n",
      "Iteration 953, loss = 0.28146472\n",
      "Iteration 954, loss = 0.28108763\n",
      "Iteration 955, loss = 0.28071044\n",
      "Iteration 956, loss = 0.28033347\n",
      "Iteration 957, loss = 0.27995746\n",
      "Iteration 958, loss = 0.27958121\n",
      "Iteration 959, loss = 0.27920471\n",
      "Iteration 960, loss = 0.27882837\n",
      "Iteration 961, loss = 0.27845309\n",
      "Iteration 962, loss = 0.27807763\n",
      "Iteration 963, loss = 0.27770199\n",
      "Iteration 964, loss = 0.27732618\n",
      "Iteration 965, loss = 0.27695085\n",
      "Iteration 966, loss = 0.27657615\n",
      "Iteration 967, loss = 0.27620125\n",
      "Iteration 968, loss = 0.27582628\n",
      "Iteration 969, loss = 0.27545174\n",
      "Iteration 970, loss = 0.27507765\n",
      "Iteration 971, loss = 0.27470345\n",
      "Iteration 972, loss = 0.27432929\n",
      "Iteration 973, loss = 0.27395551\n",
      "Iteration 974, loss = 0.27358206\n",
      "Iteration 975, loss = 0.27320857\n",
      "Iteration 976, loss = 0.27283519\n",
      "Iteration 977, loss = 0.27246211\n",
      "Iteration 978, loss = 0.27208936\n",
      "Iteration 979, loss = 0.27171653\n",
      "Iteration 980, loss = 0.27134402\n",
      "Iteration 981, loss = 0.27097169\n",
      "Iteration 982, loss = 0.27059955\n",
      "Iteration 983, loss = 0.27022761\n",
      "Iteration 984, loss = 0.26985585\n",
      "Iteration 985, loss = 0.26948428\n",
      "Iteration 986, loss = 0.26911297\n",
      "Iteration 987, loss = 0.26874175\n",
      "Iteration 988, loss = 0.26837079\n",
      "Iteration 989, loss = 0.26800003\n",
      "Iteration 990, loss = 0.26762946\n",
      "Iteration 991, loss = 0.26725910\n",
      "Iteration 992, loss = 0.26688893\n",
      "Iteration 993, loss = 0.26651897\n",
      "Iteration 994, loss = 0.26614920\n",
      "Iteration 995, loss = 0.26577971\n",
      "Iteration 996, loss = 0.26541031\n",
      "Iteration 997, loss = 0.26504119\n",
      "Iteration 998, loss = 0.26467228\n",
      "Iteration 999, loss = 0.26430360\n",
      "Iteration 1000, loss = 0.26393511\n",
      "Iteration 1, loss = 1.52120330\n",
      "Iteration 2, loss = 1.50387085\n",
      "Iteration 3, loss = 1.48680851\n",
      "Iteration 4, loss = 1.47002260\n",
      "Iteration 5, loss = 1.45352743\n",
      "Iteration 6, loss = 1.43733226\n",
      "Iteration 7, loss = 1.42140490\n",
      "Iteration 8, loss = 1.40575499\n",
      "Iteration 9, loss = 1.39039000\n",
      "Iteration 10, loss = 1.37527431\n",
      "Iteration 11, loss = 1.36038744\n",
      "Iteration 12, loss = 1.34578289\n",
      "Iteration 13, loss = 1.33141919\n",
      "Iteration 14, loss = 1.31731310\n",
      "Iteration 15, loss = 1.30346591\n",
      "Iteration 16, loss = 1.28985740\n",
      "Iteration 17, loss = 1.27654667\n",
      "Iteration 18, loss = 1.26345428\n",
      "Iteration 19, loss = 1.25059679\n",
      "Iteration 20, loss = 1.23794619\n",
      "Iteration 21, loss = 1.22548556\n",
      "Iteration 22, loss = 1.21324726\n",
      "Iteration 23, loss = 1.20121212\n",
      "Iteration 24, loss = 1.18937279\n",
      "Iteration 25, loss = 1.17771185\n",
      "Iteration 26, loss = 1.16624368\n",
      "Iteration 27, loss = 1.15497580\n",
      "Iteration 28, loss = 1.14387870\n",
      "Iteration 29, loss = 1.13294994\n",
      "Iteration 30, loss = 1.12219438\n",
      "Iteration 31, loss = 1.11162057\n",
      "Iteration 32, loss = 1.10123325\n",
      "Iteration 33, loss = 1.09103951\n",
      "Iteration 34, loss = 1.08101745\n",
      "Iteration 35, loss = 1.07117089\n",
      "Iteration 36, loss = 1.06148523\n",
      "Iteration 37, loss = 1.05197974\n",
      "Iteration 38, loss = 1.04263215\n",
      "Iteration 39, loss = 1.03344776\n",
      "Iteration 40, loss = 1.02442006\n",
      "Iteration 41, loss = 1.01556864\n",
      "Iteration 42, loss = 1.00687374\n",
      "Iteration 43, loss = 0.99833524\n",
      "Iteration 44, loss = 0.98995297\n",
      "Iteration 45, loss = 0.98171112\n",
      "Iteration 46, loss = 0.97360248\n",
      "Iteration 47, loss = 0.96562532\n",
      "Iteration 48, loss = 0.95779498\n",
      "Iteration 49, loss = 0.95014605\n",
      "Iteration 50, loss = 0.94264516\n",
      "Iteration 51, loss = 0.93528829\n",
      "Iteration 52, loss = 0.92809560\n",
      "Iteration 53, loss = 0.92106837\n",
      "Iteration 54, loss = 0.91423600\n",
      "Iteration 55, loss = 0.90757779\n",
      "Iteration 56, loss = 0.90107043\n",
      "Iteration 57, loss = 0.89475349\n",
      "Iteration 58, loss = 0.88860012\n",
      "Iteration 59, loss = 0.88263536\n",
      "Iteration 60, loss = 0.87685384\n",
      "Iteration 61, loss = 0.87130368\n",
      "Iteration 62, loss = 0.86593906\n",
      "Iteration 63, loss = 0.86073745\n",
      "Iteration 64, loss = 0.85572898\n",
      "Iteration 65, loss = 0.85091666\n",
      "Iteration 66, loss = 0.84630203\n",
      "Iteration 67, loss = 0.84185632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 68, loss = 0.83757560\n",
      "Iteration 69, loss = 0.83347769\n",
      "Iteration 70, loss = 0.82954444\n",
      "Iteration 71, loss = 0.82576137\n",
      "Iteration 72, loss = 0.82213977\n",
      "Iteration 73, loss = 0.81867442\n",
      "Iteration 74, loss = 0.81536748\n",
      "Iteration 75, loss = 0.81220312\n",
      "Iteration 76, loss = 0.80916620\n",
      "Iteration 77, loss = 0.80624447\n",
      "Iteration 78, loss = 0.80344466\n",
      "Iteration 79, loss = 0.80075849\n",
      "Iteration 80, loss = 0.79817789\n",
      "Iteration 81, loss = 0.79570292\n",
      "Iteration 82, loss = 0.79332906\n",
      "Iteration 83, loss = 0.79105502\n",
      "Iteration 84, loss = 0.78888307\n",
      "Iteration 85, loss = 0.78679910\n",
      "Iteration 86, loss = 0.78479691\n",
      "Iteration 87, loss = 0.78287212\n",
      "Iteration 88, loss = 0.78101557\n",
      "Iteration 89, loss = 0.77922314\n",
      "Iteration 90, loss = 0.77749082\n",
      "Iteration 91, loss = 0.77581472\n",
      "Iteration 92, loss = 0.77419105\n",
      "Iteration 93, loss = 0.77261685\n",
      "Iteration 94, loss = 0.77109553\n",
      "Iteration 95, loss = 0.76961718\n",
      "Iteration 96, loss = 0.76818741\n",
      "Iteration 97, loss = 0.76679500\n",
      "Iteration 98, loss = 0.76543696\n",
      "Iteration 99, loss = 0.76411050\n",
      "Iteration 100, loss = 0.76281294\n",
      "Iteration 101, loss = 0.76154176\n",
      "Iteration 102, loss = 0.76029605\n",
      "Iteration 103, loss = 0.75907908\n",
      "Iteration 104, loss = 0.75788266\n",
      "Iteration 105, loss = 0.75670482\n",
      "Iteration 106, loss = 0.75555215\n",
      "Iteration 107, loss = 0.75442263\n",
      "Iteration 108, loss = 0.75330801\n",
      "Iteration 109, loss = 0.75220677\n",
      "Iteration 110, loss = 0.75111753\n",
      "Iteration 111, loss = 0.75003902\n",
      "Iteration 112, loss = 0.74897006\n",
      "Iteration 113, loss = 0.74790959\n",
      "Iteration 114, loss = 0.74686268\n",
      "Iteration 115, loss = 0.74582517\n",
      "Iteration 116, loss = 0.74480113\n",
      "Iteration 117, loss = 0.74378336\n",
      "Iteration 118, loss = 0.74277112\n",
      "Iteration 119, loss = 0.74176385\n",
      "Iteration 120, loss = 0.74076746\n",
      "Iteration 121, loss = 0.73977528\n",
      "Iteration 122, loss = 0.73880510\n",
      "Iteration 123, loss = 0.73784083\n",
      "Iteration 124, loss = 0.73688029\n",
      "Iteration 125, loss = 0.73592836\n",
      "Iteration 126, loss = 0.73498100\n",
      "Iteration 127, loss = 0.73403644\n",
      "Iteration 128, loss = 0.73309438\n",
      "Iteration 129, loss = 0.73215632\n",
      "Iteration 130, loss = 0.73122683\n",
      "Iteration 131, loss = 0.73031227\n",
      "Iteration 132, loss = 0.72940254\n",
      "Iteration 133, loss = 0.72849823\n",
      "Iteration 134, loss = 0.72759743\n",
      "Iteration 135, loss = 0.72669868\n",
      "Iteration 136, loss = 0.72580182\n",
      "Iteration 137, loss = 0.72490668\n",
      "Iteration 138, loss = 0.72401774\n",
      "Iteration 139, loss = 0.72313097\n",
      "Iteration 140, loss = 0.72224592\n",
      "Iteration 141, loss = 0.72136244\n",
      "Iteration 142, loss = 0.72048042\n",
      "Iteration 143, loss = 0.71959973\n",
      "Iteration 144, loss = 0.71872584\n",
      "Iteration 145, loss = 0.71785556\n",
      "Iteration 146, loss = 0.71698668\n",
      "Iteration 147, loss = 0.71611910\n",
      "Iteration 148, loss = 0.71525273\n",
      "Iteration 149, loss = 0.71438751\n",
      "Iteration 150, loss = 0.71352336\n",
      "Iteration 151, loss = 0.71266021\n",
      "Iteration 152, loss = 0.71179956\n",
      "Iteration 153, loss = 0.71093984\n",
      "Iteration 154, loss = 0.71008182\n",
      "Iteration 155, loss = 0.70922524\n",
      "Iteration 156, loss = 0.70837152\n",
      "Iteration 157, loss = 0.70752032\n",
      "Iteration 158, loss = 0.70667383\n",
      "Iteration 159, loss = 0.70582839\n",
      "Iteration 160, loss = 0.70498391\n",
      "Iteration 161, loss = 0.70414036\n",
      "Iteration 162, loss = 0.70329772\n",
      "Iteration 163, loss = 0.70245597\n",
      "Iteration 164, loss = 0.70161792\n",
      "Iteration 165, loss = 0.70078080\n",
      "Iteration 166, loss = 0.69994455\n",
      "Iteration 167, loss = 0.69910943\n",
      "Iteration 168, loss = 0.69827708\n",
      "Iteration 169, loss = 0.69744554\n",
      "Iteration 170, loss = 0.69661481\n",
      "Iteration 171, loss = 0.69578492\n",
      "Iteration 172, loss = 0.69495589\n",
      "Iteration 173, loss = 0.69412772\n",
      "Iteration 174, loss = 0.69330044\n",
      "Iteration 175, loss = 0.69247405\n",
      "Iteration 176, loss = 0.69164857\n",
      "Iteration 177, loss = 0.69082400\n",
      "Iteration 178, loss = 0.69000035\n",
      "Iteration 179, loss = 0.68917764\n",
      "Iteration 180, loss = 0.68835586\n",
      "Iteration 181, loss = 0.68753503\n",
      "Iteration 182, loss = 0.68671515\n",
      "Iteration 183, loss = 0.68589623\n",
      "Iteration 184, loss = 0.68507826\n",
      "Iteration 185, loss = 0.68426125\n",
      "Iteration 186, loss = 0.68344522\n",
      "Iteration 187, loss = 0.68263015\n",
      "Iteration 188, loss = 0.68181605\n",
      "Iteration 189, loss = 0.68100294\n",
      "Iteration 190, loss = 0.68019133\n",
      "Iteration 191, loss = 0.67938112\n",
      "Iteration 192, loss = 0.67857194\n",
      "Iteration 193, loss = 0.67776382\n",
      "Iteration 194, loss = 0.67695676\n",
      "Iteration 195, loss = 0.67615077\n",
      "Iteration 196, loss = 0.67534586\n",
      "Iteration 197, loss = 0.67454204\n",
      "Iteration 198, loss = 0.67373932\n",
      "Iteration 199, loss = 0.67293769\n",
      "Iteration 200, loss = 0.67213718\n",
      "Iteration 201, loss = 0.67133779\n",
      "Iteration 202, loss = 0.67053952\n",
      "Iteration 203, loss = 0.66974238\n",
      "Iteration 204, loss = 0.66894638\n",
      "Iteration 205, loss = 0.66815153\n",
      "Iteration 206, loss = 0.66735831\n",
      "Iteration 207, loss = 0.66656644\n",
      "Iteration 208, loss = 0.66577575\n",
      "Iteration 209, loss = 0.66498625\n",
      "Iteration 210, loss = 0.66419797\n",
      "Iteration 211, loss = 0.66341091\n",
      "Iteration 212, loss = 0.66262510\n",
      "Iteration 213, loss = 0.66184053\n",
      "Iteration 214, loss = 0.66105723\n",
      "Iteration 215, loss = 0.66027520\n",
      "Iteration 216, loss = 0.65949446\n",
      "Iteration 217, loss = 0.65871500\n",
      "Iteration 218, loss = 0.65793685\n",
      "Iteration 219, loss = 0.65716000\n",
      "Iteration 220, loss = 0.65638447\n",
      "Iteration 221, loss = 0.65561027\n",
      "Iteration 222, loss = 0.65483740\n",
      "Iteration 223, loss = 0.65406586\n",
      "Iteration 224, loss = 0.65329568\n",
      "Iteration 225, loss = 0.65252685\n",
      "Iteration 226, loss = 0.65175938\n",
      "Iteration 227, loss = 0.65099327\n",
      "Iteration 228, loss = 0.65022854\n",
      "Iteration 229, loss = 0.64946519\n",
      "Iteration 230, loss = 0.64870323\n",
      "Iteration 231, loss = 0.64794266\n",
      "Iteration 232, loss = 0.64718349\n",
      "Iteration 233, loss = 0.64642572\n",
      "Iteration 234, loss = 0.64566938\n",
      "Iteration 235, loss = 0.64491500\n",
      "Iteration 236, loss = 0.64416205\n",
      "Iteration 237, loss = 0.64341055\n",
      "Iteration 238, loss = 0.64266052\n",
      "Iteration 239, loss = 0.64191196\n",
      "Iteration 240, loss = 0.64116488\n",
      "Iteration 241, loss = 0.64041930\n",
      "Iteration 242, loss = 0.63967522\n",
      "Iteration 243, loss = 0.63893264\n",
      "Iteration 244, loss = 0.63819159\n",
      "Iteration 245, loss = 0.63745206\n",
      "Iteration 246, loss = 0.63671407\n",
      "Iteration 247, loss = 0.63597761\n",
      "Iteration 248, loss = 0.63524269\n",
      "Iteration 249, loss = 0.63450932\n",
      "Iteration 250, loss = 0.63377751\n",
      "Iteration 251, loss = 0.63304726\n",
      "Iteration 252, loss = 0.63231857\n",
      "Iteration 253, loss = 0.63159144\n",
      "Iteration 254, loss = 0.63086589\n",
      "Iteration 255, loss = 0.63014191\n",
      "Iteration 256, loss = 0.62941952\n",
      "Iteration 257, loss = 0.62869870\n",
      "Iteration 258, loss = 0.62797947\n",
      "Iteration 259, loss = 0.62726182\n",
      "Iteration 260, loss = 0.62654577\n",
      "Iteration 261, loss = 0.62583131\n",
      "Iteration 262, loss = 0.62511844\n",
      "Iteration 263, loss = 0.62440718\n",
      "Iteration 264, loss = 0.62369751\n",
      "Iteration 265, loss = 0.62298945\n",
      "Iteration 266, loss = 0.62228300\n",
      "Iteration 267, loss = 0.62157815\n",
      "Iteration 268, loss = 0.62087492\n",
      "Iteration 269, loss = 0.62017329\n",
      "Iteration 270, loss = 0.61947329\n",
      "Iteration 271, loss = 0.61877489\n",
      "Iteration 272, loss = 0.61807812\n",
      "Iteration 273, loss = 0.61738297\n",
      "Iteration 274, loss = 0.61668943\n",
      "Iteration 275, loss = 0.61599752\n",
      "Iteration 276, loss = 0.61530724\n",
      "Iteration 277, loss = 0.61461858\n",
      "Iteration 278, loss = 0.61393154\n",
      "Iteration 279, loss = 0.61324614\n",
      "Iteration 280, loss = 0.61256236\n",
      "Iteration 281, loss = 0.61188021\n",
      "Iteration 282, loss = 0.61119969\n",
      "Iteration 283, loss = 0.61052081\n",
      "Iteration 284, loss = 0.60984355\n",
      "Iteration 285, loss = 0.60916793\n",
      "Iteration 286, loss = 0.60849394\n",
      "Iteration 287, loss = 0.60782158\n",
      "Iteration 288, loss = 0.60715086\n",
      "Iteration 289, loss = 0.60648176\n",
      "Iteration 290, loss = 0.60581430\n",
      "Iteration 291, loss = 0.60514848\n",
      "Iteration 292, loss = 0.60448428\n",
      "Iteration 293, loss = 0.60382172\n",
      "Iteration 294, loss = 0.60316079\n",
      "Iteration 295, loss = 0.60250149\n",
      "Iteration 296, loss = 0.60184382\n",
      "Iteration 297, loss = 0.60118778\n",
      "Iteration 298, loss = 0.60053351\n",
      "Iteration 299, loss = 0.59988085\n",
      "Iteration 300, loss = 0.59922979\n",
      "Iteration 301, loss = 0.59858033\n",
      "Iteration 302, loss = 0.59793247\n",
      "Iteration 303, loss = 0.59728622\n",
      "Iteration 304, loss = 0.59664159\n",
      "Iteration 305, loss = 0.59599857\n",
      "Iteration 306, loss = 0.59535730\n",
      "Iteration 307, loss = 0.59471765\n",
      "Iteration 308, loss = 0.59408001\n",
      "Iteration 309, loss = 0.59344451\n",
      "Iteration 310, loss = 0.59281064\n",
      "Iteration 311, loss = 0.59217842\n",
      "Iteration 312, loss = 0.59154782\n",
      "Iteration 313, loss = 0.59091886\n",
      "Iteration 314, loss = 0.59029156\n",
      "Iteration 315, loss = 0.58966637\n",
      "Iteration 316, loss = 0.58904291\n",
      "Iteration 317, loss = 0.58842105\n",
      "Iteration 318, loss = 0.58780079\n",
      "Iteration 319, loss = 0.58718216\n",
      "Iteration 320, loss = 0.58656516\n",
      "Iteration 321, loss = 0.58594980\n",
      "Iteration 322, loss = 0.58533612\n",
      "Iteration 323, loss = 0.58472408\n",
      "Iteration 324, loss = 0.58411367\n",
      "Iteration 325, loss = 0.58350495\n",
      "Iteration 326, loss = 0.58289788\n",
      "Iteration 327, loss = 0.58229253\n",
      "Iteration 328, loss = 0.58168890\n",
      "Iteration 329, loss = 0.58108693\n",
      "Iteration 330, loss = 0.58048661\n",
      "Iteration 331, loss = 0.57988794\n",
      "Iteration 332, loss = 0.57929090\n",
      "Iteration 333, loss = 0.57869551\n",
      "Iteration 334, loss = 0.57810174\n",
      "Iteration 335, loss = 0.57750960\n",
      "Iteration 336, loss = 0.57691907\n",
      "Iteration 337, loss = 0.57633017\n",
      "Iteration 338, loss = 0.57574286\n",
      "Iteration 339, loss = 0.57515716\n",
      "Iteration 340, loss = 0.57457305\n",
      "Iteration 341, loss = 0.57399053\n",
      "Iteration 342, loss = 0.57340959\n",
      "Iteration 343, loss = 0.57283022\n",
      "Iteration 344, loss = 0.57225241\n",
      "Iteration 345, loss = 0.57167616\n",
      "Iteration 346, loss = 0.57110147\n",
      "Iteration 347, loss = 0.57052832\n",
      "Iteration 348, loss = 0.56995671\n",
      "Iteration 349, loss = 0.56938662\n",
      "Iteration 350, loss = 0.56881807\n",
      "Iteration 351, loss = 0.56825103\n",
      "Iteration 352, loss = 0.56768550\n",
      "Iteration 353, loss = 0.56712148\n",
      "Iteration 354, loss = 0.56655896\n",
      "Iteration 355, loss = 0.56599793\n",
      "Iteration 356, loss = 0.56543839\n",
      "Iteration 357, loss = 0.56488033\n",
      "Iteration 358, loss = 0.56432374\n",
      "Iteration 359, loss = 0.56376862\n",
      "Iteration 360, loss = 0.56321497\n",
      "Iteration 361, loss = 0.56266277\n",
      "Iteration 362, loss = 0.56211202\n",
      "Iteration 363, loss = 0.56156272\n",
      "Iteration 364, loss = 0.56101485\n",
      "Iteration 365, loss = 0.56046842\n",
      "Iteration 366, loss = 0.55992360\n",
      "Iteration 367, loss = 0.55938031\n",
      "Iteration 368, loss = 0.55883842\n",
      "Iteration 369, loss = 0.55829790\n",
      "Iteration 370, loss = 0.55775878\n",
      "Iteration 371, loss = 0.55722104\n",
      "Iteration 372, loss = 0.55668469\n",
      "Iteration 373, loss = 0.55614972\n",
      "Iteration 374, loss = 0.55561614\n",
      "Iteration 375, loss = 0.55508394\n",
      "Iteration 376, loss = 0.55455311\n",
      "Iteration 377, loss = 0.55402366\n",
      "Iteration 378, loss = 0.55349558\n",
      "Iteration 379, loss = 0.55296886\n",
      "Iteration 380, loss = 0.55244350\n",
      "Iteration 381, loss = 0.55191949\n",
      "Iteration 382, loss = 0.55139683\n",
      "Iteration 383, loss = 0.55087562\n",
      "Iteration 384, loss = 0.55035568\n",
      "Iteration 385, loss = 0.54983700\n",
      "Iteration 386, loss = 0.54931958\n",
      "Iteration 387, loss = 0.54880349\n",
      "Iteration 388, loss = 0.54828876\n",
      "Iteration 389, loss = 0.54777532\n",
      "Iteration 390, loss = 0.54726317\n",
      "Iteration 391, loss = 0.54675230\n",
      "Iteration 392, loss = 0.54624271\n",
      "Iteration 393, loss = 0.54573438\n",
      "Iteration 394, loss = 0.54522791\n",
      "Iteration 395, loss = 0.54472303\n",
      "Iteration 396, loss = 0.54421945\n",
      "Iteration 397, loss = 0.54371716\n",
      "Iteration 398, loss = 0.54321615\n",
      "Iteration 399, loss = 0.54271643\n",
      "Iteration 400, loss = 0.54221797\n",
      "Iteration 401, loss = 0.54172078\n",
      "Iteration 402, loss = 0.54122485\n",
      "Iteration 403, loss = 0.54073017\n",
      "Iteration 404, loss = 0.54023673\n",
      "Iteration 405, loss = 0.53974453\n",
      "Iteration 406, loss = 0.53925355\n",
      "Iteration 407, loss = 0.53876381\n",
      "Iteration 408, loss = 0.53827527\n",
      "Iteration 409, loss = 0.53778795\n",
      "Iteration 410, loss = 0.53730183\n",
      "Iteration 411, loss = 0.53681690\n",
      "Iteration 412, loss = 0.53633316\n",
      "Iteration 413, loss = 0.53585060\n",
      "Iteration 414, loss = 0.53536922\n",
      "Iteration 415, loss = 0.53488900\n",
      "Iteration 416, loss = 0.53440993\n",
      "Iteration 417, loss = 0.53393202\n",
      "Iteration 418, loss = 0.53345525\n",
      "Iteration 419, loss = 0.53297962\n",
      "Iteration 420, loss = 0.53250511\n",
      "Iteration 421, loss = 0.53203173\n",
      "Iteration 422, loss = 0.53155996\n",
      "Iteration 423, loss = 0.53108957\n",
      "Iteration 424, loss = 0.53062033\n",
      "Iteration 425, loss = 0.53015224\n",
      "Iteration 426, loss = 0.52968536\n",
      "Iteration 427, loss = 0.52921979\n",
      "Iteration 428, loss = 0.52875534\n",
      "Iteration 429, loss = 0.52829200\n",
      "Iteration 430, loss = 0.52782976\n",
      "Iteration 431, loss = 0.52736863\n",
      "Iteration 432, loss = 0.52690859\n",
      "Iteration 433, loss = 0.52644963\n",
      "Iteration 434, loss = 0.52599175\n",
      "Iteration 435, loss = 0.52553495\n",
      "Iteration 436, loss = 0.52507920\n",
      "Iteration 437, loss = 0.52462452\n",
      "Iteration 438, loss = 0.52417123\n",
      "Iteration 439, loss = 0.52371940\n",
      "Iteration 440, loss = 0.52326864\n",
      "Iteration 441, loss = 0.52281895\n",
      "Iteration 442, loss = 0.52237032\n",
      "Iteration 443, loss = 0.52192275\n",
      "Iteration 444, loss = 0.52147640\n",
      "Iteration 445, loss = 0.52103123\n",
      "Iteration 446, loss = 0.52058709\n",
      "Iteration 447, loss = 0.52014400\n",
      "Iteration 448, loss = 0.51970194\n",
      "Iteration 449, loss = 0.51926093\n",
      "Iteration 450, loss = 0.51882095\n",
      "Iteration 451, loss = 0.51838200\n",
      "Iteration 452, loss = 0.51794409\n",
      "Iteration 453, loss = 0.51750719\n",
      "Iteration 454, loss = 0.51707131\n",
      "Iteration 455, loss = 0.51663644\n",
      "Iteration 456, loss = 0.51620257\n",
      "Iteration 457, loss = 0.51576969\n",
      "Iteration 458, loss = 0.51533779\n",
      "Iteration 459, loss = 0.51490687\n",
      "Iteration 460, loss = 0.51447692\n",
      "Iteration 461, loss = 0.51404813\n",
      "Iteration 462, loss = 0.51362037\n",
      "Iteration 463, loss = 0.51319357\n",
      "Iteration 464, loss = 0.51276774\n",
      "Iteration 465, loss = 0.51234285\n",
      "Iteration 466, loss = 0.51191892\n",
      "Iteration 467, loss = 0.51149593\n",
      "Iteration 468, loss = 0.51107387\n",
      "Iteration 469, loss = 0.51065275\n",
      "Iteration 470, loss = 0.51023256\n",
      "Iteration 471, loss = 0.50981328\n",
      "Iteration 472, loss = 0.50939490\n",
      "Iteration 473, loss = 0.50897743\n",
      "Iteration 474, loss = 0.50856086\n",
      "Iteration 475, loss = 0.50814517\n",
      "Iteration 476, loss = 0.50773036\n",
      "Iteration 477, loss = 0.50731641\n",
      "Iteration 478, loss = 0.50690333\n",
      "Iteration 479, loss = 0.50649111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 480, loss = 0.50607973\n",
      "Iteration 481, loss = 0.50566919\n",
      "Iteration 482, loss = 0.50525948\n",
      "Iteration 483, loss = 0.50485059\n",
      "Iteration 484, loss = 0.50444252\n",
      "Iteration 485, loss = 0.50403526\n",
      "Iteration 486, loss = 0.50362881\n",
      "Iteration 487, loss = 0.50322315\n",
      "Iteration 488, loss = 0.50281828\n",
      "Iteration 489, loss = 0.50241419\n",
      "Iteration 490, loss = 0.50201088\n",
      "Iteration 491, loss = 0.50160833\n",
      "Iteration 492, loss = 0.50120655\n",
      "Iteration 493, loss = 0.50080553\n",
      "Iteration 494, loss = 0.50040525\n",
      "Iteration 495, loss = 0.50000572\n",
      "Iteration 496, loss = 0.49960693\n",
      "Iteration 497, loss = 0.49920887\n",
      "Iteration 498, loss = 0.49881153\n",
      "Iteration 499, loss = 0.49841491\n",
      "Iteration 500, loss = 0.49801901\n",
      "Iteration 501, loss = 0.49762382\n",
      "Iteration 502, loss = 0.49722932\n",
      "Iteration 503, loss = 0.49683553\n",
      "Iteration 504, loss = 0.49644242\n",
      "Iteration 505, loss = 0.49605000\n",
      "Iteration 506, loss = 0.49565825\n",
      "Iteration 507, loss = 0.49526718\n",
      "Iteration 508, loss = 0.49487678\n",
      "Iteration 509, loss = 0.49448704\n",
      "Iteration 510, loss = 0.49409796\n",
      "Iteration 511, loss = 0.49370952\n",
      "Iteration 512, loss = 0.49332173\n",
      "Iteration 513, loss = 0.49293459\n",
      "Iteration 514, loss = 0.49254807\n",
      "Iteration 515, loss = 0.49216219\n",
      "Iteration 516, loss = 0.49177693\n",
      "Iteration 517, loss = 0.49139228\n",
      "Iteration 518, loss = 0.49100825\n",
      "Iteration 519, loss = 0.49062483\n",
      "Iteration 520, loss = 0.49024201\n",
      "Iteration 521, loss = 0.48985979\n",
      "Iteration 522, loss = 0.48947816\n",
      "Iteration 523, loss = 0.48909710\n",
      "Iteration 524, loss = 0.48871227\n",
      "Iteration 525, loss = 0.48832460\n",
      "Iteration 526, loss = 0.48792324\n",
      "Iteration 527, loss = 0.48749549\n",
      "Iteration 528, loss = 0.48697000\n",
      "Iteration 529, loss = 0.48639192\n",
      "Iteration 530, loss = 0.48575904\n",
      "Iteration 531, loss = 0.48508760\n",
      "Iteration 532, loss = 0.48438024\n",
      "Iteration 533, loss = 0.48363893\n",
      "Iteration 534, loss = 0.48289820\n",
      "Iteration 535, loss = 0.48230651\n",
      "Iteration 536, loss = 0.48203754\n",
      "Iteration 537, loss = 0.48168549\n",
      "Iteration 538, loss = 0.48112484\n",
      "Iteration 539, loss = 0.48045668\n",
      "Iteration 540, loss = 0.47978174\n",
      "Iteration 541, loss = 0.47917585\n",
      "Iteration 542, loss = 0.47867892\n",
      "Iteration 543, loss = 0.47819710\n",
      "Iteration 544, loss = 0.47770051\n",
      "Iteration 545, loss = 0.47718082\n",
      "Iteration 546, loss = 0.47663281\n",
      "Iteration 547, loss = 0.47605713\n",
      "Iteration 548, loss = 0.47545953\n",
      "Iteration 549, loss = 0.47484436\n",
      "Iteration 550, loss = 0.47422948\n",
      "Iteration 551, loss = 0.47362801\n",
      "Iteration 552, loss = 0.47306163\n",
      "Iteration 553, loss = 0.47251348\n",
      "Iteration 554, loss = 0.47195806\n",
      "Iteration 555, loss = 0.47138264\n",
      "Iteration 556, loss = 0.47078951\n",
      "Iteration 557, loss = 0.47018223\n",
      "Iteration 558, loss = 0.46956589\n",
      "Iteration 559, loss = 0.46896383\n",
      "Iteration 560, loss = 0.46836750\n",
      "Iteration 561, loss = 0.46778093\n",
      "Iteration 562, loss = 0.46719482\n",
      "Iteration 563, loss = 0.46660849\n",
      "Iteration 564, loss = 0.46601163\n",
      "Iteration 565, loss = 0.46540531\n",
      "Iteration 566, loss = 0.46480049\n",
      "Iteration 567, loss = 0.46419370\n",
      "Iteration 568, loss = 0.46359243\n",
      "Iteration 569, loss = 0.46299459\n",
      "Iteration 570, loss = 0.46239602\n",
      "Iteration 571, loss = 0.46179931\n",
      "Iteration 572, loss = 0.46120204\n",
      "Iteration 573, loss = 0.46060355\n",
      "Iteration 574, loss = 0.46000424\n",
      "Iteration 575, loss = 0.45940445\n",
      "Iteration 576, loss = 0.45880455\n",
      "Iteration 577, loss = 0.45820483\n",
      "Iteration 578, loss = 0.45760559\n",
      "Iteration 579, loss = 0.45700884\n",
      "Iteration 580, loss = 0.45641405\n",
      "Iteration 581, loss = 0.45582041\n",
      "Iteration 582, loss = 0.45522804\n",
      "Iteration 583, loss = 0.45463807\n",
      "Iteration 584, loss = 0.45404851\n",
      "Iteration 585, loss = 0.45345952\n",
      "Iteration 586, loss = 0.45287126\n",
      "Iteration 587, loss = 0.45228437\n",
      "Iteration 588, loss = 0.45170001\n",
      "Iteration 589, loss = 0.45111733\n",
      "Iteration 590, loss = 0.45053638\n",
      "Iteration 591, loss = 0.44995820\n",
      "Iteration 592, loss = 0.44938176\n",
      "Iteration 593, loss = 0.44880718\n",
      "Iteration 594, loss = 0.44823402\n",
      "Iteration 595, loss = 0.44766271\n",
      "Iteration 596, loss = 0.44709516\n",
      "Iteration 597, loss = 0.44653067\n",
      "Iteration 598, loss = 0.44596624\n",
      "Iteration 599, loss = 0.44540490\n",
      "Iteration 600, loss = 0.44484635\n",
      "Iteration 601, loss = 0.44429009\n",
      "Iteration 602, loss = 0.44373609\n",
      "Iteration 603, loss = 0.44318437\n",
      "Iteration 604, loss = 0.44263491\n",
      "Iteration 605, loss = 0.44208770\n",
      "Iteration 606, loss = 0.44154274\n",
      "Iteration 607, loss = 0.44100001\n",
      "Iteration 608, loss = 0.44045949\n",
      "Iteration 609, loss = 0.43992119\n",
      "Iteration 610, loss = 0.43938509\n",
      "Iteration 611, loss = 0.43885117\n",
      "Iteration 612, loss = 0.43831943\n",
      "Iteration 613, loss = 0.43778986\n",
      "Iteration 614, loss = 0.43726243\n",
      "Iteration 615, loss = 0.43673715\n",
      "Iteration 616, loss = 0.43621425\n",
      "Iteration 617, loss = 0.43569427\n",
      "Iteration 618, loss = 0.43517621\n",
      "Iteration 619, loss = 0.43466013\n",
      "Iteration 620, loss = 0.43414608\n",
      "Iteration 621, loss = 0.43363410\n",
      "Iteration 622, loss = 0.43312425\n",
      "Iteration 623, loss = 0.43261737\n",
      "Iteration 624, loss = 0.43211240\n",
      "Iteration 625, loss = 0.43160942\n",
      "Iteration 626, loss = 0.43110850\n",
      "Iteration 627, loss = 0.43060984\n",
      "Iteration 628, loss = 0.43011372\n",
      "Iteration 629, loss = 0.42961961\n",
      "Iteration 630, loss = 0.42912753\n",
      "Iteration 631, loss = 0.42863748\n",
      "Iteration 632, loss = 0.42814945\n",
      "Iteration 633, loss = 0.42766343\n",
      "Iteration 634, loss = 0.42717941\n",
      "Iteration 635, loss = 0.42669758\n",
      "Iteration 636, loss = 0.42621757\n",
      "Iteration 637, loss = 0.42573942\n",
      "Iteration 638, loss = 0.42526344\n",
      "Iteration 639, loss = 0.42478940\n",
      "Iteration 640, loss = 0.42431725\n",
      "Iteration 641, loss = 0.42384707\n",
      "Iteration 642, loss = 0.42337889\n",
      "Iteration 643, loss = 0.42291273\n",
      "Iteration 644, loss = 0.42244821\n",
      "Iteration 645, loss = 0.42198536\n",
      "Iteration 646, loss = 0.42152503\n",
      "Iteration 647, loss = 0.42106658\n",
      "Iteration 648, loss = 0.42060969\n",
      "Iteration 649, loss = 0.42015432\n",
      "Iteration 650, loss = 0.41970119\n",
      "Iteration 651, loss = 0.41924974\n",
      "Iteration 652, loss = 0.41879986\n",
      "Iteration 653, loss = 0.41835185\n",
      "Iteration 654, loss = 0.41790565\n",
      "Iteration 655, loss = 0.41746109\n",
      "Iteration 656, loss = 0.41701815\n",
      "Iteration 657, loss = 0.41657683\n",
      "Iteration 658, loss = 0.41613711\n",
      "Iteration 659, loss = 0.41569897\n",
      "Iteration 660, loss = 0.41526262\n",
      "Iteration 661, loss = 0.41482812\n",
      "Iteration 662, loss = 0.41439464\n",
      "Iteration 663, loss = 0.41396288\n",
      "Iteration 664, loss = 0.41353258\n",
      "Iteration 665, loss = 0.41310371\n",
      "Iteration 666, loss = 0.41267624\n",
      "Iteration 667, loss = 0.41225013\n",
      "Iteration 668, loss = 0.41182549\n",
      "Iteration 669, loss = 0.41140216\n",
      "Iteration 670, loss = 0.41098018\n",
      "Iteration 671, loss = 0.41055959\n",
      "Iteration 672, loss = 0.41014026\n",
      "Iteration 673, loss = 0.40972227\n",
      "Iteration 674, loss = 0.40930564\n",
      "Iteration 675, loss = 0.40889038\n",
      "Iteration 676, loss = 0.40847638\n",
      "Iteration 677, loss = 0.40806339\n",
      "Iteration 678, loss = 0.40765199\n",
      "Iteration 679, loss = 0.40724164\n",
      "Iteration 680, loss = 0.40683247\n",
      "Iteration 681, loss = 0.40642435\n",
      "Iteration 682, loss = 0.40601760\n",
      "Iteration 683, loss = 0.40561207\n",
      "Iteration 684, loss = 0.40520758\n",
      "Iteration 685, loss = 0.40480432\n",
      "Iteration 686, loss = 0.40440208\n",
      "Iteration 687, loss = 0.40400088\n",
      "Iteration 688, loss = 0.40360122\n",
      "Iteration 689, loss = 0.40320237\n",
      "Iteration 690, loss = 0.40280441\n",
      "Iteration 691, loss = 0.40240823\n",
      "Iteration 692, loss = 0.40201321\n",
      "Iteration 693, loss = 0.40161915\n",
      "Iteration 694, loss = 0.40122604\n",
      "Iteration 695, loss = 0.40083395\n",
      "Iteration 696, loss = 0.40044288\n",
      "Iteration 697, loss = 0.40005275\n",
      "Iteration 698, loss = 0.39966352\n",
      "Iteration 699, loss = 0.39927520\n",
      "Iteration 700, loss = 0.39888777\n",
      "Iteration 701, loss = 0.39850120\n",
      "Iteration 702, loss = 0.39811548\n",
      "Iteration 703, loss = 0.39773059\n",
      "Iteration 704, loss = 0.39734664\n",
      "Iteration 705, loss = 0.39696356\n",
      "Iteration 706, loss = 0.39658126\n",
      "Iteration 707, loss = 0.39619972\n",
      "Iteration 708, loss = 0.39581894\n",
      "Iteration 709, loss = 0.39543888\n",
      "Iteration 710, loss = 0.39505954\n",
      "Iteration 711, loss = 0.39468088\n",
      "Iteration 712, loss = 0.39430291\n",
      "Iteration 713, loss = 0.39392559\n",
      "Iteration 714, loss = 0.39354891\n",
      "Iteration 715, loss = 0.39317287\n",
      "Iteration 716, loss = 0.39279744\n",
      "Iteration 717, loss = 0.39242261\n",
      "Iteration 718, loss = 0.39204837\n",
      "Iteration 719, loss = 0.39167471\n",
      "Iteration 720, loss = 0.39130161\n",
      "Iteration 721, loss = 0.39092906\n",
      "Iteration 722, loss = 0.39055705\n",
      "Iteration 723, loss = 0.39018557\n",
      "Iteration 724, loss = 0.38981461\n",
      "Iteration 725, loss = 0.38944415\n",
      "Iteration 726, loss = 0.38907419\n",
      "Iteration 727, loss = 0.38870486\n",
      "Iteration 728, loss = 0.38833625\n",
      "Iteration 729, loss = 0.38796814\n",
      "Iteration 730, loss = 0.38760071\n",
      "Iteration 731, loss = 0.38723420\n",
      "Iteration 732, loss = 0.38686876\n",
      "Iteration 733, loss = 0.38650463\n",
      "Iteration 734, loss = 0.38614055\n",
      "Iteration 735, loss = 0.38577665\n",
      "Iteration 736, loss = 0.38541345\n",
      "Iteration 737, loss = 0.38505059\n",
      "Iteration 738, loss = 0.38468873\n",
      "Iteration 739, loss = 0.38432794\n",
      "Iteration 740, loss = 0.38396749\n",
      "Iteration 741, loss = 0.38360734\n",
      "Iteration 742, loss = 0.38324745\n",
      "Iteration 743, loss = 0.38288781\n",
      "Iteration 744, loss = 0.38252925\n",
      "Iteration 745, loss = 0.38217139\n",
      "Iteration 746, loss = 0.38181410\n",
      "Iteration 747, loss = 0.38145715\n",
      "Iteration 748, loss = 0.38110056\n",
      "Iteration 749, loss = 0.38074436\n",
      "Iteration 750, loss = 0.38038856\n",
      "Iteration 751, loss = 0.38003319\n",
      "Iteration 752, loss = 0.37967900\n",
      "Iteration 753, loss = 0.37932482\n",
      "Iteration 754, loss = 0.37897053\n",
      "Iteration 755, loss = 0.37861670\n",
      "Iteration 756, loss = 0.37826365\n",
      "Iteration 757, loss = 0.37791092\n",
      "Iteration 758, loss = 0.37755852\n",
      "Iteration 759, loss = 0.37720644\n",
      "Iteration 760, loss = 0.37685469\n",
      "Iteration 761, loss = 0.37650327\n",
      "Iteration 762, loss = 0.37615217\n",
      "Iteration 763, loss = 0.37580141\n",
      "Iteration 764, loss = 0.37545097\n",
      "Iteration 765, loss = 0.37510084\n",
      "Iteration 766, loss = 0.37475103\n",
      "Iteration 767, loss = 0.37440153\n",
      "Iteration 768, loss = 0.37405232\n",
      "Iteration 769, loss = 0.37370377\n",
      "Iteration 770, loss = 0.37335586\n",
      "Iteration 771, loss = 0.37300828\n",
      "Iteration 772, loss = 0.37266102\n",
      "Iteration 773, loss = 0.37231405\n",
      "Iteration 774, loss = 0.37196737\n",
      "Iteration 775, loss = 0.37162097\n",
      "Iteration 776, loss = 0.37127483\n",
      "Iteration 777, loss = 0.37092894\n",
      "Iteration 778, loss = 0.37058330\n",
      "Iteration 779, loss = 0.37023789\n",
      "Iteration 780, loss = 0.36989270\n",
      "Iteration 781, loss = 0.36954774\n",
      "Iteration 782, loss = 0.36920343\n",
      "Iteration 783, loss = 0.36885944\n",
      "Iteration 784, loss = 0.36851583\n",
      "Iteration 785, loss = 0.36817270\n",
      "Iteration 786, loss = 0.36782977\n",
      "Iteration 787, loss = 0.36748702\n",
      "Iteration 788, loss = 0.36714467\n",
      "Iteration 789, loss = 0.36680251\n",
      "Iteration 790, loss = 0.36646042\n",
      "Iteration 791, loss = 0.36611892\n",
      "Iteration 792, loss = 0.36577754\n",
      "Iteration 793, loss = 0.36543625\n",
      "Iteration 794, loss = 0.36509503\n",
      "Iteration 795, loss = 0.36475417\n",
      "Iteration 796, loss = 0.36441364\n",
      "Iteration 797, loss = 0.36407319\n",
      "Iteration 798, loss = 0.36373286\n",
      "Iteration 799, loss = 0.36339265\n",
      "Iteration 800, loss = 0.36305298\n",
      "Iteration 801, loss = 0.36271336\n",
      "Iteration 802, loss = 0.36237369\n",
      "Iteration 803, loss = 0.36203422\n",
      "Iteration 804, loss = 0.36169514\n",
      "Iteration 805, loss = 0.36135616\n",
      "Iteration 806, loss = 0.36101729\n",
      "Iteration 807, loss = 0.36067856\n",
      "Iteration 808, loss = 0.36033996\n",
      "Iteration 809, loss = 0.36000152\n",
      "Iteration 810, loss = 0.35966334\n",
      "Iteration 811, loss = 0.35932517\n",
      "Iteration 812, loss = 0.35898729\n",
      "Iteration 813, loss = 0.35864954\n",
      "Iteration 814, loss = 0.35831190\n",
      "Iteration 815, loss = 0.35797438\n",
      "Iteration 816, loss = 0.35763699\n",
      "Iteration 817, loss = 0.35729972\n",
      "Iteration 818, loss = 0.35696258\n",
      "Iteration 819, loss = 0.35662556\n",
      "Iteration 820, loss = 0.35628867\n",
      "Iteration 821, loss = 0.35595190\n",
      "Iteration 822, loss = 0.35561525\n",
      "Iteration 823, loss = 0.35527873\n",
      "Iteration 824, loss = 0.35494232\n",
      "Iteration 825, loss = 0.35460603\n",
      "Iteration 826, loss = 0.35426985\n",
      "Iteration 827, loss = 0.35393377\n",
      "Iteration 828, loss = 0.35359780\n",
      "Iteration 829, loss = 0.35326193\n",
      "Iteration 830, loss = 0.35292616\n",
      "Iteration 831, loss = 0.35259075\n",
      "Iteration 832, loss = 0.35225501\n",
      "Iteration 833, loss = 0.35191971\n",
      "Iteration 834, loss = 0.35158452\n",
      "Iteration 835, loss = 0.35124942\n",
      "Iteration 836, loss = 0.35091438\n",
      "Iteration 837, loss = 0.35057941\n",
      "Iteration 838, loss = 0.35024450\n",
      "Iteration 839, loss = 0.34990964\n",
      "Iteration 840, loss = 0.34957482\n",
      "Iteration 841, loss = 0.34924005\n",
      "Iteration 842, loss = 0.34890563\n",
      "Iteration 843, loss = 0.34857134\n",
      "Iteration 844, loss = 0.34823680\n",
      "Iteration 845, loss = 0.34790273\n",
      "Iteration 846, loss = 0.34756922\n",
      "Iteration 847, loss = 0.34723575\n",
      "Iteration 848, loss = 0.34690230\n",
      "Iteration 849, loss = 0.34656887\n",
      "Iteration 850, loss = 0.34623552\n",
      "Iteration 851, loss = 0.34590253\n",
      "Iteration 852, loss = 0.34556946\n",
      "Iteration 853, loss = 0.34523657\n",
      "Iteration 854, loss = 0.34490398\n",
      "Iteration 855, loss = 0.34457126\n",
      "Iteration 856, loss = 0.34423861\n",
      "Iteration 857, loss = 0.34390615\n",
      "Iteration 858, loss = 0.34357367\n",
      "Iteration 859, loss = 0.34324135\n",
      "Iteration 860, loss = 0.34290920\n",
      "Iteration 861, loss = 0.34257701\n",
      "Iteration 862, loss = 0.34224502\n",
      "Iteration 863, loss = 0.34191303\n",
      "Iteration 864, loss = 0.34158105\n",
      "Iteration 865, loss = 0.34124924\n",
      "Iteration 866, loss = 0.34091743\n",
      "Iteration 867, loss = 0.34058578\n",
      "Iteration 868, loss = 0.34025414\n",
      "Iteration 869, loss = 0.33992278\n",
      "Iteration 870, loss = 0.33959126\n",
      "Iteration 871, loss = 0.33925985\n",
      "Iteration 872, loss = 0.33892862\n",
      "Iteration 873, loss = 0.33859735\n",
      "Iteration 874, loss = 0.33826609\n",
      "Iteration 875, loss = 0.33793503\n",
      "Iteration 876, loss = 0.33760395\n",
      "Iteration 877, loss = 0.33727300\n",
      "Iteration 878, loss = 0.33694217\n",
      "Iteration 879, loss = 0.33661134\n",
      "Iteration 880, loss = 0.33628053\n",
      "Iteration 881, loss = 0.33594974\n",
      "Iteration 882, loss = 0.33561918\n",
      "Iteration 883, loss = 0.33528853\n",
      "Iteration 884, loss = 0.33495808\n",
      "Iteration 885, loss = 0.33462772\n",
      "Iteration 886, loss = 0.33429738\n",
      "Iteration 887, loss = 0.33396707\n",
      "Iteration 888, loss = 0.33363679\n",
      "Iteration 889, loss = 0.33330656\n",
      "Iteration 890, loss = 0.33297637\n",
      "Iteration 891, loss = 0.33264625\n",
      "Iteration 892, loss = 0.33231646\n",
      "Iteration 893, loss = 0.33198639\n",
      "Iteration 894, loss = 0.33165653\n",
      "Iteration 895, loss = 0.33132680\n",
      "Iteration 896, loss = 0.33099711\n",
      "Iteration 897, loss = 0.33066746\n",
      "Iteration 898, loss = 0.33033786\n",
      "Iteration 899, loss = 0.33000830\n",
      "Iteration 900, loss = 0.32967881\n",
      "Iteration 901, loss = 0.32934937\n",
      "Iteration 902, loss = 0.32901999\n",
      "Iteration 903, loss = 0.32869068\n",
      "Iteration 904, loss = 0.32836143\n",
      "Iteration 905, loss = 0.32803225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 906, loss = 0.32770314\n",
      "Iteration 907, loss = 0.32737410\n",
      "Iteration 908, loss = 0.32704513\n",
      "Iteration 909, loss = 0.32671623\n",
      "Iteration 910, loss = 0.32638739\n",
      "Iteration 911, loss = 0.32605863\n",
      "Iteration 912, loss = 0.32572994\n",
      "Iteration 913, loss = 0.32540131\n",
      "Iteration 914, loss = 0.32507275\n",
      "Iteration 915, loss = 0.32474426\n",
      "Iteration 916, loss = 0.32441583\n",
      "Iteration 917, loss = 0.32408747\n",
      "Iteration 918, loss = 0.32375917\n",
      "Iteration 919, loss = 0.32343094\n",
      "Iteration 920, loss = 0.32310277\n",
      "Iteration 921, loss = 0.32277466\n",
      "Iteration 922, loss = 0.32244661\n",
      "Iteration 923, loss = 0.32211863\n",
      "Iteration 924, loss = 0.32179071\n",
      "Iteration 925, loss = 0.32146285\n",
      "Iteration 926, loss = 0.32113506\n",
      "Iteration 927, loss = 0.32080733\n",
      "Iteration 928, loss = 0.32047966\n",
      "Iteration 929, loss = 0.32015206\n",
      "Iteration 930, loss = 0.31982452\n",
      "Iteration 931, loss = 0.31949705\n",
      "Iteration 932, loss = 0.31916964\n",
      "Iteration 933, loss = 0.31884229\n",
      "Iteration 934, loss = 0.31851502\n",
      "Iteration 935, loss = 0.31818781\n",
      "Iteration 936, loss = 0.31786067\n",
      "Iteration 937, loss = 0.31753359\n",
      "Iteration 938, loss = 0.31720659\n",
      "Iteration 939, loss = 0.31687965\n",
      "Iteration 940, loss = 0.31655278\n",
      "Iteration 941, loss = 0.31622598\n",
      "Iteration 942, loss = 0.31589926\n",
      "Iteration 943, loss = 0.31557260\n",
      "Iteration 944, loss = 0.31524602\n",
      "Iteration 945, loss = 0.31491951\n",
      "Iteration 946, loss = 0.31459307\n",
      "Iteration 947, loss = 0.31426671\n",
      "Iteration 948, loss = 0.31394042\n",
      "Iteration 949, loss = 0.31361420\n",
      "Iteration 950, loss = 0.31328806\n",
      "Iteration 951, loss = 0.31296200\n",
      "Iteration 952, loss = 0.31263601\n",
      "Iteration 953, loss = 0.31231067\n",
      "Iteration 954, loss = 0.31198508\n",
      "Iteration 955, loss = 0.31165913\n",
      "Iteration 956, loss = 0.31133335\n",
      "Iteration 957, loss = 0.31100802\n",
      "Iteration 958, loss = 0.31068277\n",
      "Iteration 959, loss = 0.31035759\n",
      "Iteration 960, loss = 0.31003247\n",
      "Iteration 961, loss = 0.30970742\n",
      "Iteration 962, loss = 0.30938242\n",
      "Iteration 963, loss = 0.30905749\n",
      "Iteration 964, loss = 0.30873261\n",
      "Iteration 965, loss = 0.30840780\n",
      "Iteration 966, loss = 0.30808337\n",
      "Iteration 967, loss = 0.30775903\n",
      "Iteration 968, loss = 0.30743450\n",
      "Iteration 969, loss = 0.30711021\n",
      "Iteration 970, loss = 0.30678620\n",
      "Iteration 971, loss = 0.30646222\n",
      "Iteration 972, loss = 0.30613828\n",
      "Iteration 973, loss = 0.30581437\n",
      "Iteration 974, loss = 0.30549050\n",
      "Iteration 975, loss = 0.30516717\n",
      "Iteration 976, loss = 0.30484380\n",
      "Iteration 977, loss = 0.30452031\n",
      "Iteration 978, loss = 0.30419674\n",
      "Iteration 979, loss = 0.30387369\n",
      "Iteration 980, loss = 0.30355065\n",
      "Iteration 981, loss = 0.30322769\n",
      "Iteration 982, loss = 0.30290485\n",
      "Iteration 983, loss = 0.30258234\n",
      "Iteration 984, loss = 0.30225982\n",
      "Iteration 985, loss = 0.30193729\n",
      "Iteration 986, loss = 0.30161494\n",
      "Iteration 987, loss = 0.30129280\n",
      "Iteration 988, loss = 0.30097062\n",
      "Iteration 989, loss = 0.30064879\n",
      "Iteration 990, loss = 0.30032703\n",
      "Iteration 991, loss = 0.30000522\n",
      "Iteration 992, loss = 0.29968359\n",
      "Iteration 993, loss = 0.29936221\n",
      "Iteration 994, loss = 0.29904083\n",
      "Iteration 995, loss = 0.29871959\n",
      "Iteration 996, loss = 0.29839852\n",
      "Iteration 997, loss = 0.29807761\n",
      "Iteration 998, loss = 0.29775682\n",
      "Iteration 999, loss = 0.29743607\n",
      "Iteration 1000, loss = 0.29711571\n",
      "Iteration 1, loss = 1.50048972\n",
      "Iteration 2, loss = 1.47058104\n",
      "Iteration 3, loss = 1.43039857\n",
      "Iteration 4, loss = 1.38364005\n",
      "Iteration 5, loss = 1.33363187\n",
      "Iteration 6, loss = 1.28306261\n",
      "Iteration 7, loss = 1.23396041\n",
      "Iteration 8, loss = 1.18758970\n",
      "Iteration 9, loss = 1.14438433\n",
      "Iteration 10, loss = 1.10462256\n",
      "Iteration 11, loss = 1.06829392\n",
      "Iteration 12, loss = 1.03481161\n",
      "Iteration 13, loss = 1.00361937\n",
      "Iteration 14, loss = 0.97439535\n",
      "Iteration 15, loss = 0.94711345\n",
      "Iteration 16, loss = 0.92175906\n",
      "Iteration 17, loss = 0.89843996\n",
      "Iteration 18, loss = 0.87711769\n",
      "Iteration 19, loss = 0.85766073\n",
      "Iteration 20, loss = 0.84015675\n",
      "Iteration 21, loss = 0.82457708\n",
      "Iteration 22, loss = 0.81102752\n",
      "Iteration 23, loss = 0.79971288\n",
      "Iteration 24, loss = 0.79060594\n",
      "Iteration 25, loss = 0.78329420\n",
      "Iteration 26, loss = 0.77761381\n",
      "Iteration 27, loss = 0.77327755\n",
      "Iteration 28, loss = 0.77014601\n",
      "Iteration 29, loss = 0.76797534\n",
      "Iteration 30, loss = 0.76637396\n",
      "Iteration 31, loss = 0.76510590\n",
      "Iteration 32, loss = 0.76411262\n",
      "Iteration 33, loss = 0.76319921\n",
      "Iteration 34, loss = 0.76225240\n",
      "Iteration 35, loss = 0.76113835\n",
      "Iteration 36, loss = 0.75987665\n",
      "Iteration 37, loss = 0.75844522\n",
      "Iteration 38, loss = 0.75678908\n",
      "Iteration 39, loss = 0.75492705\n",
      "Iteration 40, loss = 0.75288457\n",
      "Iteration 41, loss = 0.75065964\n",
      "Iteration 42, loss = 0.74827571\n",
      "Iteration 43, loss = 0.74578041\n",
      "Iteration 44, loss = 0.74326313\n",
      "Iteration 45, loss = 0.74070595\n",
      "Iteration 46, loss = 0.73811557\n",
      "Iteration 47, loss = 0.73555080\n",
      "Iteration 48, loss = 0.73303439\n",
      "Iteration 49, loss = 0.73055920\n",
      "Iteration 50, loss = 0.72816446\n",
      "Iteration 51, loss = 0.72587741\n",
      "Iteration 52, loss = 0.72369067\n",
      "Iteration 53, loss = 0.72161682\n",
      "Iteration 54, loss = 0.71961302\n",
      "Iteration 55, loss = 0.71767706\n",
      "Iteration 56, loss = 0.71580281\n",
      "Iteration 57, loss = 0.71396168\n",
      "Iteration 58, loss = 0.71214249\n",
      "Iteration 59, loss = 0.71034123\n",
      "Iteration 60, loss = 0.70854726\n",
      "Iteration 61, loss = 0.70675812\n",
      "Iteration 62, loss = 0.70497192\n",
      "Iteration 63, loss = 0.70318737\n",
      "Iteration 64, loss = 0.70140366\n",
      "Iteration 65, loss = 0.69962042\n",
      "Iteration 66, loss = 0.69783768\n",
      "Iteration 67, loss = 0.69605576\n",
      "Iteration 68, loss = 0.69427522\n",
      "Iteration 69, loss = 0.69249682\n",
      "Iteration 70, loss = 0.69072297\n",
      "Iteration 71, loss = 0.68896390\n",
      "Iteration 72, loss = 0.68721269\n",
      "Iteration 73, loss = 0.68546819\n",
      "Iteration 74, loss = 0.68373371\n",
      "Iteration 75, loss = 0.68201490\n",
      "Iteration 76, loss = 0.68031225\n",
      "Iteration 77, loss = 0.67862222\n",
      "Iteration 78, loss = 0.67694474\n",
      "Iteration 79, loss = 0.67528332\n",
      "Iteration 80, loss = 0.67363173\n",
      "Iteration 81, loss = 0.67198982\n",
      "Iteration 82, loss = 0.67035755\n",
      "Iteration 83, loss = 0.66873562\n",
      "Iteration 84, loss = 0.66713151\n",
      "Iteration 85, loss = 0.66553544\n",
      "Iteration 86, loss = 0.66394731\n",
      "Iteration 87, loss = 0.66236909\n",
      "Iteration 88, loss = 0.66079730\n",
      "Iteration 89, loss = 0.65923245\n",
      "Iteration 90, loss = 0.65767467\n",
      "Iteration 91, loss = 0.65612409\n",
      "Iteration 92, loss = 0.65458086\n",
      "Iteration 93, loss = 0.65304510\n",
      "Iteration 94, loss = 0.65151831\n",
      "Iteration 95, loss = 0.65000056\n",
      "Iteration 96, loss = 0.64849110\n",
      "Iteration 97, loss = 0.64698992\n",
      "Iteration 98, loss = 0.64549701\n",
      "Iteration 99, loss = 0.64401237\n",
      "Iteration 100, loss = 0.64253685\n",
      "Iteration 101, loss = 0.64107092\n",
      "Iteration 102, loss = 0.63961367\n",
      "Iteration 103, loss = 0.63816490\n",
      "Iteration 104, loss = 0.63672506\n",
      "Iteration 105, loss = 0.63529325\n",
      "Iteration 106, loss = 0.63386943\n",
      "Iteration 107, loss = 0.63245362\n",
      "Iteration 108, loss = 0.63104584\n",
      "Iteration 109, loss = 0.62964610\n",
      "Iteration 110, loss = 0.62825441\n",
      "Iteration 111, loss = 0.62687078\n",
      "Iteration 112, loss = 0.62549538\n",
      "Iteration 113, loss = 0.62412832\n",
      "Iteration 114, loss = 0.62276924\n",
      "Iteration 115, loss = 0.62141811\n",
      "Iteration 116, loss = 0.62007488\n",
      "Iteration 117, loss = 0.61873951\n",
      "Iteration 118, loss = 0.61741196\n",
      "Iteration 119, loss = 0.61609219\n",
      "Iteration 120, loss = 0.61478024\n",
      "Iteration 121, loss = 0.61347612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 122, loss = 0.61217970\n",
      "Iteration 123, loss = 0.61089095\n",
      "Iteration 124, loss = 0.60960992\n",
      "Iteration 125, loss = 0.60833644\n",
      "Iteration 126, loss = 0.60707052\n",
      "Iteration 127, loss = 0.60581223\n",
      "Iteration 128, loss = 0.60456133\n",
      "Iteration 129, loss = 0.60331793\n",
      "Iteration 130, loss = 0.60208195\n",
      "Iteration 131, loss = 0.60085339\n",
      "Iteration 132, loss = 0.59963212\n",
      "Iteration 133, loss = 0.59841817\n",
      "Iteration 134, loss = 0.59721146\n",
      "Iteration 135, loss = 0.59601196\n",
      "Iteration 136, loss = 0.59481962\n",
      "Iteration 137, loss = 0.59363441\n",
      "Iteration 138, loss = 0.59245626\n",
      "Iteration 139, loss = 0.59128515\n",
      "Iteration 140, loss = 0.59012102\n",
      "Iteration 141, loss = 0.58896383\n",
      "Iteration 142, loss = 0.58781352\n",
      "Iteration 143, loss = 0.58667005\n",
      "Iteration 144, loss = 0.58553337\n",
      "Iteration 145, loss = 0.58440343\n",
      "Iteration 146, loss = 0.58328018\n",
      "Iteration 147, loss = 0.58216357\n",
      "Iteration 148, loss = 0.58105356\n",
      "Iteration 149, loss = 0.57995009\n",
      "Iteration 150, loss = 0.57885311\n",
      "Iteration 151, loss = 0.57776258\n",
      "Iteration 152, loss = 0.57667844\n",
      "Iteration 153, loss = 0.57560064\n",
      "Iteration 154, loss = 0.57452914\n",
      "Iteration 155, loss = 0.57346389\n",
      "Iteration 156, loss = 0.57240483\n",
      "Iteration 157, loss = 0.57135235\n",
      "Iteration 158, loss = 0.57030701\n",
      "Iteration 159, loss = 0.56926792\n",
      "Iteration 160, loss = 0.56823490\n",
      "Iteration 161, loss = 0.56720792\n",
      "Iteration 162, loss = 0.56618696\n",
      "Iteration 163, loss = 0.56517196\n",
      "Iteration 164, loss = 0.56416290\n",
      "Iteration 165, loss = 0.56315971\n",
      "Iteration 166, loss = 0.56216237\n",
      "Iteration 167, loss = 0.56117081\n",
      "Iteration 168, loss = 0.56018500\n",
      "Iteration 169, loss = 0.55920487\n",
      "Iteration 170, loss = 0.55823039\n",
      "Iteration 171, loss = 0.55726150\n",
      "Iteration 172, loss = 0.55629815\n",
      "Iteration 173, loss = 0.55534028\n",
      "Iteration 174, loss = 0.55438785\n",
      "Iteration 175, loss = 0.55344080\n",
      "Iteration 176, loss = 0.55249910\n",
      "Iteration 177, loss = 0.55156268\n",
      "Iteration 178, loss = 0.55063150\n",
      "Iteration 179, loss = 0.54970552\n",
      "Iteration 180, loss = 0.54878468\n",
      "Iteration 181, loss = 0.54786893\n",
      "Iteration 182, loss = 0.54695824\n",
      "Iteration 183, loss = 0.54605255\n",
      "Iteration 184, loss = 0.54515182\n",
      "Iteration 185, loss = 0.54425600\n",
      "Iteration 186, loss = 0.54336504\n",
      "Iteration 187, loss = 0.54247891\n",
      "Iteration 188, loss = 0.54159754\n",
      "Iteration 189, loss = 0.54072091\n",
      "Iteration 190, loss = 0.53984897\n",
      "Iteration 191, loss = 0.53898166\n",
      "Iteration 192, loss = 0.53811895\n",
      "Iteration 193, loss = 0.53726080\n",
      "Iteration 194, loss = 0.53640715\n",
      "Iteration 195, loss = 0.53555797\n",
      "Iteration 196, loss = 0.53471321\n",
      "Iteration 197, loss = 0.53387284\n",
      "Iteration 198, loss = 0.53303680\n",
      "Iteration 199, loss = 0.53220506\n",
      "Iteration 200, loss = 0.53137758\n",
      "Iteration 201, loss = 0.53055509\n",
      "Iteration 202, loss = 0.52973697\n",
      "Iteration 203, loss = 0.52892302\n",
      "Iteration 204, loss = 0.52811321\n",
      "Iteration 205, loss = 0.52730751\n",
      "Iteration 206, loss = 0.52650588\n",
      "Iteration 207, loss = 0.52570829\n",
      "Iteration 208, loss = 0.52491470\n",
      "Iteration 209, loss = 0.52412508\n",
      "Iteration 210, loss = 0.52333940\n",
      "Iteration 211, loss = 0.52255761\n",
      "Iteration 212, loss = 0.52177968\n",
      "Iteration 213, loss = 0.52100557\n",
      "Iteration 214, loss = 0.52023525\n",
      "Iteration 215, loss = 0.51946867\n",
      "Iteration 216, loss = 0.51870580\n",
      "Iteration 217, loss = 0.51794660\n",
      "Iteration 218, loss = 0.51719103\n",
      "Iteration 219, loss = 0.51643906\n",
      "Iteration 220, loss = 0.51569065\n",
      "Iteration 221, loss = 0.51494577\n",
      "Iteration 222, loss = 0.51420437\n",
      "Iteration 223, loss = 0.51346642\n",
      "Iteration 224, loss = 0.51273189\n",
      "Iteration 225, loss = 0.51200074\n",
      "Iteration 226, loss = 0.51127294\n",
      "Iteration 227, loss = 0.51054845\n",
      "Iteration 228, loss = 0.50982725\n",
      "Iteration 229, loss = 0.50910929\n",
      "Iteration 230, loss = 0.50839454\n",
      "Iteration 231, loss = 0.50768298\n",
      "Iteration 232, loss = 0.50697457\n",
      "Iteration 233, loss = 0.50626927\n",
      "Iteration 234, loss = 0.50556707\n",
      "Iteration 235, loss = 0.50486844\n",
      "Iteration 236, loss = 0.50417303\n",
      "Iteration 237, loss = 0.50348066\n",
      "Iteration 238, loss = 0.50279130\n",
      "Iteration 239, loss = 0.50210492\n",
      "Iteration 240, loss = 0.50142150\n",
      "Iteration 241, loss = 0.50074100\n",
      "Iteration 242, loss = 0.50006341\n",
      "Iteration 243, loss = 0.49938868\n",
      "Iteration 244, loss = 0.49871679\n",
      "Iteration 245, loss = 0.49804807\n",
      "Iteration 246, loss = 0.49738231\n",
      "Iteration 247, loss = 0.49671999\n",
      "Iteration 248, loss = 0.49606046\n",
      "Iteration 249, loss = 0.49540369\n",
      "Iteration 250, loss = 0.49474966\n",
      "Iteration 251, loss = 0.49409835\n",
      "Iteration 252, loss = 0.49344973\n",
      "Iteration 253, loss = 0.49280378\n",
      "Iteration 254, loss = 0.49216046\n",
      "Iteration 255, loss = 0.49151976\n",
      "Iteration 256, loss = 0.49088165\n",
      "Iteration 257, loss = 0.49024609\n",
      "Iteration 258, loss = 0.48961306\n",
      "Iteration 259, loss = 0.48898253\n",
      "Iteration 260, loss = 0.48835485\n",
      "Iteration 261, loss = 0.48772975\n",
      "Iteration 262, loss = 0.48710711\n",
      "Iteration 263, loss = 0.48648689\n",
      "Iteration 264, loss = 0.48586906\n",
      "Iteration 265, loss = 0.48525360\n",
      "Iteration 266, loss = 0.48464047\n",
      "Iteration 267, loss = 0.48402966\n",
      "Iteration 268, loss = 0.48342114\n",
      "Iteration 269, loss = 0.48281487\n",
      "Iteration 270, loss = 0.48221085\n",
      "Iteration 271, loss = 0.48160904\n",
      "Iteration 272, loss = 0.48100942\n",
      "Iteration 273, loss = 0.48041197\n",
      "Iteration 274, loss = 0.47981667\n",
      "Iteration 275, loss = 0.47922349\n",
      "Iteration 276, loss = 0.47863241\n",
      "Iteration 277, loss = 0.47804341\n",
      "Iteration 278, loss = 0.47745647\n",
      "Iteration 279, loss = 0.47687157\n",
      "Iteration 280, loss = 0.47628869\n",
      "Iteration 281, loss = 0.47570781\n",
      "Iteration 282, loss = 0.47512890\n",
      "Iteration 283, loss = 0.47455195\n",
      "Iteration 284, loss = 0.47397694\n",
      "Iteration 285, loss = 0.47340385\n",
      "Iteration 286, loss = 0.47283266\n",
      "Iteration 287, loss = 0.47226335\n",
      "Iteration 288, loss = 0.47169590\n",
      "Iteration 289, loss = 0.47113030\n",
      "Iteration 290, loss = 0.47056652\n",
      "Iteration 291, loss = 0.47000455\n",
      "Iteration 292, loss = 0.46944437\n",
      "Iteration 293, loss = 0.46888596\n",
      "Iteration 294, loss = 0.46832931\n",
      "Iteration 295, loss = 0.46777439\n",
      "Iteration 296, loss = 0.46722120\n",
      "Iteration 297, loss = 0.46666971\n",
      "Iteration 298, loss = 0.46611991\n",
      "Iteration 299, loss = 0.46557179\n",
      "Iteration 300, loss = 0.46502532\n",
      "Iteration 301, loss = 0.46448048\n",
      "Iteration 302, loss = 0.46393728\n",
      "Iteration 303, loss = 0.46339568\n",
      "Iteration 304, loss = 0.46285568\n",
      "Iteration 305, loss = 0.46231741\n",
      "Iteration 306, loss = 0.46178154\n",
      "Iteration 307, loss = 0.46124727\n",
      "Iteration 308, loss = 0.46071459\n",
      "Iteration 309, loss = 0.46018348\n",
      "Iteration 310, loss = 0.45965392\n",
      "Iteration 311, loss = 0.45912591\n",
      "Iteration 312, loss = 0.45859942\n",
      "Iteration 313, loss = 0.45807445\n",
      "Iteration 314, loss = 0.45755097\n",
      "Iteration 315, loss = 0.45702897\n",
      "Iteration 316, loss = 0.45650845\n",
      "Iteration 317, loss = 0.45598937\n",
      "Iteration 318, loss = 0.45547173\n",
      "Iteration 319, loss = 0.45495557\n",
      "Iteration 320, loss = 0.45444173\n",
      "Iteration 321, loss = 0.45392932\n",
      "Iteration 322, loss = 0.45341836\n",
      "Iteration 323, loss = 0.45290881\n",
      "Iteration 324, loss = 0.45240068\n",
      "Iteration 325, loss = 0.45189395\n",
      "Iteration 326, loss = 0.45138860\n",
      "Iteration 327, loss = 0.45088463\n",
      "Iteration 328, loss = 0.45038202\n",
      "Iteration 329, loss = 0.44988076\n",
      "Iteration 330, loss = 0.44938084\n",
      "Iteration 331, loss = 0.44888223\n",
      "Iteration 332, loss = 0.44838493\n",
      "Iteration 333, loss = 0.44788892\n",
      "Iteration 334, loss = 0.44739418\n",
      "Iteration 335, loss = 0.44690096\n",
      "Iteration 336, loss = 0.44640926\n",
      "Iteration 337, loss = 0.44591881\n",
      "Iteration 338, loss = 0.44542962\n",
      "Iteration 339, loss = 0.44494166\n",
      "Iteration 340, loss = 0.44445492\n",
      "Iteration 341, loss = 0.44396938\n",
      "Iteration 342, loss = 0.44348503\n",
      "Iteration 343, loss = 0.44300185\n",
      "Iteration 344, loss = 0.44251987\n",
      "Iteration 345, loss = 0.44203941\n",
      "Iteration 346, loss = 0.44156012\n",
      "Iteration 347, loss = 0.44108196\n",
      "Iteration 348, loss = 0.44060494\n",
      "Iteration 349, loss = 0.44012903\n",
      "Iteration 350, loss = 0.43965423\n",
      "Iteration 351, loss = 0.43918053\n",
      "Iteration 352, loss = 0.43870791\n",
      "Iteration 353, loss = 0.43823636\n",
      "Iteration 354, loss = 0.43776587\n",
      "Iteration 355, loss = 0.43729643\n",
      "Iteration 356, loss = 0.43682804\n",
      "Iteration 357, loss = 0.43636068\n",
      "Iteration 358, loss = 0.43589433\n",
      "Iteration 359, loss = 0.43542900\n",
      "Iteration 360, loss = 0.43496467\n",
      "Iteration 361, loss = 0.43450144\n",
      "Iteration 362, loss = 0.43403934\n",
      "Iteration 363, loss = 0.43357822\n",
      "Iteration 364, loss = 0.43311808\n",
      "Iteration 365, loss = 0.43265891\n",
      "Iteration 366, loss = 0.43220071\n",
      "Iteration 367, loss = 0.43174345\n",
      "Iteration 368, loss = 0.43128713\n",
      "Iteration 369, loss = 0.43083175\n",
      "Iteration 370, loss = 0.43037729\n",
      "Iteration 371, loss = 0.42992375\n",
      "Iteration 372, loss = 0.42947111\n",
      "Iteration 373, loss = 0.42901937\n",
      "Iteration 374, loss = 0.42856852\n",
      "Iteration 375, loss = 0.42811856\n",
      "Iteration 376, loss = 0.42766947\n",
      "Iteration 377, loss = 0.42722124\n",
      "Iteration 378, loss = 0.42677388\n",
      "Iteration 379, loss = 0.42632736\n",
      "Iteration 380, loss = 0.42588169\n",
      "Iteration 381, loss = 0.42543686\n",
      "Iteration 382, loss = 0.42499286\n",
      "Iteration 383, loss = 0.42454968\n",
      "Iteration 384, loss = 0.42410732\n",
      "Iteration 385, loss = 0.42366577\n",
      "Iteration 386, loss = 0.42322502\n",
      "Iteration 387, loss = 0.42278506\n",
      "Iteration 388, loss = 0.42234590\n",
      "Iteration 389, loss = 0.42190752\n",
      "Iteration 390, loss = 0.42146991\n",
      "Iteration 391, loss = 0.42103308\n",
      "Iteration 392, loss = 0.42059733\n",
      "Iteration 393, loss = 0.42016285\n",
      "Iteration 394, loss = 0.41972917\n",
      "Iteration 395, loss = 0.41929628\n",
      "Iteration 396, loss = 0.41886417\n",
      "Iteration 397, loss = 0.41843284\n",
      "Iteration 398, loss = 0.41800227\n",
      "Iteration 399, loss = 0.41757247\n",
      "Iteration 400, loss = 0.41714342\n",
      "Iteration 401, loss = 0.41671584\n",
      "Iteration 402, loss = 0.41628999\n",
      "Iteration 403, loss = 0.41586490\n",
      "Iteration 404, loss = 0.41544058\n",
      "Iteration 405, loss = 0.41501703\n",
      "Iteration 406, loss = 0.41459426\n",
      "Iteration 407, loss = 0.41417226\n",
      "Iteration 408, loss = 0.41375105\n",
      "Iteration 409, loss = 0.41333061\n",
      "Iteration 410, loss = 0.41291093\n",
      "Iteration 411, loss = 0.41249201\n",
      "Iteration 412, loss = 0.41207385\n",
      "Iteration 413, loss = 0.41165643\n",
      "Iteration 414, loss = 0.41123975\n",
      "Iteration 415, loss = 0.41082379\n",
      "Iteration 416, loss = 0.41040856\n",
      "Iteration 417, loss = 0.40999404\n",
      "Iteration 418, loss = 0.40958023\n",
      "Iteration 419, loss = 0.40916711\n",
      "Iteration 420, loss = 0.40875469\n",
      "Iteration 421, loss = 0.40834296\n",
      "Iteration 422, loss = 0.40793190\n",
      "Iteration 423, loss = 0.40752152\n",
      "Iteration 424, loss = 0.40711180\n",
      "Iteration 425, loss = 0.40670273\n",
      "Iteration 426, loss = 0.40629431\n",
      "Iteration 427, loss = 0.40588654\n",
      "Iteration 428, loss = 0.40547940\n",
      "Iteration 429, loss = 0.40507289\n",
      "Iteration 430, loss = 0.40466700\n",
      "Iteration 431, loss = 0.40426172\n",
      "Iteration 432, loss = 0.40385706\n",
      "Iteration 433, loss = 0.40345300\n",
      "Iteration 434, loss = 0.40304954\n",
      "Iteration 435, loss = 0.40264667\n",
      "Iteration 436, loss = 0.40224440\n",
      "Iteration 437, loss = 0.40184270\n",
      "Iteration 438, loss = 0.40144159\n",
      "Iteration 439, loss = 0.40104105\n",
      "Iteration 440, loss = 0.40064108\n",
      "Iteration 441, loss = 0.40024168\n",
      "Iteration 442, loss = 0.39984284\n",
      "Iteration 443, loss = 0.39944455\n",
      "Iteration 444, loss = 0.39904682\n",
      "Iteration 445, loss = 0.39864965\n",
      "Iteration 446, loss = 0.39825301\n",
      "Iteration 447, loss = 0.39785693\n",
      "Iteration 448, loss = 0.39746137\n",
      "Iteration 449, loss = 0.39706636\n",
      "Iteration 450, loss = 0.39667188\n",
      "Iteration 451, loss = 0.39627792\n",
      "Iteration 452, loss = 0.39588449\n",
      "Iteration 453, loss = 0.39549158\n",
      "Iteration 454, loss = 0.39509919\n",
      "Iteration 455, loss = 0.39470731\n",
      "Iteration 456, loss = 0.39431595\n",
      "Iteration 457, loss = 0.39392509\n",
      "Iteration 458, loss = 0.39353474\n",
      "Iteration 459, loss = 0.39314489\n",
      "Iteration 460, loss = 0.39275554\n",
      "Iteration 461, loss = 0.39236668\n",
      "Iteration 462, loss = 0.39197832\n",
      "Iteration 463, loss = 0.39159044\n",
      "Iteration 464, loss = 0.39120305\n",
      "Iteration 465, loss = 0.39081615\n",
      "Iteration 466, loss = 0.39042973\n",
      "Iteration 467, loss = 0.39004378\n",
      "Iteration 468, loss = 0.38965831\n",
      "Iteration 469, loss = 0.38927332\n",
      "Iteration 470, loss = 0.38888879\n",
      "Iteration 471, loss = 0.38850473\n",
      "Iteration 472, loss = 0.38812129\n",
      "Iteration 473, loss = 0.38773867\n",
      "Iteration 474, loss = 0.38735653\n",
      "Iteration 475, loss = 0.38697488\n",
      "Iteration 476, loss = 0.38659371\n",
      "Iteration 477, loss = 0.38621301\n",
      "Iteration 478, loss = 0.38583279\n",
      "Iteration 479, loss = 0.38545303\n",
      "Iteration 480, loss = 0.38507374\n",
      "Iteration 481, loss = 0.38469492\n",
      "Iteration 482, loss = 0.38431654\n",
      "Iteration 483, loss = 0.38393863\n",
      "Iteration 484, loss = 0.38356116\n",
      "Iteration 485, loss = 0.38318414\n",
      "Iteration 486, loss = 0.38280757\n",
      "Iteration 487, loss = 0.38243143\n",
      "Iteration 488, loss = 0.38205573\n",
      "Iteration 489, loss = 0.38168047\n",
      "Iteration 490, loss = 0.38130564\n",
      "Iteration 491, loss = 0.38093124\n",
      "Iteration 492, loss = 0.38055727\n",
      "Iteration 493, loss = 0.38018372\n",
      "Iteration 494, loss = 0.37981060\n",
      "Iteration 495, loss = 0.37943789\n",
      "Iteration 496, loss = 0.37906560\n",
      "Iteration 497, loss = 0.37869373\n",
      "Iteration 498, loss = 0.37832227\n",
      "Iteration 499, loss = 0.37795122\n",
      "Iteration 500, loss = 0.37758058\n",
      "Iteration 501, loss = 0.37721070\n",
      "Iteration 502, loss = 0.37684158\n",
      "Iteration 503, loss = 0.37647290\n",
      "Iteration 504, loss = 0.37610466\n",
      "Iteration 505, loss = 0.37573685\n",
      "Iteration 506, loss = 0.37536948\n",
      "Iteration 507, loss = 0.37500254\n",
      "Iteration 508, loss = 0.37463602\n",
      "Iteration 509, loss = 0.37426992\n",
      "Iteration 510, loss = 0.37390424\n",
      "Iteration 511, loss = 0.37353898\n",
      "Iteration 512, loss = 0.37317412\n",
      "Iteration 513, loss = 0.37280968\n",
      "Iteration 514, loss = 0.37244564\n",
      "Iteration 515, loss = 0.37208199\n",
      "Iteration 516, loss = 0.37171875\n",
      "Iteration 517, loss = 0.37135598\n",
      "Iteration 518, loss = 0.37099399\n",
      "Iteration 519, loss = 0.37063241\n",
      "Iteration 520, loss = 0.37027125\n",
      "Iteration 521, loss = 0.36991050\n",
      "Iteration 522, loss = 0.36955015\n",
      "Iteration 523, loss = 0.36919022\n",
      "Iteration 524, loss = 0.36883068\n",
      "Iteration 525, loss = 0.36847154\n",
      "Iteration 526, loss = 0.36811279\n",
      "Iteration 527, loss = 0.36775444\n",
      "Iteration 528, loss = 0.36739648\n",
      "Iteration 529, loss = 0.36703890\n",
      "Iteration 530, loss = 0.36668171\n",
      "Iteration 531, loss = 0.36632490\n",
      "Iteration 532, loss = 0.36596847\n",
      "Iteration 533, loss = 0.36561241\n",
      "Iteration 534, loss = 0.36525673\n",
      "Iteration 535, loss = 0.36490142\n",
      "Iteration 536, loss = 0.36454648\n",
      "Iteration 537, loss = 0.36419191\n",
      "Iteration 538, loss = 0.36383770\n",
      "Iteration 539, loss = 0.36348386\n",
      "Iteration 540, loss = 0.36313038\n",
      "Iteration 541, loss = 0.36277726\n",
      "Iteration 542, loss = 0.36242450\n",
      "Iteration 543, loss = 0.36207209\n",
      "Iteration 544, loss = 0.36172004\n",
      "Iteration 545, loss = 0.36136834\n",
      "Iteration 546, loss = 0.36101700\n",
      "Iteration 547, loss = 0.36066601\n",
      "Iteration 548, loss = 0.36031536\n",
      "Iteration 549, loss = 0.35996507\n",
      "Iteration 550, loss = 0.35961512\n",
      "Iteration 551, loss = 0.35926552\n",
      "Iteration 552, loss = 0.35891626\n",
      "Iteration 553, loss = 0.35856734\n",
      "Iteration 554, loss = 0.35821877\n",
      "Iteration 555, loss = 0.35787054\n",
      "Iteration 556, loss = 0.35752265\n",
      "Iteration 557, loss = 0.35717509\n",
      "Iteration 558, loss = 0.35682788\n",
      "Iteration 559, loss = 0.35648100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 560, loss = 0.35613446\n",
      "Iteration 561, loss = 0.35578826\n",
      "Iteration 562, loss = 0.35544247\n",
      "Iteration 563, loss = 0.35509746\n",
      "Iteration 564, loss = 0.35475281\n",
      "Iteration 565, loss = 0.35440852\n",
      "Iteration 566, loss = 0.35406458\n",
      "Iteration 567, loss = 0.35372101\n",
      "Iteration 568, loss = 0.35337778\n",
      "Iteration 569, loss = 0.35303491\n",
      "Iteration 570, loss = 0.35269239\n",
      "Iteration 571, loss = 0.35235021\n",
      "Iteration 572, loss = 0.35200838\n",
      "Iteration 573, loss = 0.35166689\n",
      "Iteration 574, loss = 0.35132574\n",
      "Iteration 575, loss = 0.35098493\n",
      "Iteration 576, loss = 0.35064446\n",
      "Iteration 577, loss = 0.35030433\n",
      "Iteration 578, loss = 0.34996453\n",
      "Iteration 579, loss = 0.34962506\n",
      "Iteration 580, loss = 0.34928592\n",
      "Iteration 581, loss = 0.34894712\n",
      "Iteration 582, loss = 0.34860865\n",
      "Iteration 583, loss = 0.34827050\n",
      "Iteration 584, loss = 0.34793269\n",
      "Iteration 585, loss = 0.34759520\n",
      "Iteration 586, loss = 0.34725803\n",
      "Iteration 587, loss = 0.34692119\n",
      "Iteration 588, loss = 0.34658468\n",
      "Iteration 589, loss = 0.34624849\n",
      "Iteration 590, loss = 0.34591262\n",
      "Iteration 591, loss = 0.34557707\n",
      "Iteration 592, loss = 0.34524184\n",
      "Iteration 593, loss = 0.34490693\n",
      "Iteration 594, loss = 0.34457234\n",
      "Iteration 595, loss = 0.34423807\n",
      "Iteration 596, loss = 0.34390412\n",
      "Iteration 597, loss = 0.34357049\n",
      "Iteration 598, loss = 0.34323717\n",
      "Iteration 599, loss = 0.34290417\n",
      "Iteration 600, loss = 0.34257148\n",
      "Iteration 601, loss = 0.34223911\n",
      "Iteration 602, loss = 0.34190705\n",
      "Iteration 603, loss = 0.34157531\n",
      "Iteration 604, loss = 0.34124388\n",
      "Iteration 605, loss = 0.34091277\n",
      "Iteration 606, loss = 0.34058197\n",
      "Iteration 607, loss = 0.34025147\n",
      "Iteration 608, loss = 0.33992130\n",
      "Iteration 609, loss = 0.33959143\n",
      "Iteration 610, loss = 0.33926187\n",
      "Iteration 611, loss = 0.33893263\n",
      "Iteration 612, loss = 0.33860370\n",
      "Iteration 613, loss = 0.33827507\n",
      "Iteration 614, loss = 0.33794676\n",
      "Iteration 615, loss = 0.33761875\n",
      "Iteration 616, loss = 0.33729106\n",
      "Iteration 617, loss = 0.33696367\n",
      "Iteration 618, loss = 0.33663659\n",
      "Iteration 619, loss = 0.33630983\n",
      "Iteration 620, loss = 0.33598336\n",
      "Iteration 621, loss = 0.33565721\n",
      "Iteration 622, loss = 0.33533137\n",
      "Iteration 623, loss = 0.33500583\n",
      "Iteration 624, loss = 0.33468060\n",
      "Iteration 625, loss = 0.33435567\n",
      "Iteration 626, loss = 0.33403106\n",
      "Iteration 627, loss = 0.33370675\n",
      "Iteration 628, loss = 0.33338274\n",
      "Iteration 629, loss = 0.33305905\n",
      "Iteration 630, loss = 0.33273565\n",
      "Iteration 631, loss = 0.33241257\n",
      "Iteration 632, loss = 0.33208979\n",
      "Iteration 633, loss = 0.33176731\n",
      "Iteration 634, loss = 0.33144515\n",
      "Iteration 635, loss = 0.33112328\n",
      "Iteration 636, loss = 0.33080172\n",
      "Iteration 637, loss = 0.33048047\n",
      "Iteration 638, loss = 0.33015952\n",
      "Iteration 639, loss = 0.32983888\n",
      "Iteration 640, loss = 0.32951854\n",
      "Iteration 641, loss = 0.32919855\n",
      "Iteration 642, loss = 0.32887920\n",
      "Iteration 643, loss = 0.32856018\n",
      "Iteration 644, loss = 0.32824148\n",
      "Iteration 645, loss = 0.32792338\n",
      "Iteration 646, loss = 0.32760579\n",
      "Iteration 647, loss = 0.32728854\n",
      "Iteration 648, loss = 0.32697163\n",
      "Iteration 649, loss = 0.32665506\n",
      "Iteration 650, loss = 0.32633882\n",
      "Iteration 651, loss = 0.32602292\n",
      "Iteration 652, loss = 0.32570735\n",
      "Iteration 653, loss = 0.32539211\n",
      "Iteration 654, loss = 0.32507719\n",
      "Iteration 655, loss = 0.32476260\n",
      "Iteration 656, loss = 0.32444833\n",
      "Iteration 657, loss = 0.32413438\n",
      "Iteration 658, loss = 0.32382075\n",
      "Iteration 659, loss = 0.32350744\n",
      "Iteration 660, loss = 0.32319445\n",
      "Iteration 661, loss = 0.32288177\n",
      "Iteration 662, loss = 0.32256941\n",
      "Iteration 663, loss = 0.32225737\n",
      "Iteration 664, loss = 0.32194564\n",
      "Iteration 665, loss = 0.32163423\n",
      "Iteration 666, loss = 0.32132313\n",
      "Iteration 667, loss = 0.32101234\n",
      "Iteration 668, loss = 0.32070186\n",
      "Iteration 669, loss = 0.32039170\n",
      "Iteration 670, loss = 0.32008184\n",
      "Iteration 671, loss = 0.31977230\n",
      "Iteration 672, loss = 0.31946306\n",
      "Iteration 673, loss = 0.31915414\n",
      "Iteration 674, loss = 0.31884576\n",
      "Iteration 675, loss = 0.31853784\n",
      "Iteration 676, loss = 0.31823025\n",
      "Iteration 677, loss = 0.31792298\n",
      "Iteration 678, loss = 0.31761604\n",
      "Iteration 679, loss = 0.31730943\n",
      "Iteration 680, loss = 0.31700315\n",
      "Iteration 681, loss = 0.31669719\n",
      "Iteration 682, loss = 0.31639155\n",
      "Iteration 683, loss = 0.31608623\n",
      "Iteration 684, loss = 0.31578124\n",
      "Iteration 685, loss = 0.31547656\n",
      "Iteration 686, loss = 0.31517220\n",
      "Iteration 687, loss = 0.31486815\n",
      "Iteration 688, loss = 0.31456442\n",
      "Iteration 689, loss = 0.31426101\n",
      "Iteration 690, loss = 0.31395791\n",
      "Iteration 691, loss = 0.31365512\n",
      "Iteration 692, loss = 0.31335265\n",
      "Iteration 693, loss = 0.31305048\n",
      "Iteration 694, loss = 0.31274863\n",
      "Iteration 695, loss = 0.31244710\n",
      "Iteration 696, loss = 0.31214587\n",
      "Iteration 697, loss = 0.31184495\n",
      "Iteration 698, loss = 0.31154435\n",
      "Iteration 699, loss = 0.31124405\n",
      "Iteration 700, loss = 0.31094407\n",
      "Iteration 701, loss = 0.31064439\n",
      "Iteration 702, loss = 0.31034503\n",
      "Iteration 703, loss = 0.31004597\n",
      "Iteration 704, loss = 0.30974722\n",
      "Iteration 705, loss = 0.30944878\n",
      "Iteration 706, loss = 0.30915065\n",
      "Iteration 707, loss = 0.30885282\n",
      "Iteration 708, loss = 0.30855531\n",
      "Iteration 709, loss = 0.30825810\n",
      "Iteration 710, loss = 0.30796120\n",
      "Iteration 711, loss = 0.30766461\n",
      "Iteration 712, loss = 0.30736832\n",
      "Iteration 713, loss = 0.30707234\n",
      "Iteration 714, loss = 0.30677667\n",
      "Iteration 715, loss = 0.30648131\n",
      "Iteration 716, loss = 0.30618626\n",
      "Iteration 717, loss = 0.30589151\n",
      "Iteration 718, loss = 0.30559707\n",
      "Iteration 719, loss = 0.30530294\n",
      "Iteration 720, loss = 0.30500912\n",
      "Iteration 721, loss = 0.30471560\n",
      "Iteration 722, loss = 0.30442239\n",
      "Iteration 723, loss = 0.30412949\n",
      "Iteration 724, loss = 0.30383690\n",
      "Iteration 725, loss = 0.30354461\n",
      "Iteration 726, loss = 0.30325264\n",
      "Iteration 727, loss = 0.30296097\n",
      "Iteration 728, loss = 0.30266960\n",
      "Iteration 729, loss = 0.30237855\n",
      "Iteration 730, loss = 0.30208780\n",
      "Iteration 731, loss = 0.30179736\n",
      "Iteration 732, loss = 0.30150723\n",
      "Iteration 733, loss = 0.30121741\n",
      "Iteration 734, loss = 0.30092790\n",
      "Iteration 735, loss = 0.30063869\n",
      "Iteration 736, loss = 0.30034979\n",
      "Iteration 737, loss = 0.30006120\n",
      "Iteration 738, loss = 0.29977292\n",
      "Iteration 739, loss = 0.29948495\n",
      "Iteration 740, loss = 0.29919729\n",
      "Iteration 741, loss = 0.29890993\n",
      "Iteration 742, loss = 0.29862289\n",
      "Iteration 743, loss = 0.29833615\n",
      "Iteration 744, loss = 0.29804972\n",
      "Iteration 745, loss = 0.29776360\n",
      "Iteration 746, loss = 0.29747779\n",
      "Iteration 747, loss = 0.29719229\n",
      "Iteration 748, loss = 0.29690709\n",
      "Iteration 749, loss = 0.29662221\n",
      "Iteration 750, loss = 0.29633764\n",
      "Iteration 751, loss = 0.29605337\n",
      "Iteration 752, loss = 0.29576942\n",
      "Iteration 753, loss = 0.29548577\n",
      "Iteration 754, loss = 0.29520244\n",
      "Iteration 755, loss = 0.29491941\n",
      "Iteration 756, loss = 0.29463669\n",
      "Iteration 757, loss = 0.29435429\n",
      "Iteration 758, loss = 0.29407219\n",
      "Iteration 759, loss = 0.29379040\n",
      "Iteration 760, loss = 0.29350893\n",
      "Iteration 761, loss = 0.29322776\n",
      "Iteration 762, loss = 0.29294691\n",
      "Iteration 763, loss = 0.29266636\n",
      "Iteration 764, loss = 0.29238613\n",
      "Iteration 765, loss = 0.29210620\n",
      "Iteration 766, loss = 0.29182659\n",
      "Iteration 767, loss = 0.29154729\n",
      "Iteration 768, loss = 0.29126830\n",
      "Iteration 769, loss = 0.29098962\n",
      "Iteration 770, loss = 0.29071125\n",
      "Iteration 771, loss = 0.29043319\n",
      "Iteration 772, loss = 0.29015544\n",
      "Iteration 773, loss = 0.28987801\n",
      "Iteration 774, loss = 0.28960088\n",
      "Iteration 775, loss = 0.28932407\n",
      "Iteration 776, loss = 0.28904757\n",
      "Iteration 777, loss = 0.28877138\n",
      "Iteration 778, loss = 0.28849550\n",
      "Iteration 779, loss = 0.28821994\n",
      "Iteration 780, loss = 0.28794469\n",
      "Iteration 781, loss = 0.28766974\n",
      "Iteration 782, loss = 0.28739511\n",
      "Iteration 783, loss = 0.28712080\n",
      "Iteration 784, loss = 0.28684679\n",
      "Iteration 785, loss = 0.28657310\n",
      "Iteration 786, loss = 0.28629972\n",
      "Iteration 787, loss = 0.28602665\n",
      "Iteration 788, loss = 0.28575390\n",
      "Iteration 789, loss = 0.28548146\n",
      "Iteration 790, loss = 0.28520933\n",
      "Iteration 791, loss = 0.28493751\n",
      "Iteration 792, loss = 0.28466601\n",
      "Iteration 793, loss = 0.28439482\n",
      "Iteration 794, loss = 0.28412394\n",
      "Iteration 795, loss = 0.28385338\n",
      "Iteration 796, loss = 0.28358313\n",
      "Iteration 797, loss = 0.28331319\n",
      "Iteration 798, loss = 0.28304357\n",
      "Iteration 799, loss = 0.28277426\n",
      "Iteration 800, loss = 0.28250526\n",
      "Iteration 801, loss = 0.28223658\n",
      "Iteration 802, loss = 0.28196821\n",
      "Iteration 803, loss = 0.28170015\n",
      "Iteration 804, loss = 0.28143241\n",
      "Iteration 805, loss = 0.28116499\n",
      "Iteration 806, loss = 0.28089787\n",
      "Iteration 807, loss = 0.28063107\n",
      "Iteration 808, loss = 0.28036459\n",
      "Iteration 809, loss = 0.28009842\n",
      "Iteration 810, loss = 0.27983256\n",
      "Iteration 811, loss = 0.27956702\n",
      "Iteration 812, loss = 0.27930179\n",
      "Iteration 813, loss = 0.27903688\n",
      "Iteration 814, loss = 0.27877228\n",
      "Iteration 815, loss = 0.27850800\n",
      "Iteration 816, loss = 0.27824403\n",
      "Iteration 817, loss = 0.27798066\n",
      "Iteration 818, loss = 0.27771775\n",
      "Iteration 819, loss = 0.27745517\n",
      "Iteration 820, loss = 0.27719292\n",
      "Iteration 821, loss = 0.27693101\n",
      "Iteration 822, loss = 0.27666943\n",
      "Iteration 823, loss = 0.27640818\n",
      "Iteration 824, loss = 0.27614726\n",
      "Iteration 825, loss = 0.27588667\n",
      "Iteration 826, loss = 0.27562640\n",
      "Iteration 827, loss = 0.27536646\n",
      "Iteration 828, loss = 0.27510685\n",
      "Iteration 829, loss = 0.27484756\n",
      "Iteration 830, loss = 0.27458859\n",
      "Iteration 831, loss = 0.27432994\n",
      "Iteration 832, loss = 0.27407161\n",
      "Iteration 833, loss = 0.27381360\n",
      "Iteration 834, loss = 0.27355592\n",
      "Iteration 835, loss = 0.27329855\n",
      "Iteration 836, loss = 0.27304151\n",
      "Iteration 837, loss = 0.27278478\n",
      "Iteration 838, loss = 0.27252838\n",
      "Iteration 839, loss = 0.27227229\n",
      "Iteration 840, loss = 0.27201652\n",
      "Iteration 841, loss = 0.27176107\n",
      "Iteration 842, loss = 0.27150594\n",
      "Iteration 843, loss = 0.27125112\n",
      "Iteration 844, loss = 0.27099662\n",
      "Iteration 845, loss = 0.27074244\n",
      "Iteration 846, loss = 0.27048858\n",
      "Iteration 847, loss = 0.27023503\n",
      "Iteration 848, loss = 0.26998180\n",
      "Iteration 849, loss = 0.26972889\n",
      "Iteration 850, loss = 0.26947629\n",
      "Iteration 851, loss = 0.26922401\n",
      "Iteration 852, loss = 0.26897204\n",
      "Iteration 853, loss = 0.26872039\n",
      "Iteration 854, loss = 0.26846905\n",
      "Iteration 855, loss = 0.26821804\n",
      "Iteration 856, loss = 0.26796733\n",
      "Iteration 857, loss = 0.26771694\n",
      "Iteration 858, loss = 0.26746687\n",
      "Iteration 859, loss = 0.26721711\n",
      "Iteration 860, loss = 0.26696767\n",
      "Iteration 861, loss = 0.26671855\n",
      "Iteration 862, loss = 0.26646973\n",
      "Iteration 863, loss = 0.26622124\n",
      "Iteration 864, loss = 0.26597306\n",
      "Iteration 865, loss = 0.26572519\n",
      "Iteration 866, loss = 0.26547765\n",
      "Iteration 867, loss = 0.26523051\n",
      "Iteration 868, loss = 0.26498369\n",
      "Iteration 869, loss = 0.26473719\n",
      "Iteration 870, loss = 0.26449101\n",
      "Iteration 871, loss = 0.26424515\n",
      "Iteration 872, loss = 0.26399961\n",
      "Iteration 873, loss = 0.26375438\n",
      "Iteration 874, loss = 0.26350948\n",
      "Iteration 875, loss = 0.26326489\n",
      "Iteration 876, loss = 0.26302062\n",
      "Iteration 877, loss = 0.26277666\n",
      "Iteration 878, loss = 0.26253302\n",
      "Iteration 879, loss = 0.26228970\n",
      "Iteration 880, loss = 0.26204669\n",
      "Iteration 881, loss = 0.26180400\n",
      "Iteration 882, loss = 0.26156162\n",
      "Iteration 883, loss = 0.26131956\n",
      "Iteration 884, loss = 0.26107781\n",
      "Iteration 885, loss = 0.26083638\n",
      "Iteration 886, loss = 0.26059526\n",
      "Iteration 887, loss = 0.26035446\n",
      "Iteration 888, loss = 0.26011397\n",
      "Iteration 889, loss = 0.25987380\n",
      "Iteration 890, loss = 0.25963394\n",
      "Iteration 891, loss = 0.25939439\n",
      "Iteration 892, loss = 0.25915516\n",
      "Iteration 893, loss = 0.25891624\n",
      "Iteration 894, loss = 0.25867763\n",
      "Iteration 895, loss = 0.25843934\n",
      "Iteration 896, loss = 0.25820136\n",
      "Iteration 897, loss = 0.25796370\n",
      "Iteration 898, loss = 0.25772634\n",
      "Iteration 899, loss = 0.25748930\n",
      "Iteration 900, loss = 0.25725258\n",
      "Iteration 901, loss = 0.25701616\n",
      "Iteration 902, loss = 0.25678006\n",
      "Iteration 903, loss = 0.25654427\n",
      "Iteration 904, loss = 0.25630880\n",
      "Iteration 905, loss = 0.25607363\n",
      "Iteration 906, loss = 0.25583878\n",
      "Iteration 907, loss = 0.25560424\n",
      "Iteration 908, loss = 0.25537001\n",
      "Iteration 909, loss = 0.25513609\n",
      "Iteration 910, loss = 0.25490249\n",
      "Iteration 911, loss = 0.25466919\n",
      "Iteration 912, loss = 0.25443621\n",
      "Iteration 913, loss = 0.25420354\n",
      "Iteration 914, loss = 0.25397118\n",
      "Iteration 915, loss = 0.25373913\n",
      "Iteration 916, loss = 0.25350740\n",
      "Iteration 917, loss = 0.25327597\n",
      "Iteration 918, loss = 0.25304486\n",
      "Iteration 919, loss = 0.25281405\n",
      "Iteration 920, loss = 0.25258356\n",
      "Iteration 921, loss = 0.25235337\n",
      "Iteration 922, loss = 0.25212350\n",
      "Iteration 923, loss = 0.25189394\n",
      "Iteration 924, loss = 0.25166468\n",
      "Iteration 925, loss = 0.25143574\n",
      "Iteration 926, loss = 0.25120711\n",
      "Iteration 927, loss = 0.25097878\n",
      "Iteration 928, loss = 0.25075077\n",
      "Iteration 929, loss = 0.25052307\n",
      "Iteration 930, loss = 0.25029567\n",
      "Iteration 931, loss = 0.25006858\n",
      "Iteration 932, loss = 0.24984181\n",
      "Iteration 933, loss = 0.24961534\n",
      "Iteration 934, loss = 0.24938918\n",
      "Iteration 935, loss = 0.24916333\n",
      "Iteration 936, loss = 0.24893779\n",
      "Iteration 937, loss = 0.24871255\n",
      "Iteration 938, loss = 0.24848763\n",
      "Iteration 939, loss = 0.24826301\n",
      "Iteration 940, loss = 0.24803870\n",
      "Iteration 941, loss = 0.24781470\n",
      "Iteration 942, loss = 0.24759101\n",
      "Iteration 943, loss = 0.24736762\n",
      "Iteration 944, loss = 0.24714454\n",
      "Iteration 945, loss = 0.24692177\n",
      "Iteration 946, loss = 0.24669930\n",
      "Iteration 947, loss = 0.24647715\n",
      "Iteration 948, loss = 0.24625530\n",
      "Iteration 949, loss = 0.24603375\n",
      "Iteration 950, loss = 0.24581251\n",
      "Iteration 951, loss = 0.24559158\n",
      "Iteration 952, loss = 0.24537096\n",
      "Iteration 953, loss = 0.24515064\n",
      "Iteration 954, loss = 0.24493062\n",
      "Iteration 955, loss = 0.24471092\n",
      "Iteration 956, loss = 0.24449151\n",
      "Iteration 957, loss = 0.24427242\n",
      "Iteration 958, loss = 0.24405362\n",
      "Iteration 959, loss = 0.24383514\n",
      "Iteration 960, loss = 0.24361695\n",
      "Iteration 961, loss = 0.24339908\n",
      "Iteration 962, loss = 0.24318150\n",
      "Iteration 963, loss = 0.24296423\n",
      "Iteration 964, loss = 0.24274727\n",
      "Iteration 965, loss = 0.24253061\n",
      "Iteration 966, loss = 0.24231425\n",
      "Iteration 967, loss = 0.24209820\n",
      "Iteration 968, loss = 0.24188245\n",
      "Iteration 969, loss = 0.24166700\n",
      "Iteration 970, loss = 0.24145186\n",
      "Iteration 971, loss = 0.24123702\n",
      "Iteration 972, loss = 0.24102248\n",
      "Iteration 973, loss = 0.24080824\n",
      "Iteration 974, loss = 0.24059431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 975, loss = 0.24038068\n",
      "Iteration 976, loss = 0.24016735\n",
      "Iteration 977, loss = 0.23995432\n",
      "Iteration 978, loss = 0.23974159\n",
      "Iteration 979, loss = 0.23952917\n",
      "Iteration 980, loss = 0.23931704\n",
      "Iteration 981, loss = 0.23910522\n",
      "Iteration 982, loss = 0.23889370\n",
      "Iteration 983, loss = 0.23868248\n",
      "Iteration 984, loss = 0.23847156\n",
      "Iteration 985, loss = 0.23826094\n",
      "Iteration 986, loss = 0.23805061\n",
      "Iteration 987, loss = 0.23784059\n",
      "Iteration 988, loss = 0.23763087\n",
      "Iteration 989, loss = 0.23742145\n",
      "Iteration 990, loss = 0.23721232\n",
      "Iteration 991, loss = 0.23700350\n",
      "Iteration 992, loss = 0.23679497\n",
      "Iteration 993, loss = 0.23658674\n",
      "Iteration 994, loss = 0.23637882\n",
      "Iteration 995, loss = 0.23617118\n",
      "Iteration 996, loss = 0.23596385\n",
      "Iteration 997, loss = 0.23575681\n",
      "Iteration 998, loss = 0.23555008\n",
      "Iteration 999, loss = 0.23534363\n",
      "Iteration 1000, loss = 0.23513749\n",
      "Iteration 1, loss = 1.51234105\n",
      "Iteration 2, loss = 1.48154357\n",
      "Iteration 3, loss = 1.44021605\n",
      "Iteration 4, loss = 1.39226647\n",
      "Iteration 5, loss = 1.34100415\n",
      "Iteration 6, loss = 1.28904131\n",
      "Iteration 7, loss = 1.23903556\n",
      "Iteration 8, loss = 1.19222058\n",
      "Iteration 9, loss = 1.14880506\n",
      "Iteration 10, loss = 1.10884896\n",
      "Iteration 11, loss = 1.07252749\n",
      "Iteration 12, loss = 1.03894922\n",
      "Iteration 13, loss = 1.00758469\n",
      "Iteration 14, loss = 0.97821066\n",
      "Iteration 15, loss = 0.95079990\n",
      "Iteration 16, loss = 0.92535869\n",
      "Iteration 17, loss = 0.90190742\n",
      "Iteration 18, loss = 0.88047345\n",
      "Iteration 19, loss = 0.86100281\n",
      "Iteration 20, loss = 0.84352315\n",
      "Iteration 21, loss = 0.82799323\n",
      "Iteration 22, loss = 0.81457790\n",
      "Iteration 23, loss = 0.80336540\n",
      "Iteration 24, loss = 0.79441574\n",
      "Iteration 25, loss = 0.78756770\n",
      "Iteration 26, loss = 0.78226879\n",
      "Iteration 27, loss = 0.77835164\n",
      "Iteration 28, loss = 0.77550014\n",
      "Iteration 29, loss = 0.77367440\n",
      "Iteration 30, loss = 0.77246195\n",
      "Iteration 31, loss = 0.77157805\n",
      "Iteration 32, loss = 0.77088116\n",
      "Iteration 33, loss = 0.77015624\n",
      "Iteration 34, loss = 0.76926108\n",
      "Iteration 35, loss = 0.76814988\n",
      "Iteration 36, loss = 0.76683118\n",
      "Iteration 37, loss = 0.76531311\n",
      "Iteration 38, loss = 0.76358111\n",
      "Iteration 39, loss = 0.76161263\n",
      "Iteration 40, loss = 0.75943127\n",
      "Iteration 41, loss = 0.75707138\n",
      "Iteration 42, loss = 0.75458985\n",
      "Iteration 43, loss = 0.75200237\n",
      "Iteration 44, loss = 0.74936968\n",
      "Iteration 45, loss = 0.74673688\n",
      "Iteration 46, loss = 0.74412075\n",
      "Iteration 47, loss = 0.74157629\n",
      "Iteration 48, loss = 0.73916039\n",
      "Iteration 49, loss = 0.73686463\n",
      "Iteration 50, loss = 0.73464540\n",
      "Iteration 51, loss = 0.73248787\n",
      "Iteration 52, loss = 0.73042447\n",
      "Iteration 53, loss = 0.72843079\n",
      "Iteration 54, loss = 0.72649417\n",
      "Iteration 55, loss = 0.72461020\n",
      "Iteration 56, loss = 0.72275151\n",
      "Iteration 57, loss = 0.72092657\n",
      "Iteration 58, loss = 0.71912891\n",
      "Iteration 59, loss = 0.71734983\n",
      "Iteration 60, loss = 0.71558158\n",
      "Iteration 61, loss = 0.71381783\n",
      "Iteration 62, loss = 0.71205692\n",
      "Iteration 63, loss = 0.71029772\n",
      "Iteration 64, loss = 0.70853960\n",
      "Iteration 65, loss = 0.70678232\n",
      "Iteration 66, loss = 0.70502597\n",
      "Iteration 67, loss = 0.70327091\n",
      "Iteration 68, loss = 0.70151789\n",
      "Iteration 69, loss = 0.69976795\n",
      "Iteration 70, loss = 0.69802104\n",
      "Iteration 71, loss = 0.69627804\n",
      "Iteration 72, loss = 0.69453982\n",
      "Iteration 73, loss = 0.69281175\n",
      "Iteration 74, loss = 0.69109625\n",
      "Iteration 75, loss = 0.68938882\n",
      "Iteration 76, loss = 0.68768985\n",
      "Iteration 77, loss = 0.68600210\n",
      "Iteration 78, loss = 0.68433049\n",
      "Iteration 79, loss = 0.68266839\n",
      "Iteration 80, loss = 0.68101854\n",
      "Iteration 81, loss = 0.67938328\n",
      "Iteration 82, loss = 0.67775738\n",
      "Iteration 83, loss = 0.67614052\n",
      "Iteration 84, loss = 0.67453257\n",
      "Iteration 85, loss = 0.67293578\n",
      "Iteration 86, loss = 0.67134959\n",
      "Iteration 87, loss = 0.66977157\n",
      "Iteration 88, loss = 0.66820164\n",
      "Iteration 89, loss = 0.66664072\n",
      "Iteration 90, loss = 0.66508844\n",
      "Iteration 91, loss = 0.66354342\n",
      "Iteration 92, loss = 0.66200571\n",
      "Iteration 93, loss = 0.66047536\n",
      "Iteration 94, loss = 0.65895244\n",
      "Iteration 95, loss = 0.65743700\n",
      "Iteration 96, loss = 0.65592909\n",
      "Iteration 97, loss = 0.65442879\n",
      "Iteration 98, loss = 0.65293612\n",
      "Iteration 99, loss = 0.65145114\n",
      "Iteration 100, loss = 0.64997479\n",
      "Iteration 101, loss = 0.64850718\n",
      "Iteration 102, loss = 0.64704750\n",
      "Iteration 103, loss = 0.64559572\n",
      "Iteration 104, loss = 0.64415180\n",
      "Iteration 105, loss = 0.64271572\n",
      "Iteration 106, loss = 0.64128743\n",
      "Iteration 107, loss = 0.63986690\n",
      "Iteration 108, loss = 0.63845410\n",
      "Iteration 109, loss = 0.63704900\n",
      "Iteration 110, loss = 0.63565156\n",
      "Iteration 111, loss = 0.63426174\n",
      "Iteration 112, loss = 0.63287953\n",
      "Iteration 113, loss = 0.63150487\n",
      "Iteration 114, loss = 0.63013775\n",
      "Iteration 115, loss = 0.62877813\n",
      "Iteration 116, loss = 0.62742598\n",
      "Iteration 117, loss = 0.62608142\n",
      "Iteration 118, loss = 0.62474458\n",
      "Iteration 119, loss = 0.62341505\n",
      "Iteration 120, loss = 0.62209283\n",
      "Iteration 121, loss = 0.62077793\n",
      "Iteration 122, loss = 0.61947063\n",
      "Iteration 123, loss = 0.61817156\n",
      "Iteration 124, loss = 0.61687981\n",
      "Iteration 125, loss = 0.61559538\n",
      "Iteration 126, loss = 0.61431824\n",
      "Iteration 127, loss = 0.61304839\n",
      "Iteration 128, loss = 0.61178593\n",
      "Iteration 129, loss = 0.61053051\n",
      "Iteration 130, loss = 0.60928225\n",
      "Iteration 131, loss = 0.60804117\n",
      "Iteration 132, loss = 0.60680719\n",
      "Iteration 133, loss = 0.60558025\n",
      "Iteration 134, loss = 0.60436032\n",
      "Iteration 135, loss = 0.60314735\n",
      "Iteration 136, loss = 0.60194130\n",
      "Iteration 137, loss = 0.60074214\n",
      "Iteration 138, loss = 0.59954980\n",
      "Iteration 139, loss = 0.59836426\n",
      "Iteration 140, loss = 0.59718546\n",
      "Iteration 141, loss = 0.59601335\n",
      "Iteration 142, loss = 0.59484791\n",
      "Iteration 143, loss = 0.59368907\n",
      "Iteration 144, loss = 0.59253680\n",
      "Iteration 145, loss = 0.59139106\n",
      "Iteration 146, loss = 0.59025179\n",
      "Iteration 147, loss = 0.58911896\n",
      "Iteration 148, loss = 0.58799308\n",
      "Iteration 149, loss = 0.58687360\n",
      "Iteration 150, loss = 0.58576083\n",
      "Iteration 151, loss = 0.58465470\n",
      "Iteration 152, loss = 0.58355471\n",
      "Iteration 153, loss = 0.58246085\n",
      "Iteration 154, loss = 0.58137312\n",
      "Iteration 155, loss = 0.58029150\n",
      "Iteration 156, loss = 0.57921599\n",
      "Iteration 157, loss = 0.57814655\n",
      "Iteration 158, loss = 0.57708316\n",
      "Iteration 159, loss = 0.57602578\n",
      "Iteration 160, loss = 0.57497437\n",
      "Iteration 161, loss = 0.57392889\n",
      "Iteration 162, loss = 0.57288929\n",
      "Iteration 163, loss = 0.57185553\n",
      "Iteration 164, loss = 0.57082759\n",
      "Iteration 165, loss = 0.56980535\n",
      "Iteration 166, loss = 0.56878877\n",
      "Iteration 167, loss = 0.56777834\n",
      "Iteration 168, loss = 0.56677395\n",
      "Iteration 169, loss = 0.56577512\n",
      "Iteration 170, loss = 0.56478180\n",
      "Iteration 171, loss = 0.56379398\n",
      "Iteration 172, loss = 0.56281160\n",
      "Iteration 173, loss = 0.56183463\n",
      "Iteration 174, loss = 0.56086305\n",
      "Iteration 175, loss = 0.55989680\n",
      "Iteration 176, loss = 0.55893584\n",
      "Iteration 177, loss = 0.55798015\n",
      "Iteration 178, loss = 0.55702967\n",
      "Iteration 179, loss = 0.55608437\n",
      "Iteration 180, loss = 0.55514421\n",
      "Iteration 181, loss = 0.55420913\n",
      "Iteration 182, loss = 0.55327909\n",
      "Iteration 183, loss = 0.55235406\n",
      "Iteration 184, loss = 0.55143399\n",
      "Iteration 185, loss = 0.55051882\n",
      "Iteration 186, loss = 0.54960852\n",
      "Iteration 187, loss = 0.54870305\n",
      "Iteration 188, loss = 0.54780234\n",
      "Iteration 189, loss = 0.54690637\n",
      "Iteration 190, loss = 0.54601509\n",
      "Iteration 191, loss = 0.54512845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 192, loss = 0.54424640\n",
      "Iteration 193, loss = 0.54336891\n",
      "Iteration 194, loss = 0.54249593\n",
      "Iteration 195, loss = 0.54162741\n",
      "Iteration 196, loss = 0.54076332\n",
      "Iteration 197, loss = 0.53990361\n",
      "Iteration 198, loss = 0.53904823\n",
      "Iteration 199, loss = 0.53819716\n",
      "Iteration 200, loss = 0.53735034\n",
      "Iteration 201, loss = 0.53650773\n",
      "Iteration 202, loss = 0.53566930\n",
      "Iteration 203, loss = 0.53483500\n",
      "Iteration 204, loss = 0.53400480\n",
      "Iteration 205, loss = 0.53317865\n",
      "Iteration 206, loss = 0.53235652\n",
      "Iteration 207, loss = 0.53153837\n",
      "Iteration 208, loss = 0.53072415\n",
      "Iteration 209, loss = 0.52991383\n",
      "Iteration 210, loss = 0.52910738\n",
      "Iteration 211, loss = 0.52830475\n",
      "Iteration 212, loss = 0.52750661\n",
      "Iteration 213, loss = 0.52671300\n",
      "Iteration 214, loss = 0.52592317\n",
      "Iteration 215, loss = 0.52513710\n",
      "Iteration 216, loss = 0.52435476\n",
      "Iteration 217, loss = 0.52357612\n",
      "Iteration 218, loss = 0.52280114\n",
      "Iteration 219, loss = 0.52202980\n",
      "Iteration 220, loss = 0.52126207\n",
      "Iteration 221, loss = 0.52049789\n",
      "Iteration 222, loss = 0.51973725\n",
      "Iteration 223, loss = 0.51898011\n",
      "Iteration 224, loss = 0.51822642\n",
      "Iteration 225, loss = 0.51747615\n",
      "Iteration 226, loss = 0.51672926\n",
      "Iteration 227, loss = 0.51598573\n",
      "Iteration 228, loss = 0.51524551\n",
      "Iteration 229, loss = 0.51450857\n",
      "Iteration 230, loss = 0.51377488\n",
      "Iteration 231, loss = 0.51304440\n",
      "Iteration 232, loss = 0.51231710\n",
      "Iteration 233, loss = 0.51159295\n",
      "Iteration 234, loss = 0.51087191\n",
      "Iteration 235, loss = 0.51015396\n",
      "Iteration 236, loss = 0.50943907\n",
      "Iteration 237, loss = 0.50872720\n",
      "Iteration 238, loss = 0.50801832\n",
      "Iteration 239, loss = 0.50731241\n",
      "Iteration 240, loss = 0.50661008\n",
      "Iteration 241, loss = 0.50591090\n",
      "Iteration 242, loss = 0.50521464\n",
      "Iteration 243, loss = 0.50452127\n",
      "Iteration 244, loss = 0.50383077\n",
      "Iteration 245, loss = 0.50314312\n",
      "Iteration 246, loss = 0.50245828\n",
      "Iteration 247, loss = 0.50177623\n",
      "Iteration 248, loss = 0.50109695\n",
      "Iteration 249, loss = 0.50042041\n",
      "Iteration 250, loss = 0.49974658\n",
      "Iteration 251, loss = 0.49907544\n",
      "Iteration 252, loss = 0.49840695\n",
      "Iteration 253, loss = 0.49774109\n",
      "Iteration 254, loss = 0.49707783\n",
      "Iteration 255, loss = 0.49641714\n",
      "Iteration 256, loss = 0.49575901\n",
      "Iteration 257, loss = 0.49510339\n",
      "Iteration 258, loss = 0.49445026\n",
      "Iteration 259, loss = 0.49379960\n",
      "Iteration 260, loss = 0.49315138\n",
      "Iteration 261, loss = 0.49250558\n",
      "Iteration 262, loss = 0.49186217\n",
      "Iteration 263, loss = 0.49122112\n",
      "Iteration 264, loss = 0.49058241\n",
      "Iteration 265, loss = 0.48994602\n",
      "Iteration 266, loss = 0.48931192\n",
      "Iteration 267, loss = 0.48868009\n",
      "Iteration 268, loss = 0.48805050\n",
      "Iteration 269, loss = 0.48742314\n",
      "Iteration 270, loss = 0.48679798\n",
      "Iteration 271, loss = 0.48617500\n",
      "Iteration 272, loss = 0.48555418\n",
      "Iteration 273, loss = 0.48493549\n",
      "Iteration 274, loss = 0.48431891\n",
      "Iteration 275, loss = 0.48370443\n",
      "Iteration 276, loss = 0.48309201\n",
      "Iteration 277, loss = 0.48248165\n",
      "Iteration 278, loss = 0.48187332\n",
      "Iteration 279, loss = 0.48126700\n",
      "Iteration 280, loss = 0.48066267\n",
      "Iteration 281, loss = 0.48006032\n",
      "Iteration 282, loss = 0.47945991\n",
      "Iteration 283, loss = 0.47886144\n",
      "Iteration 284, loss = 0.47826488\n",
      "Iteration 285, loss = 0.47767022\n",
      "Iteration 286, loss = 0.47707743\n",
      "Iteration 287, loss = 0.47648651\n",
      "Iteration 288, loss = 0.47589742\n",
      "Iteration 289, loss = 0.47531016\n",
      "Iteration 290, loss = 0.47472470\n",
      "Iteration 291, loss = 0.47414103\n",
      "Iteration 292, loss = 0.47355913\n",
      "Iteration 293, loss = 0.47297898\n",
      "Iteration 294, loss = 0.47240057\n",
      "Iteration 295, loss = 0.47182388\n",
      "Iteration 296, loss = 0.47124889\n",
      "Iteration 297, loss = 0.47067560\n",
      "Iteration 298, loss = 0.47010397\n",
      "Iteration 299, loss = 0.46953400\n",
      "Iteration 300, loss = 0.46896566\n",
      "Iteration 301, loss = 0.46839896\n",
      "Iteration 302, loss = 0.46783386\n",
      "Iteration 303, loss = 0.46727035\n",
      "Iteration 304, loss = 0.46670842\n",
      "Iteration 305, loss = 0.46614806\n",
      "Iteration 306, loss = 0.46558925\n",
      "Iteration 307, loss = 0.46503197\n",
      "Iteration 308, loss = 0.46447621\n",
      "Iteration 309, loss = 0.46392196\n",
      "Iteration 310, loss = 0.46336920\n",
      "Iteration 311, loss = 0.46281792\n",
      "Iteration 312, loss = 0.46226811\n",
      "Iteration 313, loss = 0.46171975\n",
      "Iteration 314, loss = 0.46117282\n",
      "Iteration 315, loss = 0.46062733\n",
      "Iteration 316, loss = 0.46008324\n",
      "Iteration 317, loss = 0.45954055\n",
      "Iteration 318, loss = 0.45899925\n",
      "Iteration 319, loss = 0.45845933\n",
      "Iteration 320, loss = 0.45792076\n",
      "Iteration 321, loss = 0.45738355\n",
      "Iteration 322, loss = 0.45684767\n",
      "Iteration 323, loss = 0.45631330\n",
      "Iteration 324, loss = 0.45578045\n",
      "Iteration 325, loss = 0.45524893\n",
      "Iteration 326, loss = 0.45471872\n",
      "Iteration 327, loss = 0.45419181\n",
      "Iteration 328, loss = 0.45366845\n",
      "Iteration 329, loss = 0.45314676\n",
      "Iteration 330, loss = 0.45262622\n",
      "Iteration 331, loss = 0.45210689\n",
      "Iteration 332, loss = 0.45158882\n",
      "Iteration 333, loss = 0.45107206\n",
      "Iteration 334, loss = 0.45055664\n",
      "Iteration 335, loss = 0.45004258\n",
      "Iteration 336, loss = 0.44952989\n",
      "Iteration 337, loss = 0.44901857\n",
      "Iteration 338, loss = 0.44850861\n",
      "Iteration 339, loss = 0.44800001\n",
      "Iteration 340, loss = 0.44749274\n",
      "Iteration 341, loss = 0.44698683\n",
      "Iteration 342, loss = 0.44648263\n",
      "Iteration 343, loss = 0.44597975\n",
      "Iteration 344, loss = 0.44547817\n",
      "Iteration 345, loss = 0.44497787\n",
      "Iteration 346, loss = 0.44447882\n",
      "Iteration 347, loss = 0.44398102\n",
      "Iteration 348, loss = 0.44348445\n",
      "Iteration 349, loss = 0.44298908\n",
      "Iteration 350, loss = 0.44249491\n",
      "Iteration 351, loss = 0.44200191\n",
      "Iteration 352, loss = 0.44151008\n",
      "Iteration 353, loss = 0.44101938\n",
      "Iteration 354, loss = 0.44052982\n",
      "Iteration 355, loss = 0.44004138\n",
      "Iteration 356, loss = 0.43955403\n",
      "Iteration 357, loss = 0.43906777\n",
      "Iteration 358, loss = 0.43858258\n",
      "Iteration 359, loss = 0.43809846\n",
      "Iteration 360, loss = 0.43761538\n",
      "Iteration 361, loss = 0.43713334\n",
      "Iteration 362, loss = 0.43665232\n",
      "Iteration 363, loss = 0.43617231\n",
      "Iteration 364, loss = 0.43569336\n",
      "Iteration 365, loss = 0.43521585\n",
      "Iteration 366, loss = 0.43473937\n",
      "Iteration 367, loss = 0.43426389\n",
      "Iteration 368, loss = 0.43378942\n",
      "Iteration 369, loss = 0.43331595\n",
      "Iteration 370, loss = 0.43284348\n",
      "Iteration 371, loss = 0.43237198\n",
      "Iteration 372, loss = 0.43190146\n",
      "Iteration 373, loss = 0.43143191\n",
      "Iteration 374, loss = 0.43096331\n",
      "Iteration 375, loss = 0.43049566\n",
      "Iteration 376, loss = 0.43002895\n",
      "Iteration 377, loss = 0.42956316\n",
      "Iteration 378, loss = 0.42909828\n",
      "Iteration 379, loss = 0.42863432\n",
      "Iteration 380, loss = 0.42817125\n",
      "Iteration 381, loss = 0.42770907\n",
      "Iteration 382, loss = 0.42724778\n",
      "Iteration 383, loss = 0.42678736\n",
      "Iteration 384, loss = 0.42632780\n",
      "Iteration 385, loss = 0.42586910\n",
      "Iteration 386, loss = 0.42541124\n",
      "Iteration 387, loss = 0.42495437\n",
      "Iteration 388, loss = 0.42449841\n",
      "Iteration 389, loss = 0.42404330\n",
      "Iteration 390, loss = 0.42358902\n",
      "Iteration 391, loss = 0.42313556\n",
      "Iteration 392, loss = 0.42268293\n",
      "Iteration 393, loss = 0.42223111\n",
      "Iteration 394, loss = 0.42178010\n",
      "Iteration 395, loss = 0.42132990\n",
      "Iteration 396, loss = 0.42088048\n",
      "Iteration 397, loss = 0.42043185\n",
      "Iteration 398, loss = 0.41998400\n",
      "Iteration 399, loss = 0.41953692\n",
      "Iteration 400, loss = 0.41909061\n",
      "Iteration 401, loss = 0.41864505\n",
      "Iteration 402, loss = 0.41820025\n",
      "Iteration 403, loss = 0.41775619\n",
      "Iteration 404, loss = 0.41731286\n",
      "Iteration 405, loss = 0.41687027\n",
      "Iteration 406, loss = 0.41642840\n",
      "Iteration 407, loss = 0.41598725\n",
      "Iteration 408, loss = 0.41554681\n",
      "Iteration 409, loss = 0.41510707\n",
      "Iteration 410, loss = 0.41466804\n",
      "Iteration 411, loss = 0.41422969\n",
      "Iteration 412, loss = 0.41379203\n",
      "Iteration 413, loss = 0.41335506\n",
      "Iteration 414, loss = 0.41291875\n",
      "Iteration 415, loss = 0.41248313\n",
      "Iteration 416, loss = 0.41204860\n",
      "Iteration 417, loss = 0.41161474\n",
      "Iteration 418, loss = 0.41118156\n",
      "Iteration 419, loss = 0.41074906\n",
      "Iteration 420, loss = 0.41031723\n",
      "Iteration 421, loss = 0.40988607\n",
      "Iteration 422, loss = 0.40945557\n",
      "Iteration 423, loss = 0.40902572\n",
      "Iteration 424, loss = 0.40859653\n",
      "Iteration 425, loss = 0.40816798\n",
      "Iteration 426, loss = 0.40774007\n",
      "Iteration 427, loss = 0.40731280\n",
      "Iteration 428, loss = 0.40688615\n",
      "Iteration 429, loss = 0.40646013\n",
      "Iteration 430, loss = 0.40603472\n",
      "Iteration 431, loss = 0.40560993\n",
      "Iteration 432, loss = 0.40518574\n",
      "Iteration 433, loss = 0.40476216\n",
      "Iteration 434, loss = 0.40433917\n",
      "Iteration 435, loss = 0.40391677\n",
      "Iteration 436, loss = 0.40349496\n",
      "Iteration 437, loss = 0.40307374\n",
      "Iteration 438, loss = 0.40265309\n",
      "Iteration 439, loss = 0.40223301\n",
      "Iteration 440, loss = 0.40181350\n",
      "Iteration 441, loss = 0.40139456\n",
      "Iteration 442, loss = 0.40097617\n",
      "Iteration 443, loss = 0.40055834\n",
      "Iteration 444, loss = 0.40014106\n",
      "Iteration 445, loss = 0.39972433\n",
      "Iteration 446, loss = 0.39930813\n",
      "Iteration 447, loss = 0.39889248\n",
      "Iteration 448, loss = 0.39847735\n",
      "Iteration 449, loss = 0.39806276\n",
      "Iteration 450, loss = 0.39764869\n",
      "Iteration 451, loss = 0.39723515\n",
      "Iteration 452, loss = 0.39682212\n",
      "Iteration 453, loss = 0.39640961\n",
      "Iteration 454, loss = 0.39599760\n",
      "Iteration 455, loss = 0.39558611\n",
      "Iteration 456, loss = 0.39517512\n",
      "Iteration 457, loss = 0.39476462\n",
      "Iteration 458, loss = 0.39435463\n",
      "Iteration 459, loss = 0.39394512\n",
      "Iteration 460, loss = 0.39353611\n",
      "Iteration 461, loss = 0.39312759\n",
      "Iteration 462, loss = 0.39271954\n",
      "Iteration 463, loss = 0.39231198\n",
      "Iteration 464, loss = 0.39190489\n",
      "Iteration 465, loss = 0.39149828\n",
      "Iteration 466, loss = 0.39109214\n",
      "Iteration 467, loss = 0.39068647\n",
      "Iteration 468, loss = 0.39028126\n",
      "Iteration 469, loss = 0.38987652\n",
      "Iteration 470, loss = 0.38947224\n",
      "Iteration 471, loss = 0.38906841\n",
      "Iteration 472, loss = 0.38866503\n",
      "Iteration 473, loss = 0.38826211\n",
      "Iteration 474, loss = 0.38785963\n",
      "Iteration 475, loss = 0.38745761\n",
      "Iteration 476, loss = 0.38705602\n",
      "Iteration 477, loss = 0.38665487\n",
      "Iteration 478, loss = 0.38625458\n",
      "Iteration 479, loss = 0.38585497\n",
      "Iteration 480, loss = 0.38545582\n",
      "Iteration 481, loss = 0.38505714\n",
      "Iteration 482, loss = 0.38465892\n",
      "Iteration 483, loss = 0.38426117\n",
      "Iteration 484, loss = 0.38386386\n",
      "Iteration 485, loss = 0.38346701\n",
      "Iteration 486, loss = 0.38307061\n",
      "Iteration 487, loss = 0.38267466\n",
      "Iteration 488, loss = 0.38227914\n",
      "Iteration 489, loss = 0.38188407\n",
      "Iteration 490, loss = 0.38148942\n",
      "Iteration 491, loss = 0.38109521\n",
      "Iteration 492, loss = 0.38070142\n",
      "Iteration 493, loss = 0.38030806\n",
      "Iteration 494, loss = 0.37991529\n",
      "Iteration 495, loss = 0.37952320\n",
      "Iteration 496, loss = 0.37913154\n",
      "Iteration 497, loss = 0.37874033\n",
      "Iteration 498, loss = 0.37834955\n",
      "Iteration 499, loss = 0.37795920\n",
      "Iteration 500, loss = 0.37756928\n",
      "Iteration 501, loss = 0.37717979\n",
      "Iteration 502, loss = 0.37679071\n",
      "Iteration 503, loss = 0.37640206\n",
      "Iteration 504, loss = 0.37601382\n",
      "Iteration 505, loss = 0.37562599\n",
      "Iteration 506, loss = 0.37523857\n",
      "Iteration 507, loss = 0.37485156\n",
      "Iteration 508, loss = 0.37446495\n",
      "Iteration 509, loss = 0.37407874\n",
      "Iteration 510, loss = 0.37369293\n",
      "Iteration 511, loss = 0.37330752\n",
      "Iteration 512, loss = 0.37292250\n",
      "Iteration 513, loss = 0.37253787\n",
      "Iteration 514, loss = 0.37215362\n",
      "Iteration 515, loss = 0.37176977\n",
      "Iteration 516, loss = 0.37138629\n",
      "Iteration 517, loss = 0.37100320\n",
      "Iteration 518, loss = 0.37062049\n",
      "Iteration 519, loss = 0.37023815\n",
      "Iteration 520, loss = 0.36985619\n",
      "Iteration 521, loss = 0.36947460\n",
      "Iteration 522, loss = 0.36909338\n",
      "Iteration 523, loss = 0.36871254\n",
      "Iteration 524, loss = 0.36833206\n",
      "Iteration 525, loss = 0.36795194\n",
      "Iteration 526, loss = 0.36757220\n",
      "Iteration 527, loss = 0.36719281\n",
      "Iteration 528, loss = 0.36681379\n",
      "Iteration 529, loss = 0.36643512\n",
      "Iteration 530, loss = 0.36605682\n",
      "Iteration 531, loss = 0.36567887\n",
      "Iteration 532, loss = 0.36530128\n",
      "Iteration 533, loss = 0.36492405\n",
      "Iteration 534, loss = 0.36454716\n",
      "Iteration 535, loss = 0.36417063\n",
      "Iteration 536, loss = 0.36379445\n",
      "Iteration 537, loss = 0.36341862\n",
      "Iteration 538, loss = 0.36304314\n",
      "Iteration 539, loss = 0.36266801\n",
      "Iteration 540, loss = 0.36229322\n",
      "Iteration 541, loss = 0.36191878\n",
      "Iteration 542, loss = 0.36154469\n",
      "Iteration 543, loss = 0.36117093\n",
      "Iteration 544, loss = 0.36079752\n",
      "Iteration 545, loss = 0.36042450\n",
      "Iteration 546, loss = 0.36005228\n",
      "Iteration 547, loss = 0.35968044\n",
      "Iteration 548, loss = 0.35930896\n",
      "Iteration 549, loss = 0.35893784\n",
      "Iteration 550, loss = 0.35856709\n",
      "Iteration 551, loss = 0.35819669\n",
      "Iteration 552, loss = 0.35782666\n",
      "Iteration 553, loss = 0.35745698\n",
      "Iteration 554, loss = 0.35708765\n",
      "Iteration 555, loss = 0.35671868\n",
      "Iteration 556, loss = 0.35635005\n",
      "Iteration 557, loss = 0.35598178\n",
      "Iteration 558, loss = 0.35561384\n",
      "Iteration 559, loss = 0.35524626\n",
      "Iteration 560, loss = 0.35487901\n",
      "Iteration 561, loss = 0.35451211\n",
      "Iteration 562, loss = 0.35414554\n",
      "Iteration 563, loss = 0.35377932\n",
      "Iteration 564, loss = 0.35341343\n",
      "Iteration 565, loss = 0.35304788\n",
      "Iteration 566, loss = 0.35268267\n",
      "Iteration 567, loss = 0.35231779\n",
      "Iteration 568, loss = 0.35195324\n",
      "Iteration 569, loss = 0.35158902\n",
      "Iteration 570, loss = 0.35122514\n",
      "Iteration 571, loss = 0.35086159\n",
      "Iteration 572, loss = 0.35049837\n",
      "Iteration 573, loss = 0.35013547\n",
      "Iteration 574, loss = 0.34977291\n",
      "Iteration 575, loss = 0.34941067\n",
      "Iteration 576, loss = 0.34904875\n",
      "Iteration 577, loss = 0.34868717\n",
      "Iteration 578, loss = 0.34832591\n",
      "Iteration 579, loss = 0.34796497\n",
      "Iteration 580, loss = 0.34760435\n",
      "Iteration 581, loss = 0.34724406\n",
      "Iteration 582, loss = 0.34688410\n",
      "Iteration 583, loss = 0.34652445\n",
      "Iteration 584, loss = 0.34616513\n",
      "Iteration 585, loss = 0.34580613\n",
      "Iteration 586, loss = 0.34544745\n",
      "Iteration 587, loss = 0.34508908\n",
      "Iteration 588, loss = 0.34473104\n",
      "Iteration 589, loss = 0.34437332\n",
      "Iteration 590, loss = 0.34401592\n",
      "Iteration 591, loss = 0.34365884\n",
      "Iteration 592, loss = 0.34330207\n",
      "Iteration 593, loss = 0.34294563\n",
      "Iteration 594, loss = 0.34258950\n",
      "Iteration 595, loss = 0.34223369\n",
      "Iteration 596, loss = 0.34187819\n",
      "Iteration 597, loss = 0.34152302\n",
      "Iteration 598, loss = 0.34116816\n",
      "Iteration 599, loss = 0.34081361\n",
      "Iteration 600, loss = 0.34045939\n",
      "Iteration 601, loss = 0.34010548\n",
      "Iteration 602, loss = 0.33975188\n",
      "Iteration 603, loss = 0.33939860\n",
      "Iteration 604, loss = 0.33904564\n",
      "Iteration 605, loss = 0.33869299\n",
      "Iteration 606, loss = 0.33834066\n",
      "Iteration 607, loss = 0.33798864\n",
      "Iteration 608, loss = 0.33763694\n",
      "Iteration 609, loss = 0.33728555\n",
      "Iteration 610, loss = 0.33693448\n",
      "Iteration 611, loss = 0.33658372\n",
      "Iteration 612, loss = 0.33623327\n",
      "Iteration 613, loss = 0.33588314\n",
      "Iteration 614, loss = 0.33553333\n",
      "Iteration 615, loss = 0.33518383\n",
      "Iteration 616, loss = 0.33483464\n",
      "Iteration 617, loss = 0.33448577\n",
      "Iteration 618, loss = 0.33413737\n",
      "Iteration 619, loss = 0.33378952\n",
      "Iteration 620, loss = 0.33344200\n",
      "Iteration 621, loss = 0.33309482\n",
      "Iteration 622, loss = 0.33274797\n",
      "Iteration 623, loss = 0.33240146\n",
      "Iteration 624, loss = 0.33205527\n",
      "Iteration 625, loss = 0.33170942\n",
      "Iteration 626, loss = 0.33136400\n",
      "Iteration 627, loss = 0.33101927\n",
      "Iteration 628, loss = 0.33067489\n",
      "Iteration 629, loss = 0.33033085\n",
      "Iteration 630, loss = 0.32998716\n",
      "Iteration 631, loss = 0.32964382\n",
      "Iteration 632, loss = 0.32930081\n",
      "Iteration 633, loss = 0.32895815\n",
      "Iteration 634, loss = 0.32861582\n",
      "Iteration 635, loss = 0.32827383\n",
      "Iteration 636, loss = 0.32793218\n",
      "Iteration 637, loss = 0.32759086\n",
      "Iteration 638, loss = 0.32724986\n",
      "Iteration 639, loss = 0.32690920\n",
      "Iteration 640, loss = 0.32656887\n",
      "Iteration 641, loss = 0.32622887\n",
      "Iteration 642, loss = 0.32588920\n",
      "Iteration 643, loss = 0.32554985\n",
      "Iteration 644, loss = 0.32521083\n",
      "Iteration 645, loss = 0.32487214\n",
      "Iteration 646, loss = 0.32453378\n",
      "Iteration 647, loss = 0.32419573\n",
      "Iteration 648, loss = 0.32385802\n",
      "Iteration 649, loss = 0.32352062\n",
      "Iteration 650, loss = 0.32318355\n",
      "Iteration 651, loss = 0.32284680\n",
      "Iteration 652, loss = 0.32251038\n",
      "Iteration 653, loss = 0.32217427\n",
      "Iteration 654, loss = 0.32183849\n",
      "Iteration 655, loss = 0.32150334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 656, loss = 0.32116861\n",
      "Iteration 657, loss = 0.32083422\n",
      "Iteration 658, loss = 0.32050017\n",
      "Iteration 659, loss = 0.32016647\n",
      "Iteration 660, loss = 0.31983311\n",
      "Iteration 661, loss = 0.31950009\n",
      "Iteration 662, loss = 0.31916741\n",
      "Iteration 663, loss = 0.31883506\n",
      "Iteration 664, loss = 0.31850306\n",
      "Iteration 665, loss = 0.31817138\n",
      "Iteration 666, loss = 0.31784004\n",
      "Iteration 667, loss = 0.31750904\n",
      "Iteration 668, loss = 0.31717836\n",
      "Iteration 669, loss = 0.31684801\n",
      "Iteration 670, loss = 0.31651800\n",
      "Iteration 671, loss = 0.31618831\n",
      "Iteration 672, loss = 0.31585896\n",
      "Iteration 673, loss = 0.31552993\n",
      "Iteration 674, loss = 0.31520123\n",
      "Iteration 675, loss = 0.31487286\n",
      "Iteration 676, loss = 0.31454482\n",
      "Iteration 677, loss = 0.31421711\n",
      "Iteration 678, loss = 0.31388972\n",
      "Iteration 679, loss = 0.31356266\n",
      "Iteration 680, loss = 0.31323593\n",
      "Iteration 681, loss = 0.31290952\n",
      "Iteration 682, loss = 0.31258344\n",
      "Iteration 683, loss = 0.31225769\n",
      "Iteration 684, loss = 0.31193227\n",
      "Iteration 685, loss = 0.31160717\n",
      "Iteration 686, loss = 0.31128239\n",
      "Iteration 687, loss = 0.31095794\n",
      "Iteration 688, loss = 0.31063382\n",
      "Iteration 689, loss = 0.31031003\n",
      "Iteration 690, loss = 0.30998656\n",
      "Iteration 691, loss = 0.30966342\n",
      "Iteration 692, loss = 0.30934060\n",
      "Iteration 693, loss = 0.30901811\n",
      "Iteration 694, loss = 0.30869595\n",
      "Iteration 695, loss = 0.30837411\n",
      "Iteration 696, loss = 0.30805260\n",
      "Iteration 697, loss = 0.30773142\n",
      "Iteration 698, loss = 0.30741057\n",
      "Iteration 699, loss = 0.30709004\n",
      "Iteration 700, loss = 0.30676984\n",
      "Iteration 701, loss = 0.30644996\n",
      "Iteration 702, loss = 0.30613042\n",
      "Iteration 703, loss = 0.30581120\n",
      "Iteration 704, loss = 0.30549231\n",
      "Iteration 705, loss = 0.30517375\n",
      "Iteration 706, loss = 0.30485552\n",
      "Iteration 707, loss = 0.30453761\n",
      "Iteration 708, loss = 0.30422004\n",
      "Iteration 709, loss = 0.30390279\n",
      "Iteration 710, loss = 0.30358588\n",
      "Iteration 711, loss = 0.30326929\n",
      "Iteration 712, loss = 0.30295303\n",
      "Iteration 713, loss = 0.30263710\n",
      "Iteration 714, loss = 0.30232150\n",
      "Iteration 715, loss = 0.30200623\n",
      "Iteration 716, loss = 0.30169130\n",
      "Iteration 717, loss = 0.30137669\n",
      "Iteration 718, loss = 0.30106241\n",
      "Iteration 719, loss = 0.30074847\n",
      "Iteration 720, loss = 0.30043485\n",
      "Iteration 721, loss = 0.30012157\n",
      "Iteration 722, loss = 0.29980862\n",
      "Iteration 723, loss = 0.29949600\n",
      "Iteration 724, loss = 0.29918371\n",
      "Iteration 725, loss = 0.29887176\n",
      "Iteration 726, loss = 0.29856014\n",
      "Iteration 727, loss = 0.29824885\n",
      "Iteration 728, loss = 0.29793789\n",
      "Iteration 729, loss = 0.29762727\n",
      "Iteration 730, loss = 0.29731698\n",
      "Iteration 731, loss = 0.29700702\n",
      "Iteration 732, loss = 0.29669740\n",
      "Iteration 733, loss = 0.29638811\n",
      "Iteration 734, loss = 0.29607916\n",
      "Iteration 735, loss = 0.29577054\n",
      "Iteration 736, loss = 0.29546226\n",
      "Iteration 737, loss = 0.29515431\n",
      "Iteration 738, loss = 0.29484670\n",
      "Iteration 739, loss = 0.29453942\n",
      "Iteration 740, loss = 0.29423248\n",
      "Iteration 741, loss = 0.29392587\n",
      "Iteration 742, loss = 0.29361960\n",
      "Iteration 743, loss = 0.29331367\n",
      "Iteration 744, loss = 0.29300808\n",
      "Iteration 745, loss = 0.29270282\n",
      "Iteration 746, loss = 0.29239789\n",
      "Iteration 747, loss = 0.29209331\n",
      "Iteration 748, loss = 0.29178906\n",
      "Iteration 749, loss = 0.29148515\n",
      "Iteration 750, loss = 0.29118158\n",
      "Iteration 751, loss = 0.29087835\n",
      "Iteration 752, loss = 0.29057546\n",
      "Iteration 753, loss = 0.29027290\n",
      "Iteration 754, loss = 0.28997069\n",
      "Iteration 755, loss = 0.28966881\n",
      "Iteration 756, loss = 0.28936727\n",
      "Iteration 757, loss = 0.28906608\n",
      "Iteration 758, loss = 0.28876522\n",
      "Iteration 759, loss = 0.28846470\n",
      "Iteration 760, loss = 0.28816453\n",
      "Iteration 761, loss = 0.28786469\n",
      "Iteration 762, loss = 0.28756519\n",
      "Iteration 763, loss = 0.28726604\n",
      "Iteration 764, loss = 0.28696723\n",
      "Iteration 765, loss = 0.28666876\n",
      "Iteration 766, loss = 0.28637063\n",
      "Iteration 767, loss = 0.28607284\n",
      "Iteration 768, loss = 0.28577539\n",
      "Iteration 769, loss = 0.28547829\n",
      "Iteration 770, loss = 0.28518153\n",
      "Iteration 771, loss = 0.28488511\n",
      "Iteration 772, loss = 0.28458904\n",
      "Iteration 773, loss = 0.28429331\n",
      "Iteration 774, loss = 0.28399792\n",
      "Iteration 775, loss = 0.28370287\n",
      "Iteration 776, loss = 0.28340817\n",
      "Iteration 777, loss = 0.28311382\n",
      "Iteration 778, loss = 0.28281981\n",
      "Iteration 779, loss = 0.28252614\n",
      "Iteration 780, loss = 0.28223282\n",
      "Iteration 781, loss = 0.28193984\n",
      "Iteration 782, loss = 0.28164721\n",
      "Iteration 783, loss = 0.28135492\n",
      "Iteration 784, loss = 0.28106298\n",
      "Iteration 785, loss = 0.28077138\n",
      "Iteration 786, loss = 0.28048013\n",
      "Iteration 787, loss = 0.28018923\n",
      "Iteration 788, loss = 0.27989894\n",
      "Iteration 789, loss = 0.27960917\n",
      "Iteration 790, loss = 0.27931977\n",
      "Iteration 791, loss = 0.27903073\n",
      "Iteration 792, loss = 0.27874206\n",
      "Iteration 793, loss = 0.27845376\n",
      "Iteration 794, loss = 0.27816582\n",
      "Iteration 795, loss = 0.27787824\n",
      "Iteration 796, loss = 0.27759102\n",
      "Iteration 797, loss = 0.27730417\n",
      "Iteration 798, loss = 0.27701767\n",
      "Iteration 799, loss = 0.27673162\n",
      "Iteration 800, loss = 0.27644596\n",
      "Iteration 801, loss = 0.27616066\n",
      "Iteration 802, loss = 0.27587572\n",
      "Iteration 803, loss = 0.27559114\n",
      "Iteration 804, loss = 0.27530691\n",
      "Iteration 805, loss = 0.27502305\n",
      "Iteration 806, loss = 0.27473955\n",
      "Iteration 807, loss = 0.27445640\n",
      "Iteration 808, loss = 0.27417361\n",
      "Iteration 809, loss = 0.27389117\n",
      "Iteration 810, loss = 0.27360909\n",
      "Iteration 811, loss = 0.27332737\n",
      "Iteration 812, loss = 0.27304600\n",
      "Iteration 813, loss = 0.27276498\n",
      "Iteration 814, loss = 0.27248432\n",
      "Iteration 815, loss = 0.27220402\n",
      "Iteration 816, loss = 0.27192407\n",
      "Iteration 817, loss = 0.27164447\n",
      "Iteration 818, loss = 0.27136522\n",
      "Iteration 819, loss = 0.27108633\n",
      "Iteration 820, loss = 0.27080779\n",
      "Iteration 821, loss = 0.27052961\n",
      "Iteration 822, loss = 0.27025177\n",
      "Iteration 823, loss = 0.26997429\n",
      "Iteration 824, loss = 0.26969716\n",
      "Iteration 825, loss = 0.26942038\n",
      "Iteration 826, loss = 0.26914396\n",
      "Iteration 827, loss = 0.26886789\n",
      "Iteration 828, loss = 0.26859217\n",
      "Iteration 829, loss = 0.26831680\n",
      "Iteration 830, loss = 0.26804178\n",
      "Iteration 831, loss = 0.26776712\n",
      "Iteration 832, loss = 0.26749280\n",
      "Iteration 833, loss = 0.26721884\n",
      "Iteration 834, loss = 0.26694523\n",
      "Iteration 835, loss = 0.26667197\n",
      "Iteration 836, loss = 0.26639907\n",
      "Iteration 837, loss = 0.26612651\n",
      "Iteration 838, loss = 0.26585431\n",
      "Iteration 839, loss = 0.26558246\n",
      "Iteration 840, loss = 0.26531096\n",
      "Iteration 841, loss = 0.26503981\n",
      "Iteration 842, loss = 0.26476901\n",
      "Iteration 843, loss = 0.26449857\n",
      "Iteration 844, loss = 0.26422848\n",
      "Iteration 845, loss = 0.26395873\n",
      "Iteration 846, loss = 0.26368935\n",
      "Iteration 847, loss = 0.26342031\n",
      "Iteration 848, loss = 0.26315162\n",
      "Iteration 849, loss = 0.26288329\n",
      "Iteration 850, loss = 0.26261531\n",
      "Iteration 851, loss = 0.26234768\n",
      "Iteration 852, loss = 0.26208040\n",
      "Iteration 853, loss = 0.26181348\n",
      "Iteration 854, loss = 0.26154690\n",
      "Iteration 855, loss = 0.26128068\n",
      "Iteration 856, loss = 0.26101481\n",
      "Iteration 857, loss = 0.26074929\n",
      "Iteration 858, loss = 0.26048413\n",
      "Iteration 859, loss = 0.26021931\n",
      "Iteration 860, loss = 0.25995485\n",
      "Iteration 861, loss = 0.25969074\n",
      "Iteration 862, loss = 0.25942698\n",
      "Iteration 863, loss = 0.25916358\n",
      "Iteration 864, loss = 0.25890053\n",
      "Iteration 865, loss = 0.25863782\n",
      "Iteration 866, loss = 0.25837548\n",
      "Iteration 867, loss = 0.25811348\n",
      "Iteration 868, loss = 0.25785183\n",
      "Iteration 869, loss = 0.25759054\n",
      "Iteration 870, loss = 0.25732960\n",
      "Iteration 871, loss = 0.25706901\n",
      "Iteration 872, loss = 0.25680877\n",
      "Iteration 873, loss = 0.25654889\n",
      "Iteration 874, loss = 0.25628935\n",
      "Iteration 875, loss = 0.25603017\n",
      "Iteration 876, loss = 0.25577134\n",
      "Iteration 877, loss = 0.25551286\n",
      "Iteration 878, loss = 0.25525474\n",
      "Iteration 879, loss = 0.25499696\n",
      "Iteration 880, loss = 0.25473954\n",
      "Iteration 881, loss = 0.25448247\n",
      "Iteration 882, loss = 0.25422575\n",
      "Iteration 883, loss = 0.25396938\n",
      "Iteration 884, loss = 0.25371337\n",
      "Iteration 885, loss = 0.25345770\n",
      "Iteration 886, loss = 0.25320239\n",
      "Iteration 887, loss = 0.25294743\n",
      "Iteration 888, loss = 0.25269282\n",
      "Iteration 889, loss = 0.25243856\n",
      "Iteration 890, loss = 0.25218465\n",
      "Iteration 891, loss = 0.25193110\n",
      "Iteration 892, loss = 0.25167789\n",
      "Iteration 893, loss = 0.25142504\n",
      "Iteration 894, loss = 0.25117253\n",
      "Iteration 895, loss = 0.25092038\n",
      "Iteration 896, loss = 0.25066858\n",
      "Iteration 897, loss = 0.25041713\n",
      "Iteration 898, loss = 0.25016603\n",
      "Iteration 899, loss = 0.24991528\n",
      "Iteration 900, loss = 0.24966488\n",
      "Iteration 901, loss = 0.24941483\n",
      "Iteration 902, loss = 0.24916514\n",
      "Iteration 903, loss = 0.24891579\n",
      "Iteration 904, loss = 0.24866679\n",
      "Iteration 905, loss = 0.24841815\n",
      "Iteration 906, loss = 0.24816985\n",
      "Iteration 907, loss = 0.24792190\n",
      "Iteration 908, loss = 0.24767431\n",
      "Iteration 909, loss = 0.24742706\n",
      "Iteration 910, loss = 0.24718016\n",
      "Iteration 911, loss = 0.24693361\n",
      "Iteration 912, loss = 0.24668741\n",
      "Iteration 913, loss = 0.24644156\n",
      "Iteration 914, loss = 0.24619606\n",
      "Iteration 915, loss = 0.24595091\n",
      "Iteration 916, loss = 0.24570611\n",
      "Iteration 917, loss = 0.24546166\n",
      "Iteration 918, loss = 0.24521756\n",
      "Iteration 919, loss = 0.24497380\n",
      "Iteration 920, loss = 0.24473039\n",
      "Iteration 921, loss = 0.24448733\n",
      "Iteration 922, loss = 0.24424462\n",
      "Iteration 923, loss = 0.24400226\n",
      "Iteration 924, loss = 0.24376025\n",
      "Iteration 925, loss = 0.24351858\n",
      "Iteration 926, loss = 0.24327726\n",
      "Iteration 927, loss = 0.24303629\n",
      "Iteration 928, loss = 0.24279567\n",
      "Iteration 929, loss = 0.24255539\n",
      "Iteration 930, loss = 0.24231547\n",
      "Iteration 931, loss = 0.24207588\n",
      "Iteration 932, loss = 0.24183665\n",
      "Iteration 933, loss = 0.24159776\n",
      "Iteration 934, loss = 0.24135922\n",
      "Iteration 935, loss = 0.24112102\n",
      "Iteration 936, loss = 0.24088318\n",
      "Iteration 937, loss = 0.24064567\n",
      "Iteration 938, loss = 0.24040852\n",
      "Iteration 939, loss = 0.24017171\n",
      "Iteration 940, loss = 0.23993524\n",
      "Iteration 941, loss = 0.23969912\n",
      "Iteration 942, loss = 0.23946335\n",
      "Iteration 943, loss = 0.23922792\n",
      "Iteration 944, loss = 0.23899283\n",
      "Iteration 945, loss = 0.23875809\n",
      "Iteration 946, loss = 0.23852370\n",
      "Iteration 947, loss = 0.23828965\n",
      "Iteration 948, loss = 0.23805594\n",
      "Iteration 949, loss = 0.23782258\n",
      "Iteration 950, loss = 0.23758956\n",
      "Iteration 951, loss = 0.23735689\n",
      "Iteration 952, loss = 0.23712456\n",
      "Iteration 953, loss = 0.23689257\n",
      "Iteration 954, loss = 0.23666093\n",
      "Iteration 955, loss = 0.23642963\n",
      "Iteration 956, loss = 0.23619867\n",
      "Iteration 957, loss = 0.23596805\n",
      "Iteration 958, loss = 0.23573778\n",
      "Iteration 959, loss = 0.23550785\n",
      "Iteration 960, loss = 0.23527826\n",
      "Iteration 961, loss = 0.23504901\n",
      "Iteration 962, loss = 0.23482010\n",
      "Iteration 963, loss = 0.23459154\n",
      "Iteration 964, loss = 0.23436331\n",
      "Iteration 965, loss = 0.23413543\n",
      "Iteration 966, loss = 0.23390789\n",
      "Iteration 967, loss = 0.23368069\n",
      "Iteration 968, loss = 0.23345383\n",
      "Iteration 969, loss = 0.23322730\n",
      "Iteration 970, loss = 0.23300112\n",
      "Iteration 971, loss = 0.23277528\n",
      "Iteration 972, loss = 0.23254978\n",
      "Iteration 973, loss = 0.23232461\n",
      "Iteration 974, loss = 0.23209982\n",
      "Iteration 975, loss = 0.23187542\n",
      "Iteration 976, loss = 0.23165135\n",
      "Iteration 977, loss = 0.23142763\n",
      "Iteration 978, loss = 0.23120425\n",
      "Iteration 979, loss = 0.23098122\n",
      "Iteration 980, loss = 0.23075852\n",
      "Iteration 981, loss = 0.23053616\n",
      "Iteration 982, loss = 0.23031414\n",
      "Iteration 983, loss = 0.23009246\n",
      "Iteration 984, loss = 0.22987112\n",
      "Iteration 985, loss = 0.22965011\n",
      "Iteration 986, loss = 0.22942945\n",
      "Iteration 987, loss = 0.22920911\n",
      "Iteration 988, loss = 0.22898912\n",
      "Iteration 989, loss = 0.22876946\n",
      "Iteration 990, loss = 0.22855014\n",
      "Iteration 991, loss = 0.22833115\n",
      "Iteration 992, loss = 0.22811250\n",
      "Iteration 993, loss = 0.22789418\n",
      "Iteration 994, loss = 0.22767620\n",
      "Iteration 995, loss = 0.22745855\n",
      "Iteration 996, loss = 0.22724124\n",
      "Iteration 997, loss = 0.22702425\n",
      "Iteration 998, loss = 0.22680761\n",
      "Iteration 999, loss = 0.22659129\n",
      "Iteration 1000, loss = 0.22637531\n",
      "Iteration 1, loss = 1.50852130\n",
      "Iteration 2, loss = 1.47841785\n",
      "Iteration 3, loss = 1.43794932\n",
      "Iteration 4, loss = 1.39069743\n",
      "Iteration 5, loss = 1.34007257\n",
      "Iteration 6, loss = 1.28879600\n",
      "Iteration 7, loss = 1.23892040\n",
      "Iteration 8, loss = 1.19194340\n",
      "Iteration 9, loss = 1.14822164\n",
      "Iteration 10, loss = 1.10802780\n",
      "Iteration 11, loss = 1.07138053\n",
      "Iteration 12, loss = 1.03763660\n",
      "Iteration 13, loss = 1.00608573\n",
      "Iteration 14, loss = 0.97655429\n",
      "Iteration 15, loss = 0.94897773\n",
      "Iteration 16, loss = 0.92344784\n",
      "Iteration 17, loss = 0.89998670\n",
      "Iteration 18, loss = 0.87859880\n",
      "Iteration 19, loss = 0.85912938\n",
      "Iteration 20, loss = 0.84159781\n",
      "Iteration 21, loss = 0.82590126\n",
      "Iteration 22, loss = 0.81193818\n",
      "Iteration 23, loss = 0.80006550\n",
      "Iteration 24, loss = 0.79037421\n",
      "Iteration 25, loss = 0.78277165\n",
      "Iteration 26, loss = 0.77698775\n",
      "Iteration 27, loss = 0.77248504\n",
      "Iteration 28, loss = 0.76922873\n",
      "Iteration 29, loss = 0.76716818\n",
      "Iteration 30, loss = 0.76572189\n",
      "Iteration 31, loss = 0.76464315\n",
      "Iteration 32, loss = 0.76379871\n",
      "Iteration 33, loss = 0.76294174\n",
      "Iteration 34, loss = 0.76202450\n",
      "Iteration 35, loss = 0.76094282\n",
      "Iteration 36, loss = 0.75969991\n",
      "Iteration 37, loss = 0.75823845\n",
      "Iteration 38, loss = 0.75653616\n",
      "Iteration 39, loss = 0.75460762\n",
      "Iteration 40, loss = 0.75249328\n",
      "Iteration 41, loss = 0.75018879\n",
      "Iteration 42, loss = 0.74772263\n",
      "Iteration 43, loss = 0.74512565\n",
      "Iteration 44, loss = 0.74250203\n",
      "Iteration 45, loss = 0.73986691\n",
      "Iteration 46, loss = 0.73722375\n",
      "Iteration 47, loss = 0.73462336\n",
      "Iteration 48, loss = 0.73208825\n",
      "Iteration 49, loss = 0.72963819\n",
      "Iteration 50, loss = 0.72732785\n",
      "Iteration 51, loss = 0.72512962\n",
      "Iteration 52, loss = 0.72302541\n",
      "Iteration 53, loss = 0.72100428\n",
      "Iteration 54, loss = 0.71904955\n",
      "Iteration 55, loss = 0.71712473\n",
      "Iteration 56, loss = 0.71523263\n",
      "Iteration 57, loss = 0.71336774\n",
      "Iteration 58, loss = 0.71151994\n",
      "Iteration 59, loss = 0.70968350\n",
      "Iteration 60, loss = 0.70785585\n",
      "Iteration 61, loss = 0.70603487\n",
      "Iteration 62, loss = 0.70421887\n",
      "Iteration 63, loss = 0.70240662\n",
      "Iteration 64, loss = 0.70059727\n",
      "Iteration 65, loss = 0.69879036\n",
      "Iteration 66, loss = 0.69698574\n",
      "Iteration 67, loss = 0.69518353\n",
      "Iteration 68, loss = 0.69338410\n",
      "Iteration 69, loss = 0.69158794\n",
      "Iteration 70, loss = 0.68979679\n",
      "Iteration 71, loss = 0.68801188\n",
      "Iteration 72, loss = 0.68623221\n",
      "Iteration 73, loss = 0.68446510\n",
      "Iteration 74, loss = 0.68271194\n",
      "Iteration 75, loss = 0.68096679\n",
      "Iteration 76, loss = 0.67923020\n",
      "Iteration 77, loss = 0.67750267\n",
      "Iteration 78, loss = 0.67578459\n",
      "Iteration 79, loss = 0.67408694\n",
      "Iteration 80, loss = 0.67241150\n",
      "Iteration 81, loss = 0.67075075\n",
      "Iteration 82, loss = 0.66909994\n",
      "Iteration 83, loss = 0.66745929\n",
      "Iteration 84, loss = 0.66582796\n",
      "Iteration 85, loss = 0.66420575\n",
      "Iteration 86, loss = 0.66259605\n",
      "Iteration 87, loss = 0.66099791\n",
      "Iteration 88, loss = 0.65940997\n",
      "Iteration 89, loss = 0.65782756\n",
      "Iteration 90, loss = 0.65625127\n",
      "Iteration 91, loss = 0.65468133\n",
      "Iteration 92, loss = 0.65311798\n",
      "Iteration 93, loss = 0.65156145\n",
      "Iteration 94, loss = 0.65001197\n",
      "Iteration 95, loss = 0.64846975\n",
      "Iteration 96, loss = 0.64693578\n",
      "Iteration 97, loss = 0.64541267\n",
      "Iteration 98, loss = 0.64390056\n",
      "Iteration 99, loss = 0.64239742\n",
      "Iteration 100, loss = 0.64090245\n",
      "Iteration 101, loss = 0.63941560\n",
      "Iteration 102, loss = 0.63793683\n",
      "Iteration 103, loss = 0.63646609\n",
      "Iteration 104, loss = 0.63500336\n",
      "Iteration 105, loss = 0.63354860\n",
      "Iteration 106, loss = 0.63210193\n",
      "Iteration 107, loss = 0.63066354\n",
      "Iteration 108, loss = 0.62923311"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 109, loss = 0.62781064\n",
      "Iteration 110, loss = 0.62639609\n",
      "Iteration 111, loss = 0.62498944\n",
      "Iteration 112, loss = 0.62359069\n",
      "Iteration 113, loss = 0.62219979\n",
      "Iteration 114, loss = 0.62081672\n",
      "Iteration 115, loss = 0.61944159\n",
      "Iteration 116, loss = 0.61807450\n",
      "Iteration 117, loss = 0.61671517\n",
      "Iteration 118, loss = 0.61536358\n",
      "Iteration 119, loss = 0.61401968\n",
      "Iteration 120, loss = 0.61268347\n",
      "Iteration 121, loss = 0.61135490\n",
      "Iteration 122, loss = 0.61003394\n",
      "Iteration 123, loss = 0.60872054\n",
      "Iteration 124, loss = 0.60741468\n",
      "Iteration 125, loss = 0.60611631\n",
      "Iteration 126, loss = 0.60482539\n",
      "Iteration 127, loss = 0.60354187\n",
      "Iteration 128, loss = 0.60226571\n",
      "Iteration 129, loss = 0.60099687\n",
      "Iteration 130, loss = 0.59973530\n",
      "Iteration 131, loss = 0.59848096\n",
      "Iteration 132, loss = 0.59723381\n",
      "Iteration 133, loss = 0.59599379\n",
      "Iteration 134, loss = 0.59476087\n",
      "Iteration 135, loss = 0.59353500\n",
      "Iteration 136, loss = 0.59231613\n",
      "Iteration 137, loss = 0.59110422\n",
      "Iteration 138, loss = 0.58989922\n",
      "Iteration 139, loss = 0.58870109\n",
      "Iteration 140, loss = 0.58750977\n",
      "Iteration 141, loss = 0.58632522\n",
      "Iteration 142, loss = 0.58514739\n",
      "Iteration 143, loss = 0.58397624\n",
      "Iteration 144, loss = 0.58281171\n",
      "Iteration 145, loss = 0.58165376\n",
      "Iteration 146, loss = 0.58050234\n",
      "Iteration 147, loss = 0.57935740\n",
      "Iteration 148, loss = 0.57821964\n",
      "Iteration 149, loss = 0.57708853\n",
      "Iteration 150, loss = 0.57596378\n",
      "Iteration 151, loss = 0.57484536\n",
      "Iteration 152, loss = 0.57373323\n",
      "Iteration 153, loss = 0.57262736\n",
      "Iteration 154, loss = 0.57152769\n",
      "Iteration 155, loss = 0.57043420\n",
      "Iteration 156, loss = 0.56934683\n",
      "Iteration 157, loss = 0.56826554\n",
      "Iteration 158, loss = 0.56719029\n",
      "Iteration 159, loss = 0.56612103\n",
      "Iteration 160, loss = 0.56505770\n",
      "Iteration 161, loss = 0.56400027\n",
      "Iteration 162, loss = 0.56294868\n",
      "Iteration 163, loss = 0.56190288\n",
      "Iteration 164, loss = 0.56086283\n",
      "Iteration 165, loss = 0.55982883\n",
      "Iteration 166, loss = 0.55880096\n",
      "Iteration 167, loss = 0.55777872\n",
      "Iteration 168, loss = 0.55676206\n",
      "Iteration 169, loss = 0.55575095\n",
      "Iteration 170, loss = 0.55474532\n",
      "Iteration 171, loss = 0.55374512\n",
      "Iteration 172, loss = 0.55275032\n",
      "Iteration 173, loss = 0.55176112\n",
      "Iteration 174, loss = 0.55077769\n",
      "Iteration 175, loss = 0.54979954\n",
      "Iteration 176, loss = 0.54882664\n",
      "Iteration 177, loss = 0.54785894\n",
      "Iteration 178, loss = 0.54689641\n",
      "Iteration 179, loss = 0.54593899\n",
      "Iteration 180, loss = 0.54498666\n",
      "Iteration 181, loss = 0.54403935\n",
      "Iteration 182, loss = 0.54309703\n",
      "Iteration 183, loss = 0.54215965\n",
      "Iteration 184, loss = 0.54122716\n",
      "Iteration 185, loss = 0.54029952\n",
      "Iteration 186, loss = 0.53937668\n",
      "Iteration 187, loss = 0.53845859\n",
      "Iteration 188, loss = 0.53754522\n",
      "Iteration 189, loss = 0.53663651\n",
      "Iteration 190, loss = 0.53573242\n",
      "Iteration 191, loss = 0.53483290\n",
      "Iteration 192, loss = 0.53393791\n",
      "Iteration 193, loss = 0.53304741\n",
      "Iteration 194, loss = 0.53216135\n",
      "Iteration 195, loss = 0.53127969\n",
      "Iteration 196, loss = 0.53040239\n",
      "Iteration 197, loss = 0.52952939\n",
      "Iteration 198, loss = 0.52866066\n",
      "Iteration 199, loss = 0.52779616\n",
      "Iteration 200, loss = 0.52693585\n",
      "Iteration 201, loss = 0.52607967\n",
      "Iteration 202, loss = 0.52522760\n",
      "Iteration 203, loss = 0.52437959\n",
      "Iteration 204, loss = 0.52353560\n",
      "Iteration 205, loss = 0.52269559\n",
      "Iteration 206, loss = 0.52185952\n",
      "Iteration 207, loss = 0.52102735\n",
      "Iteration 208, loss = 0.52019904\n",
      "Iteration 209, loss = 0.51937456\n",
      "Iteration 210, loss = 0.51855390\n",
      "Iteration 211, loss = 0.51773783\n",
      "Iteration 212, loss = 0.51692550\n",
      "Iteration 213, loss = 0.51611690\n",
      "Iteration 214, loss = 0.51531199\n",
      "Iteration 215, loss = 0.51451074\n",
      "Iteration 216, loss = 0.51371312\n",
      "Iteration 217, loss = 0.51291910\n",
      "Iteration 218, loss = 0.51212864\n",
      "Iteration 219, loss = 0.51134172\n",
      "Iteration 220, loss = 0.51055829\n",
      "Iteration 221, loss = 0.50977832\n",
      "Iteration 222, loss = 0.50900179\n",
      "Iteration 223, loss = 0.50822864\n",
      "Iteration 224, loss = 0.50745886\n",
      "Iteration 225, loss = 0.50669239\n",
      "Iteration 226, loss = 0.50592922\n",
      "Iteration 227, loss = 0.50516930\n",
      "Iteration 228, loss = 0.50441261\n",
      "Iteration 229, loss = 0.50365910\n",
      "Iteration 230, loss = 0.50290874\n",
      "Iteration 231, loss = 0.50216151\n",
      "Iteration 232, loss = 0.50141737\n",
      "Iteration 233, loss = 0.50067629\n",
      "Iteration 234, loss = 0.49993823\n",
      "Iteration 235, loss = 0.49920316\n",
      "Iteration 236, loss = 0.49847106\n",
      "Iteration 237, loss = 0.49774253\n",
      "Iteration 238, loss = 0.49701721\n",
      "Iteration 239, loss = 0.49629481\n",
      "Iteration 240, loss = 0.49557533\n",
      "Iteration 241, loss = 0.49485872\n",
      "Iteration 242, loss = 0.49414496\n",
      "Iteration 243, loss = 0.49343403\n",
      "Iteration 244, loss = 0.49272590\n",
      "Iteration 245, loss = 0.49202054\n",
      "Iteration 246, loss = 0.49131792\n",
      "Iteration 247, loss = 0.49061842\n",
      "Iteration 248, loss = 0.48992178\n",
      "Iteration 249, loss = 0.48922782\n",
      "Iteration 250, loss = 0.48853653\n",
      "Iteration 251, loss = 0.48784787\n",
      "Iteration 252, loss = 0.48716182\n",
      "Iteration 253, loss = 0.48647833\n",
      "Iteration 254, loss = 0.48579739\n",
      "Iteration 255, loss = 0.48511896\n",
      "Iteration 256, loss = 0.48444302\n",
      "Iteration 257, loss = 0.48376953\n",
      "Iteration 258, loss = 0.48309847\n",
      "Iteration 259, loss = 0.48242982\n",
      "Iteration 260, loss = 0.48176354\n",
      "Iteration 261, loss = 0.48109962\n",
      "Iteration 262, loss = 0.48043802\n",
      "Iteration 263, loss = 0.47977873\n",
      "Iteration 264, loss = 0.47912171\n",
      "Iteration 265, loss = 0.47846694\n",
      "Iteration 266, loss = 0.47781441\n",
      "Iteration 267, loss = 0.47716408\n",
      "Iteration 268, loss = 0.47651594\n",
      "Iteration 269, loss = 0.47586996\n",
      "Iteration 270, loss = 0.47522613\n",
      "Iteration 271, loss = 0.47458441\n",
      "Iteration 272, loss = 0.47394479\n",
      "Iteration 273, loss = 0.47330724\n",
      "Iteration 274, loss = 0.47267175\n",
      "Iteration 275, loss = 0.47203829\n",
      "Iteration 276, loss = 0.47140685\n",
      "Iteration 277, loss = 0.47077740\n",
      "Iteration 278, loss = 0.47014993\n",
      "Iteration 279, loss = 0.46952440\n",
      "Iteration 280, loss = 0.46890081\n",
      "Iteration 281, loss = 0.46827914\n",
      "Iteration 282, loss = 0.46765936\n",
      "Iteration 283, loss = 0.46704146\n",
      "Iteration 284, loss = 0.46642542\n",
      "Iteration 285, loss = 0.46581121\n",
      "Iteration 286, loss = 0.46519883\n",
      "Iteration 287, loss = 0.46458826\n",
      "Iteration 288, loss = 0.46397946\n",
      "Iteration 289, loss = 0.46337244\n",
      "Iteration 290, loss = 0.46276717\n",
      "Iteration 291, loss = 0.46216364\n",
      "Iteration 292, loss = 0.46156182\n",
      "Iteration 293, loss = 0.46096171\n",
      "Iteration 294, loss = 0.46036328\n",
      "Iteration 295, loss = 0.45976652\n",
      "Iteration 296, loss = 0.45917141\n",
      "Iteration 297, loss = 0.45857793\n",
      "Iteration 298, loss = 0.45798608\n",
      "Iteration 299, loss = 0.45739627\n",
      "Iteration 300, loss = 0.45680881\n",
      "Iteration 301, loss = 0.45622299\n",
      "Iteration 302, loss = 0.45563879\n",
      "Iteration 303, loss = 0.45505619\n",
      "Iteration 304, loss = 0.45447519\n",
      "Iteration 305, loss = 0.45389577\n",
      "Iteration 306, loss = 0.45331790\n",
      "Iteration 307, loss = 0.45274158\n",
      "Iteration 308, loss = 0.45216678\n",
      "Iteration 309, loss = 0.45159350\n",
      "Iteration 310, loss = 0.45102171\n",
      "Iteration 311, loss = 0.45045140\n",
      "Iteration 312, loss = 0.44988256\n",
      "Iteration 313, loss = 0.44931516\n",
      "Iteration 314, loss = 0.44874958\n",
      "Iteration 315, loss = 0.44818603\n",
      "Iteration 316, loss = 0.44762394\n",
      "Iteration 317, loss = 0.44706330\n",
      "Iteration 318, loss = 0.44650410\n",
      "Iteration 319, loss = 0.44594632\n",
      "Iteration 320, loss = 0.44538994\n",
      "Iteration 321, loss = 0.44483495\n",
      "Iteration 322, loss = 0.44428135\n",
      "Iteration 323, loss = 0.44372910\n",
      "Iteration 324, loss = 0.44317820\n",
      "Iteration 325, loss = 0.44262864\n",
      "Iteration 326, loss = 0.44208049\n",
      "Iteration 327, loss = 0.44153497\n",
      "Iteration 328, loss = 0.44099101\n",
      "Iteration 329, loss = 0.44044842\n",
      "Iteration 330, loss = 0.43990719\n",
      "Iteration 331, loss = 0.43936731\n",
      "Iteration 332, loss = 0.43882875\n",
      "Iteration 333, loss = 0.43829150\n",
      "Iteration 334, loss = 0.43775555\n",
      "Iteration 335, loss = 0.43722087\n",
      "Iteration 336, loss = 0.43668786\n",
      "Iteration 337, loss = 0.43615621\n",
      "Iteration 338, loss = 0.43562581\n",
      "Iteration 339, loss = 0.43509665\n",
      "Iteration 340, loss = 0.43456871\n",
      "Iteration 341, loss = 0.43404196\n",
      "Iteration 342, loss = 0.43351640\n",
      "Iteration 343, loss = 0.43299201\n",
      "Iteration 344, loss = 0.43246877\n",
      "Iteration 345, loss = 0.43194667\n",
      "Iteration 346, loss = 0.43142569\n",
      "Iteration 347, loss = 0.43090583\n",
      "Iteration 348, loss = 0.43038706\n",
      "Iteration 349, loss = 0.42986949\n",
      "Iteration 350, loss = 0.42935321\n",
      "Iteration 351, loss = 0.42883800\n",
      "Iteration 352, loss = 0.42832385\n",
      "Iteration 353, loss = 0.42781077\n",
      "Iteration 354, loss = 0.42729872\n",
      "Iteration 355, loss = 0.42678771\n",
      "Iteration 356, loss = 0.42627772\n",
      "Iteration 357, loss = 0.42576874\n",
      "Iteration 358, loss = 0.42526076\n",
      "Iteration 359, loss = 0.42475378\n",
      "Iteration 360, loss = 0.42424777\n",
      "Iteration 361, loss = 0.42374274\n",
      "Iteration 362, loss = 0.42323866\n",
      "Iteration 363, loss = 0.42273554\n",
      "Iteration 364, loss = 0.42223337\n",
      "Iteration 365, loss = 0.42173213\n",
      "Iteration 366, loss = 0.42123181\n",
      "Iteration 367, loss = 0.42073242\n",
      "Iteration 368, loss = 0.42023393\n",
      "Iteration 369, loss = 0.41973635\n",
      "Iteration 370, loss = 0.41923965\n",
      "Iteration 371, loss = 0.41874385\n",
      "Iteration 372, loss = 0.41824892\n",
      "Iteration 373, loss = 0.41775486\n",
      "Iteration 374, loss = 0.41726166\n",
      "Iteration 375, loss = 0.41676931\n",
      "Iteration 376, loss = 0.41627781\n",
      "Iteration 377, loss = 0.41578716\n",
      "Iteration 378, loss = 0.41529733\n",
      "Iteration 379, loss = 0.41480833\n",
      "Iteration 380, loss = 0.41432015\n",
      "Iteration 381, loss = 0.41383278\n",
      "Iteration 382, loss = 0.41334621\n",
      "Iteration 383, loss = 0.41286044\n",
      "Iteration 384, loss = 0.41237546\n",
      "Iteration 385, loss = 0.41189127\n",
      "Iteration 386, loss = 0.41140786\n",
      "Iteration 387, loss = 0.41092522\n",
      "Iteration 388, loss = 0.41044334\n",
      "Iteration 389, loss = 0.40996223\n",
      "Iteration 390, loss = 0.40948187\n",
      "Iteration 391, loss = 0.40900226\n",
      "Iteration 392, loss = 0.40852339\n",
      "Iteration 393, loss = 0.40804525\n",
      "Iteration 394, loss = 0.40756785\n",
      "Iteration 395, loss = 0.40709117\n",
      "Iteration 396, loss = 0.40661522\n",
      "Iteration 397, loss = 0.40613997\n",
      "Iteration 398, loss = 0.40566544\n",
      "Iteration 399, loss = 0.40519183\n",
      "Iteration 400, loss = 0.40471959\n",
      "Iteration 401, loss = 0.40424809\n",
      "Iteration 402, loss = 0.40377733\n",
      "Iteration 403, loss = 0.40330730\n",
      "Iteration 404, loss = 0.40283861\n",
      "Iteration 405, loss = 0.40237265\n",
      "Iteration 406, loss = 0.40190733\n",
      "Iteration 407, loss = 0.40144270\n",
      "Iteration 408, loss = 0.40097879\n",
      "Iteration 409, loss = 0.40051562\n",
      "Iteration 410, loss = 0.40005322\n",
      "Iteration 411, loss = 0.39959159\n",
      "Iteration 412, loss = 0.39913073\n",
      "Iteration 413, loss = 0.39867065\n",
      "Iteration 414, loss = 0.39821134\n",
      "Iteration 415, loss = 0.39775279\n",
      "Iteration 416, loss = 0.39729500\n",
      "Iteration 417, loss = 0.39683795\n",
      "Iteration 418, loss = 0.39638164\n",
      "Iteration 419, loss = 0.39592607\n",
      "Iteration 420, loss = 0.39547122\n",
      "Iteration 421, loss = 0.39501709\n",
      "Iteration 422, loss = 0.39456368\n",
      "Iteration 423, loss = 0.39411098\n",
      "Iteration 424, loss = 0.39365898\n",
      "Iteration 425, loss = 0.39320768\n",
      "Iteration 426, loss = 0.39275707\n",
      "Iteration 427, loss = 0.39230714\n",
      "Iteration 428, loss = 0.39185787\n",
      "Iteration 429, loss = 0.39140928\n",
      "Iteration 430, loss = 0.39096133\n",
      "Iteration 431, loss = 0.39051403\n",
      "Iteration 432, loss = 0.39006737\n",
      "Iteration 433, loss = 0.38962134\n",
      "Iteration 434, loss = 0.38917593\n",
      "Iteration 435, loss = 0.38873135\n",
      "Iteration 436, loss = 0.38828757\n",
      "Iteration 437, loss = 0.38784460\n",
      "Iteration 438, loss = 0.38740222\n",
      "Iteration 439, loss = 0.38696046\n",
      "Iteration 440, loss = 0.38651931\n",
      "Iteration 441, loss = 0.38607879\n",
      "Iteration 442, loss = 0.38563890\n",
      "Iteration 443, loss = 0.38519965\n",
      "Iteration 444, loss = 0.38476104\n",
      "Iteration 445, loss = 0.38432305\n",
      "Iteration 446, loss = 0.38388569\n",
      "Iteration 447, loss = 0.38344894\n",
      "Iteration 448, loss = 0.38301281\n",
      "Iteration 449, loss = 0.38257728\n",
      "Iteration 450, loss = 0.38214234\n",
      "Iteration 451, loss = 0.38170801\n",
      "Iteration 452, loss = 0.38127426\n",
      "Iteration 453, loss = 0.38084109\n",
      "Iteration 454, loss = 0.38040851\n",
      "Iteration 455, loss = 0.37997650\n",
      "Iteration 456, loss = 0.37954507\n",
      "Iteration 457, loss = 0.37911420\n",
      "Iteration 458, loss = 0.37868390\n",
      "Iteration 459, loss = 0.37825416\n",
      "Iteration 460, loss = 0.37782498\n",
      "Iteration 461, loss = 0.37739635\n",
      "Iteration 462, loss = 0.37696826\n",
      "Iteration 463, loss = 0.37654072\n",
      "Iteration 464, loss = 0.37611372\n",
      "Iteration 465, loss = 0.37568725\n",
      "Iteration 466, loss = 0.37526130\n",
      "Iteration 467, loss = 0.37483589\n",
      "Iteration 468, loss = 0.37441099\n",
      "Iteration 469, loss = 0.37398661\n",
      "Iteration 470, loss = 0.37356275\n",
      "Iteration 471, loss = 0.37313940\n",
      "Iteration 472, loss = 0.37271655\n",
      "Iteration 473, loss = 0.37229421\n",
      "Iteration 474, loss = 0.37187237\n",
      "Iteration 475, loss = 0.37145102\n",
      "Iteration 476, loss = 0.37103017\n",
      "Iteration 477, loss = 0.37060982\n",
      "Iteration 478, loss = 0.37018994\n",
      "Iteration 479, loss = 0.36977056\n",
      "Iteration 480, loss = 0.36935166\n",
      "Iteration 481, loss = 0.36893323\n",
      "Iteration 482, loss = 0.36851528\n",
      "Iteration 483, loss = 0.36809781\n",
      "Iteration 484, loss = 0.36768081\n",
      "Iteration 485, loss = 0.36726427\n",
      "Iteration 486, loss = 0.36684820\n",
      "Iteration 487, loss = 0.36643260\n",
      "Iteration 488, loss = 0.36601745\n",
      "Iteration 489, loss = 0.36560276\n",
      "Iteration 490, loss = 0.36518853\n",
      "Iteration 491, loss = 0.36477475\n",
      "Iteration 492, loss = 0.36436142\n",
      "Iteration 493, loss = 0.36394854\n",
      "Iteration 494, loss = 0.36353610\n",
      "Iteration 495, loss = 0.36312411\n",
      "Iteration 496, loss = 0.36271256\n",
      "Iteration 497, loss = 0.36230145\n",
      "Iteration 498, loss = 0.36189078\n",
      "Iteration 499, loss = 0.36148054\n",
      "Iteration 500, loss = 0.36107074\n",
      "Iteration 501, loss = 0.36066136\n",
      "Iteration 502, loss = 0.36025242\n",
      "Iteration 503, loss = 0.35984390\n",
      "Iteration 504, loss = 0.35943581\n",
      "Iteration 505, loss = 0.35902814\n",
      "Iteration 506, loss = 0.35862089\n",
      "Iteration 507, loss = 0.35821406\n",
      "Iteration 508, loss = 0.35780782\n",
      "Iteration 509, loss = 0.35740204\n",
      "Iteration 510, loss = 0.35699669\n",
      "Iteration 511, loss = 0.35659177\n",
      "Iteration 512, loss = 0.35618727\n",
      "Iteration 513, loss = 0.35578320\n",
      "Iteration 514, loss = 0.35537954\n",
      "Iteration 515, loss = 0.35497631\n",
      "Iteration 516, loss = 0.35457349\n",
      "Iteration 517, loss = 0.35417108\n",
      "Iteration 518, loss = 0.35376909\n",
      "Iteration 519, loss = 0.35336751\n",
      "Iteration 520, loss = 0.35296633\n",
      "Iteration 521, loss = 0.35256557\n",
      "Iteration 522, loss = 0.35216521\n",
      "Iteration 523, loss = 0.35176525\n",
      "Iteration 524, loss = 0.35136569\n",
      "Iteration 525, loss = 0.35096657\n",
      "Iteration 526, loss = 0.35056818\n",
      "Iteration 527, loss = 0.35017020\n",
      "Iteration 528, loss = 0.34977263\n",
      "Iteration 529, loss = 0.34937548\n",
      "Iteration 530, loss = 0.34897874\n",
      "Iteration 531, loss = 0.34858241\n",
      "Iteration 532, loss = 0.34818649\n",
      "Iteration 533, loss = 0.34779098\n",
      "Iteration 534, loss = 0.34739588\n",
      "Iteration 535, loss = 0.34700119\n",
      "Iteration 536, loss = 0.34660689\n",
      "Iteration 537, loss = 0.34621300\n",
      "Iteration 538, loss = 0.34581951\n",
      "Iteration 539, loss = 0.34542641\n",
      "Iteration 540, loss = 0.34503372\n",
      "Iteration 541, loss = 0.34464141\n",
      "Iteration 542, loss = 0.34424950\n",
      "Iteration 543, loss = 0.34385804\n",
      "Iteration 544, loss = 0.34346713\n",
      "Iteration 545, loss = 0.34307662\n",
      "Iteration 546, loss = 0.34268651\n",
      "Iteration 547, loss = 0.34229680\n",
      "Iteration 548, loss = 0.34190749\n",
      "Iteration 549, loss = 0.34151858\n",
      "Iteration 550, loss = 0.34113006\n",
      "Iteration 551, loss = 0.34074194\n",
      "Iteration 552, loss = 0.34035421\n",
      "Iteration 553, loss = 0.33996687\n",
      "Iteration 554, loss = 0.33957993\n",
      "Iteration 555, loss = 0.33919337\n",
      "Iteration 556, loss = 0.33880719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 557, loss = 0.33842141\n",
      "Iteration 558, loss = 0.33803600\n",
      "Iteration 559, loss = 0.33765098\n",
      "Iteration 560, loss = 0.33726634\n",
      "Iteration 561, loss = 0.33688208\n",
      "Iteration 562, loss = 0.33649820\n",
      "Iteration 563, loss = 0.33611470\n",
      "Iteration 564, loss = 0.33573158\n",
      "Iteration 565, loss = 0.33534883\n",
      "Iteration 566, loss = 0.33496645\n",
      "Iteration 567, loss = 0.33458445\n",
      "Iteration 568, loss = 0.33420282\n",
      "Iteration 569, loss = 0.33382157\n",
      "Iteration 570, loss = 0.33344068\n",
      "Iteration 571, loss = 0.33306017\n",
      "Iteration 572, loss = 0.33268002\n",
      "Iteration 573, loss = 0.33230025\n",
      "Iteration 574, loss = 0.33192084\n",
      "Iteration 575, loss = 0.33154180\n",
      "Iteration 576, loss = 0.33116312\n",
      "Iteration 577, loss = 0.33078482\n",
      "Iteration 578, loss = 0.33040687\n",
      "Iteration 579, loss = 0.33002930\n",
      "Iteration 580, loss = 0.32965208\n",
      "Iteration 581, loss = 0.32927523\n",
      "Iteration 582, loss = 0.32889875\n",
      "Iteration 583, loss = 0.32852262\n",
      "Iteration 584, loss = 0.32814686\n",
      "Iteration 585, loss = 0.32777146\n",
      "Iteration 586, loss = 0.32739643\n",
      "Iteration 587, loss = 0.32702175\n",
      "Iteration 588, loss = 0.32664743\n",
      "Iteration 589, loss = 0.32627348\n",
      "Iteration 590, loss = 0.32589988\n",
      "Iteration 591, loss = 0.32552665\n",
      "Iteration 592, loss = 0.32515377\n",
      "Iteration 593, loss = 0.32478125\n",
      "Iteration 594, loss = 0.32440909\n",
      "Iteration 595, loss = 0.32403729\n",
      "Iteration 596, loss = 0.32366585\n",
      "Iteration 597, loss = 0.32329477\n",
      "Iteration 598, loss = 0.32292432\n",
      "Iteration 599, loss = 0.32255434\n",
      "Iteration 600, loss = 0.32218474\n",
      "Iteration 601, loss = 0.32181552\n",
      "Iteration 602, loss = 0.32144668\n",
      "Iteration 603, loss = 0.32107820\n",
      "Iteration 604, loss = 0.32071011\n",
      "Iteration 605, loss = 0.32034239\n",
      "Iteration 606, loss = 0.31997504\n",
      "Iteration 607, loss = 0.31960806\n",
      "Iteration 608, loss = 0.31924145\n",
      "Iteration 609, loss = 0.31887521\n",
      "Iteration 610, loss = 0.31850934\n",
      "Iteration 611, loss = 0.31814384\n",
      "Iteration 612, loss = 0.31777870\n",
      "Iteration 613, loss = 0.31741393\n",
      "Iteration 614, loss = 0.31704952\n",
      "Iteration 615, loss = 0.31668548\n",
      "Iteration 616, loss = 0.31632180\n",
      "Iteration 617, loss = 0.31595849\n",
      "Iteration 618, loss = 0.31559554\n",
      "Iteration 619, loss = 0.31523295\n",
      "Iteration 620, loss = 0.31487088\n",
      "Iteration 621, loss = 0.31450926\n",
      "Iteration 622, loss = 0.31414801\n",
      "Iteration 623, loss = 0.31378714\n",
      "Iteration 624, loss = 0.31342664\n",
      "Iteration 625, loss = 0.31306652\n",
      "Iteration 626, loss = 0.31270677\n",
      "Iteration 627, loss = 0.31234739\n",
      "Iteration 628, loss = 0.31198838\n",
      "Iteration 629, loss = 0.31162974\n",
      "Iteration 630, loss = 0.31127148\n",
      "Iteration 631, loss = 0.31091358\n",
      "Iteration 632, loss = 0.31055605\n",
      "Iteration 633, loss = 0.31019889\n",
      "Iteration 634, loss = 0.30984210\n",
      "Iteration 635, loss = 0.30948568\n",
      "Iteration 636, loss = 0.30912962\n",
      "Iteration 637, loss = 0.30877393\n",
      "Iteration 638, loss = 0.30841860\n",
      "Iteration 639, loss = 0.30806364\n",
      "Iteration 640, loss = 0.30770904\n",
      "Iteration 641, loss = 0.30735481\n",
      "Iteration 642, loss = 0.30700095\n",
      "Iteration 643, loss = 0.30664744\n",
      "Iteration 644, loss = 0.30629431\n",
      "Iteration 645, loss = 0.30594153\n",
      "Iteration 646, loss = 0.30558912\n",
      "Iteration 647, loss = 0.30523708\n",
      "Iteration 648, loss = 0.30488540\n",
      "Iteration 649, loss = 0.30453408\n",
      "Iteration 650, loss = 0.30418312\n",
      "Iteration 651, loss = 0.30383253\n",
      "Iteration 652, loss = 0.30348230\n",
      "Iteration 653, loss = 0.30313243\n",
      "Iteration 654, loss = 0.30278293\n",
      "Iteration 655, loss = 0.30243379\n",
      "Iteration 656, loss = 0.30208501\n",
      "Iteration 657, loss = 0.30173660\n",
      "Iteration 658, loss = 0.30138855\n",
      "Iteration 659, loss = 0.30104086\n",
      "Iteration 660, loss = 0.30069354\n",
      "Iteration 661, loss = 0.30034657\n",
      "Iteration 662, loss = 0.29999998\n",
      "Iteration 663, loss = 0.29965374\n",
      "Iteration 664, loss = 0.29930787\n",
      "Iteration 665, loss = 0.29896236\n",
      "Iteration 666, loss = 0.29861722\n",
      "Iteration 667, loss = 0.29827244\n",
      "Iteration 668, loss = 0.29792802\n",
      "Iteration 669, loss = 0.29758397\n",
      "Iteration 670, loss = 0.29724028\n",
      "Iteration 671, loss = 0.29689696\n",
      "Iteration 672, loss = 0.29655400\n",
      "Iteration 673, loss = 0.29621141\n",
      "Iteration 674, loss = 0.29586918\n",
      "Iteration 675, loss = 0.29552731\n",
      "Iteration 676, loss = 0.29518581\n",
      "Iteration 677, loss = 0.29484468\n",
      "Iteration 678, loss = 0.29450391\n",
      "Iteration 679, loss = 0.29416350\n",
      "Iteration 680, loss = 0.29382347\n",
      "Iteration 681, loss = 0.29348379\n",
      "Iteration 682, loss = 0.29314449\n",
      "Iteration 683, loss = 0.29280555\n",
      "Iteration 684, loss = 0.29246697\n",
      "Iteration 685, loss = 0.29212877\n",
      "Iteration 686, loss = 0.29179093\n",
      "Iteration 687, loss = 0.29145345\n",
      "Iteration 688, loss = 0.29111635\n",
      "Iteration 689, loss = 0.29077961\n",
      "Iteration 690, loss = 0.29044324\n",
      "Iteration 691, loss = 0.29010724\n",
      "Iteration 692, loss = 0.28977160\n",
      "Iteration 693, loss = 0.28943633\n",
      "Iteration 694, loss = 0.28910144\n",
      "Iteration 695, loss = 0.28876691\n",
      "Iteration 696, loss = 0.28843274\n",
      "Iteration 697, loss = 0.28809895\n",
      "Iteration 698, loss = 0.28776553\n",
      "Iteration 699, loss = 0.28743248\n",
      "Iteration 700, loss = 0.28709979\n",
      "Iteration 701, loss = 0.28676748\n",
      "Iteration 702, loss = 0.28643554\n",
      "Iteration 703, loss = 0.28610396\n",
      "Iteration 704, loss = 0.28577276\n",
      "Iteration 705, loss = 0.28544193\n",
      "Iteration 706, loss = 0.28511147\n",
      "Iteration 707, loss = 0.28478138\n",
      "Iteration 708, loss = 0.28445166\n",
      "Iteration 709, loss = 0.28412232\n",
      "Iteration 710, loss = 0.28379334\n",
      "Iteration 711, loss = 0.28346474\n",
      "Iteration 712, loss = 0.28313651\n",
      "Iteration 713, loss = 0.28280866\n",
      "Iteration 714, loss = 0.28248117\n",
      "Iteration 715, loss = 0.28215406\n",
      "Iteration 716, loss = 0.28182732\n",
      "Iteration 717, loss = 0.28150096\n",
      "Iteration 718, loss = 0.28117497\n",
      "Iteration 719, loss = 0.28084935\n",
      "Iteration 720, loss = 0.28052411\n",
      "Iteration 721, loss = 0.28019925\n",
      "Iteration 722, loss = 0.27987475\n",
      "Iteration 723, loss = 0.27955064\n",
      "Iteration 724, loss = 0.27922689\n",
      "Iteration 725, loss = 0.27890353\n",
      "Iteration 726, loss = 0.27858054\n",
      "Iteration 727, loss = 0.27825792\n",
      "Iteration 728, loss = 0.27793568\n",
      "Iteration 729, loss = 0.27761382\n",
      "Iteration 730, loss = 0.27729233\n",
      "Iteration 731, loss = 0.27697122\n",
      "Iteration 732, loss = 0.27665049\n",
      "Iteration 733, loss = 0.27633014\n",
      "Iteration 734, loss = 0.27601016\n",
      "Iteration 735, loss = 0.27569056\n",
      "Iteration 736, loss = 0.27537134\n",
      "Iteration 737, loss = 0.27505250\n",
      "Iteration 738, loss = 0.27473403\n",
      "Iteration 739, loss = 0.27441594\n",
      "Iteration 740, loss = 0.27409824\n",
      "Iteration 741, loss = 0.27378091\n",
      "Iteration 742, loss = 0.27346396\n",
      "Iteration 743, loss = 0.27314739\n",
      "Iteration 744, loss = 0.27283120\n",
      "Iteration 745, loss = 0.27251539\n",
      "Iteration 746, loss = 0.27219996\n",
      "Iteration 747, loss = 0.27188491\n",
      "Iteration 748, loss = 0.27157024\n",
      "Iteration 749, loss = 0.27125595\n",
      "Iteration 750, loss = 0.27094205\n",
      "Iteration 751, loss = 0.27062852\n",
      "Iteration 752, loss = 0.27031538\n",
      "Iteration 753, loss = 0.27000261\n",
      "Iteration 754, loss = 0.26969023\n",
      "Iteration 755, loss = 0.26937823\n",
      "Iteration 756, loss = 0.26906662\n",
      "Iteration 757, loss = 0.26875538\n",
      "Iteration 758, loss = 0.26844453\n",
      "Iteration 759, loss = 0.26813406\n",
      "Iteration 760, loss = 0.26782398\n",
      "Iteration 761, loss = 0.26751428\n",
      "Iteration 762, loss = 0.26720496\n",
      "Iteration 763, loss = 0.26689602\n",
      "Iteration 764, loss = 0.26658747\n",
      "Iteration 765, loss = 0.26627930\n",
      "Iteration 766, loss = 0.26597152\n",
      "Iteration 767, loss = 0.26566412\n",
      "Iteration 768, loss = 0.26535710\n",
      "Iteration 769, loss = 0.26505047\n",
      "Iteration 770, loss = 0.26474423\n",
      "Iteration 771, loss = 0.26443837\n",
      "Iteration 772, loss = 0.26413289\n",
      "Iteration 773, loss = 0.26382780\n",
      "Iteration 774, loss = 0.26352310\n",
      "Iteration 775, loss = 0.26321878\n",
      "Iteration 776, loss = 0.26291485\n",
      "Iteration 777, loss = 0.26261130\n",
      "Iteration 778, loss = 0.26230814\n",
      "Iteration 779, loss = 0.26200537\n",
      "Iteration 780, loss = 0.26170298\n",
      "Iteration 781, loss = 0.26140098\n",
      "Iteration 782, loss = 0.26109937\n",
      "Iteration 783, loss = 0.26079814\n",
      "Iteration 784, loss = 0.26049730\n",
      "Iteration 785, loss = 0.26019685\n",
      "Iteration 786, loss = 0.25989678\n",
      "Iteration 787, loss = 0.25959710\n",
      "Iteration 788, loss = 0.25929781\n",
      "Iteration 789, loss = 0.25899891\n",
      "Iteration 790, loss = 0.25870040\n",
      "Iteration 791, loss = 0.25840227\n",
      "Iteration 792, loss = 0.25810453\n",
      "Iteration 793, loss = 0.25780718\n",
      "Iteration 794, loss = 0.25751022\n",
      "Iteration 795, loss = 0.25721364\n",
      "Iteration 796, loss = 0.25691746\n",
      "Iteration 797, loss = 0.25662166\n",
      "Iteration 798, loss = 0.25632626\n",
      "Iteration 799, loss = 0.25603124\n",
      "Iteration 800, loss = 0.25573661\n",
      "Iteration 801, loss = 0.25544237\n",
      "Iteration 802, loss = 0.25514852\n",
      "Iteration 803, loss = 0.25485505\n",
      "Iteration 804, loss = 0.25456198\n",
      "Iteration 805, loss = 0.25426930\n",
      "Iteration 806, loss = 0.25397701\n",
      "Iteration 807, loss = 0.25368510\n",
      "Iteration 808, loss = 0.25339359\n",
      "Iteration 809, loss = 0.25310247\n",
      "Iteration 810, loss = 0.25281173\n",
      "Iteration 811, loss = 0.25252139\n",
      "Iteration 812, loss = 0.25223143\n",
      "Iteration 813, loss = 0.25194187\n",
      "Iteration 814, loss = 0.25165270\n",
      "Iteration 815, loss = 0.25136391\n",
      "Iteration 816, loss = 0.25107552\n",
      "Iteration 817, loss = 0.25078752\n",
      "Iteration 818, loss = 0.25049990\n",
      "Iteration 819, loss = 0.25021268\n",
      "Iteration 820, loss = 0.24992585\n",
      "Iteration 821, loss = 0.24963941\n",
      "Iteration 822, loss = 0.24935336\n",
      "Iteration 823, loss = 0.24906770\n",
      "Iteration 824, loss = 0.24878243\n",
      "Iteration 825, loss = 0.24849756\n",
      "Iteration 826, loss = 0.24821307\n",
      "Iteration 827, loss = 0.24792897\n",
      "Iteration 828, loss = 0.24764527\n",
      "Iteration 829, loss = 0.24736195\n",
      "Iteration 830, loss = 0.24707903\n",
      "Iteration 831, loss = 0.24679650\n",
      "Iteration 832, loss = 0.24651435\n",
      "Iteration 833, loss = 0.24623260\n",
      "Iteration 834, loss = 0.24595124\n",
      "Iteration 835, loss = 0.24567027\n",
      "Iteration 836, loss = 0.24538969\n",
      "Iteration 837, loss = 0.24510951\n",
      "Iteration 838, loss = 0.24482971\n",
      "Iteration 839, loss = 0.24455031\n",
      "Iteration 840, loss = 0.24427129\n",
      "Iteration 841, loss = 0.24399267\n",
      "Iteration 842, loss = 0.24371443\n",
      "Iteration 843, loss = 0.24343659\n",
      "Iteration 844, loss = 0.24315914\n",
      "Iteration 845, loss = 0.24288208\n",
      "Iteration 846, loss = 0.24260541\n",
      "Iteration 847, loss = 0.24232913\n",
      "Iteration 848, loss = 0.24205325\n",
      "Iteration 849, loss = 0.24177775\n",
      "Iteration 850, loss = 0.24150264\n",
      "Iteration 851, loss = 0.24122793\n",
      "Iteration 852, loss = 0.24095360\n",
      "Iteration 853, loss = 0.24067967\n",
      "Iteration 854, loss = 0.24040612\n",
      "Iteration 855, loss = 0.24013297\n",
      "Iteration 856, loss = 0.23986021\n",
      "Iteration 857, loss = 0.23958784\n",
      "Iteration 858, loss = 0.23931585\n",
      "Iteration 859, loss = 0.23904426\n",
      "Iteration 860, loss = 0.23877306\n",
      "Iteration 861, loss = 0.23850235\n",
      "Iteration 862, loss = 0.23823224\n",
      "Iteration 863, loss = 0.23796253\n",
      "Iteration 864, loss = 0.23769322\n",
      "Iteration 865, loss = 0.23742432\n",
      "Iteration 866, loss = 0.23715582\n",
      "Iteration 867, loss = 0.23688772\n",
      "Iteration 868, loss = 0.23662002\n",
      "Iteration 869, loss = 0.23635271\n",
      "Iteration 870, loss = 0.23608581\n",
      "Iteration 871, loss = 0.23581930\n",
      "Iteration 872, loss = 0.23555318\n",
      "Iteration 873, loss = 0.23528746\n",
      "Iteration 874, loss = 0.23502213\n",
      "Iteration 875, loss = 0.23475719\n",
      "Iteration 876, loss = 0.23449265\n",
      "Iteration 877, loss = 0.23422850\n",
      "Iteration 878, loss = 0.23396474\n",
      "Iteration 879, loss = 0.23370137\n",
      "Iteration 880, loss = 0.23343839\n",
      "Iteration 881, loss = 0.23317580\n",
      "Iteration 882, loss = 0.23291360\n",
      "Iteration 883, loss = 0.23265179\n",
      "Iteration 884, loss = 0.23239037\n",
      "Iteration 885, loss = 0.23212933\n",
      "Iteration 886, loss = 0.23186869\n",
      "Iteration 887, loss = 0.23160843\n",
      "Iteration 888, loss = 0.23134856\n",
      "Iteration 889, loss = 0.23108908\n",
      "Iteration 890, loss = 0.23082998\n",
      "Iteration 891, loss = 0.23057128\n",
      "Iteration 892, loss = 0.23031295\n",
      "Iteration 893, loss = 0.23005502\n",
      "Iteration 894, loss = 0.22979746\n",
      "Iteration 895, loss = 0.22954030\n",
      "Iteration 896, loss = 0.22928352\n",
      "Iteration 897, loss = 0.22902712\n",
      "Iteration 898, loss = 0.22877111\n",
      "Iteration 899, loss = 0.22851549\n",
      "Iteration 900, loss = 0.22826025\n",
      "Iteration 901, loss = 0.22800539\n",
      "Iteration 902, loss = 0.22775092\n",
      "Iteration 903, loss = 0.22749683\n",
      "Iteration 904, loss = 0.22724312\n",
      "Iteration 905, loss = 0.22698980\n",
      "Iteration 906, loss = 0.22673686\n",
      "Iteration 907, loss = 0.22648430\n",
      "Iteration 908, loss = 0.22623212\n",
      "Iteration 909, loss = 0.22598033\n",
      "Iteration 910, loss = 0.22572892\n",
      "Iteration 911, loss = 0.22547789\n",
      "Iteration 912, loss = 0.22522725\n",
      "Iteration 913, loss = 0.22497698\n",
      "Iteration 914, loss = 0.22472710\n",
      "Iteration 915, loss = 0.22447759\n",
      "Iteration 916, loss = 0.22422847\n",
      "Iteration 917, loss = 0.22397973\n",
      "Iteration 918, loss = 0.22373137\n",
      "Iteration 919, loss = 0.22348339\n",
      "Iteration 920, loss = 0.22323579\n",
      "Iteration 921, loss = 0.22298856\n",
      "Iteration 922, loss = 0.22274172\n",
      "Iteration 923, loss = 0.22249526\n",
      "Iteration 924, loss = 0.22224917\n",
      "Iteration 925, loss = 0.22200347\n",
      "Iteration 926, loss = 0.22175814\n",
      "Iteration 927, loss = 0.22151319\n",
      "Iteration 928, loss = 0.22126862\n",
      "Iteration 929, loss = 0.22102443\n",
      "Iteration 930, loss = 0.22078061\n",
      "Iteration 931, loss = 0.22053718\n",
      "Iteration 932, loss = 0.22029411\n",
      "Iteration 933, loss = 0.22005143\n",
      "Iteration 934, loss = 0.21980912\n",
      "Iteration 935, loss = 0.21956719\n",
      "Iteration 936, loss = 0.21932563\n",
      "Iteration 937, loss = 0.21908445\n",
      "Iteration 938, loss = 0.21884365\n",
      "Iteration 939, loss = 0.21860322\n",
      "Iteration 940, loss = 0.21836316\n",
      "Iteration 941, loss = 0.21812348\n",
      "Iteration 942, loss = 0.21788418\n",
      "Iteration 943, loss = 0.21764525\n",
      "Iteration 944, loss = 0.21740669\n",
      "Iteration 945, loss = 0.21716850\n",
      "Iteration 946, loss = 0.21693069\n",
      "Iteration 947, loss = 0.21669325\n",
      "Iteration 948, loss = 0.21645619\n",
      "Iteration 949, loss = 0.21621949\n",
      "Iteration 950, loss = 0.21598317\n",
      "Iteration 951, loss = 0.21574722\n",
      "Iteration 952, loss = 0.21551165\n",
      "Iteration 953, loss = 0.21527644\n",
      "Iteration 954, loss = 0.21504161\n",
      "Iteration 955, loss = 0.21480714\n",
      "Iteration 956, loss = 0.21457305\n",
      "Iteration 957, loss = 0.21433932\n",
      "Iteration 958, loss = 0.21410597\n",
      "Iteration 959, loss = 0.21387298\n",
      "Iteration 960, loss = 0.21364037\n",
      "Iteration 961, loss = 0.21340812\n",
      "Iteration 962, loss = 0.21317624\n",
      "Iteration 963, loss = 0.21294473\n",
      "Iteration 964, loss = 0.21271359\n",
      "Iteration 965, loss = 0.21248281\n",
      "Iteration 966, loss = 0.21225241\n",
      "Iteration 967, loss = 0.21202237\n",
      "Iteration 968, loss = 0.21179269\n",
      "Iteration 969, loss = 0.21156338\n",
      "Iteration 970, loss = 0.21133444\n",
      "Iteration 971, loss = 0.21110587\n",
      "Iteration 972, loss = 0.21087766\n",
      "Iteration 973, loss = 0.21064981\n",
      "Iteration 974, loss = 0.21042233\n",
      "Iteration 975, loss = 0.21019521\n",
      "Iteration 976, loss = 0.20996846\n",
      "Iteration 977, loss = 0.20974207\n",
      "Iteration 978, loss = 0.20951605\n",
      "Iteration 979, loss = 0.20929038\n",
      "Iteration 980, loss = 0.20906508\n",
      "Iteration 981, loss = 0.20884015\n",
      "Iteration 982, loss = 0.20861557\n",
      "Iteration 983, loss = 0.20839136\n",
      "Iteration 984, loss = 0.20816751\n",
      "Iteration 985, loss = 0.20794401\n",
      "Iteration 986, loss = 0.20772088\n",
      "Iteration 987, loss = 0.20749811\n",
      "Iteration 988, loss = 0.20727570\n",
      "Iteration 989, loss = 0.20705365\n",
      "Iteration 990, loss = 0.20683196\n",
      "Iteration 991, loss = 0.20661063\n",
      "Iteration 992, loss = 0.20638965\n",
      "Iteration 993, loss = 0.20616903\n",
      "Iteration 994, loss = 0.20594878\n",
      "Iteration 995, loss = 0.20572887\n",
      "Iteration 996, loss = 0.20550933\n",
      "Iteration 997, loss = 0.20529014\n",
      "Iteration 998, loss = 0.20507131\n",
      "Iteration 999, loss = 0.20485284\n",
      "Iteration 1000, loss = 0.20463472\n",
      "Iteration 1, loss = 1.50041490\n",
      "Iteration 2, loss = 1.47049154\n",
      "Iteration 3, loss = 1.43024280\n",
      "Iteration 4, loss = 1.38318168\n",
      "Iteration 5, loss = 1.33269593\n",
      "Iteration 6, loss = 1.28165095\n",
      "Iteration 7, loss = 1.23228800\n",
      "Iteration 8, loss = 1.18581140\n",
      "Iteration 9, loss = 1.14267257\n",
      "Iteration 10, loss = 1.10296769\n",
      "Iteration 11, loss = 1.06674396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 1.03330549\n",
      "Iteration 13, loss = 1.00205547\n",
      "Iteration 14, loss = 0.97280295\n",
      "Iteration 15, loss = 0.94553057\n",
      "Iteration 16, loss = 0.92027485\n",
      "Iteration 17, loss = 0.89703053\n",
      "Iteration 18, loss = 0.87582734\n",
      "Iteration 19, loss = 0.85655658\n",
      "Iteration 20, loss = 0.83926182\n",
      "Iteration 21, loss = 0.82386005\n",
      "Iteration 22, loss = 0.81040691\n",
      "Iteration 23, loss = 0.79903878\n",
      "Iteration 24, loss = 0.78977439\n",
      "Iteration 25, loss = 0.78246551\n",
      "Iteration 26, loss = 0.77680189\n",
      "Iteration 27, loss = 0.77255082\n",
      "Iteration 28, loss = 0.76952213\n",
      "Iteration 29, loss = 0.76747475\n",
      "Iteration 30, loss = 0.76603297\n",
      "Iteration 31, loss = 0.76498289\n",
      "Iteration 32, loss = 0.76415972\n",
      "Iteration 33, loss = 0.76336705\n",
      "Iteration 34, loss = 0.76249580\n",
      "Iteration 35, loss = 0.76144275\n",
      "Iteration 36, loss = 0.76018769\n",
      "Iteration 37, loss = 0.75869577\n",
      "Iteration 38, loss = 0.75695990\n",
      "Iteration 39, loss = 0.75499849\n",
      "Iteration 40, loss = 0.75283528\n",
      "Iteration 41, loss = 0.75048710\n",
      "Iteration 42, loss = 0.74804652\n",
      "Iteration 43, loss = 0.74553513\n",
      "Iteration 44, loss = 0.74296229\n",
      "Iteration 45, loss = 0.74038160\n",
      "Iteration 46, loss = 0.73779745\n",
      "Iteration 47, loss = 0.73524950\n",
      "Iteration 48, loss = 0.73281060\n",
      "Iteration 49, loss = 0.73044613\n",
      "Iteration 50, loss = 0.72817813\n",
      "Iteration 51, loss = 0.72602618\n",
      "Iteration 52, loss = 0.72394803\n",
      "Iteration 53, loss = 0.72194815\n",
      "Iteration 54, loss = 0.72001166\n",
      "Iteration 55, loss = 0.71811248\n",
      "Iteration 56, loss = 0.71625540\n",
      "Iteration 57, loss = 0.71443071\n",
      "Iteration 58, loss = 0.71261796\n",
      "Iteration 59, loss = 0.71081420\n",
      "Iteration 60, loss = 0.70901699\n",
      "Iteration 61, loss = 0.70722433\n",
      "Iteration 62, loss = 0.70543473\n",
      "Iteration 63, loss = 0.70364715\n",
      "Iteration 64, loss = 0.70186095\n",
      "Iteration 65, loss = 0.70007589\n",
      "Iteration 66, loss = 0.69829203\n",
      "Iteration 67, loss = 0.69650969\n",
      "Iteration 68, loss = 0.69472939\n",
      "Iteration 69, loss = 0.69295259\n",
      "Iteration 70, loss = 0.69118067\n",
      "Iteration 71, loss = 0.68941295\n",
      "Iteration 72, loss = 0.68765897\n",
      "Iteration 73, loss = 0.68591402\n",
      "Iteration 74, loss = 0.68418209\n",
      "Iteration 75, loss = 0.68246483\n",
      "Iteration 76, loss = 0.68075789\n",
      "Iteration 77, loss = 0.67906094\n",
      "Iteration 78, loss = 0.67738215\n",
      "Iteration 79, loss = 0.67571606\n",
      "Iteration 80, loss = 0.67406482\n",
      "Iteration 81, loss = 0.67242306\n",
      "Iteration 82, loss = 0.67079159\n",
      "Iteration 83, loss = 0.66917350\n",
      "Iteration 84, loss = 0.66756379\n",
      "Iteration 85, loss = 0.66596235\n",
      "Iteration 86, loss = 0.66436906\n",
      "Iteration 87, loss = 0.66278385\n",
      "Iteration 88, loss = 0.66120665\n",
      "Iteration 89, loss = 0.65963743\n",
      "Iteration 90, loss = 0.65807615\n",
      "Iteration 91, loss = 0.65652281\n",
      "Iteration 92, loss = 0.65497738\n",
      "Iteration 93, loss = 0.65343987\n",
      "Iteration 94, loss = 0.65191028\n",
      "Iteration 95, loss = 0.65038860\n",
      "Iteration 96, loss = 0.64887483\n",
      "Iteration 97, loss = 0.64736898\n",
      "Iteration 98, loss = 0.64587104\n",
      "Iteration 99, loss = 0.64438100\n",
      "Iteration 100, loss = 0.64289885\n",
      "Iteration 101, loss = 0.64142458\n",
      "Iteration 102, loss = 0.63995817\n",
      "Iteration 103, loss = 0.63849960\n",
      "Iteration 104, loss = 0.63704884\n",
      "Iteration 105, loss = 0.63560588\n",
      "Iteration 106, loss = 0.63417069\n",
      "Iteration 107, loss = 0.63274323\n",
      "Iteration 108, loss = 0.63132347\n",
      "Iteration 109, loss = 0.62991139\n",
      "Iteration 110, loss = 0.62850695\n",
      "Iteration 111, loss = 0.62711011\n",
      "Iteration 112, loss = 0.62572085\n",
      "Iteration 113, loss = 0.62433977\n",
      "Iteration 114, loss = 0.62296629\n",
      "Iteration 115, loss = 0.62160034\n",
      "Iteration 116, loss = 0.62024189\n",
      "Iteration 117, loss = 0.61889091\n",
      "Iteration 118, loss = 0.61754736\n",
      "Iteration 119, loss = 0.61621122\n",
      "Iteration 120, loss = 0.61488358\n",
      "Iteration 121, loss = 0.61356356\n",
      "Iteration 122, loss = 0.61225123\n",
      "Iteration 123, loss = 0.61094632\n",
      "Iteration 124, loss = 0.60964867\n",
      "Iteration 125, loss = 0.60835828\n",
      "Iteration 126, loss = 0.60707513\n",
      "Iteration 127, loss = 0.60579920\n",
      "Iteration 128, loss = 0.60453046\n",
      "Iteration 129, loss = 0.60326887\n",
      "Iteration 130, loss = 0.60201441\n",
      "Iteration 131, loss = 0.60076704\n",
      "Iteration 132, loss = 0.59952671\n",
      "Iteration 133, loss = 0.59829339\n",
      "Iteration 134, loss = 0.59706704\n",
      "Iteration 135, loss = 0.59584761\n",
      "Iteration 136, loss = 0.59463505\n",
      "Iteration 137, loss = 0.59342932\n",
      "Iteration 138, loss = 0.59223038\n",
      "Iteration 139, loss = 0.59103818\n",
      "Iteration 140, loss = 0.58985267\n",
      "Iteration 141, loss = 0.58867382\n",
      "Iteration 142, loss = 0.58750156\n",
      "Iteration 143, loss = 0.58633586\n",
      "Iteration 144, loss = 0.58517667\n",
      "Iteration 145, loss = 0.58402394\n",
      "Iteration 146, loss = 0.58287763\n",
      "Iteration 147, loss = 0.58173769\n",
      "Iteration 148, loss = 0.58060408\n",
      "Iteration 149, loss = 0.57947674\n",
      "Iteration 150, loss = 0.57835563\n",
      "Iteration 151, loss = 0.57724070\n",
      "Iteration 152, loss = 0.57613190\n",
      "Iteration 153, loss = 0.57502920\n",
      "Iteration 154, loss = 0.57393253\n",
      "Iteration 155, loss = 0.57284186\n",
      "Iteration 156, loss = 0.57175817\n",
      "Iteration 157, loss = 0.57068042\n",
      "Iteration 158, loss = 0.56960884\n",
      "Iteration 159, loss = 0.56854367\n",
      "Iteration 160, loss = 0.56748436\n",
      "Iteration 161, loss = 0.56643087\n",
      "Iteration 162, loss = 0.56538318\n",
      "Iteration 163, loss = 0.56434125\n",
      "Iteration 164, loss = 0.56330503\n",
      "Iteration 165, loss = 0.56227449\n",
      "Iteration 166, loss = 0.56124958\n",
      "Iteration 167, loss = 0.56023025\n",
      "Iteration 168, loss = 0.55921646\n",
      "Iteration 169, loss = 0.55820815\n",
      "Iteration 170, loss = 0.55720528\n",
      "Iteration 171, loss = 0.55620779\n",
      "Iteration 172, loss = 0.55521565\n",
      "Iteration 173, loss = 0.55422878\n",
      "Iteration 174, loss = 0.55324716\n",
      "Iteration 175, loss = 0.55227072\n",
      "Iteration 176, loss = 0.55129943\n",
      "Iteration 177, loss = 0.55033323\n",
      "Iteration 178, loss = 0.54937207\n",
      "Iteration 179, loss = 0.54841592\n",
      "Iteration 180, loss = 0.54746472\n",
      "Iteration 181, loss = 0.54651843\n",
      "Iteration 182, loss = 0.54557701\n",
      "Iteration 183, loss = 0.54464041\n",
      "Iteration 184, loss = 0.54370859\n",
      "Iteration 185, loss = 0.54278149\n",
      "Iteration 186, loss = 0.54185909\n",
      "Iteration 187, loss = 0.54094133\n",
      "Iteration 188, loss = 0.54002831\n",
      "Iteration 189, loss = 0.53912046\n",
      "Iteration 190, loss = 0.53821717\n",
      "Iteration 191, loss = 0.53731839\n",
      "Iteration 192, loss = 0.53642408\n",
      "Iteration 193, loss = 0.53553422\n",
      "Iteration 194, loss = 0.53464877\n",
      "Iteration 195, loss = 0.53376768\n",
      "Iteration 196, loss = 0.53289091\n",
      "Iteration 197, loss = 0.53201844\n",
      "Iteration 198, loss = 0.53115021\n",
      "Iteration 199, loss = 0.53028618\n",
      "Iteration 200, loss = 0.52942633\n",
      "Iteration 201, loss = 0.52857060\n",
      "Iteration 202, loss = 0.52771895\n",
      "Iteration 203, loss = 0.52687136\n",
      "Iteration 204, loss = 0.52602776\n",
      "Iteration 205, loss = 0.52518814\n",
      "Iteration 206, loss = 0.52435243\n",
      "Iteration 207, loss = 0.52352062\n",
      "Iteration 208, loss = 0.52269265\n",
      "Iteration 209, loss = 0.52186925\n",
      "Iteration 210, loss = 0.52104968\n",
      "Iteration 211, loss = 0.52023389\n",
      "Iteration 212, loss = 0.51942186\n",
      "Iteration 213, loss = 0.51861355\n",
      "Iteration 214, loss = 0.51780894\n",
      "Iteration 215, loss = 0.51700799\n",
      "Iteration 216, loss = 0.51621067\n",
      "Iteration 217, loss = 0.51541693\n",
      "Iteration 218, loss = 0.51462676\n",
      "Iteration 219, loss = 0.51384010\n",
      "Iteration 220, loss = 0.51305693\n",
      "Iteration 221, loss = 0.51227720\n",
      "Iteration 222, loss = 0.51150089\n",
      "Iteration 223, loss = 0.51072795\n",
      "Iteration 224, loss = 0.50995835\n",
      "Iteration 225, loss = 0.50919206\n",
      "Iteration 226, loss = 0.50842904\n",
      "Iteration 227, loss = 0.50766926\n",
      "Iteration 228, loss = 0.50691268\n",
      "Iteration 229, loss = 0.50615926\n",
      "Iteration 230, loss = 0.50540899\n",
      "Iteration 231, loss = 0.50466181\n",
      "Iteration 232, loss = 0.50391771\n",
      "Iteration 233, loss = 0.50317664\n",
      "Iteration 234, loss = 0.50243858\n",
      "Iteration 235, loss = 0.50170350\n",
      "Iteration 236, loss = 0.50097136\n",
      "Iteration 237, loss = 0.50024214\n",
      "Iteration 238, loss = 0.49951580\n",
      "Iteration 239, loss = 0.49879232\n",
      "Iteration 240, loss = 0.49807167\n",
      "Iteration 241, loss = 0.49735381\n",
      "Iteration 242, loss = 0.49663873\n",
      "Iteration 243, loss = 0.49592639\n",
      "Iteration 244, loss = 0.49521676\n",
      "Iteration 245, loss = 0.49450982\n",
      "Iteration 246, loss = 0.49380553\n",
      "Iteration 247, loss = 0.49310388\n",
      "Iteration 248, loss = 0.49240534\n",
      "Iteration 249, loss = 0.49170991\n",
      "Iteration 250, loss = 0.49101754\n",
      "Iteration 251, loss = 0.49032794\n",
      "Iteration 252, loss = 0.48964094\n",
      "Iteration 253, loss = 0.48895652\n",
      "Iteration 254, loss = 0.48827465\n",
      "Iteration 255, loss = 0.48759530\n",
      "Iteration 256, loss = 0.48691845\n",
      "Iteration 257, loss = 0.48624408\n",
      "Iteration 258, loss = 0.48557214\n",
      "Iteration 259, loss = 0.48490262\n",
      "Iteration 260, loss = 0.48423549\n",
      "Iteration 261, loss = 0.48357072\n",
      "Iteration 262, loss = 0.48290829\n",
      "Iteration 263, loss = 0.48224816\n",
      "Iteration 264, loss = 0.48159032\n",
      "Iteration 265, loss = 0.48093473\n",
      "Iteration 266, loss = 0.48028138\n",
      "Iteration 267, loss = 0.47963023\n",
      "Iteration 268, loss = 0.47898127\n",
      "Iteration 269, loss = 0.47833447\n",
      "Iteration 270, loss = 0.47768981\n",
      "Iteration 271, loss = 0.47704727\n",
      "Iteration 272, loss = 0.47640682\n",
      "Iteration 273, loss = 0.47576845\n",
      "Iteration 274, loss = 0.47513213\n",
      "Iteration 275, loss = 0.47449784\n",
      "Iteration 276, loss = 0.47386556\n",
      "Iteration 277, loss = 0.47323527\n",
      "Iteration 278, loss = 0.47260695\n",
      "Iteration 279, loss = 0.47198059\n",
      "Iteration 280, loss = 0.47135615\n",
      "Iteration 281, loss = 0.47073362\n",
      "Iteration 282, loss = 0.47011298\n",
      "Iteration 283, loss = 0.46949422\n",
      "Iteration 284, loss = 0.46887837\n",
      "Iteration 285, loss = 0.46826463\n",
      "Iteration 286, loss = 0.46765276\n",
      "Iteration 287, loss = 0.46704277\n",
      "Iteration 288, loss = 0.46643462\n",
      "Iteration 289, loss = 0.46582831\n",
      "Iteration 290, loss = 0.46522383\n",
      "Iteration 291, loss = 0.46462114\n",
      "Iteration 292, loss = 0.46402024\n",
      "Iteration 293, loss = 0.46342111\n",
      "Iteration 294, loss = 0.46282373\n",
      "Iteration 295, loss = 0.46222808\n",
      "Iteration 296, loss = 0.46163414\n",
      "Iteration 297, loss = 0.46104189\n",
      "Iteration 298, loss = 0.46045133\n",
      "Iteration 299, loss = 0.45986241\n",
      "Iteration 300, loss = 0.45927514\n",
      "Iteration 301, loss = 0.45868949\n",
      "Iteration 302, loss = 0.45810544\n",
      "Iteration 303, loss = 0.45752299\n",
      "Iteration 304, loss = 0.45694210\n",
      "Iteration 305, loss = 0.45636276\n",
      "Iteration 306, loss = 0.45578497\n",
      "Iteration 307, loss = 0.45520870\n",
      "Iteration 308, loss = 0.45463394\n",
      "Iteration 309, loss = 0.45406066\n",
      "Iteration 310, loss = 0.45348887\n",
      "Iteration 311, loss = 0.45291854\n",
      "Iteration 312, loss = 0.45234999\n",
      "Iteration 313, loss = 0.45178318\n",
      "Iteration 314, loss = 0.45121782\n",
      "Iteration 315, loss = 0.45065390\n",
      "Iteration 316, loss = 0.45009139\n",
      "Iteration 317, loss = 0.44953029\n",
      "Iteration 318, loss = 0.44897058\n",
      "Iteration 319, loss = 0.44841224\n",
      "Iteration 320, loss = 0.44785526\n",
      "Iteration 321, loss = 0.44729962\n",
      "Iteration 322, loss = 0.44674531\n",
      "Iteration 323, loss = 0.44619232\n",
      "Iteration 324, loss = 0.44564087\n",
      "Iteration 325, loss = 0.44509097\n",
      "Iteration 326, loss = 0.44454238\n",
      "Iteration 327, loss = 0.44399507\n",
      "Iteration 328, loss = 0.44344904\n",
      "Iteration 329, loss = 0.44290427\n",
      "Iteration 330, loss = 0.44236075\n",
      "Iteration 331, loss = 0.44181846\n",
      "Iteration 332, loss = 0.44127740\n",
      "Iteration 333, loss = 0.44073754\n",
      "Iteration 334, loss = 0.44019887\n",
      "Iteration 335, loss = 0.43966139\n",
      "Iteration 336, loss = 0.43912508\n",
      "Iteration 337, loss = 0.43858992\n",
      "Iteration 338, loss = 0.43805592\n",
      "Iteration 339, loss = 0.43752305\n",
      "Iteration 340, loss = 0.43699147\n",
      "Iteration 341, loss = 0.43646116\n",
      "Iteration 342, loss = 0.43593198\n",
      "Iteration 343, loss = 0.43540390\n",
      "Iteration 344, loss = 0.43487787\n",
      "Iteration 345, loss = 0.43435304\n",
      "Iteration 346, loss = 0.43382935\n",
      "Iteration 347, loss = 0.43330677\n",
      "Iteration 348, loss = 0.43278530\n",
      "Iteration 349, loss = 0.43226493\n",
      "Iteration 350, loss = 0.43174563\n",
      "Iteration 351, loss = 0.43122741\n",
      "Iteration 352, loss = 0.43071024\n",
      "Iteration 353, loss = 0.43019411\n",
      "Iteration 354, loss = 0.42967914\n",
      "Iteration 355, loss = 0.42916623\n",
      "Iteration 356, loss = 0.42865440\n",
      "Iteration 357, loss = 0.42814361\n",
      "Iteration 358, loss = 0.42763387\n",
      "Iteration 359, loss = 0.42712517\n",
      "Iteration 360, loss = 0.42661772\n",
      "Iteration 361, loss = 0.42611180\n",
      "Iteration 362, loss = 0.42560690\n",
      "Iteration 363, loss = 0.42510302\n",
      "Iteration 364, loss = 0.42460016\n",
      "Iteration 365, loss = 0.42409831\n",
      "Iteration 366, loss = 0.42359748\n",
      "Iteration 367, loss = 0.42309765\n",
      "Iteration 368, loss = 0.42259881\n",
      "Iteration 369, loss = 0.42210096\n",
      "Iteration 370, loss = 0.42160408\n",
      "Iteration 371, loss = 0.42110816\n",
      "Iteration 372, loss = 0.42061324\n",
      "Iteration 373, loss = 0.42011979\n",
      "Iteration 374, loss = 0.41962730\n",
      "Iteration 375, loss = 0.41913577\n",
      "Iteration 376, loss = 0.41864520\n",
      "Iteration 377, loss = 0.41815557\n",
      "Iteration 378, loss = 0.41766689\n",
      "Iteration 379, loss = 0.41717915\n",
      "Iteration 380, loss = 0.41669232\n",
      "Iteration 381, loss = 0.41620642\n",
      "Iteration 382, loss = 0.41572141\n",
      "Iteration 383, loss = 0.41523731\n",
      "Iteration 384, loss = 0.41475408\n",
      "Iteration 385, loss = 0.41427173\n",
      "Iteration 386, loss = 0.41379025\n",
      "Iteration 387, loss = 0.41330961\n",
      "Iteration 388, loss = 0.41282982\n",
      "Iteration 389, loss = 0.41235087\n",
      "Iteration 390, loss = 0.41187274\n",
      "Iteration 391, loss = 0.41139542\n",
      "Iteration 392, loss = 0.41091891\n",
      "Iteration 393, loss = 0.41044320\n",
      "Iteration 394, loss = 0.40996827\n",
      "Iteration 395, loss = 0.40949413\n",
      "Iteration 396, loss = 0.40902090\n",
      "Iteration 397, loss = 0.40854856\n",
      "Iteration 398, loss = 0.40807700\n",
      "Iteration 399, loss = 0.40760621\n",
      "Iteration 400, loss = 0.40713619\n",
      "Iteration 401, loss = 0.40666693\n",
      "Iteration 402, loss = 0.40619843\n",
      "Iteration 403, loss = 0.40573067\n",
      "Iteration 404, loss = 0.40526365\n",
      "Iteration 405, loss = 0.40479737\n",
      "Iteration 406, loss = 0.40433181\n",
      "Iteration 407, loss = 0.40386697\n",
      "Iteration 408, loss = 0.40340284\n",
      "Iteration 409, loss = 0.40293942\n",
      "Iteration 410, loss = 0.40247670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 411, loss = 0.40201467\n",
      "Iteration 412, loss = 0.40155333\n",
      "Iteration 413, loss = 0.40109267\n",
      "Iteration 414, loss = 0.40063269\n",
      "Iteration 415, loss = 0.40017338\n",
      "Iteration 416, loss = 0.39971473\n",
      "Iteration 417, loss = 0.39925674\n",
      "Iteration 418, loss = 0.39879961\n",
      "Iteration 419, loss = 0.39834343\n",
      "Iteration 420, loss = 0.39788791\n",
      "Iteration 421, loss = 0.39743307\n",
      "Iteration 422, loss = 0.39697888\n",
      "Iteration 423, loss = 0.39652536\n",
      "Iteration 424, loss = 0.39607250\n",
      "Iteration 425, loss = 0.39562028\n",
      "Iteration 426, loss = 0.39516871\n",
      "Iteration 427, loss = 0.39471779\n",
      "Iteration 428, loss = 0.39426750\n",
      "Iteration 429, loss = 0.39381784\n",
      "Iteration 430, loss = 0.39336880\n",
      "Iteration 431, loss = 0.39292039\n",
      "Iteration 432, loss = 0.39247259\n",
      "Iteration 433, loss = 0.39202540\n",
      "Iteration 434, loss = 0.39157881\n",
      "Iteration 435, loss = 0.39113282\n",
      "Iteration 436, loss = 0.39068743\n",
      "Iteration 437, loss = 0.39024263\n",
      "Iteration 438, loss = 0.38979842\n",
      "Iteration 439, loss = 0.38935479\n",
      "Iteration 440, loss = 0.38891173\n",
      "Iteration 441, loss = 0.38846925\n",
      "Iteration 442, loss = 0.38802734\n",
      "Iteration 443, loss = 0.38758599\n",
      "Iteration 444, loss = 0.38714536\n",
      "Iteration 445, loss = 0.38670549\n",
      "Iteration 446, loss = 0.38626620\n",
      "Iteration 447, loss = 0.38582749\n",
      "Iteration 448, loss = 0.38538934\n",
      "Iteration 449, loss = 0.38495175\n",
      "Iteration 450, loss = 0.38451473\n",
      "Iteration 451, loss = 0.38407826\n",
      "Iteration 452, loss = 0.38364234\n",
      "Iteration 453, loss = 0.38320698\n",
      "Iteration 454, loss = 0.38277215\n",
      "Iteration 455, loss = 0.38233786\n",
      "Iteration 456, loss = 0.38190411\n",
      "Iteration 457, loss = 0.38147089\n",
      "Iteration 458, loss = 0.38103819\n",
      "Iteration 459, loss = 0.38060602\n",
      "Iteration 460, loss = 0.38017437\n",
      "Iteration 461, loss = 0.37974323\n",
      "Iteration 462, loss = 0.37931260\n",
      "Iteration 463, loss = 0.37888248\n",
      "Iteration 464, loss = 0.37845286\n",
      "Iteration 465, loss = 0.37802374\n",
      "Iteration 466, loss = 0.37759513\n",
      "Iteration 467, loss = 0.37716700\n",
      "Iteration 468, loss = 0.37673937\n",
      "Iteration 469, loss = 0.37631223\n",
      "Iteration 470, loss = 0.37588557\n",
      "Iteration 471, loss = 0.37545939\n",
      "Iteration 472, loss = 0.37503370\n",
      "Iteration 473, loss = 0.37460848\n",
      "Iteration 474, loss = 0.37418373\n",
      "Iteration 475, loss = 0.37375946\n",
      "Iteration 476, loss = 0.37333565\n",
      "Iteration 477, loss = 0.37291231\n",
      "Iteration 478, loss = 0.37248943\n",
      "Iteration 479, loss = 0.37206702\n",
      "Iteration 480, loss = 0.37164506\n",
      "Iteration 481, loss = 0.37122356\n",
      "Iteration 482, loss = 0.37080251\n",
      "Iteration 483, loss = 0.37038191\n",
      "Iteration 484, loss = 0.36996177\n",
      "Iteration 485, loss = 0.36954238\n",
      "Iteration 486, loss = 0.36912374\n",
      "Iteration 487, loss = 0.36870558\n",
      "Iteration 488, loss = 0.36828790\n",
      "Iteration 489, loss = 0.36787068\n",
      "Iteration 490, loss = 0.36745394\n",
      "Iteration 491, loss = 0.36703766\n",
      "Iteration 492, loss = 0.36662184\n",
      "Iteration 493, loss = 0.36620649\n",
      "Iteration 494, loss = 0.36579159\n",
      "Iteration 495, loss = 0.36537715\n",
      "Iteration 496, loss = 0.36496315\n",
      "Iteration 497, loss = 0.36454961\n",
      "Iteration 498, loss = 0.36413650\n",
      "Iteration 499, loss = 0.36372384\n",
      "Iteration 500, loss = 0.36331162\n",
      "Iteration 501, loss = 0.36289983\n",
      "Iteration 502, loss = 0.36248847\n",
      "Iteration 503, loss = 0.36207755\n",
      "Iteration 504, loss = 0.36166706\n",
      "Iteration 505, loss = 0.36125699\n",
      "Iteration 506, loss = 0.36084735\n",
      "Iteration 507, loss = 0.36043813\n",
      "Iteration 508, loss = 0.36002934\n",
      "Iteration 509, loss = 0.35962096\n",
      "Iteration 510, loss = 0.35921300\n",
      "Iteration 511, loss = 0.35880545\n",
      "Iteration 512, loss = 0.35839832\n",
      "Iteration 513, loss = 0.35799160\n",
      "Iteration 514, loss = 0.35758529\n",
      "Iteration 515, loss = 0.35717938\n",
      "Iteration 516, loss = 0.35677388\n",
      "Iteration 517, loss = 0.35636879\n",
      "Iteration 518, loss = 0.35596410\n",
      "Iteration 519, loss = 0.35555981\n",
      "Iteration 520, loss = 0.35515592\n",
      "Iteration 521, loss = 0.35475244\n",
      "Iteration 522, loss = 0.35434934\n",
      "Iteration 523, loss = 0.35394665\n",
      "Iteration 524, loss = 0.35354435\n",
      "Iteration 525, loss = 0.35314244\n",
      "Iteration 526, loss = 0.35274093\n",
      "Iteration 527, loss = 0.35233981\n",
      "Iteration 528, loss = 0.35193908\n",
      "Iteration 529, loss = 0.35153873\n",
      "Iteration 530, loss = 0.35113878\n",
      "Iteration 531, loss = 0.35073921\n",
      "Iteration 532, loss = 0.35034003\n",
      "Iteration 533, loss = 0.34994124\n",
      "Iteration 534, loss = 0.34954282\n",
      "Iteration 535, loss = 0.34914480\n",
      "Iteration 536, loss = 0.34874715\n",
      "Iteration 537, loss = 0.34834989\n",
      "Iteration 538, loss = 0.34795300\n",
      "Iteration 539, loss = 0.34755650\n",
      "Iteration 540, loss = 0.34716038\n",
      "Iteration 541, loss = 0.34676463\n",
      "Iteration 542, loss = 0.34636926\n",
      "Iteration 543, loss = 0.34597427\n",
      "Iteration 544, loss = 0.34557965\n",
      "Iteration 545, loss = 0.34518541\n",
      "Iteration 546, loss = 0.34479155\n",
      "Iteration 547, loss = 0.34439806\n",
      "Iteration 548, loss = 0.34400494\n",
      "Iteration 549, loss = 0.34361220\n",
      "Iteration 550, loss = 0.34322018\n",
      "Iteration 551, loss = 0.34282874\n",
      "Iteration 552, loss = 0.34243770\n",
      "Iteration 553, loss = 0.34204705\n",
      "Iteration 554, loss = 0.34165681\n",
      "Iteration 555, loss = 0.34126696\n",
      "Iteration 556, loss = 0.34087751\n",
      "Iteration 557, loss = 0.34048845\n",
      "Iteration 558, loss = 0.34009978\n",
      "Iteration 559, loss = 0.33971150\n",
      "Iteration 560, loss = 0.33932360\n",
      "Iteration 561, loss = 0.33893609\n",
      "Iteration 562, loss = 0.33854896\n",
      "Iteration 563, loss = 0.33816221\n",
      "Iteration 564, loss = 0.33777584\n",
      "Iteration 565, loss = 0.33738985\n",
      "Iteration 566, loss = 0.33700423\n",
      "Iteration 567, loss = 0.33661900\n",
      "Iteration 568, loss = 0.33623413\n",
      "Iteration 569, loss = 0.33584964\n",
      "Iteration 570, loss = 0.33546553\n",
      "Iteration 571, loss = 0.33508179\n",
      "Iteration 572, loss = 0.33469842\n",
      "Iteration 573, loss = 0.33431542\n",
      "Iteration 574, loss = 0.33393279\n",
      "Iteration 575, loss = 0.33355053\n",
      "Iteration 576, loss = 0.33316864\n",
      "Iteration 577, loss = 0.33278712\n",
      "Iteration 578, loss = 0.33240596\n",
      "Iteration 579, loss = 0.33202517\n",
      "Iteration 580, loss = 0.33164475\n",
      "Iteration 581, loss = 0.33126470\n",
      "Iteration 582, loss = 0.33088501\n",
      "Iteration 583, loss = 0.33050568\n",
      "Iteration 584, loss = 0.33012672\n",
      "Iteration 585, loss = 0.32974812\n",
      "Iteration 586, loss = 0.32936989\n",
      "Iteration 587, loss = 0.32899202\n",
      "Iteration 588, loss = 0.32861452\n",
      "Iteration 589, loss = 0.32823737\n",
      "Iteration 590, loss = 0.32786060\n",
      "Iteration 591, loss = 0.32748418\n",
      "Iteration 592, loss = 0.32710813\n",
      "Iteration 593, loss = 0.32673243\n",
      "Iteration 594, loss = 0.32635710\n",
      "Iteration 595, loss = 0.32598214\n",
      "Iteration 596, loss = 0.32560753\n",
      "Iteration 597, loss = 0.32523329\n",
      "Iteration 598, loss = 0.32485940\n",
      "Iteration 599, loss = 0.32448588\n",
      "Iteration 600, loss = 0.32411272\n",
      "Iteration 601, loss = 0.32373992\n",
      "Iteration 602, loss = 0.32336749\n",
      "Iteration 603, loss = 0.32299541\n",
      "Iteration 604, loss = 0.32262369\n",
      "Iteration 605, loss = 0.32225234\n",
      "Iteration 606, loss = 0.32188135\n",
      "Iteration 607, loss = 0.32151072\n",
      "Iteration 608, loss = 0.32114045\n",
      "Iteration 609, loss = 0.32077054\n",
      "Iteration 610, loss = 0.32040099\n",
      "Iteration 611, loss = 0.32003180\n",
      "Iteration 612, loss = 0.31966298\n",
      "Iteration 613, loss = 0.31929452\n",
      "Iteration 614, loss = 0.31892641\n",
      "Iteration 615, loss = 0.31855867\n",
      "Iteration 616, loss = 0.31819129\n",
      "Iteration 617, loss = 0.31782427\n",
      "Iteration 618, loss = 0.31745762\n",
      "Iteration 619, loss = 0.31709132\n",
      "Iteration 620, loss = 0.31672539\n",
      "Iteration 621, loss = 0.31635982\n",
      "Iteration 622, loss = 0.31599461\n",
      "Iteration 623, loss = 0.31562976\n",
      "Iteration 624, loss = 0.31526528\n",
      "Iteration 625, loss = 0.31490131\n",
      "Iteration 626, loss = 0.31453839\n",
      "Iteration 627, loss = 0.31417595\n",
      "Iteration 628, loss = 0.31381390\n",
      "Iteration 629, loss = 0.31345225\n",
      "Iteration 630, loss = 0.31309101\n",
      "Iteration 631, loss = 0.31273016\n",
      "Iteration 632, loss = 0.31236972\n",
      "Iteration 633, loss = 0.31200967\n",
      "Iteration 634, loss = 0.31165001\n",
      "Iteration 635, loss = 0.31129074\n",
      "Iteration 636, loss = 0.31093186\n",
      "Iteration 637, loss = 0.31057337\n",
      "Iteration 638, loss = 0.31021526\n",
      "Iteration 639, loss = 0.30985754\n",
      "Iteration 640, loss = 0.30950019\n",
      "Iteration 641, loss = 0.30914323\n",
      "Iteration 642, loss = 0.30878664\n",
      "Iteration 643, loss = 0.30843044\n",
      "Iteration 644, loss = 0.30807461\n",
      "Iteration 645, loss = 0.30771916\n",
      "Iteration 646, loss = 0.30736409\n",
      "Iteration 647, loss = 0.30700940\n",
      "Iteration 648, loss = 0.30665509\n",
      "Iteration 649, loss = 0.30630115\n",
      "Iteration 650, loss = 0.30594758\n",
      "Iteration 651, loss = 0.30559439\n",
      "Iteration 652, loss = 0.30524158\n",
      "Iteration 653, loss = 0.30488914\n",
      "Iteration 654, loss = 0.30453707\n",
      "Iteration 655, loss = 0.30418537\n",
      "Iteration 656, loss = 0.30383405\n",
      "Iteration 657, loss = 0.30348310\n",
      "Iteration 658, loss = 0.30313252\n",
      "Iteration 659, loss = 0.30278231\n",
      "Iteration 660, loss = 0.30243248\n",
      "Iteration 661, loss = 0.30208302\n",
      "Iteration 662, loss = 0.30173392\n",
      "Iteration 663, loss = 0.30138521\n",
      "Iteration 664, loss = 0.30103686\n",
      "Iteration 665, loss = 0.30068888\n",
      "Iteration 666, loss = 0.30034128\n",
      "Iteration 667, loss = 0.29999405\n",
      "Iteration 668, loss = 0.29964719\n",
      "Iteration 669, loss = 0.29930070\n",
      "Iteration 670, loss = 0.29895458\n",
      "Iteration 671, loss = 0.29860884\n",
      "Iteration 672, loss = 0.29826347\n",
      "Iteration 673, loss = 0.29791847\n",
      "Iteration 674, loss = 0.29757384\n",
      "Iteration 675, loss = 0.29722959\n",
      "Iteration 676, loss = 0.29688571\n",
      "Iteration 677, loss = 0.29654220\n",
      "Iteration 678, loss = 0.29619907\n",
      "Iteration 679, loss = 0.29585631\n",
      "Iteration 680, loss = 0.29551392\n",
      "Iteration 681, loss = 0.29517191\n",
      "Iteration 682, loss = 0.29483027\n",
      "Iteration 683, loss = 0.29448900\n",
      "Iteration 684, loss = 0.29414811\n",
      "Iteration 685, loss = 0.29380760\n",
      "Iteration 686, loss = 0.29346746\n",
      "Iteration 687, loss = 0.29312769\n",
      "Iteration 688, loss = 0.29278830\n",
      "Iteration 689, loss = 0.29244928\n",
      "Iteration 690, loss = 0.29211064\n",
      "Iteration 691, loss = 0.29177238\n",
      "Iteration 692, loss = 0.29143449\n",
      "Iteration 693, loss = 0.29109698\n",
      "Iteration 694, loss = 0.29075985\n",
      "Iteration 695, loss = 0.29042309\n",
      "Iteration 696, loss = 0.29008671\n",
      "Iteration 697, loss = 0.28975070\n",
      "Iteration 698, loss = 0.28941508\n",
      "Iteration 699, loss = 0.28907983\n",
      "Iteration 700, loss = 0.28874496\n",
      "Iteration 701, loss = 0.28841047\n",
      "Iteration 702, loss = 0.28807635\n",
      "Iteration 703, loss = 0.28774262\n",
      "Iteration 704, loss = 0.28740926\n",
      "Iteration 705, loss = 0.28707628\n",
      "Iteration 706, loss = 0.28674368\n",
      "Iteration 707, loss = 0.28641147\n",
      "Iteration 708, loss = 0.28607963\n",
      "Iteration 709, loss = 0.28574817\n",
      "Iteration 710, loss = 0.28541709\n",
      "Iteration 711, loss = 0.28508639\n",
      "Iteration 712, loss = 0.28475607\n",
      "Iteration 713, loss = 0.28442614\n",
      "Iteration 714, loss = 0.28409658\n",
      "Iteration 715, loss = 0.28376741\n",
      "Iteration 716, loss = 0.28343861\n",
      "Iteration 717, loss = 0.28311020\n",
      "Iteration 718, loss = 0.28278217\n",
      "Iteration 719, loss = 0.28245453\n",
      "Iteration 720, loss = 0.28212726\n",
      "Iteration 721, loss = 0.28180038\n",
      "Iteration 722, loss = 0.28147388\n",
      "Iteration 723, loss = 0.28114777\n",
      "Iteration 724, loss = 0.28082203\n",
      "Iteration 725, loss = 0.28049668\n",
      "Iteration 726, loss = 0.28017172\n",
      "Iteration 727, loss = 0.27984714\n",
      "Iteration 728, loss = 0.27952294\n",
      "Iteration 729, loss = 0.27919913\n",
      "Iteration 730, loss = 0.27887570\n",
      "Iteration 731, loss = 0.27855266\n",
      "Iteration 732, loss = 0.27823000\n",
      "Iteration 733, loss = 0.27790773\n",
      "Iteration 734, loss = 0.27758585\n",
      "Iteration 735, loss = 0.27726434\n",
      "Iteration 736, loss = 0.27694323\n",
      "Iteration 737, loss = 0.27662250\n",
      "Iteration 738, loss = 0.27630216\n",
      "Iteration 739, loss = 0.27598220\n",
      "Iteration 740, loss = 0.27566263\n",
      "Iteration 741, loss = 0.27534345\n",
      "Iteration 742, loss = 0.27502466\n",
      "Iteration 743, loss = 0.27470625\n",
      "Iteration 744, loss = 0.27438823\n",
      "Iteration 745, loss = 0.27407060\n",
      "Iteration 746, loss = 0.27375335\n",
      "Iteration 747, loss = 0.27343650\n",
      "Iteration 748, loss = 0.27312003\n",
      "Iteration 749, loss = 0.27280395\n",
      "Iteration 750, loss = 0.27248826\n",
      "Iteration 751, loss = 0.27217296\n",
      "Iteration 752, loss = 0.27185805\n",
      "Iteration 753, loss = 0.27154353\n",
      "Iteration 754, loss = 0.27122939\n",
      "Iteration 755, loss = 0.27091565\n",
      "Iteration 756, loss = 0.27060229\n",
      "Iteration 757, loss = 0.27028933\n",
      "Iteration 758, loss = 0.26997676\n",
      "Iteration 759, loss = 0.26966457\n",
      "Iteration 760, loss = 0.26935278\n",
      "Iteration 761, loss = 0.26904138\n",
      "Iteration 762, loss = 0.26873036\n",
      "Iteration 763, loss = 0.26841974\n",
      "Iteration 764, loss = 0.26810951\n",
      "Iteration 765, loss = 0.26779967\n",
      "Iteration 766, loss = 0.26749023\n",
      "Iteration 767, loss = 0.26718117\n",
      "Iteration 768, loss = 0.26687251\n",
      "Iteration 769, loss = 0.26656424\n",
      "Iteration 770, loss = 0.26625642\n",
      "Iteration 771, loss = 0.26594946\n",
      "Iteration 772, loss = 0.26564292\n",
      "Iteration 773, loss = 0.26533680\n",
      "Iteration 774, loss = 0.26503110\n",
      "Iteration 775, loss = 0.26472581\n",
      "Iteration 776, loss = 0.26442093\n",
      "Iteration 777, loss = 0.26411646\n",
      "Iteration 778, loss = 0.26381240\n",
      "Iteration 779, loss = 0.26350875\n",
      "Iteration 780, loss = 0.26320551\n",
      "Iteration 781, loss = 0.26290267\n",
      "Iteration 782, loss = 0.26260023\n",
      "Iteration 783, loss = 0.26229820\n",
      "Iteration 784, loss = 0.26199656\n",
      "Iteration 785, loss = 0.26169533\n",
      "Iteration 786, loss = 0.26139449\n",
      "Iteration 787, loss = 0.26109406\n",
      "Iteration 788, loss = 0.26079402\n",
      "Iteration 789, loss = 0.26049439\n",
      "Iteration 790, loss = 0.26019515\n",
      "Iteration 791, loss = 0.25989631\n",
      "Iteration 792, loss = 0.25959787\n",
      "Iteration 793, loss = 0.25929983\n",
      "Iteration 794, loss = 0.25900218\n",
      "Iteration 795, loss = 0.25870494\n",
      "Iteration 796, loss = 0.25840808\n",
      "Iteration 797, loss = 0.25811162\n",
      "Iteration 798, loss = 0.25781556\n",
      "Iteration 799, loss = 0.25751990\n",
      "Iteration 800, loss = 0.25722463\n",
      "Iteration 801, loss = 0.25692975\n",
      "Iteration 802, loss = 0.25663527\n",
      "Iteration 803, loss = 0.25634118\n",
      "Iteration 804, loss = 0.25604749\n",
      "Iteration 805, loss = 0.25575420\n",
      "Iteration 806, loss = 0.25546130\n",
      "Iteration 807, loss = 0.25516879\n",
      "Iteration 808, loss = 0.25487674\n",
      "Iteration 809, loss = 0.25458515\n",
      "Iteration 810, loss = 0.25429396\n",
      "Iteration 811, loss = 0.25400318\n",
      "Iteration 812, loss = 0.25371280\n",
      "Iteration 813, loss = 0.25342281\n",
      "Iteration 814, loss = 0.25313323\n",
      "Iteration 815, loss = 0.25284405\n",
      "Iteration 816, loss = 0.25255526\n",
      "Iteration 817, loss = 0.25226687\n",
      "Iteration 818, loss = 0.25197888\n",
      "Iteration 819, loss = 0.25169128\n",
      "Iteration 820, loss = 0.25140408\n",
      "Iteration 821, loss = 0.25111728\n",
      "Iteration 822, loss = 0.25083088\n",
      "Iteration 823, loss = 0.25054487\n",
      "Iteration 824, loss = 0.25025925\n",
      "Iteration 825, loss = 0.24997403\n",
      "Iteration 826, loss = 0.24968920\n",
      "Iteration 827, loss = 0.24940477\n",
      "Iteration 828, loss = 0.24912074\n",
      "Iteration 829, loss = 0.24883710\n",
      "Iteration 830, loss = 0.24855385\n",
      "Iteration 831, loss = 0.24827100\n",
      "Iteration 832, loss = 0.24798854\n",
      "Iteration 833, loss = 0.24770647\n",
      "Iteration 834, loss = 0.24742480\n",
      "Iteration 835, loss = 0.24714352\n",
      "Iteration 836, loss = 0.24686263\n",
      "Iteration 837, loss = 0.24658214\n",
      "Iteration 838, loss = 0.24630204\n",
      "Iteration 839, loss = 0.24602233\n",
      "Iteration 840, loss = 0.24574302\n",
      "Iteration 841, loss = 0.24546410\n",
      "Iteration 842, loss = 0.24518557\n",
      "Iteration 843, loss = 0.24490744\n",
      "Iteration 844, loss = 0.24462969\n",
      "Iteration 845, loss = 0.24435234\n",
      "Iteration 846, loss = 0.24407538\n",
      "Iteration 847, loss = 0.24379881\n",
      "Iteration 848, loss = 0.24352264\n",
      "Iteration 849, loss = 0.24324686\n",
      "Iteration 850, loss = 0.24297146\n",
      "Iteration 851, loss = 0.24269646\n",
      "Iteration 852, loss = 0.24242185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 853, loss = 0.24214764\n",
      "Iteration 854, loss = 0.24187381\n",
      "Iteration 855, loss = 0.24160038\n",
      "Iteration 856, loss = 0.24132733\n",
      "Iteration 857, loss = 0.24105468\n",
      "Iteration 858, loss = 0.24078242\n",
      "Iteration 859, loss = 0.24051054\n",
      "Iteration 860, loss = 0.24023906\n",
      "Iteration 861, loss = 0.23996797\n",
      "Iteration 862, loss = 0.23969727\n",
      "Iteration 863, loss = 0.23942696\n",
      "Iteration 864, loss = 0.23915704\n",
      "Iteration 865, loss = 0.23888751\n",
      "Iteration 866, loss = 0.23861837\n",
      "Iteration 867, loss = 0.23834962\n",
      "Iteration 868, loss = 0.23808126\n",
      "Iteration 869, loss = 0.23781329\n",
      "Iteration 870, loss = 0.23754570\n",
      "Iteration 871, loss = 0.23727851\n",
      "Iteration 872, loss = 0.23701170\n",
      "Iteration 873, loss = 0.23674529\n",
      "Iteration 874, loss = 0.23647926\n",
      "Iteration 875, loss = 0.23621362\n",
      "Iteration 876, loss = 0.23594837\n",
      "Iteration 877, loss = 0.23568351\n",
      "Iteration 878, loss = 0.23541903\n",
      "Iteration 879, loss = 0.23515494\n",
      "Iteration 880, loss = 0.23489125\n",
      "Iteration 881, loss = 0.23462793\n",
      "Iteration 882, loss = 0.23436501\n",
      "Iteration 883, loss = 0.23410247\n",
      "Iteration 884, loss = 0.23384032\n",
      "Iteration 885, loss = 0.23357856\n",
      "Iteration 886, loss = 0.23331718\n",
      "Iteration 887, loss = 0.23305619\n",
      "Iteration 888, loss = 0.23279558\n",
      "Iteration 889, loss = 0.23253536\n",
      "Iteration 890, loss = 0.23227553\n",
      "Iteration 891, loss = 0.23201608\n",
      "Iteration 892, loss = 0.23175702\n",
      "Iteration 893, loss = 0.23149835\n",
      "Iteration 894, loss = 0.23124005\n",
      "Iteration 895, loss = 0.23098215\n",
      "Iteration 896, loss = 0.23072463\n",
      "Iteration 897, loss = 0.23046749\n",
      "Iteration 898, loss = 0.23021074\n",
      "Iteration 899, loss = 0.22995437\n",
      "Iteration 900, loss = 0.22969838\n",
      "Iteration 901, loss = 0.22944278\n",
      "Iteration 902, loss = 0.22918756\n",
      "Iteration 903, loss = 0.22893272\n",
      "Iteration 904, loss = 0.22867827\n",
      "Iteration 905, loss = 0.22842420\n",
      "Iteration 906, loss = 0.22817051\n",
      "Iteration 907, loss = 0.22791721\n",
      "Iteration 908, loss = 0.22766429\n",
      "Iteration 909, loss = 0.22741175\n",
      "Iteration 910, loss = 0.22715959\n",
      "Iteration 911, loss = 0.22690781\n",
      "Iteration 912, loss = 0.22665641\n",
      "Iteration 913, loss = 0.22640539\n",
      "Iteration 914, loss = 0.22615476\n",
      "Iteration 915, loss = 0.22590450\n",
      "Iteration 916, loss = 0.22565463\n",
      "Iteration 917, loss = 0.22540513\n",
      "Iteration 918, loss = 0.22515602\n",
      "Iteration 919, loss = 0.22490728\n",
      "Iteration 920, loss = 0.22465892\n",
      "Iteration 921, loss = 0.22441094\n",
      "Iteration 922, loss = 0.22416334\n",
      "Iteration 923, loss = 0.22391612\n",
      "Iteration 924, loss = 0.22366928\n",
      "Iteration 925, loss = 0.22342281\n",
      "Iteration 926, loss = 0.22317672\n",
      "Iteration 927, loss = 0.22293101\n",
      "Iteration 928, loss = 0.22268568\n",
      "Iteration 929, loss = 0.22244072\n",
      "Iteration 930, loss = 0.22219614\n",
      "Iteration 931, loss = 0.22195193\n",
      "Iteration 932, loss = 0.22170810\n",
      "Iteration 933, loss = 0.22146465\n",
      "Iteration 934, loss = 0.22122157\n",
      "Iteration 935, loss = 0.22097887\n",
      "Iteration 936, loss = 0.22073654\n",
      "Iteration 937, loss = 0.22049459\n",
      "Iteration 938, loss = 0.22025301\n",
      "Iteration 939, loss = 0.22001180\n",
      "Iteration 940, loss = 0.21977097\n",
      "Iteration 941, loss = 0.21953051\n",
      "Iteration 942, loss = 0.21929042\n",
      "Iteration 943, loss = 0.21905071\n",
      "Iteration 944, loss = 0.21881136\n",
      "Iteration 945, loss = 0.21857239\n",
      "Iteration 946, loss = 0.21833380\n",
      "Iteration 947, loss = 0.21809557\n",
      "Iteration 948, loss = 0.21785772\n",
      "Iteration 949, loss = 0.21762023\n",
      "Iteration 950, loss = 0.21738312\n",
      "Iteration 951, loss = 0.21714637\n",
      "Iteration 952, loss = 0.21691000\n",
      "Iteration 953, loss = 0.21667399\n",
      "Iteration 954, loss = 0.21643836\n",
      "Iteration 955, loss = 0.21620309\n",
      "Iteration 956, loss = 0.21596819\n",
      "Iteration 957, loss = 0.21573366\n",
      "Iteration 958, loss = 0.21549950\n",
      "Iteration 959, loss = 0.21526571\n",
      "Iteration 960, loss = 0.21503228\n",
      "Iteration 961, loss = 0.21479922\n",
      "Iteration 962, loss = 0.21456653\n",
      "Iteration 963, loss = 0.21433420\n",
      "Iteration 964, loss = 0.21410224\n",
      "Iteration 965, loss = 0.21387064\n",
      "Iteration 966, loss = 0.21363941\n",
      "Iteration 967, loss = 0.21340860\n",
      "Iteration 968, loss = 0.21317818\n",
      "Iteration 969, loss = 0.21294813\n",
      "Iteration 970, loss = 0.21271845\n",
      "Iteration 971, loss = 0.21248913\n",
      "Iteration 972, loss = 0.21226018\n",
      "Iteration 973, loss = 0.21203160\n",
      "Iteration 974, loss = 0.21180338\n",
      "Iteration 975, loss = 0.21157553\n",
      "Iteration 976, loss = 0.21134804\n",
      "Iteration 977, loss = 0.21112091\n",
      "Iteration 978, loss = 0.21089415\n",
      "Iteration 979, loss = 0.21066775\n",
      "Iteration 980, loss = 0.21044171\n",
      "Iteration 981, loss = 0.21021603\n",
      "Iteration 982, loss = 0.20999071\n",
      "Iteration 983, loss = 0.20976575\n",
      "Iteration 984, loss = 0.20954114\n",
      "Iteration 985, loss = 0.20931690\n",
      "Iteration 986, loss = 0.20909302\n",
      "Iteration 987, loss = 0.20886949\n",
      "Iteration 988, loss = 0.20864633\n",
      "Iteration 989, loss = 0.20842352\n",
      "Iteration 990, loss = 0.20820106\n",
      "Iteration 991, loss = 0.20797897\n",
      "Iteration 992, loss = 0.20775722\n",
      "Iteration 993, loss = 0.20753584\n",
      "Iteration 994, loss = 0.20731481\n",
      "Iteration 995, loss = 0.20709413\n",
      "Iteration 996, loss = 0.20687381\n",
      "Iteration 997, loss = 0.20665384\n",
      "Iteration 998, loss = 0.20643422\n",
      "Iteration 999, loss = 0.20621496\n",
      "Iteration 1000, loss = 0.20599605\n",
      "Iteration 1, loss = 1.52120330\n",
      "Iteration 2, loss = 1.48985044\n",
      "Iteration 3, loss = 1.44771214\n",
      "Iteration 4, loss = 1.39858879\n",
      "Iteration 5, loss = 1.34609089\n",
      "Iteration 6, loss = 1.29320654\n",
      "Iteration 7, loss = 1.24215391\n",
      "Iteration 8, loss = 1.19392355\n",
      "Iteration 9, loss = 1.14913820\n",
      "Iteration 10, loss = 1.10805340\n",
      "Iteration 11, loss = 1.07099013\n",
      "Iteration 12, loss = 1.03694464\n",
      "Iteration 13, loss = 1.00537055\n",
      "Iteration 14, loss = 0.97587809\n",
      "Iteration 15, loss = 0.94852650\n",
      "Iteration 16, loss = 0.92331085\n",
      "Iteration 17, loss = 0.90029529\n",
      "Iteration 18, loss = 0.87951101\n",
      "Iteration 19, loss = 0.86086116\n",
      "Iteration 20, loss = 0.84423086\n",
      "Iteration 21, loss = 0.82951793\n",
      "Iteration 22, loss = 0.81661473\n",
      "Iteration 23, loss = 0.80539319\n",
      "Iteration 24, loss = 0.79587562\n",
      "Iteration 25, loss = 0.78824916\n",
      "Iteration 26, loss = 0.78258820\n",
      "Iteration 27, loss = 0.77833685\n",
      "Iteration 28, loss = 0.77512561\n",
      "Iteration 29, loss = 0.77275295\n",
      "Iteration 30, loss = 0.77091134\n",
      "Iteration 31, loss = 0.76939702\n",
      "Iteration 32, loss = 0.76805675\n",
      "Iteration 33, loss = 0.76684038\n",
      "Iteration 34, loss = 0.76565978\n",
      "Iteration 35, loss = 0.76441752\n",
      "Iteration 36, loss = 0.76307222\n",
      "Iteration 37, loss = 0.76154820\n",
      "Iteration 38, loss = 0.75983816\n",
      "Iteration 39, loss = 0.75797092\n",
      "Iteration 40, loss = 0.75590805\n",
      "Iteration 41, loss = 0.75366051\n",
      "Iteration 42, loss = 0.75125733\n",
      "Iteration 43, loss = 0.74875417\n",
      "Iteration 44, loss = 0.74619420\n",
      "Iteration 45, loss = 0.74357612\n",
      "Iteration 46, loss = 0.74094799\n",
      "Iteration 47, loss = 0.73832660\n",
      "Iteration 48, loss = 0.73575432\n",
      "Iteration 49, loss = 0.73330756\n",
      "Iteration 50, loss = 0.73098726\n",
      "Iteration 51, loss = 0.72873423\n",
      "Iteration 52, loss = 0.72655504\n",
      "Iteration 53, loss = 0.72447099\n",
      "Iteration 54, loss = 0.72245445\n",
      "Iteration 55, loss = 0.72049845\n",
      "Iteration 56, loss = 0.71858938\n",
      "Iteration 57, loss = 0.71672628\n",
      "Iteration 58, loss = 0.71489897\n",
      "Iteration 59, loss = 0.71309535\n",
      "Iteration 60, loss = 0.71130531\n",
      "Iteration 61, loss = 0.70952690\n",
      "Iteration 62, loss = 0.70776221\n",
      "Iteration 63, loss = 0.70600465\n",
      "Iteration 64, loss = 0.70424779\n",
      "Iteration 65, loss = 0.70248914\n",
      "Iteration 66, loss = 0.70072867\n",
      "Iteration 67, loss = 0.69896680\n",
      "Iteration 68, loss = 0.69720674\n",
      "Iteration 69, loss = 0.69545275\n",
      "Iteration 70, loss = 0.69370223\n",
      "Iteration 71, loss = 0.69195611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 72, loss = 0.69021529\n",
      "Iteration 73, loss = 0.68848065\n",
      "Iteration 74, loss = 0.68675987\n",
      "Iteration 75, loss = 0.68504987\n",
      "Iteration 76, loss = 0.68334943\n",
      "Iteration 77, loss = 0.68166695\n",
      "Iteration 78, loss = 0.67999443\n",
      "Iteration 79, loss = 0.67833188\n",
      "Iteration 80, loss = 0.67667925\n",
      "Iteration 81, loss = 0.67504150\n",
      "Iteration 82, loss = 0.67341579\n",
      "Iteration 83, loss = 0.67179955\n",
      "Iteration 84, loss = 0.67019263\n",
      "Iteration 85, loss = 0.66859492\n",
      "Iteration 86, loss = 0.66700633\n",
      "Iteration 87, loss = 0.66542678\n",
      "Iteration 88, loss = 0.66385775\n",
      "Iteration 89, loss = 0.66229888\n",
      "Iteration 90, loss = 0.66074837\n",
      "Iteration 91, loss = 0.65920618\n",
      "Iteration 92, loss = 0.65767230\n",
      "Iteration 93, loss = 0.65614672\n",
      "Iteration 94, loss = 0.65462941\n",
      "Iteration 95, loss = 0.65312039\n",
      "Iteration 96, loss = 0.65161963\n",
      "Iteration 97, loss = 0.65012714\n",
      "Iteration 98, loss = 0.64864290\n",
      "Iteration 99, loss = 0.64716691\n",
      "Iteration 100, loss = 0.64569916\n",
      "Iteration 101, loss = 0.64423965\n",
      "Iteration 102, loss = 0.64278835\n",
      "Iteration 103, loss = 0.64134526\n",
      "Iteration 104, loss = 0.63991035\n",
      "Iteration 105, loss = 0.63848362\n",
      "Iteration 106, loss = 0.63706504\n",
      "Iteration 107, loss = 0.63565458\n",
      "Iteration 108, loss = 0.63425222\n",
      "Iteration 109, loss = 0.63285865\n",
      "Iteration 110, loss = 0.63147384\n",
      "Iteration 111, loss = 0.63009704\n",
      "Iteration 112, loss = 0.62872821\n",
      "Iteration 113, loss = 0.62736731\n",
      "Iteration 114, loss = 0.62601435\n",
      "Iteration 115, loss = 0.62466986\n",
      "Iteration 116, loss = 0.62333324\n",
      "Iteration 117, loss = 0.62200446\n",
      "Iteration 118, loss = 0.62068350\n",
      "Iteration 119, loss = 0.61937032\n",
      "Iteration 120, loss = 0.61806520\n",
      "Iteration 121, loss = 0.61676773\n",
      "Iteration 122, loss = 0.61547793\n",
      "Iteration 123, loss = 0.61419580\n",
      "Iteration 124, loss = 0.61292130\n",
      "Iteration 125, loss = 0.61165441\n",
      "Iteration 126, loss = 0.61039510\n",
      "Iteration 127, loss = 0.60914332\n",
      "Iteration 128, loss = 0.60789904\n",
      "Iteration 129, loss = 0.60666221\n",
      "Iteration 130, loss = 0.60543279\n",
      "Iteration 131, loss = 0.60421074\n",
      "Iteration 132, loss = 0.60299599\n",
      "Iteration 133, loss = 0.60178851\n",
      "Iteration 134, loss = 0.60058825\n",
      "Iteration 135, loss = 0.59939515\n",
      "Iteration 136, loss = 0.59820918\n",
      "Iteration 137, loss = 0.59703027\n",
      "Iteration 138, loss = 0.59585838\n",
      "Iteration 139, loss = 0.59469347\n",
      "Iteration 140, loss = 0.59353548\n",
      "Iteration 141, loss = 0.59238437\n",
      "Iteration 142, loss = 0.59124009\n",
      "Iteration 143, loss = 0.59010258\n",
      "Iteration 144, loss = 0.58897181\n",
      "Iteration 145, loss = 0.58784772\n",
      "Iteration 146, loss = 0.58673026\n",
      "Iteration 147, loss = 0.58561938\n",
      "Iteration 148, loss = 0.58451504\n",
      "Iteration 149, loss = 0.58341718\n",
      "Iteration 150, loss = 0.58232575\n",
      "Iteration 151, loss = 0.58124071\n",
      "Iteration 152, loss = 0.58016200\n",
      "Iteration 153, loss = 0.57908956\n",
      "Iteration 154, loss = 0.57802336\n",
      "Iteration 155, loss = 0.57696334\n",
      "Iteration 156, loss = 0.57590945\n",
      "Iteration 157, loss = 0.57486164\n",
      "Iteration 158, loss = 0.57381986\n",
      "Iteration 159, loss = 0.57278406\n",
      "Iteration 160, loss = 0.57175419\n",
      "Iteration 161, loss = 0.57073020\n",
      "Iteration 162, loss = 0.56971205\n",
      "Iteration 163, loss = 0.56869967\n",
      "Iteration 164, loss = 0.56769302\n",
      "Iteration 165, loss = 0.56669206\n",
      "Iteration 166, loss = 0.56569674\n",
      "Iteration 167, loss = 0.56470700\n",
      "Iteration 168, loss = 0.56372279\n",
      "Iteration 169, loss = 0.56274500\n",
      "Iteration 170, loss = 0.56177279\n",
      "Iteration 171, loss = 0.56080602\n",
      "Iteration 172, loss = 0.55984465\n",
      "Iteration 173, loss = 0.55888863\n",
      "Iteration 174, loss = 0.55793793\n",
      "Iteration 175, loss = 0.55699249\n",
      "Iteration 176, loss = 0.55605229\n",
      "Iteration 177, loss = 0.55511726\n",
      "Iteration 178, loss = 0.55418737\n",
      "Iteration 179, loss = 0.55326257\n",
      "Iteration 180, loss = 0.55234281\n",
      "Iteration 181, loss = 0.55142805\n",
      "Iteration 182, loss = 0.55051824\n",
      "Iteration 183, loss = 0.54961333\n",
      "Iteration 184, loss = 0.54871329\n",
      "Iteration 185, loss = 0.54781805\n",
      "Iteration 186, loss = 0.54692758\n",
      "Iteration 187, loss = 0.54604183\n",
      "Iteration 188, loss = 0.54516149\n",
      "Iteration 189, loss = 0.54428599\n",
      "Iteration 190, loss = 0.54341512\n",
      "Iteration 191, loss = 0.54254885\n",
      "Iteration 192, loss = 0.54168715\n",
      "Iteration 193, loss = 0.54082997\n",
      "Iteration 194, loss = 0.53997728\n",
      "Iteration 195, loss = 0.53912904\n",
      "Iteration 196, loss = 0.53828521\n",
      "Iteration 197, loss = 0.53744574\n",
      "Iteration 198, loss = 0.53661059\n",
      "Iteration 199, loss = 0.53577973\n",
      "Iteration 200, loss = 0.53495310\n",
      "Iteration 201, loss = 0.53413067\n",
      "Iteration 202, loss = 0.53331239\n",
      "Iteration 203, loss = 0.53249822\n",
      "Iteration 204, loss = 0.53168811\n",
      "Iteration 205, loss = 0.53088203\n",
      "Iteration 206, loss = 0.53007993\n",
      "Iteration 207, loss = 0.52928177\n",
      "Iteration 208, loss = 0.52848752\n",
      "Iteration 209, loss = 0.52769712\n",
      "Iteration 210, loss = 0.52691054\n",
      "Iteration 211, loss = 0.52612775\n",
      "Iteration 212, loss = 0.52534870\n",
      "Iteration 213, loss = 0.52457335\n",
      "Iteration 214, loss = 0.52380167\n",
      "Iteration 215, loss = 0.52303362\n",
      "Iteration 216, loss = 0.52226917\n",
      "Iteration 217, loss = 0.52150827\n",
      "Iteration 218, loss = 0.52075089\n",
      "Iteration 219, loss = 0.51999700\n",
      "Iteration 220, loss = 0.51924656\n",
      "Iteration 221, loss = 0.51849954\n",
      "Iteration 222, loss = 0.51775590\n",
      "Iteration 223, loss = 0.51701571\n",
      "Iteration 224, loss = 0.51627981\n",
      "Iteration 225, loss = 0.51554724\n",
      "Iteration 226, loss = 0.51481798\n",
      "Iteration 227, loss = 0.51409200\n",
      "Iteration 228, loss = 0.51336926\n",
      "Iteration 229, loss = 0.51264974\n",
      "Iteration 230, loss = 0.51193341\n",
      "Iteration 231, loss = 0.51122023\n",
      "Iteration 232, loss = 0.51051017\n",
      "Iteration 233, loss = 0.50980321\n",
      "Iteration 234, loss = 0.50909931\n",
      "Iteration 235, loss = 0.50839845\n",
      "Iteration 236, loss = 0.50770058\n",
      "Iteration 237, loss = 0.50700568\n",
      "Iteration 238, loss = 0.50631371\n",
      "Iteration 239, loss = 0.50562465\n",
      "Iteration 240, loss = 0.50493860\n",
      "Iteration 241, loss = 0.50425588\n",
      "Iteration 242, loss = 0.50357600\n",
      "Iteration 243, loss = 0.50289894\n",
      "Iteration 244, loss = 0.50222467\n",
      "Iteration 245, loss = 0.50155315\n",
      "Iteration 246, loss = 0.50088436\n",
      "Iteration 247, loss = 0.50021825\n",
      "Iteration 248, loss = 0.49955531\n",
      "Iteration 249, loss = 0.49889546\n",
      "Iteration 250, loss = 0.49823828\n",
      "Iteration 251, loss = 0.49758406\n",
      "Iteration 252, loss = 0.49693269\n",
      "Iteration 253, loss = 0.49628394\n",
      "Iteration 254, loss = 0.49563778\n",
      "Iteration 255, loss = 0.49499417\n",
      "Iteration 256, loss = 0.49435311\n",
      "Iteration 257, loss = 0.49371454\n",
      "Iteration 258, loss = 0.49307846\n",
      "Iteration 259, loss = 0.49244482\n",
      "Iteration 260, loss = 0.49181359\n",
      "Iteration 261, loss = 0.49118477\n",
      "Iteration 262, loss = 0.49055830\n",
      "Iteration 263, loss = 0.48993417\n",
      "Iteration 264, loss = 0.48931235\n",
      "Iteration 265, loss = 0.48869282\n",
      "Iteration 266, loss = 0.48807555\n",
      "Iteration 267, loss = 0.48746051\n",
      "Iteration 268, loss = 0.48684781\n",
      "Iteration 269, loss = 0.48623812\n",
      "Iteration 270, loss = 0.48563064\n",
      "Iteration 271, loss = 0.48502536\n",
      "Iteration 272, loss = 0.48442225\n",
      "Iteration 273, loss = 0.48382130\n",
      "Iteration 274, loss = 0.48322249\n",
      "Iteration 275, loss = 0.48262580\n",
      "Iteration 276, loss = 0.48203121\n",
      "Iteration 277, loss = 0.48143870\n",
      "Iteration 278, loss = 0.48084824\n",
      "Iteration 279, loss = 0.48025983\n",
      "Iteration 280, loss = 0.47967343\n",
      "Iteration 281, loss = 0.47908903\n",
      "Iteration 282, loss = 0.47850660\n",
      "Iteration 283, loss = 0.47792612\n",
      "Iteration 284, loss = 0.47734757\n",
      "Iteration 285, loss = 0.47677094\n",
      "Iteration 286, loss = 0.47619619\n",
      "Iteration 287, loss = 0.47562331\n",
      "Iteration 288, loss = 0.47505228\n",
      "Iteration 289, loss = 0.47448308\n",
      "Iteration 290, loss = 0.47391569\n",
      "Iteration 291, loss = 0.47335009\n",
      "Iteration 292, loss = 0.47278626\n",
      "Iteration 293, loss = 0.47222418\n",
      "Iteration 294, loss = 0.47166384\n",
      "Iteration 295, loss = 0.47110521\n",
      "Iteration 296, loss = 0.47054828\n",
      "Iteration 297, loss = 0.46999303\n",
      "Iteration 298, loss = 0.46943944\n",
      "Iteration 299, loss = 0.46888750\n",
      "Iteration 300, loss = 0.46833718\n",
      "Iteration 301, loss = 0.46778848\n",
      "Iteration 302, loss = 0.46724138\n",
      "Iteration 303, loss = 0.46669585\n",
      "Iteration 304, loss = 0.46615188\n",
      "Iteration 305, loss = 0.46560947\n",
      "Iteration 306, loss = 0.46506859\n",
      "Iteration 307, loss = 0.46452922\n",
      "Iteration 308, loss = 0.46399136\n",
      "Iteration 309, loss = 0.46345498\n",
      "Iteration 310, loss = 0.46292008\n",
      "Iteration 311, loss = 0.46238663\n",
      "Iteration 312, loss = 0.46185463\n",
      "Iteration 313, loss = 0.46132406\n",
      "Iteration 314, loss = 0.46079491\n",
      "Iteration 315, loss = 0.46026715\n",
      "Iteration 316, loss = 0.45974079\n",
      "Iteration 317, loss = 0.45921580\n",
      "Iteration 318, loss = 0.45869217\n",
      "Iteration 319, loss = 0.45816989\n",
      "Iteration 320, loss = 0.45764895\n",
      "Iteration 321, loss = 0.45712932\n",
      "Iteration 322, loss = 0.45661101\n",
      "Iteration 323, loss = 0.45609400\n",
      "Iteration 324, loss = 0.45557827\n",
      "Iteration 325, loss = 0.45506382\n",
      "Iteration 326, loss = 0.45455103\n",
      "Iteration 327, loss = 0.45403962\n",
      "Iteration 328, loss = 0.45352947\n",
      "Iteration 329, loss = 0.45302058\n",
      "Iteration 330, loss = 0.45251292\n",
      "Iteration 331, loss = 0.45200650\n",
      "Iteration 332, loss = 0.45150128\n",
      "Iteration 333, loss = 0.45099736\n",
      "Iteration 334, loss = 0.45049498\n",
      "Iteration 335, loss = 0.44999379\n",
      "Iteration 336, loss = 0.44949380\n",
      "Iteration 337, loss = 0.44899497\n",
      "Iteration 338, loss = 0.44849730\n",
      "Iteration 339, loss = 0.44800152\n",
      "Iteration 340, loss = 0.44750711\n",
      "Iteration 341, loss = 0.44701388\n",
      "Iteration 342, loss = 0.44652181\n",
      "Iteration 343, loss = 0.44603089\n",
      "Iteration 344, loss = 0.44554110\n",
      "Iteration 345, loss = 0.44505243\n",
      "Iteration 346, loss = 0.44456486\n",
      "Iteration 347, loss = 0.44407838\n",
      "Iteration 348, loss = 0.44359299\n",
      "Iteration 349, loss = 0.44310866\n",
      "Iteration 350, loss = 0.44262546\n",
      "Iteration 351, loss = 0.44214351\n",
      "Iteration 352, loss = 0.44166261\n",
      "Iteration 353, loss = 0.44118415\n",
      "Iteration 354, loss = 0.44070798\n",
      "Iteration 355, loss = 0.44023276\n",
      "Iteration 356, loss = 0.43975852\n",
      "Iteration 357, loss = 0.43928534\n",
      "Iteration 358, loss = 0.43881324\n",
      "Iteration 359, loss = 0.43834209\n",
      "Iteration 360, loss = 0.43787195\n",
      "Iteration 361, loss = 0.43740284\n",
      "Iteration 362, loss = 0.43693479\n",
      "Iteration 363, loss = 0.43646793\n",
      "Iteration 364, loss = 0.43600221\n",
      "Iteration 365, loss = 0.43553754\n",
      "Iteration 366, loss = 0.43507393\n",
      "Iteration 367, loss = 0.43461134\n",
      "Iteration 368, loss = 0.43414976\n",
      "Iteration 369, loss = 0.43368918\n",
      "Iteration 370, loss = 0.43322960\n",
      "Iteration 371, loss = 0.43277099\n",
      "Iteration 372, loss = 0.43231335\n",
      "Iteration 373, loss = 0.43185667\n",
      "Iteration 374, loss = 0.43140093\n",
      "Iteration 375, loss = 0.43094620\n",
      "Iteration 376, loss = 0.43049246\n",
      "Iteration 377, loss = 0.43003966\n",
      "Iteration 378, loss = 0.42958778\n",
      "Iteration 379, loss = 0.42913681\n",
      "Iteration 380, loss = 0.42868676\n",
      "Iteration 381, loss = 0.42823760\n",
      "Iteration 382, loss = 0.42778934\n",
      "Iteration 383, loss = 0.42734196\n",
      "Iteration 384, loss = 0.42689546\n",
      "Iteration 385, loss = 0.42644983\n",
      "Iteration 386, loss = 0.42600506\n",
      "Iteration 387, loss = 0.42556114\n",
      "Iteration 388, loss = 0.42511806\n",
      "Iteration 389, loss = 0.42467581\n",
      "Iteration 390, loss = 0.42423439\n",
      "Iteration 391, loss = 0.42379378\n",
      "Iteration 392, loss = 0.42335398\n",
      "Iteration 393, loss = 0.42291497\n",
      "Iteration 394, loss = 0.42247676\n",
      "Iteration 395, loss = 0.42203933\n",
      "Iteration 396, loss = 0.42160266\n",
      "Iteration 397, loss = 0.42116677\n",
      "Iteration 398, loss = 0.42073163\n",
      "Iteration 399, loss = 0.42029724\n",
      "Iteration 400, loss = 0.41986360\n",
      "Iteration 401, loss = 0.41943069\n",
      "Iteration 402, loss = 0.41899851\n",
      "Iteration 403, loss = 0.41856705\n",
      "Iteration 404, loss = 0.41813633\n",
      "Iteration 405, loss = 0.41770670\n",
      "Iteration 406, loss = 0.41727779\n",
      "Iteration 407, loss = 0.41684961\n",
      "Iteration 408, loss = 0.41642214\n",
      "Iteration 409, loss = 0.41599540\n",
      "Iteration 410, loss = 0.41556936\n",
      "Iteration 411, loss = 0.41514402\n",
      "Iteration 412, loss = 0.41471939\n",
      "Iteration 413, loss = 0.41429546\n",
      "Iteration 414, loss = 0.41387221\n",
      "Iteration 415, loss = 0.41344964\n",
      "Iteration 416, loss = 0.41302775\n",
      "Iteration 417, loss = 0.41260653\n",
      "Iteration 418, loss = 0.41218597\n",
      "Iteration 419, loss = 0.41176607\n",
      "Iteration 420, loss = 0.41134682\n",
      "Iteration 421, loss = 0.41092821\n",
      "Iteration 422, loss = 0.41051025\n",
      "Iteration 423, loss = 0.41009291\n",
      "Iteration 424, loss = 0.40967621\n",
      "Iteration 425, loss = 0.40926013\n",
      "Iteration 426, loss = 0.40884467\n",
      "Iteration 427, loss = 0.40842982\n",
      "Iteration 428, loss = 0.40801557\n",
      "Iteration 429, loss = 0.40760193\n",
      "Iteration 430, loss = 0.40718888\n",
      "Iteration 431, loss = 0.40677643\n",
      "Iteration 432, loss = 0.40636456\n",
      "Iteration 433, loss = 0.40595327\n",
      "Iteration 434, loss = 0.40554262\n",
      "Iteration 435, loss = 0.40513266\n",
      "Iteration 436, loss = 0.40472328\n",
      "Iteration 437, loss = 0.40431447\n",
      "Iteration 438, loss = 0.40390623\n",
      "Iteration 439, loss = 0.40349856\n",
      "Iteration 440, loss = 0.40309146\n",
      "Iteration 441, loss = 0.40268492\n",
      "Iteration 442, loss = 0.40227892\n",
      "Iteration 443, loss = 0.40187348\n",
      "Iteration 444, loss = 0.40146859\n",
      "Iteration 445, loss = 0.40106423\n",
      "Iteration 446, loss = 0.40066041\n",
      "Iteration 447, loss = 0.40025713\n",
      "Iteration 448, loss = 0.39985437\n",
      "Iteration 449, loss = 0.39945213\n",
      "Iteration 450, loss = 0.39905041\n",
      "Iteration 451, loss = 0.39864920\n",
      "Iteration 452, loss = 0.39824851\n",
      "Iteration 453, loss = 0.39784832\n",
      "Iteration 454, loss = 0.39744863\n",
      "Iteration 455, loss = 0.39704944\n",
      "Iteration 456, loss = 0.39665075\n",
      "Iteration 457, loss = 0.39625254\n",
      "Iteration 458, loss = 0.39585483\n",
      "Iteration 459, loss = 0.39545759\n",
      "Iteration 460, loss = 0.39506084\n",
      "Iteration 461, loss = 0.39466456\n",
      "Iteration 462, loss = 0.39426876\n",
      "Iteration 463, loss = 0.39387342\n",
      "Iteration 464, loss = 0.39347855\n",
      "Iteration 465, loss = 0.39308414\n",
      "Iteration 466, loss = 0.39269019\n",
      "Iteration 467, loss = 0.39229670\n",
      "Iteration 468, loss = 0.39190366\n",
      "Iteration 469, loss = 0.39151137\n",
      "Iteration 470, loss = 0.39111964\n",
      "Iteration 471, loss = 0.39072838\n",
      "Iteration 472, loss = 0.39033758\n",
      "Iteration 473, loss = 0.38994724\n",
      "Iteration 474, loss = 0.38955737\n",
      "Iteration 475, loss = 0.38916796\n",
      "Iteration 476, loss = 0.38877900\n",
      "Iteration 477, loss = 0.38839049\n",
      "Iteration 478, loss = 0.38800243\n",
      "Iteration 479, loss = 0.38761482\n",
      "Iteration 480, loss = 0.38722765\n",
      "Iteration 481, loss = 0.38684091\n",
      "Iteration 482, loss = 0.38645461\n",
      "Iteration 483, loss = 0.38606874\n",
      "Iteration 484, loss = 0.38568330\n",
      "Iteration 485, loss = 0.38529829\n",
      "Iteration 486, loss = 0.38491369\n",
      "Iteration 487, loss = 0.38452952\n",
      "Iteration 488, loss = 0.38414579\n",
      "Iteration 489, loss = 0.38376272\n",
      "Iteration 490, loss = 0.38338008\n",
      "Iteration 491, loss = 0.38299787\n",
      "Iteration 492, loss = 0.38261608\n",
      "Iteration 493, loss = 0.38223471\n",
      "Iteration 494, loss = 0.38185376\n",
      "Iteration 495, loss = 0.38147323\n",
      "Iteration 496, loss = 0.38109310\n",
      "Iteration 497, loss = 0.38071339\n",
      "Iteration 498, loss = 0.38033409\n",
      "Iteration 499, loss = 0.37995519\n",
      "Iteration 500, loss = 0.37957669\n",
      "Iteration 501, loss = 0.37919859\n",
      "Iteration 502, loss = 0.37882088\n",
      "Iteration 503, loss = 0.37844357\n",
      "Iteration 504, loss = 0.37806665\n",
      "Iteration 505, loss = 0.37769012\n",
      "Iteration 506, loss = 0.37731398\n",
      "Iteration 507, loss = 0.37693821\n",
      "Iteration 508, loss = 0.37656283\n",
      "Iteration 509, loss = 0.37618783\n",
      "Iteration 510, loss = 0.37581320\n",
      "Iteration 511, loss = 0.37543895\n",
      "Iteration 512, loss = 0.37506507\n",
      "Iteration 513, loss = 0.37469156\n",
      "Iteration 514, loss = 0.37431842\n",
      "Iteration 515, loss = 0.37394564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 516, loss = 0.37357323\n",
      "Iteration 517, loss = 0.37320118\n",
      "Iteration 518, loss = 0.37282949\n",
      "Iteration 519, loss = 0.37245816\n",
      "Iteration 520, loss = 0.37208719\n",
      "Iteration 521, loss = 0.37171657\n",
      "Iteration 522, loss = 0.37134630\n",
      "Iteration 523, loss = 0.37097639\n",
      "Iteration 524, loss = 0.37060683\n",
      "Iteration 525, loss = 0.37023762\n",
      "Iteration 526, loss = 0.36986875\n",
      "Iteration 527, loss = 0.36950023\n",
      "Iteration 528, loss = 0.36913206\n",
      "Iteration 529, loss = 0.36876423\n",
      "Iteration 530, loss = 0.36839674\n",
      "Iteration 531, loss = 0.36802959\n",
      "Iteration 532, loss = 0.36766278\n",
      "Iteration 533, loss = 0.36729631\n",
      "Iteration 534, loss = 0.36693018\n",
      "Iteration 535, loss = 0.36656438\n",
      "Iteration 536, loss = 0.36619891\n",
      "Iteration 537, loss = 0.36583378\n",
      "Iteration 538, loss = 0.36546898\n",
      "Iteration 539, loss = 0.36510451\n",
      "Iteration 540, loss = 0.36474037\n",
      "Iteration 541, loss = 0.36437656\n",
      "Iteration 542, loss = 0.36401308\n",
      "Iteration 543, loss = 0.36364993\n",
      "Iteration 544, loss = 0.36328710\n",
      "Iteration 545, loss = 0.36292459\n",
      "Iteration 546, loss = 0.36256241\n",
      "Iteration 547, loss = 0.36220067\n",
      "Iteration 548, loss = 0.36183957\n",
      "Iteration 549, loss = 0.36147881\n",
      "Iteration 550, loss = 0.36111839\n",
      "Iteration 551, loss = 0.36075832\n",
      "Iteration 552, loss = 0.36039859\n",
      "Iteration 553, loss = 0.36003919\n",
      "Iteration 554, loss = 0.35968014\n",
      "Iteration 555, loss = 0.35932141\n",
      "Iteration 556, loss = 0.35896302\n",
      "Iteration 557, loss = 0.35860496\n",
      "Iteration 558, loss = 0.35824723\n",
      "Iteration 559, loss = 0.35788983\n",
      "Iteration 560, loss = 0.35753275\n",
      "Iteration 561, loss = 0.35717599\n",
      "Iteration 562, loss = 0.35681956\n",
      "Iteration 563, loss = 0.35646345\n",
      "Iteration 564, loss = 0.35610766\n",
      "Iteration 565, loss = 0.35575219\n",
      "Iteration 566, loss = 0.35539703\n",
      "Iteration 567, loss = 0.35504219\n",
      "Iteration 568, loss = 0.35468767\n",
      "Iteration 569, loss = 0.35433346\n",
      "Iteration 570, loss = 0.35397957\n",
      "Iteration 571, loss = 0.35362599\n",
      "Iteration 572, loss = 0.35327272\n",
      "Iteration 573, loss = 0.35291977\n",
      "Iteration 574, loss = 0.35256712\n",
      "Iteration 575, loss = 0.35221479\n",
      "Iteration 576, loss = 0.35186276\n",
      "Iteration 577, loss = 0.35151105\n",
      "Iteration 578, loss = 0.35115964\n",
      "Iteration 579, loss = 0.35080854\n",
      "Iteration 580, loss = 0.35045774\n",
      "Iteration 581, loss = 0.35010725\n",
      "Iteration 582, loss = 0.34975707\n",
      "Iteration 583, loss = 0.34940719\n",
      "Iteration 584, loss = 0.34905762\n",
      "Iteration 585, loss = 0.34870835\n",
      "Iteration 586, loss = 0.34835939\n",
      "Iteration 587, loss = 0.34801072\n",
      "Iteration 588, loss = 0.34766237\n",
      "Iteration 589, loss = 0.34731431\n",
      "Iteration 590, loss = 0.34696656\n",
      "Iteration 591, loss = 0.34661911\n",
      "Iteration 592, loss = 0.34627196\n",
      "Iteration 593, loss = 0.34592511\n",
      "Iteration 594, loss = 0.34557856\n",
      "Iteration 595, loss = 0.34523231\n",
      "Iteration 596, loss = 0.34488637\n",
      "Iteration 597, loss = 0.34454072\n",
      "Iteration 598, loss = 0.34419538\n",
      "Iteration 599, loss = 0.34385033\n",
      "Iteration 600, loss = 0.34350558\n",
      "Iteration 601, loss = 0.34316114\n",
      "Iteration 602, loss = 0.34281699\n",
      "Iteration 603, loss = 0.34247314\n",
      "Iteration 604, loss = 0.34212959\n",
      "Iteration 605, loss = 0.34178634\n",
      "Iteration 606, loss = 0.34144339\n",
      "Iteration 607, loss = 0.34110074\n",
      "Iteration 608, loss = 0.34075838\n",
      "Iteration 609, loss = 0.34041644\n",
      "Iteration 610, loss = 0.34007515\n",
      "Iteration 611, loss = 0.33973418\n",
      "Iteration 612, loss = 0.33939353\n",
      "Iteration 613, loss = 0.33905321\n",
      "Iteration 614, loss = 0.33871320\n",
      "Iteration 615, loss = 0.33837352\n",
      "Iteration 616, loss = 0.33803415\n",
      "Iteration 617, loss = 0.33769509\n",
      "Iteration 618, loss = 0.33735635\n",
      "Iteration 619, loss = 0.33701793\n",
      "Iteration 620, loss = 0.33667981\n",
      "Iteration 621, loss = 0.33634201\n",
      "Iteration 622, loss = 0.33600451\n",
      "Iteration 623, loss = 0.33566733\n",
      "Iteration 624, loss = 0.33533045\n",
      "Iteration 625, loss = 0.33499387\n",
      "Iteration 626, loss = 0.33465761\n",
      "Iteration 627, loss = 0.33432165\n",
      "Iteration 628, loss = 0.33398600\n",
      "Iteration 629, loss = 0.33365065\n",
      "Iteration 630, loss = 0.33331561\n",
      "Iteration 631, loss = 0.33298087\n",
      "Iteration 632, loss = 0.33264644\n",
      "Iteration 633, loss = 0.33231231\n",
      "Iteration 634, loss = 0.33197848\n",
      "Iteration 635, loss = 0.33164496\n",
      "Iteration 636, loss = 0.33131175\n",
      "Iteration 637, loss = 0.33097883\n",
      "Iteration 638, loss = 0.33064622\n",
      "Iteration 639, loss = 0.33031391\n",
      "Iteration 640, loss = 0.32998190\n",
      "Iteration 641, loss = 0.32965020\n",
      "Iteration 642, loss = 0.32931880\n",
      "Iteration 643, loss = 0.32898770\n",
      "Iteration 644, loss = 0.32865690\n",
      "Iteration 645, loss = 0.32832641\n",
      "Iteration 646, loss = 0.32799622\n",
      "Iteration 647, loss = 0.32766633\n",
      "Iteration 648, loss = 0.32733674\n",
      "Iteration 649, loss = 0.32700746\n",
      "Iteration 650, loss = 0.32667848\n",
      "Iteration 651, loss = 0.32634980\n",
      "Iteration 652, loss = 0.32602143\n",
      "Iteration 653, loss = 0.32569335\n",
      "Iteration 654, loss = 0.32536558\n",
      "Iteration 655, loss = 0.32503812\n",
      "Iteration 656, loss = 0.32471095\n",
      "Iteration 657, loss = 0.32438409\n",
      "Iteration 658, loss = 0.32405754\n",
      "Iteration 659, loss = 0.32373129\n",
      "Iteration 660, loss = 0.32340534\n",
      "Iteration 661, loss = 0.32307969\n",
      "Iteration 662, loss = 0.32275435\n",
      "Iteration 663, loss = 0.32242932\n",
      "Iteration 664, loss = 0.32210458\n",
      "Iteration 665, loss = 0.32178016\n",
      "Iteration 666, loss = 0.32145604\n",
      "Iteration 667, loss = 0.32113222\n",
      "Iteration 668, loss = 0.32080871\n",
      "Iteration 669, loss = 0.32048550\n",
      "Iteration 670, loss = 0.32016260\n",
      "Iteration 671, loss = 0.31984001\n",
      "Iteration 672, loss = 0.31951772\n",
      "Iteration 673, loss = 0.31919574\n",
      "Iteration 674, loss = 0.31887406\n",
      "Iteration 675, loss = 0.31855269\n",
      "Iteration 676, loss = 0.31823163\n",
      "Iteration 677, loss = 0.31791088\n",
      "Iteration 678, loss = 0.31759043\n",
      "Iteration 679, loss = 0.31727029\n",
      "Iteration 680, loss = 0.31695046\n",
      "Iteration 681, loss = 0.31663094\n",
      "Iteration 682, loss = 0.31631198\n",
      "Iteration 683, loss = 0.31599341\n",
      "Iteration 684, loss = 0.31567516\n",
      "Iteration 685, loss = 0.31535724\n",
      "Iteration 686, loss = 0.31503965\n",
      "Iteration 687, loss = 0.31472239\n",
      "Iteration 688, loss = 0.31440544\n",
      "Iteration 689, loss = 0.31408923\n",
      "Iteration 690, loss = 0.31377335\n",
      "Iteration 691, loss = 0.31345782\n",
      "Iteration 692, loss = 0.31314262\n",
      "Iteration 693, loss = 0.31282776\n",
      "Iteration 694, loss = 0.31251323\n",
      "Iteration 695, loss = 0.31219904\n",
      "Iteration 696, loss = 0.31188518\n",
      "Iteration 697, loss = 0.31157165\n",
      "Iteration 698, loss = 0.31125845\n",
      "Iteration 699, loss = 0.31094557\n",
      "Iteration 700, loss = 0.31063302\n",
      "Iteration 701, loss = 0.31032080\n",
      "Iteration 702, loss = 0.31000890\n",
      "Iteration 703, loss = 0.30969733\n",
      "Iteration 704, loss = 0.30938608\n",
      "Iteration 705, loss = 0.30907515\n",
      "Iteration 706, loss = 0.30876455\n",
      "Iteration 707, loss = 0.30845428\n",
      "Iteration 708, loss = 0.30814432\n",
      "Iteration 709, loss = 0.30783469\n",
      "Iteration 710, loss = 0.30752538\n",
      "Iteration 711, loss = 0.30721640\n",
      "Iteration 712, loss = 0.30690773\n",
      "Iteration 713, loss = 0.30659939\n",
      "Iteration 714, loss = 0.30629136\n",
      "Iteration 715, loss = 0.30598366\n",
      "Iteration 716, loss = 0.30567628\n",
      "Iteration 717, loss = 0.30536922\n",
      "Iteration 718, loss = 0.30506248\n",
      "Iteration 719, loss = 0.30475606\n",
      "Iteration 720, loss = 0.30444997\n",
      "Iteration 721, loss = 0.30414419\n",
      "Iteration 722, loss = 0.30383874\n",
      "Iteration 723, loss = 0.30353360\n",
      "Iteration 724, loss = 0.30322879\n",
      "Iteration 725, loss = 0.30292430\n",
      "Iteration 726, loss = 0.30262013\n",
      "Iteration 727, loss = 0.30231640\n",
      "Iteration 728, loss = 0.30201322\n",
      "Iteration 729, loss = 0.30171037\n",
      "Iteration 730, loss = 0.30140787\n",
      "Iteration 731, loss = 0.30110570\n",
      "Iteration 732, loss = 0.30080387\n",
      "Iteration 733, loss = 0.30050238\n",
      "Iteration 734, loss = 0.30020122\n",
      "Iteration 735, loss = 0.29990040\n",
      "Iteration 736, loss = 0.29959992\n",
      "Iteration 737, loss = 0.29929977\n",
      "Iteration 738, loss = 0.29899995\n",
      "Iteration 739, loss = 0.29870046\n",
      "Iteration 740, loss = 0.29840130\n",
      "Iteration 741, loss = 0.29810248\n",
      "Iteration 742, loss = 0.29780398\n",
      "Iteration 743, loss = 0.29750582\n",
      "Iteration 744, loss = 0.29720798\n",
      "Iteration 745, loss = 0.29691048\n",
      "Iteration 746, loss = 0.29661330\n",
      "Iteration 747, loss = 0.29631646\n",
      "Iteration 748, loss = 0.29601995\n",
      "Iteration 749, loss = 0.29572376\n",
      "Iteration 750, loss = 0.29542791\n",
      "Iteration 751, loss = 0.29513239\n",
      "Iteration 752, loss = 0.29483719\n",
      "Iteration 753, loss = 0.29454233\n",
      "Iteration 754, loss = 0.29424779\n",
      "Iteration 755, loss = 0.29395359\n",
      "Iteration 756, loss = 0.29365971\n",
      "Iteration 757, loss = 0.29336616\n",
      "Iteration 758, loss = 0.29307295\n",
      "Iteration 759, loss = 0.29278006\n",
      "Iteration 760, loss = 0.29248750\n",
      "Iteration 761, loss = 0.29219527\n",
      "Iteration 762, loss = 0.29190337\n",
      "Iteration 763, loss = 0.29161181\n",
      "Iteration 764, loss = 0.29132057\n",
      "Iteration 765, loss = 0.29102966\n",
      "Iteration 766, loss = 0.29073908\n",
      "Iteration 767, loss = 0.29044884\n",
      "Iteration 768, loss = 0.29015892\n",
      "Iteration 769, loss = 0.28986934\n",
      "Iteration 770, loss = 0.28958008\n",
      "Iteration 771, loss = 0.28929116\n",
      "Iteration 772, loss = 0.28900257\n",
      "Iteration 773, loss = 0.28871431\n",
      "Iteration 774, loss = 0.28842638\n",
      "Iteration 775, loss = 0.28813878\n",
      "Iteration 776, loss = 0.28785152\n",
      "Iteration 777, loss = 0.28756459\n",
      "Iteration 778, loss = 0.28727798\n",
      "Iteration 779, loss = 0.28699172\n",
      "Iteration 780, loss = 0.28670578\n",
      "Iteration 781, loss = 0.28642018\n",
      "Iteration 782, loss = 0.28613491\n",
      "Iteration 783, loss = 0.28584997\n",
      "Iteration 784, loss = 0.28556537\n",
      "Iteration 785, loss = 0.28528109\n",
      "Iteration 786, loss = 0.28499716\n",
      "Iteration 787, loss = 0.28471355\n",
      "Iteration 788, loss = 0.28443028\n",
      "Iteration 789, loss = 0.28414735\n",
      "Iteration 790, loss = 0.28386475\n",
      "Iteration 791, loss = 0.28358248\n",
      "Iteration 792, loss = 0.28330054\n",
      "Iteration 793, loss = 0.28301895\n",
      "Iteration 794, loss = 0.28273768\n",
      "Iteration 795, loss = 0.28245675\n",
      "Iteration 796, loss = 0.28217616\n",
      "Iteration 797, loss = 0.28189590\n",
      "Iteration 798, loss = 0.28161598\n",
      "Iteration 799, loss = 0.28133639\n",
      "Iteration 800, loss = 0.28105714\n",
      "Iteration 801, loss = 0.28077822\n",
      "Iteration 802, loss = 0.28049964\n",
      "Iteration 803, loss = 0.28022139\n",
      "Iteration 804, loss = 0.27994349\n",
      "Iteration 805, loss = 0.27966591\n",
      "Iteration 806, loss = 0.27938868\n",
      "Iteration 807, loss = 0.27911178\n",
      "Iteration 808, loss = 0.27883522\n",
      "Iteration 809, loss = 0.27855899\n",
      "Iteration 810, loss = 0.27828310\n",
      "Iteration 811, loss = 0.27800755\n",
      "Iteration 812, loss = 0.27773234\n",
      "Iteration 813, loss = 0.27745746\n",
      "Iteration 814, loss = 0.27718292\n",
      "Iteration 815, loss = 0.27690872\n",
      "Iteration 816, loss = 0.27663486\n",
      "Iteration 817, loss = 0.27636133\n",
      "Iteration 818, loss = 0.27608814\n",
      "Iteration 819, loss = 0.27581529\n",
      "Iteration 820, loss = 0.27554278\n",
      "Iteration 821, loss = 0.27527061\n",
      "Iteration 822, loss = 0.27499877\n",
      "Iteration 823, loss = 0.27472728\n",
      "Iteration 824, loss = 0.27445612\n",
      "Iteration 825, loss = 0.27418530\n",
      "Iteration 826, loss = 0.27391482\n",
      "Iteration 827, loss = 0.27364468\n",
      "Iteration 828, loss = 0.27337488\n",
      "Iteration 829, loss = 0.27310542\n",
      "Iteration 830, loss = 0.27283629\n",
      "Iteration 831, loss = 0.27256751\n",
      "Iteration 832, loss = 0.27229906\n",
      "Iteration 833, loss = 0.27203096\n",
      "Iteration 834, loss = 0.27176319\n",
      "Iteration 835, loss = 0.27149577\n",
      "Iteration 836, loss = 0.27122868\n",
      "Iteration 837, loss = 0.27096194\n",
      "Iteration 838, loss = 0.27069553\n",
      "Iteration 839, loss = 0.27042946\n",
      "Iteration 840, loss = 0.27016374\n",
      "Iteration 841, loss = 0.26989835\n",
      "Iteration 842, loss = 0.26963331\n",
      "Iteration 843, loss = 0.26936860\n",
      "Iteration 844, loss = 0.26910423\n",
      "Iteration 845, loss = 0.26884021\n",
      "Iteration 846, loss = 0.26857653\n",
      "Iteration 847, loss = 0.26831318\n",
      "Iteration 848, loss = 0.26805018\n",
      "Iteration 849, loss = 0.26778752\n",
      "Iteration 850, loss = 0.26752519\n",
      "Iteration 851, loss = 0.26726321\n",
      "Iteration 852, loss = 0.26700157\n",
      "Iteration 853, loss = 0.26674027\n",
      "Iteration 854, loss = 0.26647931\n",
      "Iteration 855, loss = 0.26621870\n",
      "Iteration 856, loss = 0.26595842\n",
      "Iteration 857, loss = 0.26569848\n",
      "Iteration 858, loss = 0.26543889\n",
      "Iteration 859, loss = 0.26517963\n",
      "Iteration 860, loss = 0.26492072\n",
      "Iteration 861, loss = 0.26466215\n",
      "Iteration 862, loss = 0.26440392\n",
      "Iteration 863, loss = 0.26414603\n",
      "Iteration 864, loss = 0.26388848\n",
      "Iteration 865, loss = 0.26363127\n",
      "Iteration 866, loss = 0.26337441\n",
      "Iteration 867, loss = 0.26311788\n",
      "Iteration 868, loss = 0.26286170\n",
      "Iteration 869, loss = 0.26260586\n",
      "Iteration 870, loss = 0.26235036\n",
      "Iteration 871, loss = 0.26209520\n",
      "Iteration 872, loss = 0.26184038\n",
      "Iteration 873, loss = 0.26158590\n",
      "Iteration 874, loss = 0.26133177\n",
      "Iteration 875, loss = 0.26107797\n",
      "Iteration 876, loss = 0.26082452\n",
      "Iteration 877, loss = 0.26057140\n",
      "Iteration 878, loss = 0.26031863\n",
      "Iteration 879, loss = 0.26006646\n",
      "Iteration 880, loss = 0.25981470\n",
      "Iteration 881, loss = 0.25956331\n",
      "Iteration 882, loss = 0.25931226\n",
      "Iteration 883, loss = 0.25906158\n",
      "Iteration 884, loss = 0.25881124\n",
      "Iteration 885, loss = 0.25856127\n",
      "Iteration 886, loss = 0.25831164\n",
      "Iteration 887, loss = 0.25806236\n",
      "Iteration 888, loss = 0.25781344\n",
      "Iteration 889, loss = 0.25756486\n",
      "Iteration 890, loss = 0.25731663\n",
      "Iteration 891, loss = 0.25706874\n",
      "Iteration 892, loss = 0.25682120\n",
      "Iteration 893, loss = 0.25657401\n",
      "Iteration 894, loss = 0.25632716\n",
      "Iteration 895, loss = 0.25608065\n",
      "Iteration 896, loss = 0.25583449\n",
      "Iteration 897, loss = 0.25558868\n",
      "Iteration 898, loss = 0.25534320\n",
      "Iteration 899, loss = 0.25509807\n",
      "Iteration 900, loss = 0.25485328\n",
      "Iteration 901, loss = 0.25460884\n",
      "Iteration 902, loss = 0.25436473\n",
      "Iteration 903, loss = 0.25412097\n",
      "Iteration 904, loss = 0.25387755\n",
      "Iteration 905, loss = 0.25363447\n",
      "Iteration 906, loss = 0.25339172\n",
      "Iteration 907, loss = 0.25314932\n",
      "Iteration 908, loss = 0.25290726\n",
      "Iteration 909, loss = 0.25266554\n",
      "Iteration 910, loss = 0.25242416\n",
      "Iteration 911, loss = 0.25218312\n",
      "Iteration 912, loss = 0.25194242\n",
      "Iteration 913, loss = 0.25170206\n",
      "Iteration 914, loss = 0.25146203\n",
      "Iteration 915, loss = 0.25122235\n",
      "Iteration 916, loss = 0.25098300\n",
      "Iteration 917, loss = 0.25074399\n",
      "Iteration 918, loss = 0.25050532\n",
      "Iteration 919, loss = 0.25026699\n",
      "Iteration 920, loss = 0.25002900\n",
      "Iteration 921, loss = 0.24979134\n",
      "Iteration 922, loss = 0.24955402\n",
      "Iteration 923, loss = 0.24931704\n",
      "Iteration 924, loss = 0.24908039\n",
      "Iteration 925, loss = 0.24884408\n",
      "Iteration 926, loss = 0.24860811\n",
      "Iteration 927, loss = 0.24837248\n",
      "Iteration 928, loss = 0.24813718\n",
      "Iteration 929, loss = 0.24790222\n",
      "Iteration 930, loss = 0.24766759\n",
      "Iteration 931, loss = 0.24743330\n",
      "Iteration 932, loss = 0.24719935\n",
      "Iteration 933, loss = 0.24696573\n",
      "Iteration 934, loss = 0.24673245\n",
      "Iteration 935, loss = 0.24649950\n",
      "Iteration 936, loss = 0.24626689\n",
      "Iteration 937, loss = 0.24603462\n",
      "Iteration 938, loss = 0.24580267\n",
      "Iteration 939, loss = 0.24557107\n",
      "Iteration 940, loss = 0.24533980\n",
      "Iteration 941, loss = 0.24510886\n",
      "Iteration 942, loss = 0.24487826\n",
      "Iteration 943, loss = 0.24464799\n",
      "Iteration 944, loss = 0.24441805\n",
      "Iteration 945, loss = 0.24418845\n",
      "Iteration 946, loss = 0.24395918\n",
      "Iteration 947, loss = 0.24373025\n",
      "Iteration 948, loss = 0.24350165\n",
      "Iteration 949, loss = 0.24327338\n",
      "Iteration 950, loss = 0.24304545\n",
      "Iteration 951, loss = 0.24281784\n",
      "Iteration 952, loss = 0.24259057\n",
      "Iteration 953, loss = 0.24236364\n",
      "Iteration 954, loss = 0.24213703\n",
      "Iteration 955, loss = 0.24191076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 956, loss = 0.24168482\n",
      "Iteration 957, loss = 0.24145921\n",
      "Iteration 958, loss = 0.24123393\n",
      "Iteration 959, loss = 0.24100898\n",
      "Iteration 960, loss = 0.24078437\n",
      "Iteration 961, loss = 0.24056008\n",
      "Iteration 962, loss = 0.24033613\n",
      "Iteration 963, loss = 0.24011251\n",
      "Iteration 964, loss = 0.23988921\n",
      "Iteration 965, loss = 0.23966625\n",
      "Iteration 966, loss = 0.23944362\n",
      "Iteration 967, loss = 0.23922131\n",
      "Iteration 968, loss = 0.23899934\n",
      "Iteration 969, loss = 0.23877769\n",
      "Iteration 970, loss = 0.23855638\n",
      "Iteration 971, loss = 0.23833539\n",
      "Iteration 972, loss = 0.23811473\n",
      "Iteration 973, loss = 0.23789440\n",
      "Iteration 974, loss = 0.23767440\n",
      "Iteration 975, loss = 0.23745472\n",
      "Iteration 976, loss = 0.23723538\n",
      "Iteration 977, loss = 0.23701636\n",
      "Iteration 978, loss = 0.23679766\n",
      "Iteration 979, loss = 0.23657930\n",
      "Iteration 980, loss = 0.23636126\n",
      "Iteration 981, loss = 0.23614355\n",
      "Iteration 982, loss = 0.23592616\n",
      "Iteration 983, loss = 0.23570910\n",
      "Iteration 984, loss = 0.23549237\n",
      "Iteration 985, loss = 0.23527596\n",
      "Iteration 986, loss = 0.23505988\n",
      "Iteration 987, loss = 0.23484412\n",
      "Iteration 988, loss = 0.23462869\n",
      "Iteration 989, loss = 0.23441358\n",
      "Iteration 990, loss = 0.23419880\n",
      "Iteration 991, loss = 0.23398434\n",
      "Iteration 992, loss = 0.23377020\n",
      "Iteration 993, loss = 0.23355639\n",
      "Iteration 994, loss = 0.23334290\n",
      "Iteration 995, loss = 0.23312974\n",
      "Iteration 996, loss = 0.23291690\n",
      "Iteration 997, loss = 0.23270438\n",
      "Iteration 998, loss = 0.23249218\n",
      "Iteration 999, loss = 0.23228031\n",
      "Iteration 1000, loss = 0.23206875\n",
      "Iteration 1, loss = 2.10901821\n",
      "Iteration 2, loss = 1.57123119\n",
      "Iteration 3, loss = 1.53076127\n",
      "Iteration 4, loss = 1.12625997\n",
      "Iteration 5, loss = 0.79129966\n",
      "Iteration 6, loss = 0.68642657\n",
      "Iteration 7, loss = 0.60454563\n",
      "Iteration 8, loss = 0.54912810\n",
      "Iteration 9, loss = 0.52156231\n",
      "Iteration 10, loss = 0.49431729\n",
      "Iteration 11, loss = 0.46996044\n",
      "Iteration 12, loss = 0.44117362\n",
      "Iteration 13, loss = 0.39102940\n",
      "Iteration 14, loss = 0.37599798\n",
      "Iteration 15, loss = 0.33251421\n",
      "Iteration 16, loss = 0.28828998\n",
      "Iteration 17, loss = 0.27998385\n",
      "Iteration 18, loss = 0.22364036\n",
      "Iteration 19, loss = 0.21497560\n",
      "Iteration 20, loss = 0.19979010\n",
      "Iteration 21, loss = 0.15806228\n",
      "Iteration 22, loss = 0.16963811\n",
      "Iteration 23, loss = 0.14453239\n",
      "Iteration 24, loss = 0.12764906\n",
      "Iteration 25, loss = 0.13582106\n",
      "Iteration 26, loss = 0.10665842\n",
      "Iteration 27, loss = 0.11432953\n",
      "Iteration 28, loss = 0.09930232\n",
      "Iteration 29, loss = 0.09708421\n",
      "Iteration 30, loss = 0.09713683\n",
      "Iteration 31, loss = 0.08596054\n",
      "Iteration 32, loss = 0.09414500\n",
      "Iteration 33, loss = 0.08086215\n",
      "Iteration 34, loss = 0.08473292\n",
      "Iteration 35, loss = 0.07956176\n",
      "Iteration 36, loss = 0.07670924\n",
      "Iteration 37, loss = 0.07934495\n",
      "Iteration 38, loss = 0.07257420\n",
      "Iteration 39, loss = 0.07777740\n",
      "Iteration 40, loss = 0.07306974\n",
      "Iteration 41, loss = 0.07183562\n",
      "Iteration 42, loss = 0.07423060\n",
      "Iteration 43, loss = 0.06868068\n",
      "Iteration 44, loss = 0.07043618\n",
      "Iteration 45, loss = 0.06940752\n",
      "Iteration 46, loss = 0.06667013\n",
      "Iteration 47, loss = 0.06836856\n",
      "Iteration 48, loss = 0.06634221\n",
      "Iteration 49, loss = 0.06559612\n",
      "Iteration 50, loss = 0.06654517\n",
      "Iteration 51, loss = 0.06464564\n",
      "Iteration 52, loss = 0.06459083\n",
      "Iteration 53, loss = 0.06516419\n",
      "Iteration 54, loss = 0.06356772\n",
      "Iteration 55, loss = 0.06352037\n",
      "Iteration 56, loss = 0.06406399\n",
      "Iteration 57, loss = 0.06283190\n",
      "Iteration 58, loss = 0.06245349\n",
      "Iteration 59, loss = 0.06297087\n",
      "Iteration 60, loss = 0.06233009\n",
      "Iteration 61, loss = 0.06161743\n",
      "Iteration 62, loss = 0.06176182\n",
      "Iteration 63, loss = 0.06179815\n",
      "Iteration 64, loss = 0.06128512\n",
      "Iteration 65, loss = 0.06083418\n",
      "Iteration 66, loss = 0.06090825\n",
      "Iteration 67, loss = 0.06097728\n",
      "Iteration 68, loss = 0.06056972\n",
      "Iteration 69, loss = 0.06015831\n",
      "Iteration 70, loss = 0.06005859\n",
      "Iteration 71, loss = 0.06010385\n",
      "Iteration 72, loss = 0.06001526\n",
      "Iteration 73, loss = 0.05969049\n",
      "Iteration 74, loss = 0.05938856\n",
      "Iteration 75, loss = 0.05923962\n",
      "Iteration 76, loss = 0.05920918\n",
      "Iteration 77, loss = 0.05918510\n",
      "Iteration 78, loss = 0.05905459\n",
      "Iteration 79, loss = 0.05886658\n",
      "Iteration 80, loss = 0.05863320\n",
      "Iteration 81, loss = 0.05842758\n",
      "Iteration 82, loss = 0.05826233\n",
      "Iteration 83, loss = 0.05813791\n",
      "Iteration 84, loss = 0.05804472\n",
      "Iteration 85, loss = 0.05797503\n",
      "Iteration 86, loss = 0.05794119\n",
      "Iteration 87, loss = 0.05794660\n",
      "Iteration 88, loss = 0.05809047\n",
      "Iteration 89, loss = 0.05838358\n",
      "Iteration 90, loss = 0.05933340\n",
      "Iteration 91, loss = 0.06078002\n",
      "Iteration 92, loss = 0.06541190\n",
      "Iteration 93, loss = 0.06879454\n",
      "Iteration 94, loss = 0.08067983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12500615\n",
      "Iteration 2, loss = 1.57754843\n",
      "Iteration 3, loss = 1.53927651\n",
      "Iteration 4, loss = 1.12935427\n",
      "Iteration 5, loss = 0.79849036\n",
      "Iteration 6, loss = 0.69576782\n",
      "Iteration 7, loss = 0.60982787\n",
      "Iteration 8, loss = 0.55134156\n",
      "Iteration 9, loss = 0.52167482\n",
      "Iteration 10, loss = 0.49535679\n",
      "Iteration 11, loss = 0.47115449\n",
      "Iteration 12, loss = 0.44869068\n",
      "Iteration 13, loss = 0.39503582\n",
      "Iteration 14, loss = 0.38182396\n",
      "Iteration 15, loss = 0.31956949\n",
      "Iteration 16, loss = 0.29982210\n",
      "Iteration 17, loss = 0.26122681\n",
      "Iteration 18, loss = 0.22019132\n",
      "Iteration 19, loss = 0.21787038\n",
      "Iteration 20, loss = 0.17587770\n",
      "Iteration 21, loss = 0.15730212\n",
      "Iteration 22, loss = 0.15573927\n",
      "Iteration 23, loss = 0.12295567\n",
      "Iteration 24, loss = 0.12095513\n",
      "Iteration 25, loss = 0.11206077\n",
      "Iteration 26, loss = 0.09459284\n",
      "Iteration 27, loss = 0.09875420\n",
      "Iteration 28, loss = 0.08455353\n",
      "Iteration 29, loss = 0.08444197\n",
      "Iteration 30, loss = 0.08239656\n",
      "Iteration 31, loss = 0.07362763\n",
      "Iteration 32, loss = 0.07912380\n",
      "Iteration 33, loss = 0.07091397\n",
      "Iteration 34, loss = 0.07073862\n",
      "Iteration 35, loss = 0.07125599\n",
      "Iteration 36, loss = 0.06514405\n",
      "Iteration 37, loss = 0.06813798\n",
      "Iteration 38, loss = 0.06470012\n",
      "Iteration 39, loss = 0.06390828\n",
      "Iteration 40, loss = 0.06540790\n",
      "Iteration 41, loss = 0.06187879\n",
      "Iteration 42, loss = 0.06371498\n",
      "Iteration 43, loss = 0.06347782\n",
      "Iteration 44, loss = 0.06071937\n",
      "Iteration 45, loss = 0.06323194\n",
      "Iteration 46, loss = 0.06243858\n",
      "Iteration 47, loss = 0.05985857\n",
      "Iteration 48, loss = 0.06261982\n",
      "Iteration 49, loss = 0.06218136\n",
      "Iteration 50, loss = 0.05910352\n",
      "Iteration 51, loss = 0.06205550\n",
      "Iteration 52, loss = 0.06253910\n",
      "Iteration 53, loss = 0.05854218\n",
      "Iteration 54, loss = 0.06154101\n",
      "Iteration 55, loss = 0.06320562\n",
      "Iteration 56, loss = 0.05818829\n",
      "Iteration 57, loss = 0.06105996\n",
      "Iteration 58, loss = 0.06384631\n",
      "Iteration 59, loss = 0.05794637\n",
      "Iteration 60, loss = 0.06063933\n",
      "Iteration 61, loss = 0.06425919\n",
      "Iteration 62, loss = 0.05770500\n",
      "Iteration 63, loss = 0.06035610\n",
      "Iteration 64, loss = 0.06450613\n",
      "Iteration 65, loss = 0.05740452\n",
      "Iteration 66, loss = 0.06031056\n",
      "Iteration 67, loss = 0.06478387\n",
      "Iteration 68, loss = 0.05703012\n",
      "Iteration 69, loss = 0.06070308\n",
      "Iteration 70, loss = 0.06542667\n",
      "Iteration 71, loss = 0.05658722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 72, loss = 0.06200305\n",
      "Iteration 73, loss = 0.06695496\n",
      "Iteration 74, loss = 0.05613339\n",
      "Iteration 75, loss = 0.06540875\n",
      "Iteration 76, loss = 0.07017010\n",
      "Iteration 77, loss = 0.05612426\n",
      "Iteration 78, loss = 0.07370489\n",
      "Iteration 79, loss = 0.07520479\n",
      "Iteration 80, loss = 0.05902908\n",
      "Iteration 81, loss = 0.08758421\n",
      "Iteration 82, loss = 0.07392375\n",
      "Iteration 83, loss = 0.06838400\n",
      "Iteration 84, loss = 0.08270519\n",
      "Iteration 85, loss = 0.05687269\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10484323\n",
      "Iteration 2, loss = 1.58500559\n",
      "Iteration 3, loss = 1.53767511\n",
      "Iteration 4, loss = 1.11866578\n",
      "Iteration 5, loss = 0.79502528\n",
      "Iteration 6, loss = 0.68872717\n",
      "Iteration 7, loss = 0.60180946\n",
      "Iteration 8, loss = 0.54544466\n",
      "Iteration 9, loss = 0.51832212\n",
      "Iteration 10, loss = 0.48917853\n",
      "Iteration 11, loss = 0.46213746\n",
      "Iteration 12, loss = 0.43873284\n",
      "Iteration 13, loss = 0.37965921\n",
      "Iteration 14, loss = 0.36298492\n",
      "Iteration 15, loss = 0.30427609\n",
      "Iteration 16, loss = 0.27245643\n",
      "Iteration 17, loss = 0.24427617\n",
      "Iteration 18, loss = 0.19236205\n",
      "Iteration 19, loss = 0.18980126\n",
      "Iteration 20, loss = 0.15551340\n",
      "Iteration 21, loss = 0.12936031\n",
      "Iteration 22, loss = 0.13126419\n",
      "Iteration 23, loss = 0.09854935\n",
      "Iteration 24, loss = 0.10031145\n",
      "Iteration 25, loss = 0.08373138\n",
      "Iteration 26, loss = 0.07916913\n",
      "Iteration 27, loss = 0.07436476\n",
      "Iteration 28, loss = 0.06661667\n",
      "Iteration 29, loss = 0.06621643\n",
      "Iteration 30, loss = 0.05804034\n",
      "Iteration 31, loss = 0.05981406\n",
      "Iteration 32, loss = 0.05378446\n",
      "Iteration 33, loss = 0.05551617\n",
      "Iteration 34, loss = 0.05024697\n",
      "Iteration 35, loss = 0.05204730\n",
      "Iteration 36, loss = 0.04848911\n",
      "Iteration 37, loss = 0.04977850\n",
      "Iteration 38, loss = 0.04670361\n",
      "Iteration 39, loss = 0.04785872\n",
      "Iteration 40, loss = 0.04574268\n",
      "Iteration 41, loss = 0.04648992\n",
      "Iteration 42, loss = 0.04460260\n",
      "Iteration 43, loss = 0.04526991\n",
      "Iteration 44, loss = 0.04387491\n",
      "Iteration 45, loss = 0.04431530\n",
      "Iteration 46, loss = 0.04302985\n",
      "Iteration 47, loss = 0.04343111\n",
      "Iteration 48, loss = 0.04241978\n",
      "Iteration 49, loss = 0.04271893\n",
      "Iteration 50, loss = 0.04177632\n",
      "Iteration 51, loss = 0.04205168\n",
      "Iteration 52, loss = 0.04127926\n",
      "Iteration 53, loss = 0.04151247\n",
      "Iteration 54, loss = 0.04079947\n",
      "Iteration 55, loss = 0.04099826\n",
      "Iteration 56, loss = 0.04040561\n",
      "Iteration 57, loss = 0.04057603\n",
      "Iteration 58, loss = 0.04004574\n",
      "Iteration 59, loss = 0.04016934\n",
      "Iteration 60, loss = 0.03973122\n",
      "Iteration 61, loss = 0.03982368\n",
      "Iteration 62, loss = 0.03945296\n",
      "Iteration 63, loss = 0.03949371\n",
      "Iteration 64, loss = 0.03919784\n",
      "Iteration 65, loss = 0.03920159\n",
      "Iteration 66, loss = 0.03897394\n",
      "Iteration 67, loss = 0.03892945\n",
      "Iteration 68, loss = 0.03876345\n",
      "Iteration 69, loss = 0.03868189\n",
      "Iteration 70, loss = 0.03857320\n",
      "Iteration 71, loss = 0.03845909\n",
      "Iteration 72, loss = 0.03839210\n",
      "Iteration 73, loss = 0.03825662\n",
      "Iteration 74, loss = 0.03821851\n",
      "Iteration 75, loss = 0.03807871\n",
      "Iteration 76, loss = 0.03805114\n",
      "Iteration 77, loss = 0.03792050\n",
      "Iteration 78, loss = 0.03788569\n",
      "Iteration 79, loss = 0.03777896\n",
      "Iteration 80, loss = 0.03772669\n",
      "Iteration 81, loss = 0.03764925\n",
      "Iteration 82, loss = 0.03757643\n",
      "Iteration 83, loss = 0.03752382\n",
      "Iteration 84, loss = 0.03743829\n",
      "Iteration 85, loss = 0.03739760\n",
      "Iteration 86, loss = 0.03731490\n",
      "Iteration 87, loss = 0.03726902\n",
      "Iteration 88, loss = 0.03720197\n",
      "Iteration 89, loss = 0.03714287\n",
      "Iteration 90, loss = 0.03709221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11317117\n",
      "Iteration 2, loss = 1.57639800\n",
      "Iteration 3, loss = 1.53639040\n",
      "Iteration 4, loss = 1.12626099\n",
      "Iteration 5, loss = 0.79574537\n",
      "Iteration 6, loss = 0.69220636\n",
      "Iteration 7, loss = 0.60812433\n",
      "Iteration 8, loss = 0.55044026\n",
      "Iteration 9, loss = 0.52052457\n",
      "Iteration 10, loss = 0.49219700\n",
      "Iteration 11, loss = 0.46381833\n",
      "Iteration 12, loss = 0.43808531\n",
      "Iteration 13, loss = 0.38043972\n",
      "Iteration 14, loss = 0.35750597\n",
      "Iteration 15, loss = 0.32523764\n",
      "Iteration 16, loss = 0.26372463\n",
      "Iteration 17, loss = 0.26131274\n",
      "Iteration 18, loss = 0.21269808\n",
      "Iteration 19, loss = 0.17832173\n",
      "Iteration 20, loss = 0.17693031\n",
      "Iteration 21, loss = 0.13319814\n",
      "Iteration 22, loss = 0.13039567\n",
      "Iteration 23, loss = 0.11773054\n",
      "Iteration 24, loss = 0.09302705\n",
      "Iteration 25, loss = 0.10122844\n",
      "Iteration 26, loss = 0.07607109\n",
      "Iteration 27, loss = 0.07966180\n",
      "Iteration 28, loss = 0.06669973\n",
      "Iteration 29, loss = 0.06442682\n",
      "Iteration 30, loss = 0.06112012\n",
      "Iteration 31, loss = 0.05451334\n",
      "Iteration 32, loss = 0.05579420\n",
      "Iteration 33, loss = 0.04683473\n",
      "Iteration 34, loss = 0.05041806\n",
      "Iteration 35, loss = 0.04269176\n",
      "Iteration 36, loss = 0.04687773\n",
      "Iteration 37, loss = 0.04009098\n",
      "Iteration 38, loss = 0.04227919\n",
      "Iteration 39, loss = 0.03819440\n",
      "Iteration 40, loss = 0.03858004\n",
      "Iteration 41, loss = 0.03757456\n",
      "Iteration 42, loss = 0.03547211\n",
      "Iteration 43, loss = 0.03671224\n",
      "Iteration 44, loss = 0.03308281\n",
      "Iteration 45, loss = 0.03467315\n",
      "Iteration 46, loss = 0.03212821\n",
      "Iteration 47, loss = 0.03245873\n",
      "Iteration 48, loss = 0.03206033\n",
      "Iteration 49, loss = 0.03040895\n",
      "Iteration 50, loss = 0.03154798\n",
      "Iteration 51, loss = 0.02960885\n",
      "Iteration 52, loss = 0.02966581\n",
      "Iteration 53, loss = 0.02960491\n",
      "Iteration 54, loss = 0.02817092\n",
      "Iteration 55, loss = 0.02868372\n",
      "Iteration 56, loss = 0.02793048\n",
      "Iteration 57, loss = 0.02730326\n",
      "Iteration 58, loss = 0.02761927\n",
      "Iteration 59, loss = 0.02678559\n",
      "Iteration 60, loss = 0.02655515\n",
      "Iteration 61, loss = 0.02666422\n",
      "Iteration 62, loss = 0.02594075\n",
      "Iteration 63, loss = 0.02580028\n",
      "Iteration 64, loss = 0.02583729\n",
      "Iteration 65, loss = 0.02524157\n",
      "Iteration 66, loss = 0.02504064\n",
      "Iteration 67, loss = 0.02507826\n",
      "Iteration 68, loss = 0.02463626\n",
      "Iteration 69, loss = 0.02431537\n",
      "Iteration 70, loss = 0.02431360\n",
      "Iteration 71, loss = 0.02408208\n",
      "Iteration 72, loss = 0.02370551\n",
      "Iteration 73, loss = 0.02354361\n",
      "Iteration 74, loss = 0.02347072\n",
      "Iteration 75, loss = 0.02323426\n",
      "Iteration 76, loss = 0.02292360\n",
      "Iteration 77, loss = 0.02275668\n",
      "Iteration 78, loss = 0.02266624\n",
      "Iteration 79, loss = 0.02247826\n",
      "Iteration 80, loss = 0.02221761\n",
      "Iteration 81, loss = 0.02199363\n",
      "Iteration 82, loss = 0.02185265\n",
      "Iteration 83, loss = 0.02173607\n",
      "Iteration 84, loss = 0.02157225\n",
      "Iteration 85, loss = 0.02136749\n",
      "Iteration 86, loss = 0.02115105\n",
      "Iteration 87, loss = 0.02096087\n",
      "Iteration 88, loss = 0.02080375\n",
      "Iteration 89, loss = 0.02066849\n",
      "Iteration 90, loss = 0.02054268\n",
      "Iteration 91, loss = 0.02041515\n",
      "Iteration 92, loss = 0.02029765\n",
      "Iteration 93, loss = 0.02018269\n",
      "Iteration 94, loss = 0.02010843\n",
      "Iteration 95, loss = 0.02006936\n",
      "Iteration 96, loss = 0.02019076\n",
      "Iteration 97, loss = 0.02046595\n",
      "Iteration 98, loss = 0.02146019\n",
      "Iteration 99, loss = 0.02282958\n",
      "Iteration 100, loss = 0.02711808\n",
      "Iteration 101, loss = 0.02847859\n",
      "Iteration 102, loss = 0.03387617\n",
      "Iteration 103, loss = 0.02524395\n",
      "Iteration 104, loss = 0.02022641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11924132\n",
      "Iteration 2, loss = 1.56909361\n",
      "Iteration 3, loss = 1.53218873\n",
      "Iteration 4, loss = 1.11627616\n",
      "Iteration 5, loss = 0.79124334\n",
      "Iteration 6, loss = 0.68608967\n",
      "Iteration 7, loss = 0.59813920\n",
      "Iteration 8, loss = 0.54023822\n",
      "Iteration 9, loss = 0.51354996\n",
      "Iteration 10, loss = 0.48929127\n",
      "Iteration 11, loss = 0.46267293\n",
      "Iteration 12, loss = 0.43566039\n",
      "Iteration 13, loss = 0.38631118\n",
      "Iteration 14, loss = 0.35459581\n",
      "Iteration 15, loss = 0.33467437\n",
      "Iteration 16, loss = 0.27405968\n",
      "Iteration 17, loss = 0.24684910\n",
      "Iteration 18, loss = 0.25098659\n",
      "Iteration 19, loss = 0.20901235\n",
      "Iteration 20, loss = 0.16927439\n",
      "Iteration 21, loss = 0.18902110\n",
      "Iteration 22, loss = 0.15387120\n",
      "Iteration 23, loss = 0.13607614\n",
      "Iteration 24, loss = 0.14825344\n",
      "Iteration 25, loss = 0.11181593\n",
      "Iteration 26, loss = 0.12367491\n",
      "Iteration 27, loss = 0.10565382\n",
      "Iteration 28, loss = 0.10253144\n",
      "Iteration 29, loss = 0.10372715\n",
      "Iteration 30, loss = 0.08933698\n",
      "Iteration 31, loss = 0.09992343\n",
      "Iteration 32, loss = 0.08323443\n",
      "Iteration 33, loss = 0.08986961\n",
      "Iteration 34, loss = 0.08163069\n",
      "Iteration 35, loss = 0.08100397\n",
      "Iteration 36, loss = 0.08232793\n",
      "Iteration 37, loss = 0.07508850\n",
      "Iteration 38, loss = 0.08150262\n",
      "Iteration 39, loss = 0.07327795\n",
      "Iteration 40, loss = 0.07547848\n",
      "Iteration 41, loss = 0.07403193\n",
      "Iteration 42, loss = 0.07004287\n",
      "Iteration 43, loss = 0.07321090\n",
      "Iteration 44, loss = 0.06858312\n",
      "Iteration 45, loss = 0.07015916\n",
      "Iteration 46, loss = 0.07005390\n",
      "Iteration 47, loss = 0.06669784\n",
      "Iteration 48, loss = 0.06959648\n",
      "Iteration 49, loss = 0.06758454\n",
      "Iteration 50, loss = 0.06558627\n",
      "Iteration 51, loss = 0.06821694\n",
      "Iteration 52, loss = 0.06600074\n",
      "Iteration 53, loss = 0.06437535\n",
      "Iteration 54, loss = 0.06675431\n",
      "Iteration 55, loss = 0.06515042\n",
      "Iteration 56, loss = 0.06317482\n",
      "Iteration 57, loss = 0.06543494\n",
      "Iteration 58, loss = 0.06487139\n",
      "Iteration 59, loss = 0.06223169\n",
      "Iteration 60, loss = 0.06409046\n",
      "Iteration 61, loss = 0.06479871\n",
      "Iteration 62, loss = 0.06174857\n",
      "Iteration 63, loss = 0.06254509\n",
      "Iteration 64, loss = 0.06422560\n",
      "Iteration 65, loss = 0.06174439\n",
      "Iteration 66, loss = 0.06102691\n",
      "Iteration 67, loss = 0.06257277\n",
      "Iteration 68, loss = 0.06184904\n",
      "Iteration 69, loss = 0.06040814\n",
      "Iteration 70, loss = 0.06052777\n",
      "Iteration 71, loss = 0.06124103\n",
      "Iteration 72, loss = 0.06091264\n",
      "Iteration 73, loss = 0.05981103\n",
      "Iteration 74, loss = 0.05980042\n",
      "Iteration 75, loss = 0.06041723\n",
      "Iteration 76, loss = 0.06014926\n",
      "Iteration 77, loss = 0.05942883\n",
      "Iteration 78, loss = 0.05905362\n",
      "Iteration 79, loss = 0.05929219\n",
      "Iteration 80, loss = 0.05956260\n",
      "Iteration 81, loss = 0.05924533\n",
      "Iteration 82, loss = 0.05875277\n",
      "Iteration 83, loss = 0.05844275\n",
      "Iteration 84, loss = 0.05847534\n",
      "Iteration 85, loss = 0.05864506\n",
      "Iteration 86, loss = 0.05862670\n",
      "Iteration 87, loss = 0.05845507\n",
      "Iteration 88, loss = 0.05811908\n",
      "Iteration 89, loss = 0.05783974\n",
      "Iteration 90, loss = 0.05766882\n",
      "Iteration 91, loss = 0.05761169\n",
      "Iteration 92, loss = 0.05763090\n",
      "Iteration 93, loss = 0.05767026\n",
      "Iteration 94, loss = 0.05775264\n",
      "Iteration 95, loss = 0.05780184\n",
      "Iteration 96, loss = 0.05797705\n",
      "Iteration 97, loss = 0.05811980\n",
      "Iteration 98, loss = 0.05863562\n",
      "Iteration 99, loss = 0.05911952\n",
      "Iteration 100, loss = 0.06069255\n",
      "Iteration 101, loss = 0.06186749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10901821\n",
      "Iteration 2, loss = 2.89268825\n",
      "Iteration 3, loss = 1.49404354\n",
      "Iteration 4, loss = 1.08524261\n",
      "Iteration 5, loss = 1.09321517\n",
      "Iteration 6, loss = 1.09103980\n",
      "Iteration 7, loss = 1.07309084\n",
      "Iteration 8, loss = 1.03944463\n",
      "Iteration 9, loss = 0.98900946\n",
      "Iteration 10, loss = 0.95795038\n",
      "Iteration 11, loss = 0.89341342\n",
      "Iteration 12, loss = 0.83424079\n",
      "Iteration 13, loss = 0.78105524\n",
      "Iteration 14, loss = 0.73543917\n",
      "Iteration 15, loss = 0.69876203\n",
      "Iteration 16, loss = 0.66889472\n",
      "Iteration 17, loss = 0.64065070\n",
      "Iteration 18, loss = 0.61769192\n",
      "Iteration 19, loss = 0.59815634\n",
      "Iteration 20, loss = 0.58112692\n",
      "Iteration 21, loss = 0.56628218\n",
      "Iteration 22, loss = 0.55340385\n",
      "Iteration 23, loss = 0.54235694\n",
      "Iteration 24, loss = 0.53297412\n",
      "Iteration 25, loss = 0.52495278\n",
      "Iteration 26, loss = 0.51807968\n",
      "Iteration 27, loss = 0.51218863\n",
      "Iteration 28, loss = 0.50713244\n",
      "Iteration 29, loss = 0.50278332\n",
      "Iteration 30, loss = 0.49903192\n",
      "Iteration 31, loss = 0.49578569\n",
      "Iteration 32, loss = 0.49296678\n",
      "Iteration 33, loss = 0.49051008\n",
      "Iteration 34, loss = 0.48836119\n",
      "Iteration 35, loss = 0.48647469\n",
      "Iteration 36, loss = 0.48481261\n",
      "Iteration 37, loss = 0.48334311\n",
      "Iteration 38, loss = 0.48203940\n",
      "Iteration 39, loss = 0.48087882\n",
      "Iteration 40, loss = 0.47984219\n",
      "Iteration 41, loss = 0.47891312\n",
      "Iteration 42, loss = 0.47806265\n",
      "Iteration 43, loss = 0.47728268\n",
      "Iteration 44, loss = 0.47655721\n",
      "Iteration 45, loss = 0.47585947\n",
      "Iteration 46, loss = 0.47521457\n",
      "Iteration 47, loss = 0.47462214\n",
      "Iteration 48, loss = 0.47408050\n",
      "Iteration 49, loss = 0.47358601\n",
      "Iteration 50, loss = 0.47313293\n",
      "Iteration 51, loss = 0.47271405\n",
      "Iteration 52, loss = 0.47229083\n",
      "Iteration 53, loss = 0.47188696\n",
      "Iteration 54, loss = 0.47150517\n",
      "Iteration 55, loss = 0.47114050\n",
      "Iteration 56, loss = 0.47078812\n",
      "Iteration 57, loss = 0.47044429\n",
      "Iteration 58, loss = 0.47008750\n",
      "Iteration 59, loss = 0.46968964\n",
      "Iteration 60, loss = 0.46909210\n",
      "Iteration 61, loss = 0.46818656\n",
      "Iteration 62, loss = 0.46574305\n",
      "Iteration 63, loss = 0.46202327\n",
      "Iteration 64, loss = 0.46045472\n",
      "Iteration 65, loss = 0.45710482\n",
      "Iteration 66, loss = 0.45294042\n",
      "Iteration 67, loss = 0.44810834\n",
      "Iteration 68, loss = 0.44197831\n",
      "Iteration 69, loss = 0.43433788\n",
      "Iteration 70, loss = 0.42431567\n",
      "Iteration 71, loss = 0.41321488\n",
      "Iteration 72, loss = 0.40413883\n",
      "Iteration 73, loss = 0.41176312\n",
      "Iteration 74, loss = 0.46279333\n",
      "Iteration 75, loss = 0.46799740\n",
      "Iteration 76, loss = 0.47139412\n",
      "Iteration 77, loss = 0.47201403\n",
      "Iteration 78, loss = 0.46848173\n",
      "Iteration 79, loss = 0.45131605\n",
      "Iteration 80, loss = 0.32824840\n",
      "Iteration 81, loss = 0.36340767\n",
      "Iteration 82, loss = 0.48633539\n",
      "Iteration 83, loss = 0.49800597\n",
      "Iteration 84, loss = 0.49715604\n",
      "Iteration 85, loss = 0.48764343\n",
      "Iteration 86, loss = 0.47282005\n",
      "Iteration 87, loss = 0.45450340\n",
      "Iteration 88, loss = 0.31604973\n",
      "Iteration 89, loss = 0.79265144\n",
      "Iteration 90, loss = 1.50562418\n",
      "Iteration 91, loss = 0.57116183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 92, loss = 0.49786496\n",
      "Iteration 93, loss = 0.48390722\n",
      "Iteration 94, loss = 0.47925277\n",
      "Iteration 95, loss = 0.47676838\n",
      "Iteration 96, loss = 0.47478640\n",
      "Iteration 97, loss = 0.47294472\n",
      "Iteration 98, loss = 0.47123915\n",
      "Iteration 99, loss = 0.46974796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12500615\n",
      "Iteration 2, loss = 2.95718273\n",
      "Iteration 3, loss = 1.50230836\n",
      "Iteration 4, loss = 1.08842466\n",
      "Iteration 5, loss = 1.09668593\n",
      "Iteration 6, loss = 1.09881756\n",
      "Iteration 7, loss = 1.09885070\n",
      "Iteration 8, loss = 1.09722679\n",
      "Iteration 9, loss = 1.08822217\n",
      "Iteration 10, loss = 1.06303768\n",
      "Iteration 11, loss = 1.02133370\n",
      "Iteration 12, loss = 0.98752076\n",
      "Iteration 13, loss = 0.94158002\n",
      "Iteration 14, loss = 0.87136636\n",
      "Iteration 15, loss = 0.81545987\n",
      "Iteration 16, loss = 0.76439039\n",
      "Iteration 17, loss = 0.72096518\n",
      "Iteration 18, loss = 0.68603982\n",
      "Iteration 19, loss = 0.65730010\n",
      "Iteration 20, loss = 0.63165236\n",
      "Iteration 21, loss = 0.61023461\n",
      "Iteration 22, loss = 0.59177863\n",
      "Iteration 23, loss = 0.57568242\n",
      "Iteration 24, loss = 0.56174134\n",
      "Iteration 25, loss = 0.54970946\n",
      "Iteration 26, loss = 0.53932709\n",
      "Iteration 27, loss = 0.53040598\n",
      "Iteration 28, loss = 0.52276249\n",
      "Iteration 29, loss = 0.51622215\n",
      "Iteration 30, loss = 0.51061672\n",
      "Iteration 31, loss = 0.50579729\n",
      "Iteration 32, loss = 0.50164459\n",
      "Iteration 33, loss = 0.49805640\n",
      "Iteration 34, loss = 0.49494602\n",
      "Iteration 35, loss = 0.49224039\n",
      "Iteration 36, loss = 0.48987821\n",
      "Iteration 37, loss = 0.48780822\n",
      "Iteration 38, loss = 0.48598752\n",
      "Iteration 39, loss = 0.48438020\n",
      "Iteration 40, loss = 0.48295615\n",
      "Iteration 41, loss = 0.48169003\n",
      "Iteration 42, loss = 0.48054791\n",
      "Iteration 43, loss = 0.47950107\n",
      "Iteration 44, loss = 0.47855207\n",
      "Iteration 45, loss = 0.47769098\n",
      "Iteration 46, loss = 0.47690923\n",
      "Iteration 47, loss = 0.47619909\n",
      "Iteration 48, loss = 0.47555325\n",
      "Iteration 49, loss = 0.47496461\n",
      "Iteration 50, loss = 0.47442420\n",
      "Iteration 51, loss = 0.47387977\n",
      "Iteration 52, loss = 0.47337506\n",
      "Iteration 53, loss = 0.47284242\n",
      "Iteration 54, loss = 0.47235588\n",
      "Iteration 55, loss = 0.47191430\n",
      "Iteration 56, loss = 0.47145253\n",
      "Iteration 57, loss = 0.47091831\n",
      "Iteration 58, loss = 0.47031227\n",
      "Iteration 59, loss = 0.46903970\n",
      "Iteration 60, loss = 0.46592160\n",
      "Iteration 61, loss = 0.46260745\n",
      "Iteration 62, loss = 0.46091420\n",
      "Iteration 63, loss = 0.45673749\n",
      "Iteration 64, loss = 0.45219616\n",
      "Iteration 65, loss = 0.44685029\n",
      "Iteration 66, loss = 0.43978563\n",
      "Iteration 67, loss = 0.43108393\n",
      "Iteration 68, loss = 0.42155032\n",
      "Iteration 69, loss = 0.41391751\n",
      "Iteration 70, loss = 0.40021105\n",
      "Iteration 71, loss = 0.41966839\n",
      "Iteration 72, loss = 0.36622347\n",
      "Iteration 73, loss = 0.36874085\n",
      "Iteration 74, loss = 0.58679280\n",
      "Iteration 75, loss = 1.12413164\n",
      "Iteration 76, loss = 0.54484444\n",
      "Iteration 77, loss = 0.50799379\n",
      "Iteration 78, loss = 0.49555412\n",
      "Iteration 79, loss = 0.48977055\n",
      "Iteration 80, loss = 0.48520191\n",
      "Iteration 81, loss = 0.47968539\n",
      "Iteration 82, loss = 0.47331111\n",
      "Iteration 83, loss = 0.46676716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10484323\n",
      "Iteration 2, loss = 2.89174426\n",
      "Iteration 3, loss = 1.49153646\n",
      "Iteration 4, loss = 1.08578994\n",
      "Iteration 5, loss = 1.09396491\n",
      "Iteration 6, loss = 1.09588691\n",
      "Iteration 7, loss = 1.09344475\n",
      "Iteration 8, loss = 1.07939908\n",
      "Iteration 9, loss = 1.05199435\n",
      "Iteration 10, loss = 1.00649918\n",
      "Iteration 11, loss = 0.98085951\n",
      "Iteration 12, loss = 0.92355738\n",
      "Iteration 13, loss = 0.85183204\n",
      "Iteration 14, loss = 0.79636123\n",
      "Iteration 15, loss = 0.74674713\n",
      "Iteration 16, loss = 0.70598968\n",
      "Iteration 17, loss = 0.67282043\n",
      "Iteration 18, loss = 0.64524909\n",
      "Iteration 19, loss = 0.62195907\n",
      "Iteration 20, loss = 0.60183410\n",
      "Iteration 21, loss = 0.58425384\n",
      "Iteration 22, loss = 0.56893779\n",
      "Iteration 23, loss = 0.55573001\n",
      "Iteration 24, loss = 0.54437107\n",
      "Iteration 25, loss = 0.53461455\n",
      "Iteration 26, loss = 0.52626069\n",
      "Iteration 27, loss = 0.51911631\n",
      "Iteration 28, loss = 0.51300098\n",
      "Iteration 29, loss = 0.50775290\n",
      "Iteration 30, loss = 0.50324338\n",
      "Iteration 31, loss = 0.49935730\n",
      "Iteration 32, loss = 0.49599658\n",
      "Iteration 33, loss = 0.49307857\n",
      "Iteration 34, loss = 0.49053418\n",
      "Iteration 35, loss = 0.48830604\n",
      "Iteration 36, loss = 0.48633024\n",
      "Iteration 37, loss = 0.48452778\n",
      "Iteration 38, loss = 0.48290505\n",
      "Iteration 39, loss = 0.48144476\n",
      "Iteration 40, loss = 0.48013338\n",
      "Iteration 41, loss = 0.47890764\n",
      "Iteration 42, loss = 0.47767813\n",
      "Iteration 43, loss = 0.47642653\n",
      "Iteration 44, loss = 0.47527981\n",
      "Iteration 45, loss = 0.47343961\n",
      "Iteration 46, loss = 0.46775667\n",
      "Iteration 47, loss = 0.45905544\n",
      "Iteration 48, loss = 0.45756603\n",
      "Iteration 49, loss = 0.44937656\n",
      "Iteration 50, loss = 0.44234251\n",
      "Iteration 51, loss = 0.43334091\n",
      "Iteration 52, loss = 0.42259218\n",
      "Iteration 53, loss = 0.40997903\n",
      "Iteration 54, loss = 0.39733327\n",
      "Iteration 55, loss = 0.39418444\n",
      "Iteration 56, loss = 0.41213382\n",
      "Iteration 57, loss = 0.47637093\n",
      "Iteration 58, loss = 0.48448697\n",
      "Iteration 59, loss = 0.49597747\n",
      "Iteration 60, loss = 0.50397163\n",
      "Iteration 61, loss = 0.49902065\n",
      "Iteration 62, loss = 0.49391479\n",
      "Iteration 63, loss = 0.48817662\n",
      "Iteration 64, loss = 0.48265550\n",
      "Iteration 65, loss = 0.47822883\n",
      "Iteration 66, loss = 0.47471994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11317117\n",
      "Iteration 2, loss = 2.92454515\n",
      "Iteration 3, loss = 1.50080052\n",
      "Iteration 4, loss = 1.08560039\n",
      "Iteration 5, loss = 1.09305617\n",
      "Iteration 6, loss = 1.09264290\n",
      "Iteration 7, loss = 1.08039628\n",
      "Iteration 8, loss = 1.05306247\n",
      "Iteration 9, loss = 1.00912336\n",
      "Iteration 10, loss = 0.98236909\n",
      "Iteration 11, loss = 0.92733118\n",
      "Iteration 12, loss = 0.85794021\n",
      "Iteration 13, loss = 0.80227381\n",
      "Iteration 14, loss = 0.75296229\n",
      "Iteration 15, loss = 0.71133152\n",
      "Iteration 16, loss = 0.67737999\n",
      "Iteration 17, loss = 0.64893287\n",
      "Iteration 18, loss = 0.62482327\n",
      "Iteration 19, loss = 0.60397762\n",
      "Iteration 20, loss = 0.58580403\n",
      "Iteration 21, loss = 0.57008814\n",
      "Iteration 22, loss = 0.55658324\n",
      "Iteration 23, loss = 0.54498738\n",
      "Iteration 24, loss = 0.53506499\n",
      "Iteration 25, loss = 0.52658222\n",
      "Iteration 26, loss = 0.51931913\n",
      "Iteration 27, loss = 0.51307983\n",
      "Iteration 28, loss = 0.50769706\n",
      "Iteration 29, loss = 0.50303238\n",
      "Iteration 30, loss = 0.49897345\n",
      "Iteration 31, loss = 0.49542977\n",
      "Iteration 32, loss = 0.49232085\n",
      "Iteration 33, loss = 0.48948312\n",
      "Iteration 34, loss = 0.48688440\n",
      "Iteration 35, loss = 0.48454438\n",
      "Iteration 36, loss = 0.48232689\n",
      "Iteration 37, loss = 0.48005322\n",
      "Iteration 38, loss = 0.47533375\n",
      "Iteration 39, loss = 0.46880018\n",
      "Iteration 40, loss = 0.46551193\n",
      "Iteration 41, loss = 0.45920519\n",
      "Iteration 42, loss = 0.45153996\n",
      "Iteration 43, loss = 0.44268520\n",
      "Iteration 44, loss = 0.43146599\n",
      "Iteration 45, loss = 0.41829575\n",
      "Iteration 46, loss = 0.40410952\n",
      "Iteration 47, loss = 0.39767291\n",
      "Iteration 48, loss = 0.44706477\n",
      "Iteration 49, loss = 0.48412788\n",
      "Iteration 50, loss = 0.51862553\n",
      "Iteration 51, loss = 0.54161929\n",
      "Iteration 52, loss = 0.50427850\n",
      "Iteration 53, loss = 0.49328790\n",
      "Iteration 54, loss = 0.48587130\n",
      "Iteration 55, loss = 0.48082565\n",
      "Iteration 56, loss = 0.47732254\n",
      "Iteration 57, loss = 0.47466776\n",
      "Iteration 58, loss = 0.47264496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11924132\n",
      "Iteration 2, loss = 2.98308157\n",
      "Iteration 3, loss = 1.50309780\n",
      "Iteration 4, loss = 1.08672196\n",
      "Iteration 5, loss = 1.09457353\n",
      "Iteration 6, loss = 1.09188801\n",
      "Iteration 7, loss = 1.07233135\n",
      "Iteration 8, loss = 1.03724118\n",
      "Iteration 9, loss = 0.98634815\n",
      "Iteration 10, loss = 0.95240966\n",
      "Iteration 11, loss = 0.88805984\n",
      "Iteration 12, loss = 0.83011309\n",
      "Iteration 13, loss = 0.78050933\n",
      "Iteration 14, loss = 0.73431464\n",
      "Iteration 15, loss = 0.69466992\n",
      "Iteration 16, loss = 0.66360518\n",
      "Iteration 17, loss = 0.63649278\n",
      "Iteration 18, loss = 0.61417863\n",
      "Iteration 19, loss = 0.59510873\n",
      "Iteration 20, loss = 0.57844930\n",
      "Iteration 21, loss = 0.56386386\n",
      "Iteration 22, loss = 0.55121036\n",
      "Iteration 23, loss = 0.54040349\n",
      "Iteration 24, loss = 0.53120009\n",
      "Iteration 25, loss = 0.52335587\n",
      "Iteration 26, loss = 0.51668304\n",
      "Iteration 27, loss = 0.51097451\n",
      "Iteration 28, loss = 0.50607884\n",
      "Iteration 29, loss = 0.50186320\n",
      "Iteration 30, loss = 0.49822581\n",
      "Iteration 31, loss = 0.49507865\n",
      "Iteration 32, loss = 0.49234718\n",
      "Iteration 33, loss = 0.48996606\n",
      "Iteration 34, loss = 0.48787791\n",
      "Iteration 35, loss = 0.48604215\n",
      "Iteration 36, loss = 0.48441669\n",
      "Iteration 37, loss = 0.48295861\n",
      "Iteration 38, loss = 0.48165658\n",
      "Iteration 39, loss = 0.48049126\n",
      "Iteration 40, loss = 0.47944628\n",
      "Iteration 41, loss = 0.47850753\n",
      "Iteration 42, loss = 0.47765424\n",
      "Iteration 43, loss = 0.47685455\n",
      "Iteration 44, loss = 0.47612947\n",
      "Iteration 45, loss = 0.47547199\n",
      "Iteration 46, loss = 0.47487424\n",
      "Iteration 47, loss = 0.47432763\n",
      "Iteration 48, loss = 0.47381449\n",
      "Iteration 49, loss = 0.47326918\n",
      "Iteration 50, loss = 0.47264127\n",
      "Iteration 51, loss = 0.47189510\n",
      "Iteration 52, loss = 0.47037923\n",
      "Iteration 53, loss = 0.46863989\n",
      "Iteration 54, loss = 0.46726762\n",
      "Iteration 55, loss = 0.46573627\n",
      "Iteration 56, loss = 0.46312217\n",
      "Iteration 57, loss = 0.46031846\n",
      "Iteration 58, loss = 0.45703173\n",
      "Iteration 59, loss = 0.45267409\n",
      "Iteration 60, loss = 0.44706740\n",
      "Iteration 61, loss = 0.43981434\n",
      "Iteration 62, loss = 0.43069778\n",
      "Iteration 63, loss = 0.41971789\n",
      "Iteration 64, loss = 0.40877889\n",
      "Iteration 65, loss = 0.40493106\n",
      "Iteration 66, loss = 0.40298875\n",
      "Iteration 67, loss = 0.45683878\n",
      "Iteration 68, loss = 0.42314183\n",
      "Iteration 69, loss = 0.36741612\n",
      "Iteration 70, loss = 0.45882430\n",
      "Iteration 71, loss = 0.37156454\n",
      "Iteration 72, loss = 0.86557052\n",
      "Iteration 73, loss = 1.96858412\n",
      "Iteration 74, loss = 1.63899633\n",
      "Iteration 75, loss = 0.56697352\n",
      "Iteration 76, loss = 0.49466369\n",
      "Iteration 77, loss = 0.47886230\n",
      "Iteration 78, loss = 0.40761491\n",
      "Iteration 79, loss = 0.40193743\n",
      "Iteration 80, loss = 0.35474886\n",
      "Iteration 81, loss = 0.29951676\n",
      "Iteration 82, loss = 0.28883069\n",
      "Iteration 83, loss = 0.27509895\n",
      "Iteration 84, loss = 0.26872309\n",
      "Iteration 85, loss = 0.25296170\n",
      "Iteration 86, loss = 0.24518354\n",
      "Iteration 87, loss = 0.23791617\n",
      "Iteration 88, loss = 0.24297998\n",
      "Iteration 89, loss = 0.22924141\n",
      "Iteration 90, loss = 0.22548428\n",
      "Iteration 91, loss = 0.21034955\n",
      "Iteration 92, loss = 0.20928650\n",
      "Iteration 93, loss = 0.19595644\n",
      "Iteration 94, loss = 0.19004540\n",
      "Iteration 95, loss = 0.18366976\n",
      "Iteration 96, loss = 0.17996467\n",
      "Iteration 97, loss = 0.17636612\n",
      "Iteration 98, loss = 0.17484824\n",
      "Iteration 99, loss = 0.17151825\n",
      "Iteration 100, loss = 0.16928931\n",
      "Iteration 101, loss = 0.16730086\n",
      "Iteration 102, loss = 0.16544951\n",
      "Iteration 103, loss = 0.16372550\n",
      "Iteration 104, loss = 0.16228425\n",
      "Iteration 105, loss = 0.16102692\n",
      "Iteration 106, loss = 0.15996556\n",
      "Iteration 107, loss = 0.15918291\n",
      "Iteration 108, loss = 0.15862083\n",
      "Iteration 109, loss = 0.15786404\n",
      "Iteration 110, loss = 0.15396837\n",
      "Iteration 111, loss = 0.15329870\n",
      "Iteration 112, loss = 0.15261061\n",
      "Iteration 113, loss = 0.15231939\n",
      "Iteration 114, loss = 0.15115806\n",
      "Iteration 115, loss = 0.14799404\n",
      "Iteration 116, loss = 0.14739575\n",
      "Iteration 117, loss = 0.14669568\n",
      "Iteration 118, loss = 0.14594124\n",
      "Iteration 119, loss = 0.14361844\n",
      "Iteration 120, loss = 0.14303373\n",
      "Iteration 121, loss = 0.14204971\n",
      "Iteration 122, loss = 0.14162965\n",
      "Iteration 123, loss = 0.13965928\n",
      "Iteration 124, loss = 0.13883792\n",
      "Iteration 125, loss = 0.13787323\n",
      "Iteration 126, loss = 0.13708121\n",
      "Iteration 127, loss = 0.13626864\n",
      "Iteration 128, loss = 0.13812561\n",
      "Iteration 129, loss = 0.13877354\n",
      "Iteration 130, loss = 0.13488922\n",
      "Iteration 131, loss = 0.13414808\n",
      "Iteration 132, loss = 0.13131729\n",
      "Iteration 133, loss = 0.13037324\n",
      "Iteration 134, loss = 0.13021513\n",
      "Iteration 135, loss = 0.13012359\n",
      "Iteration 136, loss = 0.12894397\n",
      "Iteration 137, loss = 0.13313864\n",
      "Iteration 138, loss = 0.14461638\n",
      "Iteration 139, loss = 0.15009594\n",
      "Iteration 140, loss = 0.18954800\n",
      "Iteration 141, loss = 0.13297344\n",
      "Iteration 142, loss = 0.13858151\n",
      "Iteration 143, loss = 0.14944192\n",
      "Iteration 144, loss = 0.19164317\n",
      "Iteration 145, loss = 0.13228366\n",
      "Iteration 146, loss = 0.14144467\n",
      "Iteration 147, loss = 0.15358507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10901821\n",
      "Iteration 2, loss = 1.80397879\n",
      "Iteration 3, loss = 1.53770079\n",
      "Iteration 4, loss = 1.32341983\n",
      "Iteration 5, loss = 1.17211846\n",
      "Iteration 6, loss = 1.08500027\n",
      "Iteration 7, loss = 1.05399287\n",
      "Iteration 8, loss = 1.05975918\n",
      "Iteration 9, loss = 1.07930608\n",
      "Iteration 10, loss = 1.09415405\n",
      "Iteration 11, loss = 1.09402176\n",
      "Iteration 12, loss = 1.07628764\n",
      "Iteration 13, loss = 1.04311562\n",
      "Iteration 14, loss = 0.99914902\n",
      "Iteration 15, loss = 0.94997057\n",
      "Iteration 16, loss = 0.90200565\n",
      "Iteration 17, loss = 0.86172186\n",
      "Iteration 18, loss = 0.82922319\n",
      "Iteration 19, loss = 0.80403026\n",
      "Iteration 20, loss = 0.78537051\n",
      "Iteration 21, loss = 0.77175807\n",
      "Iteration 22, loss = 0.76118560\n",
      "Iteration 23, loss = 0.75166000\n",
      "Iteration 24, loss = 0.74162978\n",
      "Iteration 25, loss = 0.73018076\n",
      "Iteration 26, loss = 0.71693014\n",
      "Iteration 27, loss = 0.70219172\n",
      "Iteration 28, loss = 0.68643598\n",
      "Iteration 29, loss = 0.67056785\n",
      "Iteration 30, loss = 0.65538317\n",
      "Iteration 31, loss = 0.64160144\n",
      "Iteration 32, loss = 0.62967366\n",
      "Iteration 33, loss = 0.61963310\n",
      "Iteration 34, loss = 0.61114245\n",
      "Iteration 35, loss = 0.60364111\n",
      "Iteration 36, loss = 0.59653738\n",
      "Iteration 37, loss = 0.58933803\n",
      "Iteration 38, loss = 0.58183901\n",
      "Iteration 39, loss = 0.57402069\n",
      "Iteration 40, loss = 0.56606765\n",
      "Iteration 41, loss = 0.55826491\n",
      "Iteration 42, loss = 0.55086549\n",
      "Iteration 43, loss = 0.54400152\n",
      "Iteration 44, loss = 0.53767107\n",
      "Iteration 45, loss = 0.53177160\n",
      "Iteration 46, loss = 0.52609406\n",
      "Iteration 47, loss = 0.52041341\n",
      "Iteration 48, loss = 0.51454286\n",
      "Iteration 49, loss = 0.50835418\n",
      "Iteration 50, loss = 0.50182229\n",
      "Iteration 51, loss = 0.49505892\n",
      "Iteration 52, loss = 0.48815897\n",
      "Iteration 53, loss = 0.48130443\n",
      "Iteration 54, loss = 0.47471963\n",
      "Iteration 55, loss = 0.46846121\n",
      "Iteration 56, loss = 0.46251307\n",
      "Iteration 57, loss = 0.45684251\n",
      "Iteration 58, loss = 0.45134345\n",
      "Iteration 59, loss = 0.44587465\n",
      "Iteration 60, loss = 0.44036031\n",
      "Iteration 61, loss = 0.43477584\n",
      "Iteration 62, loss = 0.42919050\n",
      "Iteration 63, loss = 0.42371653\n",
      "Iteration 64, loss = 0.41842935\n",
      "Iteration 65, loss = 0.41319853\n",
      "Iteration 66, loss = 0.40811338\n",
      "Iteration 67, loss = 0.40303861\n",
      "Iteration 68, loss = 0.39775709\n",
      "Iteration 69, loss = 0.39215186\n",
      "Iteration 70, loss = 0.38628655\n",
      "Iteration 71, loss = 0.38064454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.37469592\n",
      "Iteration 73, loss = 0.36898304\n",
      "Iteration 74, loss = 0.36317838\n",
      "Iteration 75, loss = 0.35729429\n",
      "Iteration 76, loss = 0.35129754\n",
      "Iteration 77, loss = 0.34516785\n",
      "Iteration 78, loss = 0.33890572\n",
      "Iteration 79, loss = 0.33251607\n",
      "Iteration 80, loss = 0.32600876\n",
      "Iteration 81, loss = 0.31939775\n",
      "Iteration 82, loss = 0.31271021\n",
      "Iteration 83, loss = 0.30600986\n",
      "Iteration 84, loss = 0.29927591\n",
      "Iteration 85, loss = 0.29253051\n",
      "Iteration 86, loss = 0.28584297\n",
      "Iteration 87, loss = 0.27928485\n",
      "Iteration 88, loss = 0.27281069\n",
      "Iteration 89, loss = 0.26639441\n",
      "Iteration 90, loss = 0.26001438\n",
      "Iteration 91, loss = 0.25366665\n",
      "Iteration 92, loss = 0.24739694\n",
      "Iteration 93, loss = 0.24123945\n",
      "Iteration 94, loss = 0.23521887\n",
      "Iteration 95, loss = 0.22935380\n",
      "Iteration 96, loss = 0.22365183\n",
      "Iteration 97, loss = 0.21811577\n",
      "Iteration 98, loss = 0.21273594\n",
      "Iteration 99, loss = 0.20750980\n",
      "Iteration 100, loss = 0.20244959\n",
      "Iteration 101, loss = 0.19755578\n",
      "Iteration 102, loss = 0.19283465\n",
      "Iteration 103, loss = 0.18829642\n",
      "Iteration 104, loss = 0.18390855\n",
      "Iteration 105, loss = 0.17969100\n",
      "Iteration 106, loss = 0.17562738\n",
      "Iteration 107, loss = 0.17172571\n",
      "Iteration 108, loss = 0.16797457\n",
      "Iteration 109, loss = 0.16438133\n",
      "Iteration 110, loss = 0.16093110\n",
      "Iteration 111, loss = 0.15763246\n",
      "Iteration 112, loss = 0.15446313\n",
      "Iteration 113, loss = 0.15143557\n",
      "Iteration 114, loss = 0.14852178\n",
      "Iteration 115, loss = 0.14574667\n",
      "Iteration 116, loss = 0.14308140\n",
      "Iteration 117, loss = 0.14053766\n",
      "Iteration 118, loss = 0.13809251\n",
      "Iteration 119, loss = 0.13576135\n",
      "Iteration 120, loss = 0.13351803\n",
      "Iteration 121, loss = 0.13137870\n",
      "Iteration 122, loss = 0.12932232\n",
      "Iteration 123, loss = 0.12735908\n",
      "Iteration 124, loss = 0.12547160\n",
      "Iteration 125, loss = 0.12366725\n",
      "Iteration 126, loss = 0.12193378\n",
      "Iteration 127, loss = 0.12027311\n",
      "Iteration 128, loss = 0.11868684\n",
      "Iteration 129, loss = 0.11715567\n",
      "Iteration 130, loss = 0.11569337\n",
      "Iteration 131, loss = 0.11428232\n",
      "Iteration 132, loss = 0.11293121\n",
      "Iteration 133, loss = 0.11163358\n",
      "Iteration 134, loss = 0.11037792\n",
      "Iteration 135, loss = 0.10917832\n",
      "Iteration 136, loss = 0.10801737\n",
      "Iteration 137, loss = 0.10690426\n",
      "Iteration 138, loss = 0.10582791\n",
      "Iteration 139, loss = 0.10479202\n",
      "Iteration 140, loss = 0.10379281\n",
      "Iteration 141, loss = 0.10282683\n",
      "Iteration 142, loss = 0.10189659\n",
      "Iteration 143, loss = 0.10099490\n",
      "Iteration 144, loss = 0.10012604\n",
      "Iteration 145, loss = 0.09928378\n",
      "Iteration 146, loss = 0.09847106\n",
      "Iteration 147, loss = 0.09769134\n",
      "Iteration 148, loss = 0.09692576\n",
      "Iteration 149, loss = 0.09619317\n",
      "Iteration 150, loss = 0.09547989\n",
      "Iteration 151, loss = 0.09478789\n",
      "Iteration 152, loss = 0.09412033\n",
      "Iteration 153, loss = 0.09346741\n",
      "Iteration 154, loss = 0.09283838\n",
      "Iteration 155, loss = 0.09222458\n",
      "Iteration 156, loss = 0.09162853\n",
      "Iteration 157, loss = 0.09105113\n",
      "Iteration 158, loss = 0.09048663\n",
      "Iteration 159, loss = 0.08994051\n",
      "Iteration 160, loss = 0.08940759\n",
      "Iteration 161, loss = 0.08888846\n",
      "Iteration 162, loss = 0.08838453\n",
      "Iteration 163, loss = 0.08789157\n",
      "Iteration 164, loss = 0.08741241\n",
      "Iteration 165, loss = 0.08694517\n",
      "Iteration 166, loss = 0.08648848\n",
      "Iteration 167, loss = 0.08604422\n",
      "Iteration 168, loss = 0.08560978\n",
      "Iteration 169, loss = 0.08518562\n",
      "Iteration 170, loss = 0.08477209\n",
      "Iteration 171, loss = 0.08436727\n",
      "Iteration 172, loss = 0.08397210\n",
      "Iteration 173, loss = 0.08358602\n",
      "Iteration 174, loss = 0.08320794\n",
      "Iteration 175, loss = 0.08283865\n",
      "Iteration 176, loss = 0.08247754\n",
      "Iteration 177, loss = 0.08212430\n",
      "Iteration 178, loss = 0.08177900\n",
      "Iteration 179, loss = 0.08144083\n",
      "Iteration 180, loss = 0.08111005\n",
      "Iteration 181, loss = 0.08078630\n",
      "Iteration 182, loss = 0.08046924\n",
      "Iteration 183, loss = 0.08015856\n",
      "Iteration 184, loss = 0.07985399\n",
      "Iteration 185, loss = 0.07955548\n",
      "Iteration 186, loss = 0.07926277\n",
      "Iteration 187, loss = 0.07897562\n",
      "Iteration 188, loss = 0.07869397\n",
      "Iteration 189, loss = 0.07841762\n",
      "Iteration 190, loss = 0.07814635\n",
      "Iteration 191, loss = 0.07788012\n",
      "Iteration 192, loss = 0.07761878\n",
      "Iteration 193, loss = 0.07736210\n",
      "Iteration 194, loss = 0.07710999\n",
      "Iteration 195, loss = 0.07686233\n",
      "Iteration 196, loss = 0.07661895\n",
      "Iteration 197, loss = 0.07637976\n",
      "Iteration 198, loss = 0.07614465\n",
      "Iteration 199, loss = 0.07591350\n",
      "Iteration 200, loss = 0.07568618\n",
      "Iteration 201, loss = 0.07546263\n",
      "Iteration 202, loss = 0.07524272\n",
      "Iteration 203, loss = 0.07502662\n",
      "Iteration 204, loss = 0.07481416\n",
      "Iteration 205, loss = 0.07460491\n",
      "Iteration 206, loss = 0.07439898\n",
      "Iteration 207, loss = 0.07419639\n",
      "Iteration 208, loss = 0.07399686\n",
      "Iteration 209, loss = 0.07380033\n",
      "Iteration 210, loss = 0.07360684\n",
      "Iteration 211, loss = 0.07341632\n",
      "Iteration 212, loss = 0.07322855\n",
      "Iteration 213, loss = 0.07304349\n",
      "Iteration 214, loss = 0.07286118\n",
      "Iteration 215, loss = 0.07268153\n",
      "Iteration 216, loss = 0.07250439\n",
      "Iteration 217, loss = 0.07232973\n",
      "Iteration 218, loss = 0.07215755\n",
      "Iteration 219, loss = 0.07198778\n",
      "Iteration 220, loss = 0.07182031\n",
      "Iteration 221, loss = 0.07165510\n",
      "Iteration 222, loss = 0.07149213\n",
      "Iteration 223, loss = 0.07133136\n",
      "Iteration 224, loss = 0.07117271\n",
      "Iteration 225, loss = 0.07101612\n",
      "Iteration 226, loss = 0.07086157\n",
      "Iteration 227, loss = 0.07070903\n",
      "Iteration 228, loss = 0.07055843\n",
      "Iteration 229, loss = 0.07040974\n",
      "Iteration 230, loss = 0.07026290\n",
      "Iteration 231, loss = 0.07011790\n",
      "Iteration 232, loss = 0.06997469\n",
      "Iteration 233, loss = 0.06983324\n",
      "Iteration 234, loss = 0.06969349\n",
      "Iteration 235, loss = 0.06955545\n",
      "Iteration 236, loss = 0.06941905\n",
      "Iteration 237, loss = 0.06928428\n",
      "Iteration 238, loss = 0.06915109\n",
      "Iteration 239, loss = 0.06901945\n",
      "Iteration 240, loss = 0.06888932\n",
      "Iteration 241, loss = 0.06876069\n",
      "Iteration 242, loss = 0.06863351\n",
      "Iteration 243, loss = 0.06850778\n",
      "Iteration 244, loss = 0.06838345\n",
      "Iteration 245, loss = 0.06826049\n",
      "Iteration 246, loss = 0.06813889\n",
      "Iteration 247, loss = 0.06801862\n",
      "Iteration 248, loss = 0.06789967\n",
      "Iteration 249, loss = 0.06778202\n",
      "Iteration 250, loss = 0.06766564\n",
      "Iteration 251, loss = 0.06755050\n",
      "Iteration 252, loss = 0.06743656\n",
      "Iteration 253, loss = 0.06732381\n",
      "Iteration 254, loss = 0.06721223\n",
      "Iteration 255, loss = 0.06710179\n",
      "Iteration 256, loss = 0.06699247\n",
      "Iteration 257, loss = 0.06688427\n",
      "Iteration 258, loss = 0.06677715\n",
      "Iteration 259, loss = 0.06667109\n",
      "Iteration 260, loss = 0.06656609\n",
      "Iteration 261, loss = 0.06646211\n",
      "Iteration 262, loss = 0.06635914\n",
      "Iteration 263, loss = 0.06625717\n",
      "Iteration 264, loss = 0.06615617\n",
      "Iteration 265, loss = 0.06605614\n",
      "Iteration 266, loss = 0.06595705\n",
      "Iteration 267, loss = 0.06585889\n",
      "Iteration 268, loss = 0.06576164\n",
      "Iteration 269, loss = 0.06566529\n",
      "Iteration 270, loss = 0.06556983\n",
      "Iteration 271, loss = 0.06547524\n",
      "Iteration 272, loss = 0.06538151\n",
      "Iteration 273, loss = 0.06528862\n",
      "Iteration 274, loss = 0.06519658\n",
      "Iteration 275, loss = 0.06510536\n",
      "Iteration 276, loss = 0.06501497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12500615\n",
      "Iteration 2, loss = 1.81811504\n",
      "Iteration 3, loss = 1.54965307\n",
      "Iteration 4, loss = 1.33301183\n",
      "Iteration 5, loss = 1.17928176\n",
      "Iteration 6, loss = 1.09014189\n",
      "Iteration 7, loss = 1.05796699\n",
      "Iteration 8, loss = 1.06415183\n",
      "Iteration 9, loss = 1.08505980\n",
      "Iteration 10, loss = 1.10132118\n",
      "Iteration 11, loss = 1.10221772\n",
      "Iteration 12, loss = 1.08496180\n",
      "Iteration 13, loss = 1.05175271\n",
      "Iteration 14, loss = 1.00731883\n",
      "Iteration 15, loss = 0.95730401\n",
      "Iteration 16, loss = 0.90774805\n",
      "Iteration 17, loss = 0.86612918\n",
      "Iteration 18, loss = 0.83373350\n",
      "Iteration 19, loss = 0.80907685\n",
      "Iteration 20, loss = 0.79101495\n",
      "Iteration 21, loss = 0.77790913\n",
      "Iteration 22, loss = 0.76766360\n",
      "Iteration 23, loss = 0.75820522\n",
      "Iteration 24, loss = 0.74798485\n",
      "Iteration 25, loss = 0.73613364\n",
      "Iteration 26, loss = 0.72243302\n",
      "Iteration 27, loss = 0.70717124\n",
      "Iteration 28, loss = 0.69110548\n",
      "Iteration 29, loss = 0.67495914\n",
      "Iteration 30, loss = 0.65960124\n",
      "Iteration 31, loss = 0.64573847\n",
      "Iteration 32, loss = 0.63377557\n",
      "Iteration 33, loss = 0.62371575\n",
      "Iteration 34, loss = 0.61519082\n",
      "Iteration 35, loss = 0.60759598\n",
      "Iteration 36, loss = 0.60034788\n",
      "Iteration 37, loss = 0.59299332\n",
      "Iteration 38, loss = 0.58531862\n",
      "Iteration 39, loss = 0.57733931\n",
      "Iteration 40, loss = 0.56927640\n",
      "Iteration 41, loss = 0.56141552\n",
      "Iteration 42, loss = 0.55399562\n",
      "Iteration 43, loss = 0.54713731\n",
      "Iteration 44, loss = 0.54082078\n",
      "Iteration 45, loss = 0.53492790\n",
      "Iteration 46, loss = 0.52924980\n",
      "Iteration 47, loss = 0.52355363\n",
      "Iteration 48, loss = 0.51765215\n",
      "Iteration 49, loss = 0.51142917\n",
      "Iteration 50, loss = 0.50485042\n",
      "Iteration 51, loss = 0.49800796\n",
      "Iteration 52, loss = 0.49099943\n",
      "Iteration 53, loss = 0.48398004\n",
      "Iteration 54, loss = 0.47719736\n",
      "Iteration 55, loss = 0.47075406\n",
      "Iteration 56, loss = 0.46461895\n",
      "Iteration 57, loss = 0.45874265\n",
      "Iteration 58, loss = 0.45263553\n",
      "Iteration 59, loss = 0.44650801\n",
      "Iteration 60, loss = 0.44020643\n",
      "Iteration 61, loss = 0.43383286\n",
      "Iteration 62, loss = 0.42768421\n",
      "Iteration 63, loss = 0.42190731\n",
      "Iteration 64, loss = 0.41625979\n",
      "Iteration 65, loss = 0.41036486\n",
      "Iteration 66, loss = 0.40400149\n",
      "Iteration 67, loss = 0.39734742\n",
      "Iteration 68, loss = 0.39076207\n",
      "Iteration 69, loss = 0.38436162\n",
      "Iteration 70, loss = 0.37802511\n",
      "Iteration 71, loss = 0.37150023\n",
      "Iteration 72, loss = 0.36466716\n",
      "Iteration 73, loss = 0.35763151\n",
      "Iteration 74, loss = 0.35061896\n",
      "Iteration 75, loss = 0.34368201\n",
      "Iteration 76, loss = 0.33664273\n",
      "Iteration 77, loss = 0.32937152\n",
      "Iteration 78, loss = 0.32196870\n",
      "Iteration 79, loss = 0.31463496\n",
      "Iteration 80, loss = 0.30750264\n",
      "Iteration 81, loss = 0.30030382\n",
      "Iteration 82, loss = 0.29303021\n",
      "Iteration 83, loss = 0.28588327\n",
      "Iteration 84, loss = 0.27889885\n",
      "Iteration 85, loss = 0.27195620\n",
      "Iteration 86, loss = 0.26517009\n",
      "Iteration 87, loss = 0.25864460\n",
      "Iteration 88, loss = 0.25210835\n",
      "Iteration 89, loss = 0.24579914\n",
      "Iteration 90, loss = 0.23955751\n",
      "Iteration 91, loss = 0.23337044\n",
      "Iteration 92, loss = 0.22742612\n",
      "Iteration 93, loss = 0.22158912\n",
      "Iteration 94, loss = 0.21593028\n",
      "Iteration 95, loss = 0.21050277\n",
      "Iteration 96, loss = 0.20521411\n",
      "Iteration 97, loss = 0.20011295\n",
      "Iteration 98, loss = 0.19520075\n",
      "Iteration 99, loss = 0.19044052\n",
      "Iteration 100, loss = 0.18586152\n",
      "Iteration 101, loss = 0.18145647\n",
      "Iteration 102, loss = 0.17719333\n",
      "Iteration 103, loss = 0.17309793\n",
      "Iteration 104, loss = 0.16916993\n",
      "Iteration 105, loss = 0.16537486\n",
      "Iteration 106, loss = 0.16176211\n",
      "Iteration 107, loss = 0.15827329\n",
      "Iteration 108, loss = 0.15492821\n",
      "Iteration 109, loss = 0.15173480\n",
      "Iteration 110, loss = 0.14865154\n",
      "Iteration 111, loss = 0.14571318\n",
      "Iteration 112, loss = 0.14288767\n",
      "Iteration 113, loss = 0.14017127\n",
      "Iteration 114, loss = 0.13758459\n",
      "Iteration 115, loss = 0.13509436\n",
      "Iteration 116, loss = 0.13271677\n",
      "Iteration 117, loss = 0.13043367\n",
      "Iteration 118, loss = 0.12825687\n",
      "Iteration 119, loss = 0.12616672\n",
      "Iteration 120, loss = 0.12416689\n",
      "Iteration 121, loss = 0.12224619\n",
      "Iteration 122, loss = 0.12040111\n",
      "Iteration 123, loss = 0.11863515\n",
      "Iteration 124, loss = 0.11693635\n",
      "Iteration 125, loss = 0.11530874\n",
      "Iteration 126, loss = 0.11374565\n",
      "Iteration 127, loss = 0.11224284\n",
      "Iteration 128, loss = 0.11080201\n",
      "Iteration 129, loss = 0.10941492\n",
      "Iteration 130, loss = 0.10808335\n",
      "Iteration 131, loss = 0.10680234\n",
      "Iteration 132, loss = 0.10556917\n",
      "Iteration 133, loss = 0.10438389\n",
      "Iteration 134, loss = 0.10324114\n",
      "Iteration 135, loss = 0.10214403\n",
      "Iteration 136, loss = 0.10108493\n",
      "Iteration 137, loss = 0.10006560\n",
      "Iteration 138, loss = 0.09908262\n",
      "Iteration 139, loss = 0.09813389\n",
      "Iteration 140, loss = 0.09721934\n",
      "Iteration 141, loss = 0.09633525\n",
      "Iteration 142, loss = 0.09548230\n",
      "Iteration 143, loss = 0.09465754\n",
      "Iteration 144, loss = 0.09386045\n",
      "Iteration 145, loss = 0.09308978\n",
      "Iteration 146, loss = 0.09234368\n",
      "Iteration 147, loss = 0.09162222\n",
      "Iteration 148, loss = 0.09092313\n",
      "Iteration 149, loss = 0.09024639\n",
      "Iteration 150, loss = 0.08959033\n",
      "Iteration 151, loss = 0.08895438\n",
      "Iteration 152, loss = 0.08833772\n",
      "Iteration 153, loss = 0.08773923\n",
      "Iteration 154, loss = 0.08715862\n",
      "Iteration 155, loss = 0.08659462\n",
      "Iteration 156, loss = 0.08604706\n",
      "Iteration 157, loss = 0.08551485\n",
      "Iteration 158, loss = 0.08499766\n",
      "Iteration 159, loss = 0.08449482\n",
      "Iteration 160, loss = 0.08400578\n",
      "Iteration 161, loss = 0.08353054\n",
      "Iteration 162, loss = 0.08306801\n",
      "Iteration 163, loss = 0.08261824\n",
      "Iteration 164, loss = 0.08217973\n",
      "Iteration 165, loss = 0.08175382\n",
      "Iteration 166, loss = 0.08133882\n",
      "Iteration 167, loss = 0.08093433\n",
      "Iteration 168, loss = 0.08053978\n",
      "Iteration 169, loss = 0.08015523\n",
      "Iteration 170, loss = 0.07977985\n",
      "Iteration 171, loss = 0.07941420\n",
      "Iteration 172, loss = 0.07905656\n",
      "Iteration 173, loss = 0.07870794\n",
      "Iteration 174, loss = 0.07836682\n",
      "Iteration 175, loss = 0.07803399\n",
      "Iteration 176, loss = 0.07770843\n",
      "Iteration 177, loss = 0.07739035\n",
      "Iteration 178, loss = 0.07707924\n",
      "Iteration 179, loss = 0.07677490\n",
      "Iteration 180, loss = 0.07647728\n",
      "Iteration 181, loss = 0.07618587\n",
      "Iteration 182, loss = 0.07590081\n",
      "Iteration 183, loss = 0.07562150\n",
      "Iteration 184, loss = 0.07534817\n",
      "Iteration 185, loss = 0.07508023\n",
      "Iteration 186, loss = 0.07481788\n",
      "Iteration 187, loss = 0.07456079\n",
      "Iteration 188, loss = 0.07430905\n",
      "Iteration 189, loss = 0.07406193\n",
      "Iteration 190, loss = 0.07381996\n",
      "Iteration 191, loss = 0.07358229\n",
      "Iteration 192, loss = 0.07334951\n",
      "Iteration 193, loss = 0.07312086\n",
      "Iteration 194, loss = 0.07289668\n",
      "Iteration 195, loss = 0.07267643\n",
      "Iteration 196, loss = 0.07246034\n",
      "Iteration 197, loss = 0.07224801\n",
      "Iteration 198, loss = 0.07203955\n",
      "Iteration 199, loss = 0.07183470\n",
      "Iteration 200, loss = 0.07163346\n",
      "Iteration 201, loss = 0.07143566\n",
      "Iteration 202, loss = 0.07124124\n",
      "Iteration 203, loss = 0.07105011\n",
      "Iteration 204, loss = 0.07086216\n",
      "Iteration 205, loss = 0.07067733\n",
      "Iteration 206, loss = 0.07049550\n",
      "Iteration 207, loss = 0.07031665\n",
      "Iteration 208, loss = 0.07014068\n",
      "Iteration 209, loss = 0.06996753\n",
      "Iteration 210, loss = 0.06979707\n",
      "Iteration 211, loss = 0.06962930\n",
      "Iteration 212, loss = 0.06946408\n",
      "Iteration 213, loss = 0.06930141\n",
      "Iteration 214, loss = 0.06914121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 215, loss = 0.06898344\n",
      "Iteration 216, loss = 0.06882799\n",
      "Iteration 217, loss = 0.06867484\n",
      "Iteration 218, loss = 0.06852390\n",
      "Iteration 219, loss = 0.06837514\n",
      "Iteration 220, loss = 0.06822849\n",
      "Iteration 221, loss = 0.06808393\n",
      "Iteration 222, loss = 0.06794138\n",
      "Iteration 223, loss = 0.06780082\n",
      "Iteration 224, loss = 0.06766218\n",
      "Iteration 225, loss = 0.06752548\n",
      "Iteration 226, loss = 0.06739062\n",
      "Iteration 227, loss = 0.06725757\n",
      "Iteration 228, loss = 0.06712628\n",
      "Iteration 229, loss = 0.06699672\n",
      "Iteration 230, loss = 0.06686884\n",
      "Iteration 231, loss = 0.06674262\n",
      "Iteration 232, loss = 0.06661800\n",
      "Iteration 233, loss = 0.06649497\n",
      "Iteration 234, loss = 0.06637348\n",
      "Iteration 235, loss = 0.06625350\n",
      "Iteration 236, loss = 0.06613501\n",
      "Iteration 237, loss = 0.06601796\n",
      "Iteration 238, loss = 0.06590233\n",
      "Iteration 239, loss = 0.06578810\n",
      "Iteration 240, loss = 0.06567522\n",
      "Iteration 241, loss = 0.06556368\n",
      "Iteration 242, loss = 0.06545345\n",
      "Iteration 243, loss = 0.06534450\n",
      "Iteration 244, loss = 0.06523680\n",
      "Iteration 245, loss = 0.06513034\n",
      "Iteration 246, loss = 0.06502509\n",
      "Iteration 247, loss = 0.06492102\n",
      "Iteration 248, loss = 0.06481811\n",
      "Iteration 249, loss = 0.06471635\n",
      "Iteration 250, loss = 0.06461570\n",
      "Iteration 251, loss = 0.06451614\n",
      "Iteration 252, loss = 0.06441766\n",
      "Iteration 253, loss = 0.06432024\n",
      "Iteration 254, loss = 0.06422387\n",
      "Iteration 255, loss = 0.06412852\n",
      "Iteration 256, loss = 0.06403417\n",
      "Iteration 257, loss = 0.06394080\n",
      "Iteration 258, loss = 0.06384840\n",
      "Iteration 259, loss = 0.06375694\n",
      "Iteration 260, loss = 0.06366641\n",
      "Iteration 261, loss = 0.06357678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10484323\n",
      "Iteration 2, loss = 1.80224795\n",
      "Iteration 3, loss = 1.53809578\n",
      "Iteration 4, loss = 1.32567763\n",
      "Iteration 5, loss = 1.17658378\n",
      "Iteration 6, loss = 1.09161946\n",
      "Iteration 7, loss = 1.06148116\n",
      "Iteration 8, loss = 1.06696921\n",
      "Iteration 9, loss = 1.08571714\n",
      "Iteration 10, loss = 1.09951259\n",
      "Iteration 11, loss = 1.09828676\n",
      "Iteration 12, loss = 1.07954716\n",
      "Iteration 13, loss = 1.04555178\n",
      "Iteration 14, loss = 1.00096157\n",
      "Iteration 15, loss = 0.95136329\n",
      "Iteration 16, loss = 0.90276197\n",
      "Iteration 17, loss = 0.86247339\n",
      "Iteration 18, loss = 0.83055421\n",
      "Iteration 19, loss = 0.80571209\n",
      "Iteration 20, loss = 0.78734301\n",
      "Iteration 21, loss = 0.77382147\n",
      "Iteration 22, loss = 0.76314687\n",
      "Iteration 23, loss = 0.75334956\n",
      "Iteration 24, loss = 0.74294931\n",
      "Iteration 25, loss = 0.73098488\n",
      "Iteration 26, loss = 0.71723785\n",
      "Iteration 27, loss = 0.70211685\n",
      "Iteration 28, loss = 0.68621710\n",
      "Iteration 29, loss = 0.67019074\n",
      "Iteration 30, loss = 0.65485205\n",
      "Iteration 31, loss = 0.64095294\n",
      "Iteration 32, loss = 0.62894650\n",
      "Iteration 33, loss = 0.61884431\n",
      "Iteration 34, loss = 0.61026616\n",
      "Iteration 35, loss = 0.60265790\n",
      "Iteration 36, loss = 0.59543938\n",
      "Iteration 37, loss = 0.58815096\n",
      "Iteration 38, loss = 0.58059036\n",
      "Iteration 39, loss = 0.57278265\n",
      "Iteration 40, loss = 0.56490608\n",
      "Iteration 41, loss = 0.55722539\n",
      "Iteration 42, loss = 0.54999347\n",
      "Iteration 43, loss = 0.54336334\n",
      "Iteration 44, loss = 0.53734861\n",
      "Iteration 45, loss = 0.53182680\n",
      "Iteration 46, loss = 0.52659982\n",
      "Iteration 47, loss = 0.52144166\n",
      "Iteration 48, loss = 0.51614634\n",
      "Iteration 49, loss = 0.51055672\n",
      "Iteration 50, loss = 0.50459778\n",
      "Iteration 51, loss = 0.49826514\n",
      "Iteration 52, loss = 0.49162629\n",
      "Iteration 53, loss = 0.48476330\n",
      "Iteration 54, loss = 0.47781236\n",
      "Iteration 55, loss = 0.47092916\n",
      "Iteration 56, loss = 0.46412799\n",
      "Iteration 57, loss = 0.45742955\n",
      "Iteration 58, loss = 0.45076150\n",
      "Iteration 59, loss = 0.44414756\n",
      "Iteration 60, loss = 0.43738396\n",
      "Iteration 61, loss = 0.43048258\n",
      "Iteration 62, loss = 0.42360021\n",
      "Iteration 63, loss = 0.41703018\n",
      "Iteration 64, loss = 0.41082473\n",
      "Iteration 65, loss = 0.40459238\n",
      "Iteration 66, loss = 0.39800123\n",
      "Iteration 67, loss = 0.39100715\n",
      "Iteration 68, loss = 0.38386803\n",
      "Iteration 69, loss = 0.37690962\n",
      "Iteration 70, loss = 0.37013692\n",
      "Iteration 71, loss = 0.36330422\n",
      "Iteration 72, loss = 0.35623277\n",
      "Iteration 73, loss = 0.34889213\n",
      "Iteration 74, loss = 0.34143474\n",
      "Iteration 75, loss = 0.33401947\n",
      "Iteration 76, loss = 0.32660951\n",
      "Iteration 77, loss = 0.31904360\n",
      "Iteration 78, loss = 0.31127611\n",
      "Iteration 79, loss = 0.30344561\n",
      "Iteration 80, loss = 0.29571160\n",
      "Iteration 81, loss = 0.28806778\n",
      "Iteration 82, loss = 0.28041980\n",
      "Iteration 83, loss = 0.27273716\n",
      "Iteration 84, loss = 0.26519756\n",
      "Iteration 85, loss = 0.25784247\n",
      "Iteration 86, loss = 0.25053884\n",
      "Iteration 87, loss = 0.24332440\n",
      "Iteration 88, loss = 0.23642684\n",
      "Iteration 89, loss = 0.22974233\n",
      "Iteration 90, loss = 0.22314692\n",
      "Iteration 91, loss = 0.21687644\n",
      "Iteration 92, loss = 0.21071549\n",
      "Iteration 93, loss = 0.20464016\n",
      "Iteration 94, loss = 0.19879327\n",
      "Iteration 95, loss = 0.19310709\n",
      "Iteration 96, loss = 0.18756916\n",
      "Iteration 97, loss = 0.18227052\n",
      "Iteration 98, loss = 0.17715293\n",
      "Iteration 99, loss = 0.17220091\n",
      "Iteration 100, loss = 0.16749723\n",
      "Iteration 101, loss = 0.16294065\n",
      "Iteration 102, loss = 0.15858661\n",
      "Iteration 103, loss = 0.15442388\n",
      "Iteration 104, loss = 0.15040695\n",
      "Iteration 105, loss = 0.14655787\n",
      "Iteration 106, loss = 0.14288088\n",
      "Iteration 107, loss = 0.13934044\n",
      "Iteration 108, loss = 0.13595071\n",
      "Iteration 109, loss = 0.13271026\n",
      "Iteration 110, loss = 0.12959495\n",
      "Iteration 111, loss = 0.12662507\n",
      "Iteration 112, loss = 0.12377906\n",
      "Iteration 113, loss = 0.12105321\n",
      "Iteration 114, loss = 0.11844350\n",
      "Iteration 115, loss = 0.11594239\n",
      "Iteration 116, loss = 0.11355445\n",
      "Iteration 117, loss = 0.11127218\n",
      "Iteration 118, loss = 0.10908622\n",
      "Iteration 119, loss = 0.10699784\n",
      "Iteration 120, loss = 0.10499177\n",
      "Iteration 121, loss = 0.10307690\n",
      "Iteration 122, loss = 0.10124329\n",
      "Iteration 123, loss = 0.09948485\n",
      "Iteration 124, loss = 0.09780302\n",
      "Iteration 125, loss = 0.09618885\n",
      "Iteration 126, loss = 0.09464144\n",
      "Iteration 127, loss = 0.09315659\n",
      "Iteration 128, loss = 0.09173041\n",
      "Iteration 129, loss = 0.09036102\n",
      "Iteration 130, loss = 0.08904553\n",
      "Iteration 131, loss = 0.08778081\n",
      "Iteration 132, loss = 0.08656518\n",
      "Iteration 133, loss = 0.08539589\n",
      "Iteration 134, loss = 0.08427059\n",
      "Iteration 135, loss = 0.08318759\n",
      "Iteration 136, loss = 0.08214434\n",
      "Iteration 137, loss = 0.08113913\n",
      "Iteration 138, loss = 0.08017029\n",
      "Iteration 139, loss = 0.07923638\n",
      "Iteration 140, loss = 0.07833665\n",
      "Iteration 141, loss = 0.07746998\n",
      "Iteration 142, loss = 0.07663154\n",
      "Iteration 143, loss = 0.07582243\n",
      "Iteration 144, loss = 0.07504094\n",
      "Iteration 145, loss = 0.07428428\n",
      "Iteration 146, loss = 0.07355345\n",
      "Iteration 147, loss = 0.07284580\n",
      "Iteration 148, loss = 0.07216032\n",
      "Iteration 149, loss = 0.07149715\n",
      "Iteration 150, loss = 0.07085385\n",
      "Iteration 151, loss = 0.07023043\n",
      "Iteration 152, loss = 0.06962606\n",
      "Iteration 153, loss = 0.06903984\n",
      "Iteration 154, loss = 0.06847108\n",
      "Iteration 155, loss = 0.06791867\n",
      "Iteration 156, loss = 0.06738226\n",
      "Iteration 157, loss = 0.06686079\n",
      "Iteration 158, loss = 0.06635389\n",
      "Iteration 159, loss = 0.06586098\n",
      "Iteration 160, loss = 0.06538130\n",
      "Iteration 161, loss = 0.06491455\n",
      "Iteration 162, loss = 0.06446009\n",
      "Iteration 163, loss = 0.06401797\n",
      "Iteration 164, loss = 0.06358705\n",
      "Iteration 165, loss = 0.06316719\n",
      "Iteration 166, loss = 0.06275790\n",
      "Iteration 167, loss = 0.06235876\n",
      "Iteration 168, loss = 0.06197018\n",
      "Iteration 169, loss = 0.06159100\n",
      "Iteration 170, loss = 0.06122093\n",
      "Iteration 171, loss = 0.06085973\n",
      "Iteration 172, loss = 0.06050693\n",
      "Iteration 173, loss = 0.06016238\n",
      "Iteration 174, loss = 0.05982574\n",
      "Iteration 175, loss = 0.05949669\n",
      "Iteration 176, loss = 0.05917509\n",
      "Iteration 177, loss = 0.05886057\n",
      "Iteration 178, loss = 0.05855296\n",
      "Iteration 179, loss = 0.05825205\n",
      "Iteration 180, loss = 0.05795756\n",
      "Iteration 181, loss = 0.05766935\n",
      "Iteration 182, loss = 0.05738719\n",
      "Iteration 183, loss = 0.05711088\n",
      "Iteration 184, loss = 0.05684031\n",
      "Iteration 185, loss = 0.05657529\n",
      "Iteration 186, loss = 0.05631563\n",
      "Iteration 187, loss = 0.05606116\n",
      "Iteration 188, loss = 0.05581172\n",
      "Iteration 189, loss = 0.05556718\n",
      "Iteration 190, loss = 0.05532737\n",
      "Iteration 191, loss = 0.05509218\n",
      "Iteration 192, loss = 0.05486147\n",
      "Iteration 193, loss = 0.05463511\n",
      "Iteration 194, loss = 0.05441299\n",
      "Iteration 195, loss = 0.05419498\n",
      "Iteration 196, loss = 0.05398098\n",
      "Iteration 197, loss = 0.05377087\n",
      "Iteration 198, loss = 0.05356456\n",
      "Iteration 199, loss = 0.05336194\n",
      "Iteration 200, loss = 0.05316292\n",
      "Iteration 201, loss = 0.05296739\n",
      "Iteration 202, loss = 0.05277528\n",
      "Iteration 203, loss = 0.05258650\n",
      "Iteration 204, loss = 0.05240096\n",
      "Iteration 205, loss = 0.05221858\n",
      "Iteration 206, loss = 0.05203928\n",
      "Iteration 207, loss = 0.05186298\n",
      "Iteration 208, loss = 0.05168962\n",
      "Iteration 209, loss = 0.05151912\n",
      "Iteration 210, loss = 0.05135142\n",
      "Iteration 211, loss = 0.05118654\n",
      "Iteration 212, loss = 0.05102439\n",
      "Iteration 213, loss = 0.05086494\n",
      "Iteration 214, loss = 0.05070791\n",
      "Iteration 215, loss = 0.05055344\n",
      "Iteration 216, loss = 0.05040139\n",
      "Iteration 217, loss = 0.05025169\n",
      "Iteration 218, loss = 0.05010437\n",
      "Iteration 219, loss = 0.04995924\n",
      "Iteration 220, loss = 0.04981638\n",
      "Iteration 221, loss = 0.04967565\n",
      "Iteration 222, loss = 0.04953704\n",
      "Iteration 223, loss = 0.04940054\n",
      "Iteration 224, loss = 0.04926601\n",
      "Iteration 225, loss = 0.04913349\n",
      "Iteration 226, loss = 0.04900290\n",
      "Iteration 227, loss = 0.04887418\n",
      "Iteration 228, loss = 0.04874740\n",
      "Iteration 229, loss = 0.04862241\n",
      "Iteration 230, loss = 0.04849922\n",
      "Iteration 231, loss = 0.04837776\n",
      "Iteration 232, loss = 0.04825800\n",
      "Iteration 233, loss = 0.04813991\n",
      "Iteration 234, loss = 0.04802345\n",
      "Iteration 235, loss = 0.04790859\n",
      "Iteration 236, loss = 0.04779529\n",
      "Iteration 237, loss = 0.04768352\n",
      "Iteration 238, loss = 0.04757327\n",
      "Iteration 239, loss = 0.04746447\n",
      "Iteration 240, loss = 0.04735712\n",
      "Iteration 241, loss = 0.04725119\n",
      "Iteration 242, loss = 0.04714663\n",
      "Iteration 243, loss = 0.04704344\n",
      "Iteration 244, loss = 0.04694158\n",
      "Iteration 245, loss = 0.04684103\n",
      "Iteration 246, loss = 0.04674175\n",
      "Iteration 247, loss = 0.04664375\n",
      "Iteration 248, loss = 0.04654698\n",
      "Iteration 249, loss = 0.04645141\n",
      "Iteration 250, loss = 0.04635704\n",
      "Iteration 251, loss = 0.04626383\n",
      "Iteration 252, loss = 0.04617177\n",
      "Iteration 253, loss = 0.04608083\n",
      "Iteration 254, loss = 0.04599098\n",
      "Iteration 255, loss = 0.04590222\n",
      "Iteration 256, loss = 0.04581452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11317117\n",
      "Iteration 2, loss = 1.80700948\n",
      "Iteration 3, loss = 1.53960225\n",
      "Iteration 4, loss = 1.32438366\n",
      "Iteration 5, loss = 1.17275668\n",
      "Iteration 6, loss = 1.08607340\n",
      "Iteration 7, loss = 1.05529029\n",
      "Iteration 8, loss = 1.06105867\n",
      "Iteration 9, loss = 1.08084651\n",
      "Iteration 10, loss = 1.09580797\n",
      "Iteration 11, loss = 1.09560020\n",
      "Iteration 12, loss = 1.07766673\n",
      "Iteration 13, loss = 1.04422852\n",
      "Iteration 14, loss = 0.99996474\n",
      "Iteration 15, loss = 0.95062228\n",
      "Iteration 16, loss = 0.90184654\n",
      "Iteration 17, loss = 0.86104320\n",
      "Iteration 18, loss = 0.82887785\n",
      "Iteration 19, loss = 0.80390193\n",
      "Iteration 20, loss = 0.78531422\n",
      "Iteration 21, loss = 0.77166374\n",
      "Iteration 22, loss = 0.76097219\n",
      "Iteration 23, loss = 0.75127189\n",
      "Iteration 24, loss = 0.74103771\n",
      "Iteration 25, loss = 0.72937663\n",
      "Iteration 26, loss = 0.71593843\n",
      "Iteration 27, loss = 0.70121966\n",
      "Iteration 28, loss = 0.68554434\n",
      "Iteration 29, loss = 0.66967092\n",
      "Iteration 30, loss = 0.65438219\n",
      "Iteration 31, loss = 0.64047837\n",
      "Iteration 32, loss = 0.62842247\n",
      "Iteration 33, loss = 0.61826787\n",
      "Iteration 34, loss = 0.60966005\n",
      "Iteration 35, loss = 0.60205848\n",
      "Iteration 36, loss = 0.59485445\n",
      "Iteration 37, loss = 0.58760142\n",
      "Iteration 38, loss = 0.58007682\n",
      "Iteration 39, loss = 0.57225039\n",
      "Iteration 40, loss = 0.56429996\n",
      "Iteration 41, loss = 0.55649483\n",
      "Iteration 42, loss = 0.54909434\n",
      "Iteration 43, loss = 0.54223220\n",
      "Iteration 44, loss = 0.53589722\n",
      "Iteration 45, loss = 0.52996788\n",
      "Iteration 46, loss = 0.52424848\n",
      "Iteration 47, loss = 0.51851648\n",
      "Iteration 48, loss = 0.51254561\n",
      "Iteration 49, loss = 0.50620466\n",
      "Iteration 50, loss = 0.49945362\n",
      "Iteration 51, loss = 0.49239358\n",
      "Iteration 52, loss = 0.48513643\n",
      "Iteration 53, loss = 0.47785790\n",
      "Iteration 54, loss = 0.47091002\n",
      "Iteration 55, loss = 0.46430782\n",
      "Iteration 56, loss = 0.45807847\n",
      "Iteration 57, loss = 0.45208163\n",
      "Iteration 58, loss = 0.44580696\n",
      "Iteration 59, loss = 0.43939946\n",
      "Iteration 60, loss = 0.43278903\n",
      "Iteration 61, loss = 0.42617035\n",
      "Iteration 62, loss = 0.41979607\n",
      "Iteration 63, loss = 0.41373014\n",
      "Iteration 64, loss = 0.40768338\n",
      "Iteration 65, loss = 0.40132237\n",
      "Iteration 66, loss = 0.39450137\n",
      "Iteration 67, loss = 0.38739861\n",
      "Iteration 68, loss = 0.38032555\n",
      "Iteration 69, loss = 0.37342252\n",
      "Iteration 70, loss = 0.36656282\n",
      "Iteration 71, loss = 0.35954512\n",
      "Iteration 72, loss = 0.35221875\n",
      "Iteration 73, loss = 0.34466845\n",
      "Iteration 74, loss = 0.33714249\n",
      "Iteration 75, loss = 0.32972395\n",
      "Iteration 76, loss = 0.32223772\n",
      "Iteration 77, loss = 0.31453864\n",
      "Iteration 78, loss = 0.30670983\n",
      "Iteration 79, loss = 0.29902126\n",
      "Iteration 80, loss = 0.29149869\n",
      "Iteration 81, loss = 0.28395459\n",
      "Iteration 82, loss = 0.27637604\n",
      "Iteration 83, loss = 0.26895714\n",
      "Iteration 84, loss = 0.26178161\n",
      "Iteration 85, loss = 0.25470565\n",
      "Iteration 86, loss = 0.24771032\n",
      "Iteration 87, loss = 0.24098954\n",
      "Iteration 88, loss = 0.23422415\n",
      "Iteration 89, loss = 0.22765720\n",
      "Iteration 90, loss = 0.22120599\n",
      "Iteration 91, loss = 0.21486406\n",
      "Iteration 92, loss = 0.20875276\n",
      "Iteration 93, loss = 0.20283160\n",
      "Iteration 94, loss = 0.19711775\n",
      "Iteration 95, loss = 0.19157633\n",
      "Iteration 96, loss = 0.18621436\n",
      "Iteration 97, loss = 0.18103772\n",
      "Iteration 98, loss = 0.17603184\n",
      "Iteration 99, loss = 0.17119837\n",
      "Iteration 100, loss = 0.16653777\n",
      "Iteration 101, loss = 0.16204313\n",
      "Iteration 102, loss = 0.15771193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 103, loss = 0.15353952\n",
      "Iteration 104, loss = 0.14952130\n",
      "Iteration 105, loss = 0.14567325\n",
      "Iteration 106, loss = 0.14198321\n",
      "Iteration 107, loss = 0.13844053\n",
      "Iteration 108, loss = 0.13502277\n",
      "Iteration 109, loss = 0.13175310\n",
      "Iteration 110, loss = 0.12860814\n",
      "Iteration 111, loss = 0.12558690\n",
      "Iteration 112, loss = 0.12269577\n",
      "Iteration 113, loss = 0.11991697\n",
      "Iteration 114, loss = 0.11724294\n",
      "Iteration 115, loss = 0.11468195\n",
      "Iteration 116, loss = 0.11221934\n",
      "Iteration 117, loss = 0.10985421\n",
      "Iteration 118, loss = 0.10759220\n",
      "Iteration 119, loss = 0.10540660\n",
      "Iteration 120, loss = 0.10331851\n",
      "Iteration 121, loss = 0.10130750\n",
      "Iteration 122, loss = 0.09937137\n",
      "Iteration 123, loss = 0.09751780\n",
      "Iteration 124, loss = 0.09572624\n",
      "Iteration 125, loss = 0.09401052\n",
      "Iteration 126, loss = 0.09235308\n",
      "Iteration 127, loss = 0.09075823\n",
      "Iteration 128, loss = 0.08922507\n",
      "Iteration 129, loss = 0.08774190\n",
      "Iteration 130, loss = 0.08631649\n",
      "Iteration 131, loss = 0.08493994\n",
      "Iteration 132, loss = 0.08361058\n",
      "Iteration 133, loss = 0.08233038\n",
      "Iteration 134, loss = 0.08109051\n",
      "Iteration 135, loss = 0.07989464\n",
      "Iteration 136, loss = 0.07873879\n",
      "Iteration 137, loss = 0.07762015\n",
      "Iteration 138, loss = 0.07654105\n",
      "Iteration 139, loss = 0.07549843\n",
      "Iteration 140, loss = 0.07448837\n",
      "Iteration 141, loss = 0.07351105\n",
      "Iteration 142, loss = 0.07256467\n",
      "Iteration 143, loss = 0.07164765\n",
      "Iteration 144, loss = 0.07075987\n",
      "Iteration 145, loss = 0.06989703\n",
      "Iteration 146, loss = 0.06906122\n",
      "Iteration 147, loss = 0.06824922\n",
      "Iteration 148, loss = 0.06746056\n",
      "Iteration 149, loss = 0.06669514\n",
      "Iteration 150, loss = 0.06595019\n",
      "Iteration 151, loss = 0.06522680\n",
      "Iteration 152, loss = 0.06452270\n",
      "Iteration 153, loss = 0.06383763\n",
      "Iteration 154, loss = 0.06317115\n",
      "Iteration 155, loss = 0.06252244\n",
      "Iteration 156, loss = 0.06189178\n",
      "Iteration 157, loss = 0.06127734\n",
      "Iteration 158, loss = 0.06067968\n",
      "Iteration 159, loss = 0.06009565\n",
      "Iteration 160, loss = 0.05952703\n",
      "Iteration 161, loss = 0.05897179\n",
      "Iteration 162, loss = 0.05842983\n",
      "Iteration 163, loss = 0.05790124\n",
      "Iteration 164, loss = 0.05738430\n",
      "Iteration 165, loss = 0.05688000\n",
      "Iteration 166, loss = 0.05638672\n",
      "Iteration 167, loss = 0.05590476\n",
      "Iteration 168, loss = 0.05543358\n",
      "Iteration 169, loss = 0.05497245\n",
      "Iteration 170, loss = 0.05452169\n",
      "Iteration 171, loss = 0.05408020\n",
      "Iteration 172, loss = 0.05364831\n",
      "Iteration 173, loss = 0.05322532\n",
      "Iteration 174, loss = 0.05281168\n",
      "Iteration 175, loss = 0.05240657\n",
      "Iteration 176, loss = 0.05201000\n",
      "Iteration 177, loss = 0.05162122\n",
      "Iteration 178, loss = 0.05124008\n",
      "Iteration 179, loss = 0.05086657\n",
      "Iteration 180, loss = 0.05050001\n",
      "Iteration 181, loss = 0.05014072\n",
      "Iteration 182, loss = 0.04978797\n",
      "Iteration 183, loss = 0.04944195\n",
      "Iteration 184, loss = 0.04910225\n",
      "Iteration 185, loss = 0.04876871\n",
      "Iteration 186, loss = 0.04844128\n",
      "Iteration 187, loss = 0.04811954\n",
      "Iteration 188, loss = 0.04780362\n",
      "Iteration 189, loss = 0.04749308\n",
      "Iteration 190, loss = 0.04718798\n",
      "Iteration 191, loss = 0.04688807\n",
      "Iteration 192, loss = 0.04659326\n",
      "Iteration 193, loss = 0.04630341\n",
      "Iteration 194, loss = 0.04601837\n",
      "Iteration 195, loss = 0.04573812\n",
      "Iteration 196, loss = 0.04546237\n",
      "Iteration 197, loss = 0.04519113\n",
      "Iteration 198, loss = 0.04492416\n",
      "Iteration 199, loss = 0.04466143\n",
      "Iteration 200, loss = 0.04440279\n",
      "Iteration 201, loss = 0.04414815\n",
      "Iteration 202, loss = 0.04389767\n",
      "Iteration 203, loss = 0.04365118\n",
      "Iteration 204, loss = 0.04340865\n",
      "Iteration 205, loss = 0.04316951\n",
      "Iteration 206, loss = 0.04293417\n",
      "Iteration 207, loss = 0.04270208\n",
      "Iteration 208, loss = 0.04247354\n",
      "Iteration 209, loss = 0.04224819\n",
      "Iteration 210, loss = 0.04202611\n",
      "Iteration 211, loss = 0.04180718\n",
      "Iteration 212, loss = 0.04159128\n",
      "Iteration 213, loss = 0.04137846\n",
      "Iteration 214, loss = 0.04116848\n",
      "Iteration 215, loss = 0.04096146\n",
      "Iteration 216, loss = 0.04075718\n",
      "Iteration 217, loss = 0.04055572\n",
      "Iteration 218, loss = 0.04035687\n",
      "Iteration 219, loss = 0.04016069\n",
      "Iteration 220, loss = 0.03996705\n",
      "Iteration 221, loss = 0.03977592\n",
      "Iteration 222, loss = 0.03958726\n",
      "Iteration 223, loss = 0.03940098\n",
      "Iteration 224, loss = 0.03921716\n",
      "Iteration 225, loss = 0.03903562\n",
      "Iteration 226, loss = 0.03885637\n",
      "Iteration 227, loss = 0.03867930\n",
      "Iteration 228, loss = 0.03850443\n",
      "Iteration 229, loss = 0.03833165\n",
      "Iteration 230, loss = 0.03816096\n",
      "Iteration 231, loss = 0.03799230\n",
      "Iteration 232, loss = 0.03782564\n",
      "Iteration 233, loss = 0.03766093\n",
      "Iteration 234, loss = 0.03749814\n",
      "Iteration 235, loss = 0.03733723\n",
      "Iteration 236, loss = 0.03717816\n",
      "Iteration 237, loss = 0.03702090\n",
      "Iteration 238, loss = 0.03686540\n",
      "Iteration 239, loss = 0.03671165\n",
      "Iteration 240, loss = 0.03655960\n",
      "Iteration 241, loss = 0.03640923\n",
      "Iteration 242, loss = 0.03626049\n",
      "Iteration 243, loss = 0.03611337\n",
      "Iteration 244, loss = 0.03596783\n",
      "Iteration 245, loss = 0.03582385\n",
      "Iteration 246, loss = 0.03568138\n",
      "Iteration 247, loss = 0.03554043\n",
      "Iteration 248, loss = 0.03540096\n",
      "Iteration 249, loss = 0.03526295\n",
      "Iteration 250, loss = 0.03512636\n",
      "Iteration 251, loss = 0.03499116\n",
      "Iteration 252, loss = 0.03485733\n",
      "Iteration 253, loss = 0.03472485\n",
      "Iteration 254, loss = 0.03459370\n",
      "Iteration 255, loss = 0.03446384\n",
      "Iteration 256, loss = 0.03433526\n",
      "Iteration 257, loss = 0.03420794\n",
      "Iteration 258, loss = 0.03408185\n",
      "Iteration 259, loss = 0.03395698\n",
      "Iteration 260, loss = 0.03383330\n",
      "Iteration 261, loss = 0.03371079\n",
      "Iteration 262, loss = 0.03358944\n",
      "Iteration 263, loss = 0.03346923\n",
      "Iteration 264, loss = 0.03335013\n",
      "Iteration 265, loss = 0.03323215\n",
      "Iteration 266, loss = 0.03311525\n",
      "Iteration 267, loss = 0.03299942\n",
      "Iteration 268, loss = 0.03288463\n",
      "Iteration 269, loss = 0.03277088\n",
      "Iteration 270, loss = 0.03265828\n",
      "Iteration 271, loss = 0.03254667\n",
      "Iteration 272, loss = 0.03243607\n",
      "Iteration 273, loss = 0.03232643\n",
      "Iteration 274, loss = 0.03221775\n",
      "Iteration 275, loss = 0.03211001\n",
      "Iteration 276, loss = 0.03200320\n",
      "Iteration 277, loss = 0.03189729\n",
      "Iteration 278, loss = 0.03179261\n",
      "Iteration 279, loss = 0.03168881\n",
      "Iteration 280, loss = 0.03158592\n",
      "Iteration 281, loss = 0.03148386\n",
      "Iteration 282, loss = 0.03138269\n",
      "Iteration 283, loss = 0.03128232\n",
      "Iteration 284, loss = 0.03118280\n",
      "Iteration 285, loss = 0.03108413\n",
      "Iteration 286, loss = 0.03098627\n",
      "Iteration 287, loss = 0.03088929\n",
      "Iteration 288, loss = 0.03079299\n",
      "Iteration 289, loss = 0.03069753\n",
      "Iteration 290, loss = 0.03060275\n",
      "Iteration 291, loss = 0.03050878\n",
      "Iteration 292, loss = 0.03041547\n",
      "Iteration 293, loss = 0.03032293\n",
      "Iteration 294, loss = 0.03023104\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11924132\n",
      "Iteration 2, loss = 1.80963186\n",
      "Iteration 3, loss = 1.53866701\n",
      "Iteration 4, loss = 1.31993045\n",
      "Iteration 5, loss = 1.16494330\n",
      "Iteration 6, loss = 1.07565233\n",
      "Iteration 7, loss = 1.04422304\n",
      "Iteration 8, loss = 1.05094233\n",
      "Iteration 9, loss = 1.07181067\n",
      "Iteration 10, loss = 1.08765069\n",
      "Iteration 11, loss = 1.08796981\n",
      "Iteration 12, loss = 1.07017790\n",
      "Iteration 13, loss = 1.03656900\n",
      "Iteration 14, loss = 0.99174540\n",
      "Iteration 15, loss = 0.94157006\n",
      "Iteration 16, loss = 0.89217888\n",
      "Iteration 17, loss = 0.85066209\n",
      "Iteration 18, loss = 0.81802165\n",
      "Iteration 19, loss = 0.79301179\n",
      "Iteration 20, loss = 0.77474870\n",
      "Iteration 21, loss = 0.76158740\n",
      "Iteration 22, loss = 0.75140484\n",
      "Iteration 23, loss = 0.74211942\n",
      "Iteration 24, loss = 0.73212790\n",
      "Iteration 25, loss = 0.72051833\n",
      "Iteration 26, loss = 0.70703073\n",
      "Iteration 27, loss = 0.69198696\n",
      "Iteration 28, loss = 0.67612977\n",
      "Iteration 29, loss = 0.66022993\n",
      "Iteration 30, loss = 0.64516009\n",
      "Iteration 31, loss = 0.63162033\n",
      "Iteration 32, loss = 0.62004091\n",
      "Iteration 33, loss = 0.61040614\n",
      "Iteration 34, loss = 0.60233099\n",
      "Iteration 35, loss = 0.59518159\n",
      "Iteration 36, loss = 0.58835366\n",
      "Iteration 37, loss = 0.58137654\n",
      "Iteration 38, loss = 0.57403971\n",
      "Iteration 39, loss = 0.56637041\n",
      "Iteration 40, loss = 0.55860510\n",
      "Iteration 41, loss = 0.55105632\n",
      "Iteration 42, loss = 0.54399004\n",
      "Iteration 43, loss = 0.53753974\n",
      "Iteration 44, loss = 0.53168371\n",
      "Iteration 45, loss = 0.52628499\n",
      "Iteration 46, loss = 0.52110567\n",
      "Iteration 47, loss = 0.51590430\n",
      "Iteration 48, loss = 0.51046976\n",
      "Iteration 49, loss = 0.50465926\n",
      "Iteration 50, loss = 0.49845195\n",
      "Iteration 51, loss = 0.49190142\n",
      "Iteration 52, loss = 0.48517596\n",
      "Iteration 53, loss = 0.47843376\n",
      "Iteration 54, loss = 0.47189691\n",
      "Iteration 55, loss = 0.46565664\n",
      "Iteration 56, loss = 0.45969219\n",
      "Iteration 57, loss = 0.45399415\n",
      "Iteration 58, loss = 0.44812814\n",
      "Iteration 59, loss = 0.44219268\n",
      "Iteration 60, loss = 0.43599322\n",
      "Iteration 61, loss = 0.42968267\n",
      "Iteration 62, loss = 0.42360581\n",
      "Iteration 63, loss = 0.41797370\n",
      "Iteration 64, loss = 0.41257606\n",
      "Iteration 65, loss = 0.40693251\n",
      "Iteration 66, loss = 0.40082028\n",
      "Iteration 67, loss = 0.39432876\n",
      "Iteration 68, loss = 0.38785366\n",
      "Iteration 69, loss = 0.38167718\n",
      "Iteration 70, loss = 0.37559193\n",
      "Iteration 71, loss = 0.36936490\n",
      "Iteration 72, loss = 0.36281683\n",
      "Iteration 73, loss = 0.35601093\n",
      "Iteration 74, loss = 0.34920688\n",
      "Iteration 75, loss = 0.34253604\n",
      "Iteration 76, loss = 0.33582821\n",
      "Iteration 77, loss = 0.32888150\n",
      "Iteration 78, loss = 0.32174286\n",
      "Iteration 79, loss = 0.31464120\n",
      "Iteration 80, loss = 0.30769373\n",
      "Iteration 81, loss = 0.30079545\n",
      "Iteration 82, loss = 0.29378207\n",
      "Iteration 83, loss = 0.28680397\n",
      "Iteration 84, loss = 0.28004698\n",
      "Iteration 85, loss = 0.27337826\n",
      "Iteration 86, loss = 0.26665757\n",
      "Iteration 87, loss = 0.26007974\n",
      "Iteration 88, loss = 0.25387090\n",
      "Iteration 89, loss = 0.24766664\n",
      "Iteration 90, loss = 0.24169158\n",
      "Iteration 91, loss = 0.23602525\n",
      "Iteration 92, loss = 0.23023511\n",
      "Iteration 93, loss = 0.22467512\n",
      "Iteration 94, loss = 0.21932972\n",
      "Iteration 95, loss = 0.21399965\n",
      "Iteration 96, loss = 0.20896193\n",
      "Iteration 97, loss = 0.20407092\n",
      "Iteration 98, loss = 0.19929873\n",
      "Iteration 99, loss = 0.19474561\n",
      "Iteration 100, loss = 0.19034664\n",
      "Iteration 101, loss = 0.18608366\n",
      "Iteration 102, loss = 0.18204099\n",
      "Iteration 103, loss = 0.17808746\n",
      "Iteration 104, loss = 0.17430452\n",
      "Iteration 105, loss = 0.17067743\n",
      "Iteration 106, loss = 0.16715876\n",
      "Iteration 107, loss = 0.16380711\n",
      "Iteration 108, loss = 0.16058077\n",
      "Iteration 109, loss = 0.15745588\n",
      "Iteration 110, loss = 0.15447748\n",
      "Iteration 111, loss = 0.15161049\n",
      "Iteration 112, loss = 0.14884694\n",
      "Iteration 113, loss = 0.14621854\n",
      "Iteration 114, loss = 0.14368001\n",
      "Iteration 115, loss = 0.14125427\n",
      "Iteration 116, loss = 0.13892513\n",
      "Iteration 117, loss = 0.13668431\n",
      "Iteration 118, loss = 0.13454030\n",
      "Iteration 119, loss = 0.13248020\n",
      "Iteration 120, loss = 0.13049977\n",
      "Iteration 121, loss = 0.12860636\n",
      "Iteration 122, loss = 0.12678027\n",
      "Iteration 123, loss = 0.12503319\n",
      "Iteration 124, loss = 0.12335051\n",
      "Iteration 125, loss = 0.12172986\n",
      "Iteration 126, loss = 0.12017756\n",
      "Iteration 127, loss = 0.11867896\n",
      "Iteration 128, loss = 0.11723978\n",
      "Iteration 129, loss = 0.11585569\n",
      "Iteration 130, loss = 0.11451929\n",
      "Iteration 131, loss = 0.11323619\n",
      "Iteration 132, loss = 0.11199759\n",
      "Iteration 133, loss = 0.11080459\n",
      "Iteration 134, loss = 0.10965595\n",
      "Iteration 135, loss = 0.10854536\n",
      "Iteration 136, loss = 0.10747602\n",
      "Iteration 137, loss = 0.10644249\n",
      "Iteration 138, loss = 0.10544431\n",
      "Iteration 139, loss = 0.10448101\n",
      "Iteration 140, loss = 0.10354848\n",
      "Iteration 141, loss = 0.10264835\n",
      "Iteration 142, loss = 0.10177682\n",
      "Iteration 143, loss = 0.10093347\n",
      "Iteration 144, loss = 0.10011755\n",
      "Iteration 145, loss = 0.09932640\n",
      "Iteration 146, loss = 0.09856089\n",
      "Iteration 147, loss = 0.09782043\n",
      "Iteration 148, loss = 0.09710306\n",
      "Iteration 149, loss = 0.09640887\n",
      "Iteration 150, loss = 0.09573334\n",
      "Iteration 151, loss = 0.09507992\n",
      "Iteration 152, loss = 0.09444402\n",
      "Iteration 153, loss = 0.09382725\n",
      "Iteration 154, loss = 0.09322788\n",
      "Iteration 155, loss = 0.09264665\n",
      "Iteration 156, loss = 0.09208124\n",
      "Iteration 157, loss = 0.09153086\n",
      "Iteration 158, loss = 0.09099598\n",
      "Iteration 159, loss = 0.09047441\n",
      "Iteration 160, loss = 0.08996729\n",
      "Iteration 161, loss = 0.08947266\n",
      "Iteration 162, loss = 0.08899097\n",
      "Iteration 163, loss = 0.08852135\n",
      "Iteration 164, loss = 0.08806320\n",
      "Iteration 165, loss = 0.08761662\n",
      "Iteration 166, loss = 0.08718044\n",
      "Iteration 167, loss = 0.08675507\n",
      "Iteration 168, loss = 0.08633940\n",
      "Iteration 169, loss = 0.08593422\n",
      "Iteration 170, loss = 0.08553782\n",
      "Iteration 171, loss = 0.08515085\n",
      "Iteration 172, loss = 0.08477230\n",
      "Iteration 173, loss = 0.08440223\n",
      "Iteration 174, loss = 0.08404031\n",
      "Iteration 175, loss = 0.08368605\n",
      "Iteration 176, loss = 0.08333958\n",
      "Iteration 177, loss = 0.08300017\n",
      "Iteration 178, loss = 0.08266808\n",
      "Iteration 179, loss = 0.08234260\n",
      "Iteration 180, loss = 0.08202392\n",
      "Iteration 181, loss = 0.08171153\n",
      "Iteration 182, loss = 0.08140541\n",
      "Iteration 183, loss = 0.08110529\n",
      "Iteration 184, loss = 0.08081096\n",
      "Iteration 185, loss = 0.08052235\n",
      "Iteration 186, loss = 0.08023913\n",
      "Iteration 187, loss = 0.07996131\n",
      "Iteration 188, loss = 0.07968857\n",
      "Iteration 189, loss = 0.07942090\n",
      "Iteration 190, loss = 0.07915804\n",
      "Iteration 191, loss = 0.07889995\n",
      "Iteration 192, loss = 0.07864641\n",
      "Iteration 193, loss = 0.07839735\n",
      "Iteration 194, loss = 0.07815262\n",
      "Iteration 195, loss = 0.07791210\n",
      "Iteration 196, loss = 0.07767568\n",
      "Iteration 197, loss = 0.07744324\n",
      "Iteration 198, loss = 0.07721469\n",
      "Iteration 199, loss = 0.07698990\n",
      "Iteration 200, loss = 0.07676881\n",
      "Iteration 201, loss = 0.07655127\n",
      "Iteration 202, loss = 0.07633725\n",
      "Iteration 203, loss = 0.07612660\n",
      "Iteration 204, loss = 0.07591929\n",
      "Iteration 205, loss = 0.07571519\n",
      "Iteration 206, loss = 0.07551426\n",
      "Iteration 207, loss = 0.07531638\n",
      "Iteration 208, loss = 0.07512151\n",
      "Iteration 209, loss = 0.07492956\n",
      "Iteration 210, loss = 0.07474047\n",
      "Iteration 211, loss = 0.07455415\n",
      "Iteration 212, loss = 0.07437056\n",
      "Iteration 213, loss = 0.07418962\n",
      "Iteration 214, loss = 0.07401127\n",
      "Iteration 215, loss = 0.07383546\n",
      "Iteration 216, loss = 0.07366212\n",
      "Iteration 217, loss = 0.07349119\n",
      "Iteration 218, loss = 0.07332263\n",
      "Iteration 219, loss = 0.07315638\n",
      "Iteration 220, loss = 0.07299239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.07283061\n",
      "Iteration 222, loss = 0.07267098\n",
      "Iteration 223, loss = 0.07251347\n",
      "Iteration 224, loss = 0.07235803\n",
      "Iteration 225, loss = 0.07220461\n",
      "Iteration 226, loss = 0.07205323\n",
      "Iteration 227, loss = 0.07190383\n",
      "Iteration 228, loss = 0.07175632\n",
      "Iteration 229, loss = 0.07161067\n",
      "Iteration 230, loss = 0.07146683\n",
      "Iteration 231, loss = 0.07132478\n",
      "Iteration 232, loss = 0.07118446\n",
      "Iteration 233, loss = 0.07104586\n",
      "Iteration 234, loss = 0.07090892\n",
      "Iteration 235, loss = 0.07077363\n",
      "Iteration 236, loss = 0.07063993\n",
      "Iteration 237, loss = 0.07050782\n",
      "Iteration 238, loss = 0.07037723\n",
      "Iteration 239, loss = 0.07024817\n",
      "Iteration 240, loss = 0.07012057\n",
      "Iteration 241, loss = 0.06999444\n",
      "Iteration 242, loss = 0.06986972\n",
      "Iteration 243, loss = 0.06974641\n",
      "Iteration 244, loss = 0.06962446\n",
      "Iteration 245, loss = 0.06950385\n",
      "Iteration 246, loss = 0.06938456\n",
      "Iteration 247, loss = 0.06926657\n",
      "Iteration 248, loss = 0.06914985\n",
      "Iteration 249, loss = 0.06903437\n",
      "Iteration 250, loss = 0.06892011\n",
      "Iteration 251, loss = 0.06880706\n",
      "Iteration 252, loss = 0.06869518\n",
      "Iteration 253, loss = 0.06858446\n",
      "Iteration 254, loss = 0.06847488\n",
      "Iteration 255, loss = 0.06836642\n",
      "Iteration 256, loss = 0.06825905\n",
      "Iteration 257, loss = 0.06815275\n",
      "Iteration 258, loss = 0.06804752\n",
      "Iteration 259, loss = 0.06794332\n",
      "Iteration 260, loss = 0.06784015\n",
      "Iteration 261, loss = 0.06773798\n",
      "Iteration 262, loss = 0.06763680\n",
      "Iteration 263, loss = 0.06753659\n",
      "Iteration 264, loss = 0.06743734\n",
      "Iteration 265, loss = 0.06733902\n",
      "Iteration 266, loss = 0.06724162\n",
      "Iteration 267, loss = 0.06714513\n",
      "Iteration 268, loss = 0.06704953\n",
      "Iteration 269, loss = 0.06695481\n",
      "Iteration 270, loss = 0.06686095\n",
      "Iteration 271, loss = 0.06676794\n",
      "Iteration 272, loss = 0.06667577\n",
      "Iteration 273, loss = 0.06658442\n",
      "Iteration 274, loss = 0.06649387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10901821\n",
      "Iteration 2, loss = 1.53890613\n",
      "Iteration 3, loss = 1.12015391\n",
      "Iteration 4, loss = 1.09331626\n",
      "Iteration 5, loss = 1.16457503\n",
      "Iteration 6, loss = 1.15504274\n",
      "Iteration 7, loss = 1.09001354\n",
      "Iteration 8, loss = 1.01927684\n",
      "Iteration 9, loss = 0.96401157\n",
      "Iteration 10, loss = 0.92251187\n",
      "Iteration 11, loss = 0.88877051\n",
      "Iteration 12, loss = 0.85867188\n",
      "Iteration 13, loss = 0.83063146\n",
      "Iteration 14, loss = 0.80450437\n",
      "Iteration 15, loss = 0.78091468\n",
      "Iteration 16, loss = 0.75941747\n",
      "Iteration 17, loss = 0.73936221\n",
      "Iteration 18, loss = 0.72016929\n",
      "Iteration 19, loss = 0.70276979\n",
      "Iteration 20, loss = 0.68672504\n",
      "Iteration 21, loss = 0.67183290\n",
      "Iteration 22, loss = 0.65707553\n",
      "Iteration 23, loss = 0.64261113\n",
      "Iteration 24, loss = 0.62845870\n",
      "Iteration 25, loss = 0.61472469\n",
      "Iteration 26, loss = 0.60151851\n",
      "Iteration 27, loss = 0.58894735\n",
      "Iteration 28, loss = 0.57697097\n",
      "Iteration 29, loss = 0.56557003\n",
      "Iteration 30, loss = 0.55473167\n",
      "Iteration 31, loss = 0.54445396\n",
      "Iteration 32, loss = 0.53469824\n",
      "Iteration 33, loss = 0.52544775\n",
      "Iteration 34, loss = 0.51666513\n",
      "Iteration 35, loss = 0.50832107\n",
      "Iteration 36, loss = 0.50031076\n",
      "Iteration 37, loss = 0.49266421\n",
      "Iteration 38, loss = 0.48534628\n",
      "Iteration 39, loss = 0.47831575\n",
      "Iteration 40, loss = 0.47160189\n",
      "Iteration 41, loss = 0.46549571\n",
      "Iteration 42, loss = 0.45990291\n",
      "Iteration 43, loss = 0.45468483\n",
      "Iteration 44, loss = 0.44968415\n",
      "Iteration 45, loss = 0.44481761\n",
      "Iteration 46, loss = 0.44003687\n",
      "Iteration 47, loss = 0.43529246\n",
      "Iteration 48, loss = 0.43061912\n",
      "Iteration 49, loss = 0.42601322\n",
      "Iteration 50, loss = 0.42148517\n",
      "Iteration 51, loss = 0.41700529\n",
      "Iteration 52, loss = 0.41258146\n",
      "Iteration 53, loss = 0.40827256\n",
      "Iteration 54, loss = 0.40428309\n",
      "Iteration 55, loss = 0.40031463\n",
      "Iteration 56, loss = 0.39638645\n",
      "Iteration 57, loss = 0.39249284\n",
      "Iteration 58, loss = 0.38863417\n",
      "Iteration 59, loss = 0.38481232\n",
      "Iteration 60, loss = 0.38104827\n",
      "Iteration 61, loss = 0.37735727\n",
      "Iteration 62, loss = 0.37372299\n",
      "Iteration 63, loss = 0.37013693\n",
      "Iteration 64, loss = 0.36658663\n",
      "Iteration 65, loss = 0.36306778\n",
      "Iteration 66, loss = 0.35958341\n",
      "Iteration 67, loss = 0.35613426\n",
      "Iteration 68, loss = 0.35269433\n",
      "Iteration 69, loss = 0.34927841\n",
      "Iteration 70, loss = 0.34590427\n",
      "Iteration 71, loss = 0.34257379\n",
      "Iteration 72, loss = 0.33927328\n",
      "Iteration 73, loss = 0.33600260\n",
      "Iteration 74, loss = 0.33276367\n",
      "Iteration 75, loss = 0.32954800\n",
      "Iteration 76, loss = 0.32636128\n",
      "Iteration 77, loss = 0.32320236\n",
      "Iteration 78, loss = 0.32007144\n",
      "Iteration 79, loss = 0.31696834\n",
      "Iteration 80, loss = 0.31389293\n",
      "Iteration 81, loss = 0.31085754\n",
      "Iteration 82, loss = 0.30785416\n",
      "Iteration 83, loss = 0.30488031\n",
      "Iteration 84, loss = 0.30193571\n",
      "Iteration 85, loss = 0.29902072\n",
      "Iteration 86, loss = 0.29614281\n",
      "Iteration 87, loss = 0.29331933\n",
      "Iteration 88, loss = 0.29052562\n",
      "Iteration 89, loss = 0.28777423\n",
      "Iteration 90, loss = 0.28505748\n",
      "Iteration 91, loss = 0.28237984\n",
      "Iteration 92, loss = 0.27973763\n",
      "Iteration 93, loss = 0.27713130\n",
      "Iteration 94, loss = 0.27456479\n",
      "Iteration 95, loss = 0.27202577\n",
      "Iteration 96, loss = 0.26951410\n",
      "Iteration 97, loss = 0.26702994\n",
      "Iteration 98, loss = 0.26457584\n",
      "Iteration 99, loss = 0.26215340\n",
      "Iteration 100, loss = 0.25976227\n",
      "Iteration 101, loss = 0.25740411\n",
      "Iteration 102, loss = 0.25507938\n",
      "Iteration 103, loss = 0.25278521\n",
      "Iteration 104, loss = 0.25052147\n",
      "Iteration 105, loss = 0.24829531\n",
      "Iteration 106, loss = 0.24610526\n",
      "Iteration 107, loss = 0.24394436\n",
      "Iteration 108, loss = 0.24181328\n",
      "Iteration 109, loss = 0.23971235\n",
      "Iteration 110, loss = 0.23764073\n",
      "Iteration 111, loss = 0.23559926\n",
      "Iteration 112, loss = 0.23358709\n",
      "Iteration 113, loss = 0.23160326\n",
      "Iteration 114, loss = 0.22964847\n",
      "Iteration 115, loss = 0.22772103\n",
      "Iteration 116, loss = 0.22582155\n",
      "Iteration 117, loss = 0.22395013\n",
      "Iteration 118, loss = 0.22210636\n",
      "Iteration 119, loss = 0.22029242\n",
      "Iteration 120, loss = 0.21850682\n",
      "Iteration 121, loss = 0.21674778\n",
      "Iteration 122, loss = 0.21501558\n",
      "Iteration 123, loss = 0.21331019\n",
      "Iteration 124, loss = 0.21163173\n",
      "Iteration 125, loss = 0.20997979\n",
      "Iteration 126, loss = 0.20835366\n",
      "Iteration 127, loss = 0.20675312\n",
      "Iteration 128, loss = 0.20517781\n",
      "Iteration 129, loss = 0.20362726\n",
      "Iteration 130, loss = 0.20210127\n",
      "Iteration 131, loss = 0.20059952\n",
      "Iteration 132, loss = 0.19912196\n",
      "Iteration 133, loss = 0.19766803\n",
      "Iteration 134, loss = 0.19623734\n",
      "Iteration 135, loss = 0.19482964\n",
      "Iteration 136, loss = 0.19344449\n",
      "Iteration 137, loss = 0.19208164\n",
      "Iteration 138, loss = 0.19074081\n",
      "Iteration 139, loss = 0.18942143\n",
      "Iteration 140, loss = 0.18812327\n",
      "Iteration 141, loss = 0.18684606\n",
      "Iteration 142, loss = 0.18558930\n",
      "Iteration 143, loss = 0.18435269\n",
      "Iteration 144, loss = 0.18313608\n",
      "Iteration 145, loss = 0.18193881\n",
      "Iteration 146, loss = 0.18076081\n",
      "Iteration 147, loss = 0.17960156\n",
      "Iteration 148, loss = 0.17846104\n",
      "Iteration 149, loss = 0.17733849\n",
      "Iteration 150, loss = 0.17623403\n",
      "Iteration 151, loss = 0.17514699\n",
      "Iteration 152, loss = 0.17407722\n",
      "Iteration 153, loss = 0.17302446\n",
      "Iteration 154, loss = 0.17198831\n",
      "Iteration 155, loss = 0.17096851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, loss = 0.16996475\n",
      "Iteration 157, loss = 0.16897724\n",
      "Iteration 158, loss = 0.16800588\n",
      "Iteration 159, loss = 0.16704979\n",
      "Iteration 160, loss = 0.16610865\n",
      "Iteration 161, loss = 0.16518216\n",
      "Iteration 162, loss = 0.16427007\n",
      "Iteration 163, loss = 0.16337211\n",
      "Iteration 164, loss = 0.16248804\n",
      "Iteration 165, loss = 0.16161758\n",
      "Iteration 166, loss = 0.16076049\n",
      "Iteration 167, loss = 0.15991652\n",
      "Iteration 168, loss = 0.15908542\n",
      "Iteration 169, loss = 0.15826694\n",
      "Iteration 170, loss = 0.15746085\n",
      "Iteration 171, loss = 0.15666691\n",
      "Iteration 172, loss = 0.15588489\n",
      "Iteration 173, loss = 0.15511456\n",
      "Iteration 174, loss = 0.15435570\n",
      "Iteration 175, loss = 0.15360809\n",
      "Iteration 176, loss = 0.15287151\n",
      "Iteration 177, loss = 0.15214576\n",
      "Iteration 178, loss = 0.15143062\n",
      "Iteration 179, loss = 0.15072589\n",
      "Iteration 180, loss = 0.15003139\n",
      "Iteration 181, loss = 0.14934690\n",
      "Iteration 182, loss = 0.14867242\n",
      "Iteration 183, loss = 0.14800761\n",
      "Iteration 184, loss = 0.14735227\n",
      "Iteration 185, loss = 0.14670627\n",
      "Iteration 186, loss = 0.14606934\n",
      "Iteration 187, loss = 0.14544174\n",
      "Iteration 188, loss = 0.14482316\n",
      "Iteration 189, loss = 0.14421322\n",
      "Iteration 190, loss = 0.14361179\n",
      "Iteration 191, loss = 0.14301868\n",
      "Iteration 192, loss = 0.14243374\n",
      "Iteration 193, loss = 0.14185689\n",
      "Iteration 194, loss = 0.14128777\n",
      "Iteration 195, loss = 0.14072648\n",
      "Iteration 196, loss = 0.14017276\n",
      "Iteration 197, loss = 0.13962652\n",
      "Iteration 198, loss = 0.13908763\n",
      "Iteration 199, loss = 0.13855593\n",
      "Iteration 200, loss = 0.13803130\n",
      "Iteration 201, loss = 0.13751361\n",
      "Iteration 202, loss = 0.13700273\n",
      "Iteration 203, loss = 0.13649854\n",
      "Iteration 204, loss = 0.13600092\n",
      "Iteration 205, loss = 0.13550975\n",
      "Iteration 206, loss = 0.13502492\n",
      "Iteration 207, loss = 0.13454630\n",
      "Iteration 208, loss = 0.13407382\n",
      "Iteration 209, loss = 0.13360733\n",
      "Iteration 210, loss = 0.13314674\n",
      "Iteration 211, loss = 0.13269195\n",
      "Iteration 212, loss = 0.13224285\n",
      "Iteration 213, loss = 0.13179940\n",
      "Iteration 214, loss = 0.13136153\n",
      "Iteration 215, loss = 0.13092935\n",
      "Iteration 216, loss = 0.13050250\n",
      "Iteration 217, loss = 0.13008088\n",
      "Iteration 218, loss = 0.12966442\n",
      "Iteration 219, loss = 0.12925347\n",
      "Iteration 220, loss = 0.12884742\n",
      "Iteration 221, loss = 0.12844624\n",
      "Iteration 222, loss = 0.12804987\n",
      "Iteration 223, loss = 0.12765853\n",
      "Iteration 224, loss = 0.12727145\n",
      "Iteration 225, loss = 0.12688876\n",
      "Iteration 226, loss = 0.12651041\n",
      "Iteration 227, loss = 0.12613671\n",
      "Iteration 228, loss = 0.12576764\n",
      "Iteration 229, loss = 0.12540285\n",
      "Iteration 230, loss = 0.12504224\n",
      "Iteration 231, loss = 0.12468588\n",
      "Iteration 232, loss = 0.12433354\n",
      "Iteration 233, loss = 0.12398523\n",
      "Iteration 234, loss = 0.12364087\n",
      "Iteration 235, loss = 0.12330045\n",
      "Iteration 236, loss = 0.12296378\n",
      "Iteration 237, loss = 0.12263087\n",
      "Iteration 238, loss = 0.12230170\n",
      "Iteration 239, loss = 0.12197615\n",
      "Iteration 240, loss = 0.12165416\n",
      "Iteration 241, loss = 0.12133569\n",
      "Iteration 242, loss = 0.12102071\n",
      "Iteration 243, loss = 0.12070932\n",
      "Iteration 244, loss = 0.12040127\n",
      "Iteration 245, loss = 0.12009653\n",
      "Iteration 246, loss = 0.11979506\n",
      "Iteration 247, loss = 0.11949679\n",
      "Iteration 248, loss = 0.11920169\n",
      "Iteration 249, loss = 0.11890969\n",
      "Iteration 250, loss = 0.11862076\n",
      "Iteration 251, loss = 0.11833485\n",
      "Iteration 252, loss = 0.11805191\n",
      "Iteration 253, loss = 0.11777189\n",
      "Iteration 254, loss = 0.11749477\n",
      "Iteration 255, loss = 0.11722048\n",
      "Iteration 256, loss = 0.11694899\n",
      "Iteration 257, loss = 0.11668026\n",
      "Iteration 258, loss = 0.11641424\n",
      "Iteration 259, loss = 0.11615089\n",
      "Iteration 260, loss = 0.11589018\n",
      "Iteration 261, loss = 0.11563206\n",
      "Iteration 262, loss = 0.11537650\n",
      "Iteration 263, loss = 0.11512346\n",
      "Iteration 264, loss = 0.11487290\n",
      "Iteration 265, loss = 0.11462478\n",
      "Iteration 266, loss = 0.11437908\n",
      "Iteration 267, loss = 0.11413575\n",
      "Iteration 268, loss = 0.11389476\n",
      "Iteration 269, loss = 0.11365607\n",
      "Iteration 270, loss = 0.11341966\n",
      "Iteration 271, loss = 0.11318549\n",
      "Iteration 272, loss = 0.11295353\n",
      "Iteration 273, loss = 0.11272375\n",
      "Iteration 274, loss = 0.11249611\n",
      "Iteration 275, loss = 0.11227059\n",
      "Iteration 276, loss = 0.11204716\n",
      "Iteration 277, loss = 0.11182579\n",
      "Iteration 278, loss = 0.11160645\n",
      "Iteration 279, loss = 0.11138912\n",
      "Iteration 280, loss = 0.11117376\n",
      "Iteration 281, loss = 0.11096035\n",
      "Iteration 282, loss = 0.11074886\n",
      "Iteration 283, loss = 0.11053926\n",
      "Iteration 284, loss = 0.11033154\n",
      "Iteration 285, loss = 0.11012567\n",
      "Iteration 286, loss = 0.10992162\n",
      "Iteration 287, loss = 0.10971939\n",
      "Iteration 288, loss = 0.10951895\n",
      "Iteration 289, loss = 0.10932026\n",
      "Iteration 290, loss = 0.10912330\n",
      "Iteration 291, loss = 0.10892805\n",
      "Iteration 292, loss = 0.10873449\n",
      "Iteration 293, loss = 0.10854259\n",
      "Iteration 294, loss = 0.10835233\n",
      "Iteration 295, loss = 0.10816369\n",
      "Iteration 296, loss = 0.10797665\n",
      "Iteration 297, loss = 0.10779119\n",
      "Iteration 298, loss = 0.10760730\n",
      "Iteration 299, loss = 0.10742494\n",
      "Iteration 300, loss = 0.10724411\n",
      "Iteration 301, loss = 0.10706483\n",
      "Iteration 302, loss = 0.10688708\n",
      "Iteration 303, loss = 0.10671079\n",
      "Iteration 304, loss = 0.10653596\n",
      "Iteration 305, loss = 0.10636258\n",
      "Iteration 306, loss = 0.10619062\n",
      "Iteration 307, loss = 0.10602006\n",
      "Iteration 308, loss = 0.10585088\n",
      "Iteration 309, loss = 0.10568308\n",
      "Iteration 310, loss = 0.10551664\n",
      "Iteration 311, loss = 0.10535153\n",
      "Iteration 312, loss = 0.10518773\n",
      "Iteration 313, loss = 0.10502523\n",
      "Iteration 314, loss = 0.10486401\n",
      "Iteration 315, loss = 0.10470407\n",
      "Iteration 316, loss = 0.10454537\n",
      "Iteration 317, loss = 0.10438792\n",
      "Iteration 318, loss = 0.10423169\n",
      "Iteration 319, loss = 0.10407667\n",
      "Iteration 320, loss = 0.10392285\n",
      "Iteration 321, loss = 0.10377022\n",
      "Iteration 322, loss = 0.10361875\n",
      "Iteration 323, loss = 0.10346844\n",
      "Iteration 324, loss = 0.10331933\n",
      "Iteration 325, loss = 0.10317138\n",
      "Iteration 326, loss = 0.10302455\n",
      "Iteration 327, loss = 0.10287882\n",
      "Iteration 328, loss = 0.10273419\n",
      "Iteration 329, loss = 0.10259064\n",
      "Iteration 330, loss = 0.10244815\n",
      "Iteration 331, loss = 0.10230673\n",
      "Iteration 332, loss = 0.10216634\n",
      "Iteration 333, loss = 0.10202700\n",
      "Iteration 334, loss = 0.10188868\n",
      "Iteration 335, loss = 0.10175137\n",
      "Iteration 336, loss = 0.10161506\n",
      "Iteration 337, loss = 0.10147973\n",
      "Iteration 338, loss = 0.10134539\n",
      "Iteration 339, loss = 0.10121202\n",
      "Iteration 340, loss = 0.10107960\n",
      "Iteration 341, loss = 0.10094812\n",
      "Iteration 342, loss = 0.10081759\n",
      "Iteration 343, loss = 0.10068798\n",
      "Iteration 344, loss = 0.10055929\n",
      "Iteration 345, loss = 0.10043150\n",
      "Iteration 346, loss = 0.10030461\n",
      "Iteration 347, loss = 0.10017861\n",
      "Iteration 348, loss = 0.10005349\n",
      "Iteration 349, loss = 0.09992924\n",
      "Iteration 350, loss = 0.09980586\n",
      "Iteration 351, loss = 0.09968332\n",
      "Iteration 352, loss = 0.09956162\n",
      "Iteration 353, loss = 0.09944076\n",
      "Iteration 354, loss = 0.09932073\n",
      "Iteration 355, loss = 0.09920151\n",
      "Iteration 356, loss = 0.09908309\n",
      "Iteration 357, loss = 0.09896549\n",
      "Iteration 358, loss = 0.09884866\n",
      "Iteration 359, loss = 0.09873262\n",
      "Iteration 360, loss = 0.09861736\n",
      "Iteration 361, loss = 0.09850287\n",
      "Iteration 362, loss = 0.09838913\n",
      "Iteration 363, loss = 0.09827615\n",
      "Iteration 364, loss = 0.09816391\n",
      "Iteration 365, loss = 0.09805241\n",
      "Iteration 366, loss = 0.09794163\n",
      "Iteration 367, loss = 0.09783158\n",
      "Iteration 368, loss = 0.09772225\n",
      "Iteration 369, loss = 0.09761362\n",
      "Iteration 370, loss = 0.09750569\n",
      "Iteration 371, loss = 0.09739846\n",
      "Iteration 372, loss = 0.09729192\n",
      "Iteration 373, loss = 0.09718605\n",
      "Iteration 374, loss = 0.09708086\n",
      "Iteration 375, loss = 0.09697633\n",
      "Iteration 376, loss = 0.09687247\n",
      "Iteration 377, loss = 0.09676926\n",
      "Iteration 378, loss = 0.09666670\n",
      "Iteration 379, loss = 0.09656478\n",
      "Iteration 380, loss = 0.09646350\n",
      "Iteration 381, loss = 0.09636284\n",
      "Iteration 382, loss = 0.09626281\n",
      "Iteration 383, loss = 0.09616340\n",
      "Iteration 384, loss = 0.09606460\n",
      "Iteration 385, loss = 0.09596640\n",
      "Iteration 386, loss = 0.09586881\n",
      "Iteration 387, loss = 0.09577181\n",
      "Iteration 388, loss = 0.09567540\n",
      "Iteration 389, loss = 0.09557957\n",
      "Iteration 390, loss = 0.09548433\n",
      "Iteration 391, loss = 0.09538965\n",
      "Iteration 392, loss = 0.09529555\n",
      "Iteration 393, loss = 0.09520200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12500615\n",
      "Iteration 2, loss = 1.54621109\n",
      "Iteration 3, loss = 1.12064780\n",
      "Iteration 4, loss = 1.10154337\n",
      "Iteration 5, loss = 1.17447579\n",
      "Iteration 6, loss = 1.16096062\n",
      "Iteration 7, loss = 1.09259307\n",
      "Iteration 8, loss = 1.02155656\n",
      "Iteration 9, loss = 0.96793583\n",
      "Iteration 10, loss = 0.92839432\n",
      "Iteration 11, loss = 0.89577310\n",
      "Iteration 12, loss = 0.86590425\n",
      "Iteration 13, loss = 0.83760020\n",
      "Iteration 14, loss = 0.81135161\n",
      "Iteration 15, loss = 0.78776970\n",
      "Iteration 16, loss = 0.76598856\n",
      "Iteration 17, loss = 0.74480146\n",
      "Iteration 18, loss = 0.72460853\n",
      "Iteration 19, loss = 0.70696611\n",
      "Iteration 20, loss = 0.69152911\n",
      "Iteration 21, loss = 0.67656073\n",
      "Iteration 22, loss = 0.66203564\n",
      "Iteration 23, loss = 0.64764469\n",
      "Iteration 24, loss = 0.63353737\n",
      "Iteration 25, loss = 0.61960227\n",
      "Iteration 26, loss = 0.60628426\n",
      "Iteration 27, loss = 0.59361234\n",
      "Iteration 28, loss = 0.58156445\n",
      "Iteration 29, loss = 0.57004365\n",
      "Iteration 30, loss = 0.55902390\n",
      "Iteration 31, loss = 0.54851013\n",
      "Iteration 32, loss = 0.53846252\n",
      "Iteration 33, loss = 0.52891157\n",
      "Iteration 34, loss = 0.51984482\n",
      "Iteration 35, loss = 0.51116297\n",
      "Iteration 36, loss = 0.50285031\n",
      "Iteration 37, loss = 0.49488197\n",
      "Iteration 38, loss = 0.48713677\n",
      "Iteration 39, loss = 0.47970907\n",
      "Iteration 40, loss = 0.47267688\n",
      "Iteration 41, loss = 0.46621481\n",
      "Iteration 42, loss = 0.46038256\n",
      "Iteration 43, loss = 0.45497723\n",
      "Iteration 44, loss = 0.44980242\n",
      "Iteration 45, loss = 0.44470843\n",
      "Iteration 46, loss = 0.43963556\n",
      "Iteration 47, loss = 0.43453691\n",
      "Iteration 48, loss = 0.42945664\n",
      "Iteration 49, loss = 0.42455489\n",
      "Iteration 50, loss = 0.41983303\n",
      "Iteration 51, loss = 0.41523655\n",
      "Iteration 52, loss = 0.41076424\n",
      "Iteration 53, loss = 0.40635979\n",
      "Iteration 54, loss = 0.40202526\n",
      "Iteration 55, loss = 0.39778098\n",
      "Iteration 56, loss = 0.39359781\n",
      "Iteration 57, loss = 0.38948545\n",
      "Iteration 58, loss = 0.38540989\n",
      "Iteration 59, loss = 0.38137283\n",
      "Iteration 60, loss = 0.37737362\n",
      "Iteration 61, loss = 0.37342102\n",
      "Iteration 62, loss = 0.36952079\n",
      "Iteration 63, loss = 0.36567385\n",
      "Iteration 64, loss = 0.36186922\n",
      "Iteration 65, loss = 0.35810422\n",
      "Iteration 66, loss = 0.35437531\n",
      "Iteration 67, loss = 0.35068056\n",
      "Iteration 68, loss = 0.34702386\n",
      "Iteration 69, loss = 0.34339526\n",
      "Iteration 70, loss = 0.33980057\n",
      "Iteration 71, loss = 0.33624138\n",
      "Iteration 72, loss = 0.33271630\n",
      "Iteration 73, loss = 0.32922476\n",
      "Iteration 74, loss = 0.32576856\n",
      "Iteration 75, loss = 0.32234280\n",
      "Iteration 76, loss = 0.31894247\n",
      "Iteration 77, loss = 0.31556931\n",
      "Iteration 78, loss = 0.31223044\n",
      "Iteration 79, loss = 0.30892372\n",
      "Iteration 80, loss = 0.30564987\n",
      "Iteration 81, loss = 0.30241030\n",
      "Iteration 82, loss = 0.29920705\n",
      "Iteration 83, loss = 0.29603783\n",
      "Iteration 84, loss = 0.29290295\n",
      "Iteration 85, loss = 0.28980392\n",
      "Iteration 86, loss = 0.28674805\n",
      "Iteration 87, loss = 0.28372742\n",
      "Iteration 88, loss = 0.28074752\n",
      "Iteration 89, loss = 0.27781004\n",
      "Iteration 90, loss = 0.27490864\n",
      "Iteration 91, loss = 0.27204995\n",
      "Iteration 92, loss = 0.26924099\n",
      "Iteration 93, loss = 0.26647524\n",
      "Iteration 94, loss = 0.26375040\n",
      "Iteration 95, loss = 0.26106506\n",
      "Iteration 96, loss = 0.25841525\n",
      "Iteration 97, loss = 0.25579879\n",
      "Iteration 98, loss = 0.25321505\n",
      "Iteration 99, loss = 0.25066478\n",
      "Iteration 100, loss = 0.24814893\n",
      "Iteration 101, loss = 0.24566790\n",
      "Iteration 102, loss = 0.24322354\n",
      "Iteration 103, loss = 0.24082091\n",
      "Iteration 104, loss = 0.23845458\n",
      "Iteration 105, loss = 0.23612282\n",
      "Iteration 106, loss = 0.23382498\n",
      "Iteration 107, loss = 0.23156531\n",
      "Iteration 108, loss = 0.22933934\n",
      "Iteration 109, loss = 0.22714757\n",
      "Iteration 110, loss = 0.22499004\n",
      "Iteration 111, loss = 0.22286631\n",
      "Iteration 112, loss = 0.22077586\n",
      "Iteration 113, loss = 0.21871839\n",
      "Iteration 114, loss = 0.21669381\n",
      "Iteration 115, loss = 0.21470394\n",
      "Iteration 116, loss = 0.21274693\n",
      "Iteration 117, loss = 0.21082419\n",
      "Iteration 118, loss = 0.20893117\n",
      "Iteration 119, loss = 0.20706899\n",
      "Iteration 120, loss = 0.20523682\n",
      "Iteration 121, loss = 0.20343728\n",
      "Iteration 122, loss = 0.20166962\n",
      "Iteration 123, loss = 0.19993009\n",
      "Iteration 124, loss = 0.19821946\n",
      "Iteration 125, loss = 0.19653785\n",
      "Iteration 126, loss = 0.19488520\n",
      "Iteration 127, loss = 0.19326102\n",
      "Iteration 128, loss = 0.19166440\n",
      "Iteration 129, loss = 0.19009535\n",
      "Iteration 130, loss = 0.18855281\n",
      "Iteration 131, loss = 0.18703723\n",
      "Iteration 132, loss = 0.18554822\n",
      "Iteration 133, loss = 0.18408542\n",
      "Iteration 134, loss = 0.18264833\n",
      "Iteration 135, loss = 0.18123661\n",
      "Iteration 136, loss = 0.17984978\n",
      "Iteration 137, loss = 0.17848747\n",
      "Iteration 138, loss = 0.17714923\n",
      "Iteration 139, loss = 0.17583459\n",
      "Iteration 140, loss = 0.17454314\n",
      "Iteration 141, loss = 0.17327444\n",
      "Iteration 142, loss = 0.17202810\n",
      "Iteration 143, loss = 0.17080366\n",
      "Iteration 144, loss = 0.16960073\n",
      "Iteration 145, loss = 0.16841913\n",
      "Iteration 146, loss = 0.16725847\n",
      "Iteration 147, loss = 0.16611810\n",
      "Iteration 148, loss = 0.16499761\n",
      "Iteration 149, loss = 0.16389661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150, loss = 0.16281475\n",
      "Iteration 151, loss = 0.16175190\n",
      "Iteration 152, loss = 0.16070715\n",
      "Iteration 153, loss = 0.15968062\n",
      "Iteration 154, loss = 0.15867157\n",
      "Iteration 155, loss = 0.15767986\n",
      "Iteration 156, loss = 0.15670509\n",
      "Iteration 157, loss = 0.15574694\n",
      "Iteration 158, loss = 0.15480500\n",
      "Iteration 159, loss = 0.15387895\n",
      "Iteration 160, loss = 0.15296846\n",
      "Iteration 161, loss = 0.15207323\n",
      "Iteration 162, loss = 0.15119293\n",
      "Iteration 163, loss = 0.15032725\n",
      "Iteration 164, loss = 0.14947589\n",
      "Iteration 165, loss = 0.14863854\n",
      "Iteration 166, loss = 0.14781491\n",
      "Iteration 167, loss = 0.14700471\n",
      "Iteration 168, loss = 0.14620767\n",
      "Iteration 169, loss = 0.14542351\n",
      "Iteration 170, loss = 0.14465195\n",
      "Iteration 171, loss = 0.14389275\n",
      "Iteration 172, loss = 0.14314562\n",
      "Iteration 173, loss = 0.14241066\n",
      "Iteration 174, loss = 0.14168725\n",
      "Iteration 175, loss = 0.14097520\n",
      "Iteration 176, loss = 0.14027469\n",
      "Iteration 177, loss = 0.13958516\n",
      "Iteration 178, loss = 0.13890629\n",
      "Iteration 179, loss = 0.13823789\n",
      "Iteration 180, loss = 0.13757975\n",
      "Iteration 181, loss = 0.13693168\n",
      "Iteration 182, loss = 0.13629348\n",
      "Iteration 183, loss = 0.13566504\n",
      "Iteration 184, loss = 0.13504597\n",
      "Iteration 185, loss = 0.13443622\n",
      "Iteration 186, loss = 0.13383559\n",
      "Iteration 187, loss = 0.13324390\n",
      "Iteration 188, loss = 0.13266097\n",
      "Iteration 189, loss = 0.13208661\n",
      "Iteration 190, loss = 0.13152066\n",
      "Iteration 191, loss = 0.13096296\n",
      "Iteration 192, loss = 0.13041340\n",
      "Iteration 193, loss = 0.12987169\n",
      "Iteration 194, loss = 0.12933779\n",
      "Iteration 195, loss = 0.12881146\n",
      "Iteration 196, loss = 0.12829259\n",
      "Iteration 197, loss = 0.12778107\n",
      "Iteration 198, loss = 0.12727667\n",
      "Iteration 199, loss = 0.12677932\n",
      "Iteration 200, loss = 0.12628885\n",
      "Iteration 201, loss = 0.12580514\n",
      "Iteration 202, loss = 0.12532805\n",
      "Iteration 203, loss = 0.12485746\n",
      "Iteration 204, loss = 0.12439325\n",
      "Iteration 205, loss = 0.12393530\n",
      "Iteration 206, loss = 0.12348348\n",
      "Iteration 207, loss = 0.12303769\n",
      "Iteration 208, loss = 0.12259780\n",
      "Iteration 209, loss = 0.12216372\n",
      "Iteration 210, loss = 0.12173532\n",
      "Iteration 211, loss = 0.12131252\n",
      "Iteration 212, loss = 0.12089519\n",
      "Iteration 213, loss = 0.12048326\n",
      "Iteration 214, loss = 0.12007660\n",
      "Iteration 215, loss = 0.11967513\n",
      "Iteration 216, loss = 0.11927876\n",
      "Iteration 217, loss = 0.11888739\n",
      "Iteration 218, loss = 0.11850094\n",
      "Iteration 219, loss = 0.11811932\n",
      "Iteration 220, loss = 0.11774245\n",
      "Iteration 221, loss = 0.11737023\n",
      "Iteration 222, loss = 0.11700259\n",
      "Iteration 223, loss = 0.11663945\n",
      "Iteration 224, loss = 0.11628073\n",
      "Iteration 225, loss = 0.11592635\n",
      "Iteration 226, loss = 0.11557625\n",
      "Iteration 227, loss = 0.11523034\n",
      "Iteration 228, loss = 0.11488856\n",
      "Iteration 229, loss = 0.11455083\n",
      "Iteration 230, loss = 0.11421709\n",
      "Iteration 231, loss = 0.11388727\n",
      "Iteration 232, loss = 0.11356130\n",
      "Iteration 233, loss = 0.11323913\n",
      "Iteration 234, loss = 0.11292069\n",
      "Iteration 235, loss = 0.11260591\n",
      "Iteration 236, loss = 0.11229474\n",
      "Iteration 237, loss = 0.11198711\n",
      "Iteration 238, loss = 0.11168298\n",
      "Iteration 239, loss = 0.11138228\n",
      "Iteration 240, loss = 0.11108496\n",
      "Iteration 241, loss = 0.11079096\n",
      "Iteration 242, loss = 0.11050023\n",
      "Iteration 243, loss = 0.11021273\n",
      "Iteration 244, loss = 0.10992879\n",
      "Iteration 245, loss = 0.10964801\n",
      "Iteration 246, loss = 0.10937030\n",
      "Iteration 247, loss = 0.10909562\n",
      "Iteration 248, loss = 0.10882391\n",
      "Iteration 249, loss = 0.10855513\n",
      "Iteration 250, loss = 0.10828923\n",
      "Iteration 251, loss = 0.10802618\n",
      "Iteration 252, loss = 0.10776592\n",
      "Iteration 253, loss = 0.10750842\n",
      "Iteration 254, loss = 0.10725364\n",
      "Iteration 255, loss = 0.10700152\n",
      "Iteration 256, loss = 0.10675204\n",
      "Iteration 257, loss = 0.10650515\n",
      "Iteration 258, loss = 0.10626081\n",
      "Iteration 259, loss = 0.10601899\n",
      "Iteration 260, loss = 0.10577964\n",
      "Iteration 261, loss = 0.10554274\n",
      "Iteration 262, loss = 0.10530824\n",
      "Iteration 263, loss = 0.10507611\n",
      "Iteration 264, loss = 0.10484631\n",
      "Iteration 265, loss = 0.10461881\n",
      "Iteration 266, loss = 0.10439359\n",
      "Iteration 267, loss = 0.10417059\n",
      "Iteration 268, loss = 0.10394981\n",
      "Iteration 269, loss = 0.10373130\n",
      "Iteration 270, loss = 0.10351494\n",
      "Iteration 271, loss = 0.10330068\n",
      "Iteration 272, loss = 0.10308851\n",
      "Iteration 273, loss = 0.10287838\n",
      "Iteration 274, loss = 0.10267027\n",
      "Iteration 275, loss = 0.10246416\n",
      "Iteration 276, loss = 0.10226001\n",
      "Iteration 277, loss = 0.10205780\n",
      "Iteration 278, loss = 0.10185750\n",
      "Iteration 279, loss = 0.10165908\n",
      "Iteration 280, loss = 0.10146252\n",
      "Iteration 281, loss = 0.10126779\n",
      "Iteration 282, loss = 0.10107488\n",
      "Iteration 283, loss = 0.10088374\n",
      "Iteration 284, loss = 0.10069436\n",
      "Iteration 285, loss = 0.10050672\n",
      "Iteration 286, loss = 0.10032079\n",
      "Iteration 287, loss = 0.10013655\n",
      "Iteration 288, loss = 0.09995398\n",
      "Iteration 289, loss = 0.09977305\n",
      "Iteration 290, loss = 0.09959374\n",
      "Iteration 291, loss = 0.09941603\n",
      "Iteration 292, loss = 0.09923991\n",
      "Iteration 293, loss = 0.09906534\n",
      "Iteration 294, loss = 0.09889232\n",
      "Iteration 295, loss = 0.09872081\n",
      "Iteration 296, loss = 0.09855080\n",
      "Iteration 297, loss = 0.09838227\n",
      "Iteration 298, loss = 0.09821521\n",
      "Iteration 299, loss = 0.09804958\n",
      "Iteration 300, loss = 0.09788539\n",
      "Iteration 301, loss = 0.09772259\n",
      "Iteration 302, loss = 0.09756119\n",
      "Iteration 303, loss = 0.09740116\n",
      "Iteration 304, loss = 0.09724248\n",
      "Iteration 305, loss = 0.09708514\n",
      "Iteration 306, loss = 0.09692912\n",
      "Iteration 307, loss = 0.09677441\n",
      "Iteration 308, loss = 0.09662098\n",
      "Iteration 309, loss = 0.09646919\n",
      "Iteration 310, loss = 0.09631890\n",
      "Iteration 311, loss = 0.09616988\n",
      "Iteration 312, loss = 0.09602212\n",
      "Iteration 313, loss = 0.09587558\n",
      "Iteration 314, loss = 0.09573026\n",
      "Iteration 315, loss = 0.09558614\n",
      "Iteration 316, loss = 0.09544320\n",
      "Iteration 317, loss = 0.09530143\n",
      "Iteration 318, loss = 0.09516093\n",
      "Iteration 319, loss = 0.09502157\n",
      "Iteration 320, loss = 0.09488332\n",
      "Iteration 321, loss = 0.09474617\n",
      "Iteration 322, loss = 0.09461010\n",
      "Iteration 323, loss = 0.09447512\n",
      "Iteration 324, loss = 0.09434119\n",
      "Iteration 325, loss = 0.09420831\n",
      "Iteration 326, loss = 0.09407647\n",
      "Iteration 327, loss = 0.09394565\n",
      "Iteration 328, loss = 0.09381584\n",
      "Iteration 329, loss = 0.09368703\n",
      "Iteration 330, loss = 0.09355921\n",
      "Iteration 331, loss = 0.09343237\n",
      "Iteration 332, loss = 0.09330650\n",
      "Iteration 333, loss = 0.09318158\n",
      "Iteration 334, loss = 0.09305760\n",
      "Iteration 335, loss = 0.09293455\n",
      "Iteration 336, loss = 0.09281243\n",
      "Iteration 337, loss = 0.09269122\n",
      "Iteration 338, loss = 0.09257091\n",
      "Iteration 339, loss = 0.09245150\n",
      "Iteration 340, loss = 0.09233296\n",
      "Iteration 341, loss = 0.09221530\n",
      "Iteration 342, loss = 0.09209850\n",
      "Iteration 343, loss = 0.09198255\n",
      "Iteration 344, loss = 0.09186744\n",
      "Iteration 345, loss = 0.09175317\n",
      "Iteration 346, loss = 0.09163972\n",
      "Iteration 347, loss = 0.09152709\n",
      "Iteration 348, loss = 0.09141527\n",
      "Iteration 349, loss = 0.09130424\n",
      "Iteration 350, loss = 0.09119401\n",
      "Iteration 351, loss = 0.09108455\n",
      "Iteration 352, loss = 0.09097587\n",
      "Iteration 353, loss = 0.09086795\n",
      "Iteration 354, loss = 0.09076079\n",
      "Iteration 355, loss = 0.09065440\n",
      "Iteration 356, loss = 0.09054875\n",
      "Iteration 357, loss = 0.09044384\n",
      "Iteration 358, loss = 0.09033966\n",
      "Iteration 359, loss = 0.09023622\n",
      "Iteration 360, loss = 0.09013355\n",
      "Iteration 361, loss = 0.09003166\n",
      "Iteration 362, loss = 0.08993048\n",
      "Iteration 363, loss = 0.08983000\n",
      "Iteration 364, loss = 0.08973020\n",
      "Iteration 365, loss = 0.08963107\n",
      "Iteration 366, loss = 0.08953261\n",
      "Iteration 367, loss = 0.08943482\n",
      "Iteration 368, loss = 0.08933769\n",
      "Iteration 369, loss = 0.08924120\n",
      "Iteration 370, loss = 0.08914536\n",
      "Iteration 371, loss = 0.08905016\n",
      "Iteration 372, loss = 0.08895558\n",
      "Iteration 373, loss = 0.08886163\n",
      "Iteration 374, loss = 0.08876829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10484323\n",
      "Iteration 2, loss = 1.54254810\n",
      "Iteration 3, loss = 1.12839663\n",
      "Iteration 4, loss = 1.09784149\n",
      "Iteration 5, loss = 1.16838435\n",
      "Iteration 6, loss = 1.15883981\n",
      "Iteration 7, loss = 1.09298905\n",
      "Iteration 8, loss = 1.02099510\n",
      "Iteration 9, loss = 0.96500667\n",
      "Iteration 10, loss = 0.92332725\n",
      "Iteration 11, loss = 0.88945582\n",
      "Iteration 12, loss = 0.85902629\n",
      "Iteration 13, loss = 0.83026872\n",
      "Iteration 14, loss = 0.80316829\n",
      "Iteration 15, loss = 0.77850814\n",
      "Iteration 16, loss = 0.75612581\n",
      "Iteration 17, loss = 0.73532189\n",
      "Iteration 18, loss = 0.71619388\n",
      "Iteration 19, loss = 0.69928394\n",
      "Iteration 20, loss = 0.68356304\n",
      "Iteration 21, loss = 0.66834313\n",
      "Iteration 22, loss = 0.65324755\n",
      "Iteration 23, loss = 0.63846692\n",
      "Iteration 24, loss = 0.62408619\n",
      "Iteration 25, loss = 0.61013067\n",
      "Iteration 26, loss = 0.59666328\n",
      "Iteration 27, loss = 0.58378107\n",
      "Iteration 28, loss = 0.57144637\n",
      "Iteration 29, loss = 0.55964447\n",
      "Iteration 30, loss = 0.54830246\n",
      "Iteration 31, loss = 0.53748726\n",
      "Iteration 32, loss = 0.52717599\n",
      "Iteration 33, loss = 0.51729549\n",
      "Iteration 34, loss = 0.50783845\n",
      "Iteration 35, loss = 0.49877529\n",
      "Iteration 36, loss = 0.49014540\n",
      "Iteration 37, loss = 0.48210672\n",
      "Iteration 38, loss = 0.47477907\n",
      "Iteration 39, loss = 0.46802286\n",
      "Iteration 40, loss = 0.46181287\n",
      "Iteration 41, loss = 0.45600436\n",
      "Iteration 42, loss = 0.45017426\n",
      "Iteration 43, loss = 0.44441299\n",
      "Iteration 44, loss = 0.43865435\n",
      "Iteration 45, loss = 0.43293934\n",
      "Iteration 46, loss = 0.42742092\n",
      "Iteration 47, loss = 0.42210408\n",
      "Iteration 48, loss = 0.41694432\n",
      "Iteration 49, loss = 0.41189391\n",
      "Iteration 50, loss = 0.40693873\n",
      "Iteration 51, loss = 0.40203587\n",
      "Iteration 52, loss = 0.39722003\n",
      "Iteration 53, loss = 0.39257466\n",
      "Iteration 54, loss = 0.38798303\n",
      "Iteration 55, loss = 0.38343787\n",
      "Iteration 56, loss = 0.37895404\n",
      "Iteration 57, loss = 0.37451780\n",
      "Iteration 58, loss = 0.37013099\n",
      "Iteration 59, loss = 0.36581210\n",
      "Iteration 60, loss = 0.36154143\n",
      "Iteration 61, loss = 0.35732049\n",
      "Iteration 62, loss = 0.35314462\n",
      "Iteration 63, loss = 0.34901091\n",
      "Iteration 64, loss = 0.34492329\n",
      "Iteration 65, loss = 0.34088038\n",
      "Iteration 66, loss = 0.33688480\n",
      "Iteration 67, loss = 0.33293653\n",
      "Iteration 68, loss = 0.32902531\n",
      "Iteration 69, loss = 0.32514382\n",
      "Iteration 70, loss = 0.32130495\n",
      "Iteration 71, loss = 0.31750447\n",
      "Iteration 72, loss = 0.31374173\n",
      "Iteration 73, loss = 0.31001655\n",
      "Iteration 74, loss = 0.30632980\n",
      "Iteration 75, loss = 0.30268304\n",
      "Iteration 76, loss = 0.29907492\n",
      "Iteration 77, loss = 0.29550565\n",
      "Iteration 78, loss = 0.29197657\n",
      "Iteration 79, loss = 0.28848957\n",
      "Iteration 80, loss = 0.28505467\n",
      "Iteration 81, loss = 0.28166457\n",
      "Iteration 82, loss = 0.27831635\n",
      "Iteration 83, loss = 0.27502168\n",
      "Iteration 84, loss = 0.27177609\n",
      "Iteration 85, loss = 0.26857105\n",
      "Iteration 86, loss = 0.26540968\n",
      "Iteration 87, loss = 0.26228870\n",
      "Iteration 88, loss = 0.25920314\n",
      "Iteration 89, loss = 0.25615392\n",
      "Iteration 90, loss = 0.25314515\n",
      "Iteration 91, loss = 0.25018212\n",
      "Iteration 92, loss = 0.24726258\n",
      "Iteration 93, loss = 0.24438486\n",
      "Iteration 94, loss = 0.24154919\n",
      "Iteration 95, loss = 0.23876200\n",
      "Iteration 96, loss = 0.23601734\n",
      "Iteration 97, loss = 0.23331386\n",
      "Iteration 98, loss = 0.23065146\n",
      "Iteration 99, loss = 0.22803457\n",
      "Iteration 100, loss = 0.22545968\n",
      "Iteration 101, loss = 0.22292558\n",
      "Iteration 102, loss = 0.22043192\n",
      "Iteration 103, loss = 0.21797842\n",
      "Iteration 104, loss = 0.21556485\n",
      "Iteration 105, loss = 0.21319100\n",
      "Iteration 106, loss = 0.21085655\n",
      "Iteration 107, loss = 0.20856117\n",
      "Iteration 108, loss = 0.20630450\n",
      "Iteration 109, loss = 0.20408806\n",
      "Iteration 110, loss = 0.20191076\n",
      "Iteration 111, loss = 0.19977127\n",
      "Iteration 112, loss = 0.19766883\n",
      "Iteration 113, loss = 0.19560323\n",
      "Iteration 114, loss = 0.19357767\n",
      "Iteration 115, loss = 0.19158884\n",
      "Iteration 116, loss = 0.18964058\n",
      "Iteration 117, loss = 0.18772721\n",
      "Iteration 118, loss = 0.18584829\n",
      "Iteration 119, loss = 0.18400334\n",
      "Iteration 120, loss = 0.18219190\n",
      "Iteration 121, loss = 0.18041347\n",
      "Iteration 122, loss = 0.17866754\n",
      "Iteration 123, loss = 0.17695384\n",
      "Iteration 124, loss = 0.17527261\n",
      "Iteration 125, loss = 0.17362172\n",
      "Iteration 126, loss = 0.17200132\n",
      "Iteration 127, loss = 0.17041098\n",
      "Iteration 128, loss = 0.16885305\n",
      "Iteration 129, loss = 0.16732563\n",
      "Iteration 130, loss = 0.16583009\n",
      "Iteration 131, loss = 0.16436232\n",
      "Iteration 132, loss = 0.16292374\n",
      "Iteration 133, loss = 0.16151320\n",
      "Iteration 134, loss = 0.16012872\n",
      "Iteration 135, loss = 0.15877021\n",
      "Iteration 136, loss = 0.15743630\n",
      "Iteration 137, loss = 0.15612735\n",
      "Iteration 138, loss = 0.15484213\n",
      "Iteration 139, loss = 0.15358056\n",
      "Iteration 140, loss = 0.15234198\n",
      "Iteration 141, loss = 0.15112609\n",
      "Iteration 142, loss = 0.14993219\n",
      "Iteration 143, loss = 0.14875981\n",
      "Iteration 144, loss = 0.14760883\n",
      "Iteration 145, loss = 0.14647851\n",
      "Iteration 146, loss = 0.14537089\n",
      "Iteration 147, loss = 0.14428385\n",
      "Iteration 148, loss = 0.14321609\n",
      "Iteration 149, loss = 0.14216725\n",
      "Iteration 150, loss = 0.14113698\n",
      "Iteration 151, loss = 0.14012487\n",
      "Iteration 152, loss = 0.13913194\n",
      "Iteration 153, loss = 0.13815692\n",
      "Iteration 154, loss = 0.13720001\n",
      "Iteration 155, loss = 0.13626019\n",
      "Iteration 156, loss = 0.13533632\n",
      "Iteration 157, loss = 0.13442808\n",
      "Iteration 158, loss = 0.13353518\n",
      "Iteration 159, loss = 0.13265757\n",
      "Iteration 160, loss = 0.13179448\n",
      "Iteration 161, loss = 0.13094582\n",
      "Iteration 162, loss = 0.13011152\n",
      "Iteration 163, loss = 0.12929097\n",
      "Iteration 164, loss = 0.12848400\n",
      "Iteration 165, loss = 0.12769044\n",
      "Iteration 166, loss = 0.12690988\n",
      "Iteration 167, loss = 0.12614205\n",
      "Iteration 168, loss = 0.12538675\n",
      "Iteration 169, loss = 0.12464358\n",
      "Iteration 170, loss = 0.12391240\n",
      "Iteration 171, loss = 0.12319288\n",
      "Iteration 172, loss = 0.12248490\n",
      "Iteration 173, loss = 0.12178802\n",
      "Iteration 174, loss = 0.12110217\n",
      "Iteration 175, loss = 0.12042708\n",
      "Iteration 176, loss = 0.11976247\n",
      "Iteration 177, loss = 0.11910816\n",
      "Iteration 178, loss = 0.11846394\n",
      "Iteration 179, loss = 0.11782961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 180, loss = 0.11720493\n",
      "Iteration 181, loss = 0.11658971\n",
      "Iteration 182, loss = 0.11598377\n",
      "Iteration 183, loss = 0.11538690\n",
      "Iteration 184, loss = 0.11479893\n",
      "Iteration 185, loss = 0.11421967\n",
      "Iteration 186, loss = 0.11364893\n",
      "Iteration 187, loss = 0.11308656\n",
      "Iteration 188, loss = 0.11253237\n",
      "Iteration 189, loss = 0.11198621\n",
      "Iteration 190, loss = 0.11144790\n",
      "Iteration 191, loss = 0.11091729\n",
      "Iteration 192, loss = 0.11039423\n",
      "Iteration 193, loss = 0.10987856\n",
      "Iteration 194, loss = 0.10937014\n",
      "Iteration 195, loss = 0.10886882\n",
      "Iteration 196, loss = 0.10837446\n",
      "Iteration 197, loss = 0.10788693\n",
      "Iteration 198, loss = 0.10740609\n",
      "Iteration 199, loss = 0.10693181\n",
      "Iteration 200, loss = 0.10646396\n",
      "Iteration 201, loss = 0.10600243\n",
      "Iteration 202, loss = 0.10554708\n",
      "Iteration 203, loss = 0.10509781\n",
      "Iteration 204, loss = 0.10465449\n",
      "Iteration 205, loss = 0.10421702\n",
      "Iteration 206, loss = 0.10378528\n",
      "Iteration 207, loss = 0.10335916\n",
      "Iteration 208, loss = 0.10293879\n",
      "Iteration 209, loss = 0.10252387\n",
      "Iteration 210, loss = 0.10211428\n",
      "Iteration 211, loss = 0.10170991\n",
      "Iteration 212, loss = 0.10131068\n",
      "Iteration 213, loss = 0.10091649\n",
      "Iteration 214, loss = 0.10052724\n",
      "Iteration 215, loss = 0.10014286\n",
      "Iteration 216, loss = 0.09976324\n",
      "Iteration 217, loss = 0.09938838\n",
      "Iteration 218, loss = 0.09901821\n",
      "Iteration 219, loss = 0.09865253\n",
      "Iteration 220, loss = 0.09829126\n",
      "Iteration 221, loss = 0.09793433\n",
      "Iteration 222, loss = 0.09758165\n",
      "Iteration 223, loss = 0.09723318\n",
      "Iteration 224, loss = 0.09688883\n",
      "Iteration 225, loss = 0.09654865\n",
      "Iteration 226, loss = 0.09621245\n",
      "Iteration 227, loss = 0.09588019\n",
      "Iteration 228, loss = 0.09555178\n",
      "Iteration 229, loss = 0.09522717\n",
      "Iteration 230, loss = 0.09490629\n",
      "Iteration 231, loss = 0.09458907\n",
      "Iteration 232, loss = 0.09427552\n",
      "Iteration 233, loss = 0.09396548\n",
      "Iteration 234, loss = 0.09365935\n",
      "Iteration 235, loss = 0.09335677\n",
      "Iteration 236, loss = 0.09305755\n",
      "Iteration 237, loss = 0.09276169\n",
      "Iteration 238, loss = 0.09246909\n",
      "Iteration 239, loss = 0.09217971\n",
      "Iteration 240, loss = 0.09189352\n",
      "Iteration 241, loss = 0.09161046\n",
      "Iteration 242, loss = 0.09133056\n",
      "Iteration 243, loss = 0.09105377\n",
      "Iteration 244, loss = 0.09077995\n",
      "Iteration 245, loss = 0.09050905\n",
      "Iteration 246, loss = 0.09024104\n",
      "Iteration 247, loss = 0.08997586\n",
      "Iteration 248, loss = 0.08971348\n",
      "Iteration 249, loss = 0.08945386\n",
      "Iteration 250, loss = 0.08919694\n",
      "Iteration 251, loss = 0.08894270\n",
      "Iteration 252, loss = 0.08869109\n",
      "Iteration 253, loss = 0.08844206\n",
      "Iteration 254, loss = 0.08819558\n",
      "Iteration 255, loss = 0.08795163\n",
      "Iteration 256, loss = 0.08771015\n",
      "Iteration 257, loss = 0.08747110\n",
      "Iteration 258, loss = 0.08723445\n",
      "Iteration 259, loss = 0.08700019\n",
      "Iteration 260, loss = 0.08676822\n",
      "Iteration 261, loss = 0.08653859\n",
      "Iteration 262, loss = 0.08631122\n",
      "Iteration 263, loss = 0.08608614\n",
      "Iteration 264, loss = 0.08586324\n",
      "Iteration 265, loss = 0.08564251\n",
      "Iteration 266, loss = 0.08542391\n",
      "Iteration 267, loss = 0.08520742\n",
      "Iteration 268, loss = 0.08499302\n",
      "Iteration 269, loss = 0.08478065\n",
      "Iteration 270, loss = 0.08457029\n",
      "Iteration 271, loss = 0.08436191\n",
      "Iteration 272, loss = 0.08415548\n",
      "Iteration 273, loss = 0.08395098\n",
      "Iteration 274, loss = 0.08374837\n",
      "Iteration 275, loss = 0.08354764\n",
      "Iteration 276, loss = 0.08334875\n",
      "Iteration 277, loss = 0.08315169\n",
      "Iteration 278, loss = 0.08295642\n",
      "Iteration 279, loss = 0.08276294\n",
      "Iteration 280, loss = 0.08257121\n",
      "Iteration 281, loss = 0.08238120\n",
      "Iteration 282, loss = 0.08219290\n",
      "Iteration 283, loss = 0.08200627\n",
      "Iteration 284, loss = 0.08182130\n",
      "Iteration 285, loss = 0.08163796\n",
      "Iteration 286, loss = 0.08145624\n",
      "Iteration 287, loss = 0.08127610\n",
      "Iteration 288, loss = 0.08109753\n",
      "Iteration 289, loss = 0.08092052\n",
      "Iteration 290, loss = 0.08074504\n",
      "Iteration 291, loss = 0.08057106\n",
      "Iteration 292, loss = 0.08039857\n",
      "Iteration 293, loss = 0.08022756\n",
      "Iteration 294, loss = 0.08005799\n",
      "Iteration 295, loss = 0.07988985\n",
      "Iteration 296, loss = 0.07972314\n",
      "Iteration 297, loss = 0.07955782\n",
      "Iteration 298, loss = 0.07939387\n",
      "Iteration 299, loss = 0.07923129\n",
      "Iteration 300, loss = 0.07907010\n",
      "Iteration 301, loss = 0.07891036\n",
      "Iteration 302, loss = 0.07875194\n",
      "Iteration 303, loss = 0.07859494\n",
      "Iteration 304, loss = 0.07843914\n",
      "Iteration 305, loss = 0.07828457\n",
      "Iteration 306, loss = 0.07813122\n",
      "Iteration 307, loss = 0.07797907\n",
      "Iteration 308, loss = 0.07782811\n",
      "Iteration 309, loss = 0.07767835\n",
      "Iteration 310, loss = 0.07752980\n",
      "Iteration 311, loss = 0.07738247\n",
      "Iteration 312, loss = 0.07723631\n",
      "Iteration 313, loss = 0.07709133\n",
      "Iteration 314, loss = 0.07694746\n",
      "Iteration 315, loss = 0.07680469\n",
      "Iteration 316, loss = 0.07666303\n",
      "Iteration 317, loss = 0.07652247\n",
      "Iteration 318, loss = 0.07638299\n",
      "Iteration 319, loss = 0.07624459\n",
      "Iteration 320, loss = 0.07610724\n",
      "Iteration 321, loss = 0.07597094\n",
      "Iteration 322, loss = 0.07583566\n",
      "Iteration 323, loss = 0.07570142\n",
      "Iteration 324, loss = 0.07556820\n",
      "Iteration 325, loss = 0.07543594\n",
      "Iteration 326, loss = 0.07530470\n",
      "Iteration 327, loss = 0.07517442\n",
      "Iteration 328, loss = 0.07504511\n",
      "Iteration 329, loss = 0.07491675\n",
      "Iteration 330, loss = 0.07478933\n",
      "Iteration 331, loss = 0.07466285\n",
      "Iteration 332, loss = 0.07453729\n",
      "Iteration 333, loss = 0.07441263\n",
      "Iteration 334, loss = 0.07428888\n",
      "Iteration 335, loss = 0.07416602\n",
      "Iteration 336, loss = 0.07404404\n",
      "Iteration 337, loss = 0.07392292\n",
      "Iteration 338, loss = 0.07380267\n",
      "Iteration 339, loss = 0.07368328\n",
      "Iteration 340, loss = 0.07356472\n",
      "Iteration 341, loss = 0.07344700\n",
      "Iteration 342, loss = 0.07333010\n",
      "Iteration 343, loss = 0.07321402\n",
      "Iteration 344, loss = 0.07309875\n",
      "Iteration 345, loss = 0.07298427\n",
      "Iteration 346, loss = 0.07287058\n",
      "Iteration 347, loss = 0.07275768\n",
      "Iteration 348, loss = 0.07264555\n",
      "Iteration 349, loss = 0.07253419\n",
      "Iteration 350, loss = 0.07242358\n",
      "Iteration 351, loss = 0.07231372\n",
      "Iteration 352, loss = 0.07220461\n",
      "Iteration 353, loss = 0.07209622\n",
      "Iteration 354, loss = 0.07198857\n",
      "Iteration 355, loss = 0.07188163\n",
      "Iteration 356, loss = 0.07177541\n",
      "Iteration 357, loss = 0.07166989\n",
      "Iteration 358, loss = 0.07156506\n",
      "Iteration 359, loss = 0.07146093\n",
      "Iteration 360, loss = 0.07135748\n",
      "Iteration 361, loss = 0.07125471\n",
      "Iteration 362, loss = 0.07115261\n",
      "Iteration 363, loss = 0.07105117\n",
      "Iteration 364, loss = 0.07095038\n",
      "Iteration 365, loss = 0.07085026\n",
      "Iteration 366, loss = 0.07075081\n",
      "Iteration 367, loss = 0.07065200\n",
      "Iteration 368, loss = 0.07055382\n",
      "Iteration 369, loss = 0.07045628\n",
      "Iteration 370, loss = 0.07035936\n",
      "Iteration 371, loss = 0.07026305\n",
      "Iteration 372, loss = 0.07016735\n",
      "Iteration 373, loss = 0.07007225\n",
      "Iteration 374, loss = 0.06997776\n",
      "Iteration 375, loss = 0.06988384\n",
      "Iteration 376, loss = 0.06979052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11317117\n",
      "Iteration 2, loss = 1.53718236\n",
      "Iteration 3, loss = 1.11700167\n",
      "Iteration 4, loss = 1.09451797\n",
      "Iteration 5, loss = 1.16537111\n",
      "Iteration 6, loss = 1.15287477\n",
      "Iteration 7, loss = 1.08577195\n",
      "Iteration 8, loss = 1.01475745\n",
      "Iteration 9, loss = 0.96008127\n",
      "Iteration 10, loss = 0.91926259\n",
      "Iteration 11, loss = 0.88556629\n",
      "Iteration 12, loss = 0.85518562\n",
      "Iteration 13, loss = 0.82665079\n",
      "Iteration 14, loss = 0.80014260\n",
      "Iteration 15, loss = 0.77601071\n",
      "Iteration 16, loss = 0.75375976\n",
      "Iteration 17, loss = 0.73286697\n",
      "Iteration 18, loss = 0.71356502\n",
      "Iteration 19, loss = 0.69671710\n",
      "Iteration 20, loss = 0.68127779\n",
      "Iteration 21, loss = 0.66621709\n",
      "Iteration 22, loss = 0.65140371\n",
      "Iteration 23, loss = 0.63689059\n",
      "Iteration 24, loss = 0.62268246\n",
      "Iteration 25, loss = 0.60889107\n",
      "Iteration 26, loss = 0.59560838\n",
      "Iteration 27, loss = 0.58293843\n",
      "Iteration 28, loss = 0.57083660\n",
      "Iteration 29, loss = 0.55920641\n",
      "Iteration 30, loss = 0.54799917\n",
      "Iteration 31, loss = 0.53719088\n",
      "Iteration 32, loss = 0.52672893\n",
      "Iteration 33, loss = 0.51668474\n",
      "Iteration 34, loss = 0.50711128\n",
      "Iteration 35, loss = 0.49792187\n",
      "Iteration 36, loss = 0.48918012\n",
      "Iteration 37, loss = 0.48142141\n",
      "Iteration 38, loss = 0.47446231\n",
      "Iteration 39, loss = 0.46788409\n",
      "Iteration 40, loss = 0.46157647\n",
      "Iteration 41, loss = 0.45542612\n",
      "Iteration 42, loss = 0.44934644\n",
      "Iteration 43, loss = 0.44333322\n",
      "Iteration 44, loss = 0.43746122\n",
      "Iteration 45, loss = 0.43174330\n",
      "Iteration 46, loss = 0.42617713\n",
      "Iteration 47, loss = 0.42083999\n",
      "Iteration 48, loss = 0.41565485\n",
      "Iteration 49, loss = 0.41065175\n",
      "Iteration 50, loss = 0.40573428\n",
      "Iteration 51, loss = 0.40086725\n",
      "Iteration 52, loss = 0.39606544\n",
      "Iteration 53, loss = 0.39133523\n",
      "Iteration 54, loss = 0.38669490\n",
      "Iteration 55, loss = 0.38213239\n",
      "Iteration 56, loss = 0.37764102\n",
      "Iteration 57, loss = 0.37320931\n",
      "Iteration 58, loss = 0.36882945\n",
      "Iteration 59, loss = 0.36451137\n",
      "Iteration 60, loss = 0.36022344\n",
      "Iteration 61, loss = 0.35595612\n",
      "Iteration 62, loss = 0.35174579\n",
      "Iteration 63, loss = 0.34759046\n",
      "Iteration 64, loss = 0.34348564\n",
      "Iteration 65, loss = 0.33942338\n",
      "Iteration 66, loss = 0.33540083\n",
      "Iteration 67, loss = 0.33141523\n",
      "Iteration 68, loss = 0.32746593\n",
      "Iteration 69, loss = 0.32355697\n",
      "Iteration 70, loss = 0.31969000\n",
      "Iteration 71, loss = 0.31585781\n",
      "Iteration 72, loss = 0.31206668\n",
      "Iteration 73, loss = 0.30831687\n",
      "Iteration 74, loss = 0.30460781\n",
      "Iteration 75, loss = 0.30093139\n",
      "Iteration 76, loss = 0.29729698\n",
      "Iteration 77, loss = 0.29369952\n",
      "Iteration 78, loss = 0.29014122\n",
      "Iteration 79, loss = 0.28662164\n",
      "Iteration 80, loss = 0.28314098\n",
      "Iteration 81, loss = 0.27970492\n",
      "Iteration 82, loss = 0.27632882\n",
      "Iteration 83, loss = 0.27299302\n",
      "Iteration 84, loss = 0.26970327\n",
      "Iteration 85, loss = 0.26646024\n",
      "Iteration 86, loss = 0.26326627\n",
      "Iteration 87, loss = 0.26011142\n",
      "Iteration 88, loss = 0.25699774\n",
      "Iteration 89, loss = 0.25392517\n",
      "Iteration 90, loss = 0.25089341\n",
      "Iteration 91, loss = 0.24790336\n",
      "Iteration 92, loss = 0.24495565\n",
      "Iteration 93, loss = 0.24205670\n",
      "Iteration 94, loss = 0.23920493\n",
      "Iteration 95, loss = 0.23639664\n",
      "Iteration 96, loss = 0.23363311\n",
      "Iteration 97, loss = 0.23091437\n",
      "Iteration 98, loss = 0.22823938\n",
      "Iteration 99, loss = 0.22560671\n",
      "Iteration 100, loss = 0.22301499\n",
      "Iteration 101, loss = 0.22046824\n",
      "Iteration 102, loss = 0.21796546\n",
      "Iteration 103, loss = 0.21551052\n",
      "Iteration 104, loss = 0.21310057\n",
      "Iteration 105, loss = 0.21073302\n",
      "Iteration 106, loss = 0.20840773\n",
      "Iteration 107, loss = 0.20612404\n",
      "Iteration 108, loss = 0.20387846\n",
      "Iteration 109, loss = 0.20167032\n",
      "Iteration 110, loss = 0.19949917\n",
      "Iteration 111, loss = 0.19736452\n",
      "Iteration 112, loss = 0.19526589\n",
      "Iteration 113, loss = 0.19320278\n",
      "Iteration 114, loss = 0.19117487\n",
      "Iteration 115, loss = 0.18918160\n",
      "Iteration 116, loss = 0.18722253\n",
      "Iteration 117, loss = 0.18529797\n",
      "Iteration 118, loss = 0.18340788\n",
      "Iteration 119, loss = 0.18155159\n",
      "Iteration 120, loss = 0.17972735\n",
      "Iteration 121, loss = 0.17793473\n",
      "Iteration 122, loss = 0.17617327\n",
      "Iteration 123, loss = 0.17444356\n",
      "Iteration 124, loss = 0.17274901\n",
      "Iteration 125, loss = 0.17108386\n",
      "Iteration 126, loss = 0.16944758\n",
      "Iteration 127, loss = 0.16784097\n",
      "Iteration 128, loss = 0.16626190\n",
      "Iteration 129, loss = 0.16471039\n",
      "Iteration 130, loss = 0.16318586\n",
      "Iteration 131, loss = 0.16168822\n",
      "Iteration 132, loss = 0.16021673\n",
      "Iteration 133, loss = 0.15877064\n",
      "Iteration 134, loss = 0.15734937\n",
      "Iteration 135, loss = 0.15595252\n",
      "Iteration 136, loss = 0.15457970\n",
      "Iteration 137, loss = 0.15323063\n",
      "Iteration 138, loss = 0.15190512\n",
      "Iteration 139, loss = 0.15060247\n",
      "Iteration 140, loss = 0.14932223\n",
      "Iteration 141, loss = 0.14806404\n",
      "Iteration 142, loss = 0.14682744\n",
      "Iteration 143, loss = 0.14561189\n",
      "Iteration 144, loss = 0.14441698\n",
      "Iteration 145, loss = 0.14324267\n",
      "Iteration 146, loss = 0.14208864\n",
      "Iteration 147, loss = 0.14095407\n",
      "Iteration 148, loss = 0.13983853\n",
      "Iteration 149, loss = 0.13874162\n",
      "Iteration 150, loss = 0.13766297\n",
      "Iteration 151, loss = 0.13660220\n",
      "Iteration 152, loss = 0.13555894\n",
      "Iteration 153, loss = 0.13453282\n",
      "Iteration 154, loss = 0.13352349\n",
      "Iteration 155, loss = 0.13253059\n",
      "Iteration 156, loss = 0.13155378\n",
      "Iteration 157, loss = 0.13059280\n",
      "Iteration 158, loss = 0.12964727\n",
      "Iteration 159, loss = 0.12871684\n",
      "Iteration 160, loss = 0.12780121\n",
      "Iteration 161, loss = 0.12690217\n",
      "Iteration 162, loss = 0.12601681\n",
      "Iteration 163, loss = 0.12514542\n",
      "Iteration 164, loss = 0.12428754\n",
      "Iteration 165, loss = 0.12344294\n",
      "Iteration 166, loss = 0.12261145\n",
      "Iteration 167, loss = 0.12179291\n",
      "Iteration 168, loss = 0.12098700\n",
      "Iteration 169, loss = 0.12019362\n",
      "Iteration 170, loss = 0.11941251\n",
      "Iteration 171, loss = 0.11864331\n",
      "Iteration 172, loss = 0.11788574\n",
      "Iteration 173, loss = 0.11713957\n",
      "Iteration 174, loss = 0.11640457\n",
      "Iteration 175, loss = 0.11568060\n",
      "Iteration 176, loss = 0.11496718\n",
      "Iteration 177, loss = 0.11426436\n",
      "Iteration 178, loss = 0.11357192\n",
      "Iteration 179, loss = 0.11288951\n",
      "Iteration 180, loss = 0.11221706\n",
      "Iteration 181, loss = 0.11155428\n",
      "Iteration 182, loss = 0.11090108\n",
      "Iteration 183, loss = 0.11025719\n",
      "Iteration 184, loss = 0.10962244\n",
      "Iteration 185, loss = 0.10899664\n",
      "Iteration 186, loss = 0.10837967\n",
      "Iteration 187, loss = 0.10777125\n",
      "Iteration 188, loss = 0.10717130\n",
      "Iteration 189, loss = 0.10657959\n",
      "Iteration 190, loss = 0.10599596\n",
      "Iteration 191, loss = 0.10542027\n",
      "Iteration 192, loss = 0.10485241\n",
      "Iteration 193, loss = 0.10429210\n",
      "Iteration 194, loss = 0.10373930\n",
      "Iteration 195, loss = 0.10319382\n",
      "Iteration 196, loss = 0.10265554\n",
      "Iteration 197, loss = 0.10212430\n",
      "Iteration 198, loss = 0.10160001\n",
      "Iteration 199, loss = 0.10108246\n",
      "Iteration 200, loss = 0.10057158\n",
      "Iteration 201, loss = 0.10006723\n",
      "Iteration 202, loss = 0.09956933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 203, loss = 0.09907772\n",
      "Iteration 204, loss = 0.09859228\n",
      "Iteration 205, loss = 0.09811290\n",
      "Iteration 206, loss = 0.09763948\n",
      "Iteration 207, loss = 0.09717190\n",
      "Iteration 208, loss = 0.09671006\n",
      "Iteration 209, loss = 0.09625385\n",
      "Iteration 210, loss = 0.09580318\n",
      "Iteration 211, loss = 0.09535796\n",
      "Iteration 212, loss = 0.09491807\n",
      "Iteration 213, loss = 0.09448344\n",
      "Iteration 214, loss = 0.09405395\n",
      "Iteration 215, loss = 0.09362953\n",
      "Iteration 216, loss = 0.09321009\n",
      "Iteration 217, loss = 0.09279553\n",
      "Iteration 218, loss = 0.09238579\n",
      "Iteration 219, loss = 0.09198077\n",
      "Iteration 220, loss = 0.09158039\n",
      "Iteration 221, loss = 0.09118458\n",
      "Iteration 222, loss = 0.09079328\n",
      "Iteration 223, loss = 0.09040643\n",
      "Iteration 224, loss = 0.09002393\n",
      "Iteration 225, loss = 0.08964569\n",
      "Iteration 226, loss = 0.08927166\n",
      "Iteration 227, loss = 0.08890176\n",
      "Iteration 228, loss = 0.08853592\n",
      "Iteration 229, loss = 0.08817409\n",
      "Iteration 230, loss = 0.08781619\n",
      "Iteration 231, loss = 0.08746217\n",
      "Iteration 232, loss = 0.08711196\n",
      "Iteration 233, loss = 0.08676550\n",
      "Iteration 234, loss = 0.08642273\n",
      "Iteration 235, loss = 0.08608359\n",
      "Iteration 236, loss = 0.08574804\n",
      "Iteration 237, loss = 0.08541601\n",
      "Iteration 238, loss = 0.08508764\n",
      "Iteration 239, loss = 0.08476269\n",
      "Iteration 240, loss = 0.08444133\n",
      "Iteration 241, loss = 0.08412379\n",
      "Iteration 242, loss = 0.08380955\n",
      "Iteration 243, loss = 0.08349853\n",
      "Iteration 244, loss = 0.08319070\n",
      "Iteration 245, loss = 0.08288600\n",
      "Iteration 246, loss = 0.08258438\n",
      "Iteration 247, loss = 0.08228581\n",
      "Iteration 248, loss = 0.08199023\n",
      "Iteration 249, loss = 0.08169760\n",
      "Iteration 250, loss = 0.08140787\n",
      "Iteration 251, loss = 0.08112100\n",
      "Iteration 252, loss = 0.08083695\n",
      "Iteration 253, loss = 0.08055568\n",
      "Iteration 254, loss = 0.08027714\n",
      "Iteration 255, loss = 0.08000131\n",
      "Iteration 256, loss = 0.07972818\n",
      "Iteration 257, loss = 0.07945766\n",
      "Iteration 258, loss = 0.07918971\n",
      "Iteration 259, loss = 0.07892428\n",
      "Iteration 260, loss = 0.07866135\n",
      "Iteration 261, loss = 0.07840090\n",
      "Iteration 262, loss = 0.07814289\n",
      "Iteration 263, loss = 0.07788728\n",
      "Iteration 264, loss = 0.07763403\n",
      "Iteration 265, loss = 0.07738311\n",
      "Iteration 266, loss = 0.07713449\n",
      "Iteration 267, loss = 0.07688814\n",
      "Iteration 268, loss = 0.07664402\n",
      "Iteration 269, loss = 0.07640210\n",
      "Iteration 270, loss = 0.07616237\n",
      "Iteration 271, loss = 0.07592478\n",
      "Iteration 272, loss = 0.07568930\n",
      "Iteration 273, loss = 0.07545592\n",
      "Iteration 274, loss = 0.07522460\n",
      "Iteration 275, loss = 0.07499532\n",
      "Iteration 276, loss = 0.07476815\n",
      "Iteration 277, loss = 0.07454315\n",
      "Iteration 278, loss = 0.07432011\n",
      "Iteration 279, loss = 0.07409901\n",
      "Iteration 280, loss = 0.07387981\n",
      "Iteration 281, loss = 0.07366249\n",
      "Iteration 282, loss = 0.07344703\n",
      "Iteration 283, loss = 0.07323341\n",
      "Iteration 284, loss = 0.07302159\n",
      "Iteration 285, loss = 0.07281156\n",
      "Iteration 286, loss = 0.07260329\n",
      "Iteration 287, loss = 0.07239677\n",
      "Iteration 288, loss = 0.07219205\n",
      "Iteration 289, loss = 0.07198902\n",
      "Iteration 290, loss = 0.07178766\n",
      "Iteration 291, loss = 0.07158796\n",
      "Iteration 292, loss = 0.07138989\n",
      "Iteration 293, loss = 0.07119345\n",
      "Iteration 294, loss = 0.07099858\n",
      "Iteration 295, loss = 0.07080529\n",
      "Iteration 296, loss = 0.07061356\n",
      "Iteration 297, loss = 0.07042337\n",
      "Iteration 298, loss = 0.07023470\n",
      "Iteration 299, loss = 0.07004753\n",
      "Iteration 300, loss = 0.06986184\n",
      "Iteration 301, loss = 0.06967763\n",
      "Iteration 302, loss = 0.06949486\n",
      "Iteration 303, loss = 0.06931352\n",
      "Iteration 304, loss = 0.06913360\n",
      "Iteration 305, loss = 0.06895508\n",
      "Iteration 306, loss = 0.06877794\n",
      "Iteration 307, loss = 0.06860217\n",
      "Iteration 308, loss = 0.06842774\n",
      "Iteration 309, loss = 0.06825465\n",
      "Iteration 310, loss = 0.06808288\n",
      "Iteration 311, loss = 0.06791241\n",
      "Iteration 312, loss = 0.06774325\n",
      "Iteration 313, loss = 0.06757545\n",
      "Iteration 314, loss = 0.06740890\n",
      "Iteration 315, loss = 0.06724361\n",
      "Iteration 316, loss = 0.06707954\n",
      "Iteration 317, loss = 0.06691668\n",
      "Iteration 318, loss = 0.06675503\n",
      "Iteration 319, loss = 0.06659457\n",
      "Iteration 320, loss = 0.06643528\n",
      "Iteration 321, loss = 0.06627715\n",
      "Iteration 322, loss = 0.06612017\n",
      "Iteration 323, loss = 0.06596432\n",
      "Iteration 324, loss = 0.06580960\n",
      "Iteration 325, loss = 0.06565599\n",
      "Iteration 326, loss = 0.06550348\n",
      "Iteration 327, loss = 0.06535205\n",
      "Iteration 328, loss = 0.06520170\n",
      "Iteration 329, loss = 0.06505241\n",
      "Iteration 330, loss = 0.06490417\n",
      "Iteration 331, loss = 0.06475696\n",
      "Iteration 332, loss = 0.06461079\n",
      "Iteration 333, loss = 0.06446563\n",
      "Iteration 334, loss = 0.06432148\n",
      "Iteration 335, loss = 0.06417832\n",
      "Iteration 336, loss = 0.06403615\n",
      "Iteration 337, loss = 0.06389495\n",
      "Iteration 338, loss = 0.06375472\n",
      "Iteration 339, loss = 0.06361543\n",
      "Iteration 340, loss = 0.06347710\n",
      "Iteration 341, loss = 0.06333969\n",
      "Iteration 342, loss = 0.06320321\n",
      "Iteration 343, loss = 0.06306764\n",
      "Iteration 344, loss = 0.06293299\n",
      "Iteration 345, loss = 0.06279928\n",
      "Iteration 346, loss = 0.06266646\n",
      "Iteration 347, loss = 0.06253451\n",
      "Iteration 348, loss = 0.06240344\n",
      "Iteration 349, loss = 0.06227323\n",
      "Iteration 350, loss = 0.06214388\n",
      "Iteration 351, loss = 0.06201536\n",
      "Iteration 352, loss = 0.06188769\n",
      "Iteration 353, loss = 0.06176084\n",
      "Iteration 354, loss = 0.06163486\n",
      "Iteration 355, loss = 0.06150969\n",
      "Iteration 356, loss = 0.06138533\n",
      "Iteration 357, loss = 0.06126176\n",
      "Iteration 358, loss = 0.06113897\n",
      "Iteration 359, loss = 0.06101698\n",
      "Iteration 360, loss = 0.06089576\n",
      "Iteration 361, loss = 0.06077531\n",
      "Iteration 362, loss = 0.06065562\n",
      "Iteration 363, loss = 0.06053669\n",
      "Iteration 364, loss = 0.06041851\n",
      "Iteration 365, loss = 0.06030107\n",
      "Iteration 366, loss = 0.06018436\n",
      "Iteration 367, loss = 0.06006840\n",
      "Iteration 368, loss = 0.05995315\n",
      "Iteration 369, loss = 0.05983862\n",
      "Iteration 370, loss = 0.05972479\n",
      "Iteration 371, loss = 0.05961165\n",
      "Iteration 372, loss = 0.05949921\n",
      "Iteration 373, loss = 0.05938747\n",
      "Iteration 374, loss = 0.05927640\n",
      "Iteration 375, loss = 0.05916600\n",
      "Iteration 376, loss = 0.05905627\n",
      "Iteration 377, loss = 0.05894720\n",
      "Iteration 378, loss = 0.05883879\n",
      "Iteration 379, loss = 0.05873102\n",
      "Iteration 380, loss = 0.05862390\n",
      "Iteration 381, loss = 0.05851741\n",
      "Iteration 382, loss = 0.05841155\n",
      "Iteration 383, loss = 0.05830632\n",
      "Iteration 384, loss = 0.05820170\n",
      "Iteration 385, loss = 0.05809769\n",
      "Iteration 386, loss = 0.05799429\n",
      "Iteration 387, loss = 0.05789150\n",
      "Iteration 388, loss = 0.05778930\n",
      "Iteration 389, loss = 0.05768768\n",
      "Iteration 390, loss = 0.05758666\n",
      "Iteration 391, loss = 0.05748621\n",
      "Iteration 392, loss = 0.05738634\n",
      "Iteration 393, loss = 0.05728704\n",
      "Iteration 394, loss = 0.05718830\n",
      "Iteration 395, loss = 0.05709013\n",
      "Iteration 396, loss = 0.05699251\n",
      "Iteration 397, loss = 0.05689543\n",
      "Iteration 398, loss = 0.05679891\n",
      "Iteration 399, loss = 0.05670292\n",
      "Iteration 400, loss = 0.05660747\n",
      "Iteration 401, loss = 0.05651254\n",
      "Iteration 402, loss = 0.05641815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11924132\n",
      "Iteration 2, loss = 1.53154890\n",
      "Iteration 3, loss = 1.10678771\n",
      "Iteration 4, loss = 1.09230263\n",
      "Iteration 5, loss = 1.16249824\n",
      "Iteration 6, loss = 1.14587701\n",
      "Iteration 7, loss = 1.07627297\n",
      "Iteration 8, loss = 1.00507111\n",
      "Iteration 9, loss = 0.95102057\n",
      "Iteration 10, loss = 0.91040398\n",
      "Iteration 11, loss = 0.87669790\n",
      "Iteration 12, loss = 0.84599437\n",
      "Iteration 13, loss = 0.81717861\n",
      "Iteration 14, loss = 0.79077005\n",
      "Iteration 15, loss = 0.76717962\n",
      "Iteration 16, loss = 0.74575947\n",
      "Iteration 17, loss = 0.72558310\n",
      "Iteration 18, loss = 0.70636924\n",
      "Iteration 19, loss = 0.68907992\n",
      "Iteration 20, loss = 0.67349670\n",
      "Iteration 21, loss = 0.65843867\n",
      "Iteration 22, loss = 0.64370922\n",
      "Iteration 23, loss = 0.62942204\n",
      "Iteration 24, loss = 0.61542772\n",
      "Iteration 25, loss = 0.60179690\n",
      "Iteration 26, loss = 0.58860302\n",
      "Iteration 27, loss = 0.57608344\n",
      "Iteration 28, loss = 0.56416240\n",
      "Iteration 29, loss = 0.55282235\n",
      "Iteration 30, loss = 0.54194529\n",
      "Iteration 31, loss = 0.53153280\n",
      "Iteration 32, loss = 0.52147880\n",
      "Iteration 33, loss = 0.51182294\n",
      "Iteration 34, loss = 0.50253154\n",
      "Iteration 35, loss = 0.49364844\n",
      "Iteration 36, loss = 0.48520297\n",
      "Iteration 37, loss = 0.47735569\n",
      "Iteration 38, loss = 0.47048956\n",
      "Iteration 39, loss = 0.46415325\n",
      "Iteration 40, loss = 0.45809292\n",
      "Iteration 41, loss = 0.45224148\n",
      "Iteration 42, loss = 0.44651880\n",
      "Iteration 43, loss = 0.44087117\n",
      "Iteration 44, loss = 0.43527886\n",
      "Iteration 45, loss = 0.42984932\n",
      "Iteration 46, loss = 0.42462920\n",
      "Iteration 47, loss = 0.41966858\n",
      "Iteration 48, loss = 0.41487662\n",
      "Iteration 49, loss = 0.41021489\n",
      "Iteration 50, loss = 0.40562937\n",
      "Iteration 51, loss = 0.40111463\n",
      "Iteration 52, loss = 0.39666942\n",
      "Iteration 53, loss = 0.39230673\n",
      "Iteration 54, loss = 0.38802453\n",
      "Iteration 55, loss = 0.38382219\n",
      "Iteration 56, loss = 0.37969736\n",
      "Iteration 57, loss = 0.37563866\n",
      "Iteration 58, loss = 0.37163385\n",
      "Iteration 59, loss = 0.36768342\n",
      "Iteration 60, loss = 0.36378154\n",
      "Iteration 61, loss = 0.35992716\n",
      "Iteration 62, loss = 0.35611872\n",
      "Iteration 63, loss = 0.35235448\n",
      "Iteration 64, loss = 0.34863630\n",
      "Iteration 65, loss = 0.34497083\n",
      "Iteration 66, loss = 0.34135225\n",
      "Iteration 67, loss = 0.33777487\n",
      "Iteration 68, loss = 0.33423674\n",
      "Iteration 69, loss = 0.33073765\n",
      "Iteration 70, loss = 0.32727824\n",
      "Iteration 71, loss = 0.32385644\n",
      "Iteration 72, loss = 0.32047221\n",
      "Iteration 73, loss = 0.31712700\n",
      "Iteration 74, loss = 0.31382242\n",
      "Iteration 75, loss = 0.31055386\n",
      "Iteration 76, loss = 0.30732243\n",
      "Iteration 77, loss = 0.30412754\n",
      "Iteration 78, loss = 0.30096898\n",
      "Iteration 79, loss = 0.29784790\n",
      "Iteration 80, loss = 0.29476337\n",
      "Iteration 81, loss = 0.29171506\n",
      "Iteration 82, loss = 0.28870507\n",
      "Iteration 83, loss = 0.28572983\n",
      "Iteration 84, loss = 0.28279329\n",
      "Iteration 85, loss = 0.27989601\n",
      "Iteration 86, loss = 0.27703568\n",
      "Iteration 87, loss = 0.27421452\n",
      "Iteration 88, loss = 0.27143107\n",
      "Iteration 89, loss = 0.26869372\n",
      "Iteration 90, loss = 0.26599983\n",
      "Iteration 91, loss = 0.26335211\n",
      "Iteration 92, loss = 0.26075346\n",
      "Iteration 93, loss = 0.25819382\n",
      "Iteration 94, loss = 0.25567115\n",
      "Iteration 95, loss = 0.25318604\n",
      "Iteration 96, loss = 0.25073724\n",
      "Iteration 97, loss = 0.24832584\n",
      "Iteration 98, loss = 0.24595942\n",
      "Iteration 99, loss = 0.24363542\n",
      "Iteration 100, loss = 0.24134696\n",
      "Iteration 101, loss = 0.23909795\n",
      "Iteration 102, loss = 0.23689320\n",
      "Iteration 103, loss = 0.23473279\n",
      "Iteration 104, loss = 0.23260596\n",
      "Iteration 105, loss = 0.23051245\n",
      "Iteration 106, loss = 0.22845793\n",
      "Iteration 107, loss = 0.22643751\n",
      "Iteration 108, loss = 0.22445314\n",
      "Iteration 109, loss = 0.22249758\n",
      "Iteration 110, loss = 0.22057473\n",
      "Iteration 111, loss = 0.21867986\n",
      "Iteration 112, loss = 0.21681656\n",
      "Iteration 113, loss = 0.21498414\n",
      "Iteration 114, loss = 0.21318071\n",
      "Iteration 115, loss = 0.21140742\n",
      "Iteration 116, loss = 0.20966399\n",
      "Iteration 117, loss = 0.20795078\n",
      "Iteration 118, loss = 0.20626598\n",
      "Iteration 119, loss = 0.20461002\n",
      "Iteration 120, loss = 0.20298276\n",
      "Iteration 121, loss = 0.20138458\n",
      "Iteration 122, loss = 0.19981405\n",
      "Iteration 123, loss = 0.19827073\n",
      "Iteration 124, loss = 0.19675482\n",
      "Iteration 125, loss = 0.19526562\n",
      "Iteration 126, loss = 0.19380217\n",
      "Iteration 127, loss = 0.19236400\n",
      "Iteration 128, loss = 0.19095066\n",
      "Iteration 129, loss = 0.18956174\n",
      "Iteration 130, loss = 0.18819680\n",
      "Iteration 131, loss = 0.18685557\n",
      "Iteration 132, loss = 0.18553752\n",
      "Iteration 133, loss = 0.18424266\n",
      "Iteration 134, loss = 0.18297008\n",
      "Iteration 135, loss = 0.18171944\n",
      "Iteration 136, loss = 0.18049031\n",
      "Iteration 137, loss = 0.17928228\n",
      "Iteration 138, loss = 0.17809496\n",
      "Iteration 139, loss = 0.17692803\n",
      "Iteration 140, loss = 0.17578118\n",
      "Iteration 141, loss = 0.17465390\n",
      "Iteration 142, loss = 0.17354581\n",
      "Iteration 143, loss = 0.17245653\n",
      "Iteration 144, loss = 0.17138570\n",
      "Iteration 145, loss = 0.17033308\n",
      "Iteration 146, loss = 0.16929798\n",
      "Iteration 147, loss = 0.16828047\n",
      "Iteration 148, loss = 0.16728001\n",
      "Iteration 149, loss = 0.16629623\n",
      "Iteration 150, loss = 0.16532957\n",
      "Iteration 151, loss = 0.16437956\n",
      "Iteration 152, loss = 0.16344533\n",
      "Iteration 153, loss = 0.16252653\n",
      "Iteration 154, loss = 0.16162285\n",
      "Iteration 155, loss = 0.16073402\n",
      "Iteration 156, loss = 0.15985975\n",
      "Iteration 157, loss = 0.15899975\n",
      "Iteration 158, loss = 0.15815374\n",
      "Iteration 159, loss = 0.15732144\n",
      "Iteration 160, loss = 0.15650257\n",
      "Iteration 161, loss = 0.15569686\n",
      "Iteration 162, loss = 0.15490406\n",
      "Iteration 163, loss = 0.15412389\n",
      "Iteration 164, loss = 0.15335657\n",
      "Iteration 165, loss = 0.15260132\n",
      "Iteration 166, loss = 0.15185798\n",
      "Iteration 167, loss = 0.15112628\n",
      "Iteration 168, loss = 0.15040606\n",
      "Iteration 169, loss = 0.14969703\n",
      "Iteration 170, loss = 0.14899899\n",
      "Iteration 171, loss = 0.14831171\n",
      "Iteration 172, loss = 0.14763500\n",
      "Iteration 173, loss = 0.14696862\n",
      "Iteration 174, loss = 0.14631239\n",
      "Iteration 175, loss = 0.14566610\n",
      "Iteration 176, loss = 0.14502959\n",
      "Iteration 177, loss = 0.14440263\n",
      "Iteration 178, loss = 0.14378504\n",
      "Iteration 179, loss = 0.14317662\n",
      "Iteration 180, loss = 0.14257719\n",
      "Iteration 181, loss = 0.14198657\n",
      "Iteration 182, loss = 0.14140459\n",
      "Iteration 183, loss = 0.14083107\n",
      "Iteration 184, loss = 0.14026584\n",
      "Iteration 185, loss = 0.13970874\n",
      "Iteration 186, loss = 0.13915960\n",
      "Iteration 187, loss = 0.13861828\n",
      "Iteration 188, loss = 0.13808461\n",
      "Iteration 189, loss = 0.13755845\n",
      "Iteration 190, loss = 0.13703965\n",
      "Iteration 191, loss = 0.13652806\n",
      "Iteration 192, loss = 0.13602355\n",
      "Iteration 193, loss = 0.13552597\n",
      "Iteration 194, loss = 0.13503521\n",
      "Iteration 195, loss = 0.13455124\n",
      "Iteration 196, loss = 0.13407390\n",
      "Iteration 197, loss = 0.13360299\n",
      "Iteration 198, loss = 0.13313839\n",
      "Iteration 199, loss = 0.13267999\n",
      "Iteration 200, loss = 0.13222766\n",
      "Iteration 201, loss = 0.13178131\n",
      "Iteration 202, loss = 0.13134092\n",
      "Iteration 203, loss = 0.13090629\n",
      "Iteration 204, loss = 0.13047730\n",
      "Iteration 205, loss = 0.13005448\n",
      "Iteration 206, loss = 0.12963721\n",
      "Iteration 207, loss = 0.12922532\n",
      "Iteration 208, loss = 0.12881869\n",
      "Iteration 209, loss = 0.12841737\n",
      "Iteration 210, loss = 0.12802113\n",
      "Iteration 211, loss = 0.12762989\n",
      "Iteration 212, loss = 0.12724357\n",
      "Iteration 213, loss = 0.12686203\n",
      "Iteration 214, loss = 0.12648521\n",
      "Iteration 215, loss = 0.12611304\n",
      "Iteration 216, loss = 0.12574545\n",
      "Iteration 217, loss = 0.12538251\n",
      "Iteration 218, loss = 0.12502396\n",
      "Iteration 219, loss = 0.12466977\n",
      "Iteration 220, loss = 0.12431985\n",
      "Iteration 221, loss = 0.12397412\n",
      "Iteration 222, loss = 0.12363251\n",
      "Iteration 223, loss = 0.12329505\n",
      "Iteration 224, loss = 0.12296167\n",
      "Iteration 225, loss = 0.12263221\n",
      "Iteration 226, loss = 0.12230668\n",
      "Iteration 227, loss = 0.12198496\n",
      "Iteration 228, loss = 0.12166697\n",
      "Iteration 229, loss = 0.12135264\n",
      "Iteration 230, loss = 0.12104194\n",
      "Iteration 231, loss = 0.12073478\n",
      "Iteration 232, loss = 0.12043140\n",
      "Iteration 233, loss = 0.12013137\n",
      "Iteration 234, loss = 0.11983470\n",
      "Iteration 235, loss = 0.11954132\n",
      "Iteration 236, loss = 0.11925118\n",
      "Iteration 237, loss = 0.11896424\n",
      "Iteration 238, loss = 0.11868045\n",
      "Iteration 239, loss = 0.11839977\n",
      "Iteration 240, loss = 0.11812214\n",
      "Iteration 241, loss = 0.11784752\n",
      "Iteration 242, loss = 0.11757589\n",
      "Iteration 243, loss = 0.11730725\n",
      "Iteration 244, loss = 0.11704149\n",
      "Iteration 245, loss = 0.11677853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 246, loss = 0.11651835\n",
      "Iteration 247, loss = 0.11626090\n",
      "Iteration 248, loss = 0.11600617\n",
      "Iteration 249, loss = 0.11575408\n",
      "Iteration 250, loss = 0.11550460\n",
      "Iteration 251, loss = 0.11525772\n",
      "Iteration 252, loss = 0.11501333\n",
      "Iteration 253, loss = 0.11477147\n",
      "Iteration 254, loss = 0.11453203\n",
      "Iteration 255, loss = 0.11429505\n",
      "Iteration 256, loss = 0.11406042\n",
      "Iteration 257, loss = 0.11382813\n",
      "Iteration 258, loss = 0.11359817\n",
      "Iteration 259, loss = 0.11337046\n",
      "Iteration 260, loss = 0.11314503\n",
      "Iteration 261, loss = 0.11292176\n",
      "Iteration 262, loss = 0.11270071\n",
      "Iteration 263, loss = 0.11248177\n",
      "Iteration 264, loss = 0.11226495\n",
      "Iteration 265, loss = 0.11205024\n",
      "Iteration 266, loss = 0.11183756\n",
      "Iteration 267, loss = 0.11162692\n",
      "Iteration 268, loss = 0.11141827\n",
      "Iteration 269, loss = 0.11121158\n",
      "Iteration 270, loss = 0.11100682\n",
      "Iteration 271, loss = 0.11080400\n",
      "Iteration 272, loss = 0.11060304\n",
      "Iteration 273, loss = 0.11040395\n",
      "Iteration 274, loss = 0.11020670\n",
      "Iteration 275, loss = 0.11001125\n",
      "Iteration 276, loss = 0.10981757\n",
      "Iteration 277, loss = 0.10962568\n",
      "Iteration 278, loss = 0.10943550\n",
      "Iteration 279, loss = 0.10924704\n",
      "Iteration 280, loss = 0.10906028\n",
      "Iteration 281, loss = 0.10887551\n",
      "Iteration 282, loss = 0.10869242\n",
      "Iteration 283, loss = 0.10851097\n",
      "Iteration 284, loss = 0.10833111\n",
      "Iteration 285, loss = 0.10815284\n",
      "Iteration 286, loss = 0.10797613\n",
      "Iteration 287, loss = 0.10780097\n",
      "Iteration 288, loss = 0.10762733\n",
      "Iteration 289, loss = 0.10745519\n",
      "Iteration 290, loss = 0.10728453\n",
      "Iteration 291, loss = 0.10711534\n",
      "Iteration 292, loss = 0.10694759\n",
      "Iteration 293, loss = 0.10678127\n",
      "Iteration 294, loss = 0.10661637\n",
      "Iteration 295, loss = 0.10645284\n",
      "Iteration 296, loss = 0.10629070\n",
      "Iteration 297, loss = 0.10612991\n",
      "Iteration 298, loss = 0.10597045\n",
      "Iteration 299, loss = 0.10581232\n",
      "Iteration 300, loss = 0.10565549\n",
      "Iteration 301, loss = 0.10549995\n",
      "Iteration 302, loss = 0.10534568\n",
      "Iteration 303, loss = 0.10519266\n",
      "Iteration 304, loss = 0.10504089\n",
      "Iteration 305, loss = 0.10489034\n",
      "Iteration 306, loss = 0.10474099\n",
      "Iteration 307, loss = 0.10459284\n",
      "Iteration 308, loss = 0.10444587\n",
      "Iteration 309, loss = 0.10430007\n",
      "Iteration 310, loss = 0.10415541\n",
      "Iteration 311, loss = 0.10401189\n",
      "Iteration 312, loss = 0.10386949\n",
      "Iteration 313, loss = 0.10372820\n",
      "Iteration 314, loss = 0.10358801\n",
      "Iteration 315, loss = 0.10344890\n",
      "Iteration 316, loss = 0.10331085\n",
      "Iteration 317, loss = 0.10317386\n",
      "Iteration 318, loss = 0.10303792\n",
      "Iteration 319, loss = 0.10290300\n",
      "Iteration 320, loss = 0.10276911\n",
      "Iteration 321, loss = 0.10263622\n",
      "Iteration 322, loss = 0.10250432\n",
      "Iteration 323, loss = 0.10237341\n",
      "Iteration 324, loss = 0.10224347\n",
      "Iteration 325, loss = 0.10211449\n",
      "Iteration 326, loss = 0.10198645\n",
      "Iteration 327, loss = 0.10185938\n",
      "Iteration 328, loss = 0.10173323\n",
      "Iteration 329, loss = 0.10160800\n",
      "Iteration 330, loss = 0.10148368\n",
      "Iteration 331, loss = 0.10136025\n",
      "Iteration 332, loss = 0.10123771\n",
      "Iteration 333, loss = 0.10111605\n",
      "Iteration 334, loss = 0.10099526\n",
      "Iteration 335, loss = 0.10087532\n",
      "Iteration 336, loss = 0.10075623\n",
      "Iteration 337, loss = 0.10063797\n",
      "Iteration 338, loss = 0.10052055\n",
      "Iteration 339, loss = 0.10040395\n",
      "Iteration 340, loss = 0.10028815\n",
      "Iteration 341, loss = 0.10017316\n",
      "Iteration 342, loss = 0.10005897\n",
      "Iteration 343, loss = 0.09994556\n",
      "Iteration 344, loss = 0.09983292\n",
      "Iteration 345, loss = 0.09972105\n",
      "Iteration 346, loss = 0.09960994\n",
      "Iteration 347, loss = 0.09949959\n",
      "Iteration 348, loss = 0.09938997\n",
      "Iteration 349, loss = 0.09928110\n",
      "Iteration 350, loss = 0.09917295\n",
      "Iteration 351, loss = 0.09906552\n",
      "Iteration 352, loss = 0.09895880\n",
      "Iteration 353, loss = 0.09885279\n",
      "Iteration 354, loss = 0.09874748\n",
      "Iteration 355, loss = 0.09864286\n",
      "Iteration 356, loss = 0.09853892\n",
      "Iteration 357, loss = 0.09843565\n",
      "Iteration 358, loss = 0.09833306\n",
      "Iteration 359, loss = 0.09823113\n",
      "Iteration 360, loss = 0.09812985\n",
      "Iteration 361, loss = 0.09802923\n",
      "Iteration 362, loss = 0.09792924\n",
      "Iteration 363, loss = 0.09782990\n",
      "Iteration 364, loss = 0.09773118\n",
      "Iteration 365, loss = 0.09763309\n",
      "Iteration 366, loss = 0.09753561\n",
      "Iteration 367, loss = 0.09743875\n",
      "Iteration 368, loss = 0.09734250\n",
      "Iteration 369, loss = 0.09724684\n",
      "Iteration 370, loss = 0.09715178\n",
      "Iteration 371, loss = 0.09705730\n",
      "Iteration 372, loss = 0.09696341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10901821\n",
      "Iteration 2, loss = 2.07714642\n",
      "Iteration 3, loss = 2.04555469\n",
      "Iteration 4, loss = 2.01426606\n",
      "Iteration 5, loss = 1.98329016\n",
      "Iteration 6, loss = 1.95264787\n",
      "Iteration 7, loss = 1.92234870\n",
      "Iteration 8, loss = 1.89241553\n",
      "Iteration 9, loss = 1.86285726\n",
      "Iteration 10, loss = 1.83368012\n",
      "Iteration 11, loss = 1.80490403\n",
      "Iteration 12, loss = 1.77654183\n",
      "Iteration 13, loss = 1.74862145\n",
      "Iteration 14, loss = 1.72114048\n",
      "Iteration 15, loss = 1.69412777\n",
      "Iteration 16, loss = 1.66758774\n",
      "Iteration 17, loss = 1.64150927\n",
      "Iteration 18, loss = 1.61589366\n",
      "Iteration 19, loss = 1.59077130\n",
      "Iteration 20, loss = 1.56615850\n",
      "Iteration 21, loss = 1.54208224\n",
      "Iteration 22, loss = 1.51856694\n",
      "Iteration 23, loss = 1.49559325\n",
      "Iteration 24, loss = 1.47318441\n",
      "Iteration 25, loss = 1.45136515\n",
      "Iteration 26, loss = 1.43012863\n",
      "Iteration 27, loss = 1.40947133\n",
      "Iteration 28, loss = 1.38939978\n",
      "Iteration 29, loss = 1.36992585\n",
      "Iteration 30, loss = 1.35104253\n",
      "Iteration 31, loss = 1.33272991\n",
      "Iteration 32, loss = 1.31499577\n",
      "Iteration 33, loss = 1.29787592\n",
      "Iteration 34, loss = 1.28134810\n",
      "Iteration 35, loss = 1.26536032\n",
      "Iteration 36, loss = 1.24995918\n",
      "Iteration 37, loss = 1.23515717\n",
      "Iteration 38, loss = 1.22094874\n",
      "Iteration 39, loss = 1.20730765\n",
      "Iteration 40, loss = 1.19421690\n",
      "Iteration 41, loss = 1.18166622\n",
      "Iteration 42, loss = 1.16964412\n",
      "Iteration 43, loss = 1.15813168\n",
      "Iteration 44, loss = 1.14709091\n",
      "Iteration 45, loss = 1.13654702\n",
      "Iteration 46, loss = 1.12650071\n",
      "Iteration 47, loss = 1.11692423\n",
      "Iteration 48, loss = 1.10780885\n",
      "Iteration 49, loss = 1.09911551\n",
      "Iteration 50, loss = 1.09085291\n",
      "Iteration 51, loss = 1.08297219\n",
      "Iteration 52, loss = 1.07545029\n",
      "Iteration 53, loss = 1.06826529\n",
      "Iteration 54, loss = 1.06144122\n",
      "Iteration 55, loss = 1.05495883\n",
      "Iteration 56, loss = 1.04877748\n",
      "Iteration 57, loss = 1.04287083\n",
      "Iteration 58, loss = 1.03724073\n",
      "Iteration 59, loss = 1.03186080\n",
      "Iteration 60, loss = 1.02671876\n",
      "Iteration 61, loss = 1.02178452\n",
      "Iteration 62, loss = 1.01703962\n",
      "Iteration 63, loss = 1.01247066\n",
      "Iteration 64, loss = 1.00807029\n",
      "Iteration 65, loss = 1.00381453\n",
      "Iteration 66, loss = 0.99969009\n",
      "Iteration 67, loss = 0.99568679\n",
      "Iteration 68, loss = 0.99179046\n",
      "Iteration 69, loss = 0.98798675\n",
      "Iteration 70, loss = 0.98425604\n",
      "Iteration 71, loss = 0.98059556\n",
      "Iteration 72, loss = 0.97698776\n",
      "Iteration 73, loss = 0.97342898\n",
      "Iteration 74, loss = 0.96991261\n",
      "Iteration 75, loss = 0.96643124\n",
      "Iteration 76, loss = 0.96297831\n",
      "Iteration 77, loss = 0.95954401\n",
      "Iteration 78, loss = 0.95612535\n",
      "Iteration 79, loss = 0.95271516\n",
      "Iteration 80, loss = 0.94931449\n",
      "Iteration 81, loss = 0.94592020\n",
      "Iteration 82, loss = 0.94252793\n",
      "Iteration 83, loss = 0.93913973\n",
      "Iteration 84, loss = 0.93575437\n",
      "Iteration 85, loss = 0.93237019\n",
      "Iteration 86, loss = 0.92898487\n",
      "Iteration 87, loss = 0.92560094\n",
      "Iteration 88, loss = 0.92221845\n",
      "Iteration 89, loss = 0.91883767\n",
      "Iteration 90, loss = 0.91545905\n",
      "Iteration 91, loss = 0.91208314\n",
      "Iteration 92, loss = 0.90871063\n",
      "Iteration 93, loss = 0.90536790\n",
      "Iteration 94, loss = 0.90203964\n",
      "Iteration 95, loss = 0.89871965\n",
      "Iteration 96, loss = 0.89540854\n",
      "Iteration 97, loss = 0.89210690\n",
      "Iteration 98, loss = 0.88881536\n",
      "Iteration 99, loss = 0.88553454\n",
      "Iteration 100, loss = 0.88226327\n",
      "Iteration 101, loss = 0.87902320\n",
      "Iteration 102, loss = 0.87583026\n",
      "Iteration 103, loss = 0.87267370\n",
      "Iteration 104, loss = 0.86955436\n",
      "Iteration 105, loss = 0.86646603\n",
      "Iteration 106, loss = 0.86339772\n",
      "Iteration 107, loss = 0.86034920\n",
      "Iteration 108, loss = 0.85731885\n",
      "Iteration 109, loss = 0.85430721\n",
      "Iteration 110, loss = 0.85131466\n",
      "Iteration 111, loss = 0.84838537\n",
      "Iteration 112, loss = 0.84556522\n",
      "Iteration 113, loss = 0.84281090\n",
      "Iteration 114, loss = 0.84012796\n",
      "Iteration 115, loss = 0.83757443\n",
      "Iteration 116, loss = 0.83513176\n",
      "Iteration 117, loss = 0.83275398\n",
      "Iteration 118, loss = 0.83043880\n",
      "Iteration 119, loss = 0.82813176\n",
      "Iteration 120, loss = 0.82583333\n",
      "Iteration 121, loss = 0.82354396\n",
      "Iteration 122, loss = 0.82126408\n",
      "Iteration 123, loss = 0.81900396\n",
      "Iteration 124, loss = 0.81675750\n",
      "Iteration 125, loss = 0.81451765\n",
      "Iteration 126, loss = 0.81228547\n",
      "Iteration 127, loss = 0.81006172\n",
      "Iteration 128, loss = 0.80784712\n",
      "Iteration 129, loss = 0.80564227\n",
      "Iteration 130, loss = 0.80344776\n",
      "Iteration 131, loss = 0.80126406\n",
      "Iteration 132, loss = 0.79909161\n",
      "Iteration 133, loss = 0.79693079\n",
      "Iteration 134, loss = 0.79478078\n",
      "Iteration 135, loss = 0.79264227\n",
      "Iteration 136, loss = 0.79051609\n",
      "Iteration 137, loss = 0.78840245\n",
      "Iteration 138, loss = 0.78630149\n",
      "Iteration 139, loss = 0.78421334\n",
      "Iteration 140, loss = 0.78213808\n",
      "Iteration 141, loss = 0.78007575\n",
      "Iteration 142, loss = 0.77802637\n",
      "Iteration 143, loss = 0.77599051\n",
      "Iteration 144, loss = 0.77397288\n",
      "Iteration 145, loss = 0.77197057\n",
      "Iteration 146, loss = 0.76998124\n",
      "Iteration 147, loss = 0.76800472\n",
      "Iteration 148, loss = 0.76604088\n",
      "Iteration 149, loss = 0.76408955\n",
      "Iteration 150, loss = 0.76215061\n",
      "Iteration 151, loss = 0.76022393\n",
      "Iteration 152, loss = 0.75830937\n",
      "Iteration 153, loss = 0.75640683\n",
      "Iteration 154, loss = 0.75451618\n",
      "Iteration 155, loss = 0.75263732\n",
      "Iteration 156, loss = 0.75077014\n",
      "Iteration 157, loss = 0.74891456\n",
      "Iteration 158, loss = 0.74707047\n",
      "Iteration 159, loss = 0.74523778\n",
      "Iteration 160, loss = 0.74341641\n",
      "Iteration 161, loss = 0.74160628\n",
      "Iteration 162, loss = 0.73980730\n",
      "Iteration 163, loss = 0.73801940\n",
      "Iteration 164, loss = 0.73624249\n",
      "Iteration 165, loss = 0.73447652\n",
      "Iteration 166, loss = 0.73272139\n",
      "Iteration 167, loss = 0.73097716\n",
      "Iteration 168, loss = 0.72924397\n",
      "Iteration 169, loss = 0.72752144\n",
      "Iteration 170, loss = 0.72580952\n",
      "Iteration 171, loss = 0.72410812\n",
      "Iteration 172, loss = 0.72241717\n",
      "Iteration 173, loss = 0.72073660\n",
      "Iteration 174, loss = 0.71906635\n",
      "Iteration 175, loss = 0.71740633\n",
      "Iteration 176, loss = 0.71575649\n",
      "Iteration 177, loss = 0.71411675\n",
      "Iteration 178, loss = 0.71248703\n",
      "Iteration 179, loss = 0.71086728\n",
      "Iteration 180, loss = 0.70925741\n",
      "Iteration 181, loss = 0.70765737\n",
      "Iteration 182, loss = 0.70606707\n",
      "Iteration 183, loss = 0.70448645\n",
      "Iteration 184, loss = 0.70291544\n",
      "Iteration 185, loss = 0.70135397\n",
      "Iteration 186, loss = 0.69980196\n",
      "Iteration 187, loss = 0.69825935\n",
      "Iteration 188, loss = 0.69672606\n",
      "Iteration 189, loss = 0.69520203\n",
      "Iteration 190, loss = 0.69368719\n",
      "Iteration 191, loss = 0.69218146\n",
      "Iteration 192, loss = 0.69068477\n",
      "Iteration 193, loss = 0.68919706\n",
      "Iteration 194, loss = 0.68771824\n",
      "Iteration 195, loss = 0.68624826\n",
      "Iteration 196, loss = 0.68478704\n",
      "Iteration 197, loss = 0.68333451\n",
      "Iteration 198, loss = 0.68189040\n",
      "Iteration 199, loss = 0.68045456\n",
      "Iteration 200, loss = 0.67902718\n",
      "Iteration 201, loss = 0.67760819\n",
      "Iteration 202, loss = 0.67619753\n",
      "Iteration 203, loss = 0.67479668\n",
      "Iteration 204, loss = 0.67340462\n",
      "Iteration 205, loss = 0.67202085\n",
      "Iteration 206, loss = 0.67064529\n",
      "Iteration 207, loss = 0.66927789\n",
      "Iteration 208, loss = 0.66791858\n",
      "Iteration 209, loss = 0.66656728\n",
      "Iteration 210, loss = 0.66522394\n",
      "Iteration 211, loss = 0.66388865\n",
      "Iteration 212, loss = 0.66256147\n",
      "Iteration 213, loss = 0.66124248\n",
      "Iteration 214, loss = 0.65993122\n",
      "Iteration 215, loss = 0.65862829\n",
      "Iteration 216, loss = 0.65733411\n",
      "Iteration 217, loss = 0.65604752\n",
      "Iteration 218, loss = 0.65476963\n",
      "Iteration 219, loss = 0.65349917\n",
      "Iteration 220, loss = 0.65223600\n",
      "Iteration 221, loss = 0.65098012\n",
      "Iteration 222, loss = 0.64973183\n",
      "Iteration 223, loss = 0.64849084\n",
      "Iteration 224, loss = 0.64725710\n",
      "Iteration 225, loss = 0.64603068\n",
      "Iteration 226, loss = 0.64481161\n",
      "Iteration 227, loss = 0.64359969\n",
      "Iteration 228, loss = 0.64239512\n",
      "Iteration 229, loss = 0.64119771\n",
      "Iteration 230, loss = 0.64000732\n",
      "Iteration 231, loss = 0.63882390\n",
      "Iteration 232, loss = 0.63764738\n",
      "Iteration 233, loss = 0.63647771\n",
      "Iteration 234, loss = 0.63531482\n",
      "Iteration 235, loss = 0.63415865\n",
      "Iteration 236, loss = 0.63300913\n",
      "Iteration 237, loss = 0.63186621\n",
      "Iteration 238, loss = 0.63072981\n",
      "Iteration 239, loss = 0.62959988\n",
      "Iteration 240, loss = 0.62847633\n",
      "Iteration 241, loss = 0.62735912\n",
      "Iteration 242, loss = 0.62624816\n",
      "Iteration 243, loss = 0.62514339\n",
      "Iteration 244, loss = 0.62404476\n",
      "Iteration 245, loss = 0.62295218\n",
      "Iteration 246, loss = 0.62186560\n",
      "Iteration 247, loss = 0.62078543\n",
      "Iteration 248, loss = 0.61971198\n",
      "Iteration 249, loss = 0.61864450\n",
      "Iteration 250, loss = 0.61758427\n",
      "Iteration 251, loss = 0.61652994\n",
      "Iteration 252, loss = 0.61548128\n",
      "Iteration 253, loss = 0.61443832\n",
      "Iteration 254, loss = 0.61340104\n",
      "Iteration 255, loss = 0.61236944\n",
      "Iteration 256, loss = 0.61134349\n",
      "Iteration 257, loss = 0.61032318\n",
      "Iteration 258, loss = 0.60930848\n",
      "Iteration 259, loss = 0.60829936\n",
      "Iteration 260, loss = 0.60729578\n",
      "Iteration 261, loss = 0.60629771\n",
      "Iteration 262, loss = 0.60530561\n",
      "Iteration 263, loss = 0.60431902\n",
      "Iteration 264, loss = 0.60333788\n",
      "Iteration 265, loss = 0.60236226\n",
      "Iteration 266, loss = 0.60139193\n",
      "Iteration 267, loss = 0.60042686\n",
      "Iteration 268, loss = 0.59946697\n",
      "Iteration 269, loss = 0.59851221\n",
      "Iteration 270, loss = 0.59756253\n",
      "Iteration 271, loss = 0.59661786\n",
      "Iteration 272, loss = 0.59567816\n",
      "Iteration 273, loss = 0.59474336\n",
      "Iteration 274, loss = 0.59381355\n",
      "Iteration 275, loss = 0.59288856\n",
      "Iteration 276, loss = 0.59196837\n",
      "Iteration 277, loss = 0.59105286\n",
      "Iteration 278, loss = 0.59014212\n",
      "Iteration 279, loss = 0.58923597\n",
      "Iteration 280, loss = 0.58833433\n",
      "Iteration 281, loss = 0.58743715\n",
      "Iteration 282, loss = 0.58654545\n",
      "Iteration 283, loss = 0.58565842\n",
      "Iteration 284, loss = 0.58477577\n",
      "Iteration 285, loss = 0.58389755\n",
      "Iteration 286, loss = 0.58302385\n",
      "Iteration 287, loss = 0.58215425\n",
      "Iteration 288, loss = 0.58128877\n",
      "Iteration 289, loss = 0.58042748\n",
      "Iteration 290, loss = 0.57957071\n",
      "Iteration 291, loss = 0.57871817\n",
      "Iteration 292, loss = 0.57786982\n",
      "Iteration 293, loss = 0.57702560\n",
      "Iteration 294, loss = 0.57618546\n",
      "Iteration 295, loss = 0.57534937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 296, loss = 0.57451727\n",
      "Iteration 297, loss = 0.57368911\n",
      "Iteration 298, loss = 0.57286484\n",
      "Iteration 299, loss = 0.57204443\n",
      "Iteration 300, loss = 0.57122816\n",
      "Iteration 301, loss = 0.57041567\n",
      "Iteration 302, loss = 0.56960692\n",
      "Iteration 303, loss = 0.56880191\n",
      "Iteration 304, loss = 0.56800059\n",
      "Iteration 305, loss = 0.56720296\n",
      "Iteration 306, loss = 0.56640897\n",
      "Iteration 307, loss = 0.56561861\n",
      "Iteration 308, loss = 0.56483184\n",
      "Iteration 309, loss = 0.56404863\n",
      "Iteration 310, loss = 0.56326893\n",
      "Iteration 311, loss = 0.56249273\n",
      "Iteration 312, loss = 0.56171997\n",
      "Iteration 313, loss = 0.56095062\n",
      "Iteration 314, loss = 0.56018464\n",
      "Iteration 315, loss = 0.55942200\n",
      "Iteration 316, loss = 0.55866264\n",
      "Iteration 317, loss = 0.55790654\n",
      "Iteration 318, loss = 0.55715365\n",
      "Iteration 319, loss = 0.55640393\n",
      "Iteration 320, loss = 0.55565741\n",
      "Iteration 321, loss = 0.55491400\n",
      "Iteration 322, loss = 0.55417364\n",
      "Iteration 323, loss = 0.55343631\n",
      "Iteration 324, loss = 0.55270194\n",
      "Iteration 325, loss = 0.55197052\n",
      "Iteration 326, loss = 0.55124198\n",
      "Iteration 327, loss = 0.55051630\n",
      "Iteration 328, loss = 0.54979344\n",
      "Iteration 329, loss = 0.54907336\n",
      "Iteration 330, loss = 0.54835601\n",
      "Iteration 331, loss = 0.54764227\n",
      "Iteration 332, loss = 0.54693129\n",
      "Iteration 333, loss = 0.54622303\n",
      "Iteration 334, loss = 0.54551747\n",
      "Iteration 335, loss = 0.54481457\n",
      "Iteration 336, loss = 0.54411438\n",
      "Iteration 337, loss = 0.54341686\n",
      "Iteration 338, loss = 0.54272194\n",
      "Iteration 339, loss = 0.54202960\n",
      "Iteration 340, loss = 0.54133980\n",
      "Iteration 341, loss = 0.54065252\n",
      "Iteration 342, loss = 0.53996773\n",
      "Iteration 343, loss = 0.53928539\n",
      "Iteration 344, loss = 0.53860548\n",
      "Iteration 345, loss = 0.53792803\n",
      "Iteration 346, loss = 0.53725298\n",
      "Iteration 347, loss = 0.53658028\n",
      "Iteration 348, loss = 0.53590989\n",
      "Iteration 349, loss = 0.53524177\n",
      "Iteration 350, loss = 0.53457591\n",
      "Iteration 351, loss = 0.53391225\n",
      "Iteration 352, loss = 0.53325078\n",
      "Iteration 353, loss = 0.53259243\n",
      "Iteration 354, loss = 0.53193635\n",
      "Iteration 355, loss = 0.53128242\n",
      "Iteration 356, loss = 0.53063065\n",
      "Iteration 357, loss = 0.52998100\n",
      "Iteration 358, loss = 0.52933348\n",
      "Iteration 359, loss = 0.52868805\n",
      "Iteration 360, loss = 0.52804471\n",
      "Iteration 361, loss = 0.52740342\n",
      "Iteration 362, loss = 0.52676418\n",
      "Iteration 363, loss = 0.52612694\n",
      "Iteration 364, loss = 0.52549180\n",
      "Iteration 365, loss = 0.52485864\n",
      "Iteration 366, loss = 0.52422744\n",
      "Iteration 367, loss = 0.52359817\n",
      "Iteration 368, loss = 0.52297080\n",
      "Iteration 369, loss = 0.52234530\n",
      "Iteration 370, loss = 0.52172205\n",
      "Iteration 371, loss = 0.52110099\n",
      "Iteration 372, loss = 0.52048181\n",
      "Iteration 373, loss = 0.51986446\n",
      "Iteration 374, loss = 0.51924893\n",
      "Iteration 375, loss = 0.51863522\n",
      "Iteration 376, loss = 0.51802331\n",
      "Iteration 377, loss = 0.51741318\n",
      "Iteration 378, loss = 0.51680483\n",
      "Iteration 379, loss = 0.51619827\n",
      "Iteration 380, loss = 0.51559344\n",
      "Iteration 381, loss = 0.51499033\n",
      "Iteration 382, loss = 0.51438893\n",
      "Iteration 383, loss = 0.51378920\n",
      "Iteration 384, loss = 0.51319113\n",
      "Iteration 385, loss = 0.51259469\n",
      "Iteration 386, loss = 0.51199987\n",
      "Iteration 387, loss = 0.51140663\n",
      "Iteration 388, loss = 0.51081496\n",
      "Iteration 389, loss = 0.51022484\n",
      "Iteration 390, loss = 0.50963635\n",
      "Iteration 391, loss = 0.50904986\n",
      "Iteration 392, loss = 0.50846487\n",
      "Iteration 393, loss = 0.50788138\n",
      "Iteration 394, loss = 0.50729938\n",
      "Iteration 395, loss = 0.50671887\n",
      "Iteration 396, loss = 0.50613984\n",
      "Iteration 397, loss = 0.50556228\n",
      "Iteration 398, loss = 0.50498618\n",
      "Iteration 399, loss = 0.50441152\n",
      "Iteration 400, loss = 0.50383829\n",
      "Iteration 401, loss = 0.50326647\n",
      "Iteration 402, loss = 0.50269605\n",
      "Iteration 403, loss = 0.50212701\n",
      "Iteration 404, loss = 0.50155933\n",
      "Iteration 405, loss = 0.50099299\n",
      "Iteration 406, loss = 0.50042797\n",
      "Iteration 407, loss = 0.49986426\n",
      "Iteration 408, loss = 0.49930183\n",
      "Iteration 409, loss = 0.49874067\n",
      "Iteration 410, loss = 0.49818074\n",
      "Iteration 411, loss = 0.49762204\n",
      "Iteration 412, loss = 0.49706455\n",
      "Iteration 413, loss = 0.49650823\n",
      "Iteration 414, loss = 0.49595310\n",
      "Iteration 415, loss = 0.49539928\n",
      "Iteration 416, loss = 0.49484662\n",
      "Iteration 417, loss = 0.49429507\n",
      "Iteration 418, loss = 0.49374463\n",
      "Iteration 419, loss = 0.49319526\n",
      "Iteration 420, loss = 0.49264695\n",
      "Iteration 421, loss = 0.49209967\n",
      "Iteration 422, loss = 0.49155341\n",
      "Iteration 423, loss = 0.49100816\n",
      "Iteration 424, loss = 0.49046387\n",
      "Iteration 425, loss = 0.48992055\n",
      "Iteration 426, loss = 0.48937817\n",
      "Iteration 427, loss = 0.48883672\n",
      "Iteration 428, loss = 0.48829616\n",
      "Iteration 429, loss = 0.48775650\n",
      "Iteration 430, loss = 0.48721770\n",
      "Iteration 431, loss = 0.48667975\n",
      "Iteration 432, loss = 0.48614263\n",
      "Iteration 433, loss = 0.48560635\n",
      "Iteration 434, loss = 0.48507098\n",
      "Iteration 435, loss = 0.48453639\n",
      "Iteration 436, loss = 0.48400259\n",
      "Iteration 437, loss = 0.48346954\n",
      "Iteration 438, loss = 0.48293724\n",
      "Iteration 439, loss = 0.48240567\n",
      "Iteration 440, loss = 0.48187483\n",
      "Iteration 441, loss = 0.48134477\n",
      "Iteration 442, loss = 0.48081539\n",
      "Iteration 443, loss = 0.48028670\n",
      "Iteration 444, loss = 0.47975920\n",
      "Iteration 445, loss = 0.47923406\n",
      "Iteration 446, loss = 0.47870967\n",
      "Iteration 447, loss = 0.47818603\n",
      "Iteration 448, loss = 0.47766313\n",
      "Iteration 449, loss = 0.47714095\n",
      "Iteration 450, loss = 0.47661949\n",
      "Iteration 451, loss = 0.47609880\n",
      "Iteration 452, loss = 0.47557881\n",
      "Iteration 453, loss = 0.47505951\n",
      "Iteration 454, loss = 0.47454089\n",
      "Iteration 455, loss = 0.47402293\n",
      "Iteration 456, loss = 0.47350563\n",
      "Iteration 457, loss = 0.47298897\n",
      "Iteration 458, loss = 0.47247294\n",
      "Iteration 459, loss = 0.47195752\n",
      "Iteration 460, loss = 0.47144271\n",
      "Iteration 461, loss = 0.47092849\n",
      "Iteration 462, loss = 0.47041484\n",
      "Iteration 463, loss = 0.46990177\n",
      "Iteration 464, loss = 0.46938924\n",
      "Iteration 465, loss = 0.46887725\n",
      "Iteration 466, loss = 0.46836579\n",
      "Iteration 467, loss = 0.46785484\n",
      "Iteration 468, loss = 0.46734438\n",
      "Iteration 469, loss = 0.46683585\n",
      "Iteration 470, loss = 0.46632821\n",
      "Iteration 471, loss = 0.46582138\n",
      "Iteration 472, loss = 0.46531581\n",
      "Iteration 473, loss = 0.46481038\n",
      "Iteration 474, loss = 0.46430521\n",
      "Iteration 475, loss = 0.46380036\n",
      "Iteration 476, loss = 0.46329584\n",
      "Iteration 477, loss = 0.46279171\n",
      "Iteration 478, loss = 0.46228802\n",
      "Iteration 479, loss = 0.46178485\n",
      "Iteration 480, loss = 0.46128237\n",
      "Iteration 481, loss = 0.46078131\n",
      "Iteration 482, loss = 0.46028082\n",
      "Iteration 483, loss = 0.45978097\n",
      "Iteration 484, loss = 0.45928182\n",
      "Iteration 485, loss = 0.45878320\n",
      "Iteration 486, loss = 0.45828510\n",
      "Iteration 487, loss = 0.45778751\n",
      "Iteration 488, loss = 0.45729040\n",
      "Iteration 489, loss = 0.45679376\n",
      "Iteration 490, loss = 0.45629757\n",
      "Iteration 491, loss = 0.45580182\n",
      "Iteration 492, loss = 0.45530649\n",
      "Iteration 493, loss = 0.45481156\n",
      "Iteration 494, loss = 0.45431754\n",
      "Iteration 495, loss = 0.45382410\n",
      "Iteration 496, loss = 0.45333103\n",
      "Iteration 497, loss = 0.45283835\n",
      "Iteration 498, loss = 0.45234606\n",
      "Iteration 499, loss = 0.45185417\n",
      "Iteration 500, loss = 0.45136269\n",
      "Iteration 501, loss = 0.45087161\n",
      "Iteration 502, loss = 0.45038094\n",
      "Iteration 503, loss = 0.44989067\n",
      "Iteration 504, loss = 0.44940081\n",
      "Iteration 505, loss = 0.44891135\n",
      "Iteration 506, loss = 0.44842229\n",
      "Iteration 507, loss = 0.44793361\n",
      "Iteration 508, loss = 0.44744532\n",
      "Iteration 509, loss = 0.44695754\n",
      "Iteration 510, loss = 0.44646997\n",
      "Iteration 511, loss = 0.44598269\n",
      "Iteration 512, loss = 0.44549583\n",
      "Iteration 513, loss = 0.44500930\n",
      "Iteration 514, loss = 0.44452309\n",
      "Iteration 515, loss = 0.44403719\n",
      "Iteration 516, loss = 0.44355160\n",
      "Iteration 517, loss = 0.44306630\n",
      "Iteration 518, loss = 0.44258129\n",
      "Iteration 519, loss = 0.44209657\n",
      "Iteration 520, loss = 0.44161211\n",
      "Iteration 521, loss = 0.44112793\n",
      "Iteration 522, loss = 0.44064399\n",
      "Iteration 523, loss = 0.44016031\n",
      "Iteration 524, loss = 0.43967686\n",
      "Iteration 525, loss = 0.43919365\n",
      "Iteration 526, loss = 0.43871065\n",
      "Iteration 527, loss = 0.43822806\n",
      "Iteration 528, loss = 0.43774573\n",
      "Iteration 529, loss = 0.43726363\n",
      "Iteration 530, loss = 0.43678173\n",
      "Iteration 531, loss = 0.43630003\n",
      "Iteration 532, loss = 0.43581852\n",
      "Iteration 533, loss = 0.43533719\n",
      "Iteration 534, loss = 0.43485603\n",
      "Iteration 535, loss = 0.43437503\n",
      "Iteration 536, loss = 0.43389418\n",
      "Iteration 537, loss = 0.43341348\n",
      "Iteration 538, loss = 0.43293291\n",
      "Iteration 539, loss = 0.43245247\n",
      "Iteration 540, loss = 0.43197215\n",
      "Iteration 541, loss = 0.43149194\n",
      "Iteration 542, loss = 0.43101184\n",
      "Iteration 543, loss = 0.43053183\n",
      "Iteration 544, loss = 0.43005194\n",
      "Iteration 545, loss = 0.42957229\n",
      "Iteration 546, loss = 0.42909274\n",
      "Iteration 547, loss = 0.42861327\n",
      "Iteration 548, loss = 0.42813388\n",
      "Iteration 549, loss = 0.42765455\n",
      "Iteration 550, loss = 0.42717529\n",
      "Iteration 551, loss = 0.42669685\n",
      "Iteration 552, loss = 0.42621886\n",
      "Iteration 553, loss = 0.42574090\n",
      "Iteration 554, loss = 0.42526298\n",
      "Iteration 555, loss = 0.42478511\n",
      "Iteration 556, loss = 0.42430728\n",
      "Iteration 557, loss = 0.42382952\n",
      "Iteration 558, loss = 0.42335182\n",
      "Iteration 559, loss = 0.42287418\n",
      "Iteration 560, loss = 0.42239660\n",
      "Iteration 561, loss = 0.42191909\n",
      "Iteration 562, loss = 0.42144164\n",
      "Iteration 563, loss = 0.42096427\n",
      "Iteration 564, loss = 0.42048695\n",
      "Iteration 565, loss = 0.42000970\n",
      "Iteration 566, loss = 0.41953251\n",
      "Iteration 567, loss = 0.41905537\n",
      "Iteration 568, loss = 0.41857830\n",
      "Iteration 569, loss = 0.41810127\n",
      "Iteration 570, loss = 0.41762430\n",
      "Iteration 571, loss = 0.41714736\n",
      "Iteration 572, loss = 0.41667047\n",
      "Iteration 573, loss = 0.41619362\n",
      "Iteration 574, loss = 0.41571680\n",
      "Iteration 575, loss = 0.41524000\n",
      "Iteration 576, loss = 0.41476323\n",
      "Iteration 577, loss = 0.41428648\n",
      "Iteration 578, loss = 0.41380974\n",
      "Iteration 579, loss = 0.41333320\n",
      "Iteration 580, loss = 0.41285672\n",
      "Iteration 581, loss = 0.41238026\n",
      "Iteration 582, loss = 0.41190381\n",
      "Iteration 583, loss = 0.41142736\n",
      "Iteration 584, loss = 0.41095092\n",
      "Iteration 585, loss = 0.41047447\n",
      "Iteration 586, loss = 0.40999802\n",
      "Iteration 587, loss = 0.40952154\n",
      "Iteration 588, loss = 0.40904505\n",
      "Iteration 589, loss = 0.40856853\n",
      "Iteration 590, loss = 0.40809198\n",
      "Iteration 591, loss = 0.40761539\n",
      "Iteration 592, loss = 0.40713876\n",
      "Iteration 593, loss = 0.40666208\n",
      "Iteration 594, loss = 0.40618536\n",
      "Iteration 595, loss = 0.40570858\n",
      "Iteration 596, loss = 0.40523175\n",
      "Iteration 597, loss = 0.40475485\n",
      "Iteration 598, loss = 0.40427788\n",
      "Iteration 599, loss = 0.40380085\n",
      "Iteration 600, loss = 0.40332374\n",
      "Iteration 601, loss = 0.40284656\n",
      "Iteration 602, loss = 0.40236930\n",
      "Iteration 603, loss = 0.40189196\n",
      "Iteration 604, loss = 0.40141453\n",
      "Iteration 605, loss = 0.40093701\n",
      "Iteration 606, loss = 0.40045940\n",
      "Iteration 607, loss = 0.39998169\n",
      "Iteration 608, loss = 0.39950496\n",
      "Iteration 609, loss = 0.39902817\n",
      "Iteration 610, loss = 0.39855124\n",
      "Iteration 611, loss = 0.39807418\n",
      "Iteration 612, loss = 0.39759702\n",
      "Iteration 613, loss = 0.39711974\n",
      "Iteration 614, loss = 0.39664238\n",
      "Iteration 615, loss = 0.39616492\n",
      "Iteration 616, loss = 0.39568738\n",
      "Iteration 617, loss = 0.39520976\n",
      "Iteration 618, loss = 0.39473235\n",
      "Iteration 619, loss = 0.39425540\n",
      "Iteration 620, loss = 0.39377844\n",
      "Iteration 621, loss = 0.39330148\n",
      "Iteration 622, loss = 0.39282452\n",
      "Iteration 623, loss = 0.39234754\n",
      "Iteration 624, loss = 0.39187055\n",
      "Iteration 625, loss = 0.39139354\n",
      "Iteration 626, loss = 0.39091651\n",
      "Iteration 627, loss = 0.39043954\n",
      "Iteration 628, loss = 0.38996281\n",
      "Iteration 629, loss = 0.38948608\n",
      "Iteration 630, loss = 0.38900934\n",
      "Iteration 631, loss = 0.38853258\n",
      "Iteration 632, loss = 0.38805581\n",
      "Iteration 633, loss = 0.38757906\n",
      "Iteration 634, loss = 0.38710245\n",
      "Iteration 635, loss = 0.38662583\n",
      "Iteration 636, loss = 0.38614919\n",
      "Iteration 637, loss = 0.38567251\n",
      "Iteration 638, loss = 0.38519581\n",
      "Iteration 639, loss = 0.38471906\n",
      "Iteration 640, loss = 0.38424228\n",
      "Iteration 641, loss = 0.38376545\n",
      "Iteration 642, loss = 0.38328857\n",
      "Iteration 643, loss = 0.38281164\n",
      "Iteration 644, loss = 0.38233464\n",
      "Iteration 645, loss = 0.38185759\n",
      "Iteration 646, loss = 0.38138048\n",
      "Iteration 647, loss = 0.38090330\n",
      "Iteration 648, loss = 0.38042605\n",
      "Iteration 649, loss = 0.37994874\n",
      "Iteration 650, loss = 0.37947135\n",
      "Iteration 651, loss = 0.37899389\n",
      "Iteration 652, loss = 0.37851635\n",
      "Iteration 653, loss = 0.37803873\n",
      "Iteration 654, loss = 0.37756104\n",
      "Iteration 655, loss = 0.37708326\n",
      "Iteration 656, loss = 0.37660540\n",
      "Iteration 657, loss = 0.37612746\n",
      "Iteration 658, loss = 0.37564944\n",
      "Iteration 659, loss = 0.37517133\n",
      "Iteration 660, loss = 0.37469314\n",
      "Iteration 661, loss = 0.37421486\n",
      "Iteration 662, loss = 0.37373649\n",
      "Iteration 663, loss = 0.37325803\n",
      "Iteration 664, loss = 0.37277949\n",
      "Iteration 665, loss = 0.37230085\n",
      "Iteration 666, loss = 0.37182213\n",
      "Iteration 667, loss = 0.37134332\n",
      "Iteration 668, loss = 0.37086443\n",
      "Iteration 669, loss = 0.37038545\n",
      "Iteration 670, loss = 0.36990636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 671, loss = 0.36942720\n",
      "Iteration 672, loss = 0.36894794\n",
      "Iteration 673, loss = 0.36846860\n",
      "Iteration 674, loss = 0.36798917\n",
      "Iteration 675, loss = 0.36750965\n",
      "Iteration 676, loss = 0.36703004\n",
      "Iteration 677, loss = 0.36655035\n",
      "Iteration 678, loss = 0.36607057\n",
      "Iteration 679, loss = 0.36559071\n",
      "Iteration 680, loss = 0.36511076\n",
      "Iteration 681, loss = 0.36463073\n",
      "Iteration 682, loss = 0.36415061\n",
      "Iteration 683, loss = 0.36367042\n",
      "Iteration 684, loss = 0.36319103\n",
      "Iteration 685, loss = 0.36271150\n",
      "Iteration 686, loss = 0.36223179\n",
      "Iteration 687, loss = 0.36175193\n",
      "Iteration 688, loss = 0.36127192\n",
      "Iteration 689, loss = 0.36079178\n",
      "Iteration 690, loss = 0.36031154\n",
      "Iteration 691, loss = 0.35983119\n",
      "Iteration 692, loss = 0.35935075\n",
      "Iteration 693, loss = 0.35887023\n",
      "Iteration 694, loss = 0.35838964\n",
      "Iteration 695, loss = 0.35790900\n",
      "Iteration 696, loss = 0.35742830\n",
      "Iteration 697, loss = 0.35694778\n",
      "Iteration 698, loss = 0.35646749\n",
      "Iteration 699, loss = 0.35598706\n",
      "Iteration 700, loss = 0.35550650\n",
      "Iteration 701, loss = 0.35502582\n",
      "Iteration 702, loss = 0.35454502\n",
      "Iteration 703, loss = 0.35406410\n",
      "Iteration 704, loss = 0.35358337\n",
      "Iteration 705, loss = 0.35310275\n",
      "Iteration 706, loss = 0.35262207\n",
      "Iteration 707, loss = 0.35214132\n",
      "Iteration 708, loss = 0.35166052\n",
      "Iteration 709, loss = 0.35117968\n",
      "Iteration 710, loss = 0.35069880\n",
      "Iteration 711, loss = 0.35021790\n",
      "Iteration 712, loss = 0.34973696\n",
      "Iteration 713, loss = 0.34925601\n",
      "Iteration 714, loss = 0.34877522\n",
      "Iteration 715, loss = 0.34829443\n",
      "Iteration 716, loss = 0.34781350\n",
      "Iteration 717, loss = 0.34733245\n",
      "Iteration 718, loss = 0.34685161\n",
      "Iteration 719, loss = 0.34637074\n",
      "Iteration 720, loss = 0.34588987\n",
      "Iteration 721, loss = 0.34540899\n",
      "Iteration 722, loss = 0.34492812\n",
      "Iteration 723, loss = 0.34444726\n",
      "Iteration 724, loss = 0.34396643\n",
      "Iteration 725, loss = 0.34348561\n",
      "Iteration 726, loss = 0.34300481\n",
      "Iteration 727, loss = 0.34252409\n",
      "Iteration 728, loss = 0.34204330\n",
      "Iteration 729, loss = 0.34156259\n",
      "Iteration 730, loss = 0.34108190\n",
      "Iteration 731, loss = 0.34060123\n",
      "Iteration 732, loss = 0.34012059\n",
      "Iteration 733, loss = 0.33964003\n",
      "Iteration 734, loss = 0.33915945\n",
      "Iteration 735, loss = 0.33867895\n",
      "Iteration 736, loss = 0.33819849\n",
      "Iteration 737, loss = 0.33771807\n",
      "Iteration 738, loss = 0.33723769\n",
      "Iteration 739, loss = 0.33675737\n",
      "Iteration 740, loss = 0.33627710\n",
      "Iteration 741, loss = 0.33579696\n",
      "Iteration 742, loss = 0.33531675\n",
      "Iteration 743, loss = 0.33483667\n",
      "Iteration 744, loss = 0.33435666\n",
      "Iteration 745, loss = 0.33387672\n",
      "Iteration 746, loss = 0.33339684\n",
      "Iteration 747, loss = 0.33291704\n",
      "Iteration 748, loss = 0.33243731\n",
      "Iteration 749, loss = 0.33195766\n",
      "Iteration 750, loss = 0.33147810\n",
      "Iteration 751, loss = 0.33099862\n",
      "Iteration 752, loss = 0.33051924\n",
      "Iteration 753, loss = 0.33003995\n",
      "Iteration 754, loss = 0.32956076\n",
      "Iteration 755, loss = 0.32908166\n",
      "Iteration 756, loss = 0.32860267\n",
      "Iteration 757, loss = 0.32812377\n",
      "Iteration 758, loss = 0.32764499\n",
      "Iteration 759, loss = 0.32716631\n",
      "Iteration 760, loss = 0.32668775\n",
      "Iteration 761, loss = 0.32620930\n",
      "Iteration 762, loss = 0.32573097\n",
      "Iteration 763, loss = 0.32525277\n",
      "Iteration 764, loss = 0.32477469\n",
      "Iteration 765, loss = 0.32429674\n",
      "Iteration 766, loss = 0.32381892\n",
      "Iteration 767, loss = 0.32334124\n",
      "Iteration 768, loss = 0.32286370\n",
      "Iteration 769, loss = 0.32238631\n",
      "Iteration 770, loss = 0.32190905\n",
      "Iteration 771, loss = 0.32143195\n",
      "Iteration 772, loss = 0.32095500\n",
      "Iteration 773, loss = 0.32047820\n",
      "Iteration 774, loss = 0.32000157\n",
      "Iteration 775, loss = 0.31952509\n",
      "Iteration 776, loss = 0.31904879\n",
      "Iteration 777, loss = 0.31857265\n",
      "Iteration 778, loss = 0.31809668\n",
      "Iteration 779, loss = 0.31762089\n",
      "Iteration 780, loss = 0.31714528\n",
      "Iteration 781, loss = 0.31666985\n",
      "Iteration 782, loss = 0.31619460\n",
      "Iteration 783, loss = 0.31571955\n",
      "Iteration 784, loss = 0.31524469\n",
      "Iteration 785, loss = 0.31477002\n",
      "Iteration 786, loss = 0.31429556\n",
      "Iteration 787, loss = 0.31382129\n",
      "Iteration 788, loss = 0.31334724\n",
      "Iteration 789, loss = 0.31287339\n",
      "Iteration 790, loss = 0.31239976\n",
      "Iteration 791, loss = 0.31192634\n",
      "Iteration 792, loss = 0.31145314\n",
      "Iteration 793, loss = 0.31098017\n",
      "Iteration 794, loss = 0.31050743\n",
      "Iteration 795, loss = 0.31003491\n",
      "Iteration 796, loss = 0.30956264\n",
      "Iteration 797, loss = 0.30909059\n",
      "Iteration 798, loss = 0.30861880\n",
      "Iteration 799, loss = 0.30814724\n",
      "Iteration 800, loss = 0.30767594\n",
      "Iteration 801, loss = 0.30720489\n",
      "Iteration 802, loss = 0.30673409\n",
      "Iteration 803, loss = 0.30626356\n",
      "Iteration 804, loss = 0.30579328\n",
      "Iteration 805, loss = 0.30532328\n",
      "Iteration 806, loss = 0.30485354\n",
      "Iteration 807, loss = 0.30438408\n",
      "Iteration 808, loss = 0.30391490\n",
      "Iteration 809, loss = 0.30344600\n",
      "Iteration 810, loss = 0.30297739\n",
      "Iteration 811, loss = 0.30250906\n",
      "Iteration 812, loss = 0.30204103\n",
      "Iteration 813, loss = 0.30157329\n",
      "Iteration 814, loss = 0.30110585\n",
      "Iteration 815, loss = 0.30063872\n",
      "Iteration 816, loss = 0.30017189\n",
      "Iteration 817, loss = 0.29970538\n",
      "Iteration 818, loss = 0.29923918\n",
      "Iteration 819, loss = 0.29877329\n",
      "Iteration 820, loss = 0.29830773\n",
      "Iteration 821, loss = 0.29784250\n",
      "Iteration 822, loss = 0.29737759\n",
      "Iteration 823, loss = 0.29691301\n",
      "Iteration 824, loss = 0.29644878\n",
      "Iteration 825, loss = 0.29598488\n",
      "Iteration 826, loss = 0.29552133\n",
      "Iteration 827, loss = 0.29505812\n",
      "Iteration 828, loss = 0.29459527\n",
      "Iteration 829, loss = 0.29413276\n",
      "Iteration 830, loss = 0.29367062\n",
      "Iteration 831, loss = 0.29320884\n",
      "Iteration 832, loss = 0.29274743\n",
      "Iteration 833, loss = 0.29228638\n",
      "Iteration 834, loss = 0.29182571\n",
      "Iteration 835, loss = 0.29136541\n",
      "Iteration 836, loss = 0.29090550\n",
      "Iteration 837, loss = 0.29044596\n",
      "Iteration 838, loss = 0.28998682\n",
      "Iteration 839, loss = 0.28952806\n",
      "Iteration 840, loss = 0.28906970\n",
      "Iteration 841, loss = 0.28861174\n",
      "Iteration 842, loss = 0.28815418\n",
      "Iteration 843, loss = 0.28769702\n",
      "Iteration 844, loss = 0.28724027\n",
      "Iteration 845, loss = 0.28678393\n",
      "Iteration 846, loss = 0.28632801\n",
      "Iteration 847, loss = 0.28587251\n",
      "Iteration 848, loss = 0.28541743\n",
      "Iteration 849, loss = 0.28496278\n",
      "Iteration 850, loss = 0.28450855\n",
      "Iteration 851, loss = 0.28405476\n",
      "Iteration 852, loss = 0.28360141\n",
      "Iteration 853, loss = 0.28314849\n",
      "Iteration 854, loss = 0.28269602\n",
      "Iteration 855, loss = 0.28224400\n",
      "Iteration 856, loss = 0.28179242\n",
      "Iteration 857, loss = 0.28134130\n",
      "Iteration 858, loss = 0.28089063\n",
      "Iteration 859, loss = 0.28044043\n",
      "Iteration 860, loss = 0.27999069\n",
      "Iteration 861, loss = 0.27954141\n",
      "Iteration 862, loss = 0.27909261\n",
      "Iteration 863, loss = 0.27864428\n",
      "Iteration 864, loss = 0.27819642\n",
      "Iteration 865, loss = 0.27774905\n",
      "Iteration 866, loss = 0.27730216\n",
      "Iteration 867, loss = 0.27685575\n",
      "Iteration 868, loss = 0.27640984\n",
      "Iteration 869, loss = 0.27596442\n",
      "Iteration 870, loss = 0.27551949\n",
      "Iteration 871, loss = 0.27507507\n",
      "Iteration 872, loss = 0.27463114\n",
      "Iteration 873, loss = 0.27418773\n",
      "Iteration 874, loss = 0.27374482\n",
      "Iteration 875, loss = 0.27330242\n",
      "Iteration 876, loss = 0.27286054\n",
      "Iteration 877, loss = 0.27241918\n",
      "Iteration 878, loss = 0.27197834\n",
      "Iteration 879, loss = 0.27153802\n",
      "Iteration 880, loss = 0.27109823\n",
      "Iteration 881, loss = 0.27065897\n",
      "Iteration 882, loss = 0.27022024\n",
      "Iteration 883, loss = 0.26978205\n",
      "Iteration 884, loss = 0.26934440\n",
      "Iteration 885, loss = 0.26890729\n",
      "Iteration 886, loss = 0.26847073\n",
      "Iteration 887, loss = 0.26803472\n",
      "Iteration 888, loss = 0.26759925\n",
      "Iteration 889, loss = 0.26716434\n",
      "Iteration 890, loss = 0.26672999\n",
      "Iteration 891, loss = 0.26629620\n",
      "Iteration 892, loss = 0.26586297\n",
      "Iteration 893, loss = 0.26543030\n",
      "Iteration 894, loss = 0.26499820\n",
      "Iteration 895, loss = 0.26456668\n",
      "Iteration 896, loss = 0.26413572\n",
      "Iteration 897, loss = 0.26370535\n",
      "Iteration 898, loss = 0.26327555\n",
      "Iteration 899, loss = 0.26284633\n",
      "Iteration 900, loss = 0.26241770\n",
      "Iteration 901, loss = 0.26198965\n",
      "Iteration 902, loss = 0.26156220\n",
      "Iteration 903, loss = 0.26113533\n",
      "Iteration 904, loss = 0.26070907\n",
      "Iteration 905, loss = 0.26028339\n",
      "Iteration 906, loss = 0.25985832\n",
      "Iteration 907, loss = 0.25943385\n",
      "Iteration 908, loss = 0.25900999\n",
      "Iteration 909, loss = 0.25858673\n",
      "Iteration 910, loss = 0.25816408\n",
      "Iteration 911, loss = 0.25774205\n",
      "Iteration 912, loss = 0.25732062\n",
      "Iteration 913, loss = 0.25689982\n",
      "Iteration 914, loss = 0.25647963\n",
      "Iteration 915, loss = 0.25606007\n",
      "Iteration 916, loss = 0.25564113\n",
      "Iteration 917, loss = 0.25522282\n",
      "Iteration 918, loss = 0.25480513\n",
      "Iteration 919, loss = 0.25438808\n",
      "Iteration 920, loss = 0.25397166\n",
      "Iteration 921, loss = 0.25355587\n",
      "Iteration 922, loss = 0.25314072\n",
      "Iteration 923, loss = 0.25272621\n",
      "Iteration 924, loss = 0.25231234\n",
      "Iteration 925, loss = 0.25189912\n",
      "Iteration 926, loss = 0.25148654\n",
      "Iteration 927, loss = 0.25107461\n",
      "Iteration 928, loss = 0.25066333\n",
      "Iteration 929, loss = 0.25025270\n",
      "Iteration 930, loss = 0.24984273\n",
      "Iteration 931, loss = 0.24943341\n",
      "Iteration 932, loss = 0.24902475\n",
      "Iteration 933, loss = 0.24861675\n",
      "Iteration 934, loss = 0.24820941\n",
      "Iteration 935, loss = 0.24780273\n",
      "Iteration 936, loss = 0.24739672\n",
      "Iteration 937, loss = 0.24699138\n",
      "Iteration 938, loss = 0.24658671\n",
      "Iteration 939, loss = 0.24618270\n",
      "Iteration 940, loss = 0.24577938\n",
      "Iteration 941, loss = 0.24537672\n",
      "Iteration 942, loss = 0.24497474\n",
      "Iteration 943, loss = 0.24457344\n",
      "Iteration 944, loss = 0.24417282\n",
      "Iteration 945, loss = 0.24377288\n",
      "Iteration 946, loss = 0.24337362\n",
      "Iteration 947, loss = 0.24297505\n",
      "Iteration 948, loss = 0.24257716\n",
      "Iteration 949, loss = 0.24217997\n",
      "Iteration 950, loss = 0.24178346\n",
      "Iteration 951, loss = 0.24138766\n",
      "Iteration 952, loss = 0.24099254\n",
      "Iteration 953, loss = 0.24059812\n",
      "Iteration 954, loss = 0.24020440\n",
      "Iteration 955, loss = 0.23981138\n",
      "Iteration 956, loss = 0.23941905\n",
      "Iteration 957, loss = 0.23902742\n",
      "Iteration 958, loss = 0.23863650\n",
      "Iteration 959, loss = 0.23824627\n",
      "Iteration 960, loss = 0.23785675\n",
      "Iteration 961, loss = 0.23746794\n",
      "Iteration 962, loss = 0.23707983\n",
      "Iteration 963, loss = 0.23669243\n",
      "Iteration 964, loss = 0.23630574\n",
      "Iteration 965, loss = 0.23591976\n",
      "Iteration 966, loss = 0.23553449\n",
      "Iteration 967, loss = 0.23514993\n",
      "Iteration 968, loss = 0.23476608\n",
      "Iteration 969, loss = 0.23438295\n",
      "Iteration 970, loss = 0.23400053\n",
      "Iteration 971, loss = 0.23361883\n",
      "Iteration 972, loss = 0.23323785\n",
      "Iteration 973, loss = 0.23285759\n",
      "Iteration 974, loss = 0.23247804\n",
      "Iteration 975, loss = 0.23209922\n",
      "Iteration 976, loss = 0.23172112\n",
      "Iteration 977, loss = 0.23134374\n",
      "Iteration 978, loss = 0.23096708\n",
      "Iteration 979, loss = 0.23059115\n",
      "Iteration 980, loss = 0.23021594\n",
      "Iteration 981, loss = 0.22984146\n",
      "Iteration 982, loss = 0.22946771\n",
      "Iteration 983, loss = 0.22909468\n",
      "Iteration 984, loss = 0.22872238\n",
      "Iteration 985, loss = 0.22835082\n",
      "Iteration 986, loss = 0.22797998\n",
      "Iteration 987, loss = 0.22760987\n",
      "Iteration 988, loss = 0.22724049\n",
      "Iteration 989, loss = 0.22687185\n",
      "Iteration 990, loss = 0.22650393\n",
      "Iteration 991, loss = 0.22613675\n",
      "Iteration 992, loss = 0.22577031\n",
      "Iteration 993, loss = 0.22540460\n",
      "Iteration 994, loss = 0.22503962\n",
      "Iteration 995, loss = 0.22467538\n",
      "Iteration 996, loss = 0.22431188\n",
      "Iteration 997, loss = 0.22394912\n",
      "Iteration 998, loss = 0.22358709\n",
      "Iteration 999, loss = 0.22322580\n",
      "Iteration 1000, loss = 0.22286524\n",
      "Iteration 1, loss = 2.12500615\n",
      "Iteration 2, loss = 2.09294974\n",
      "Iteration 3, loss = 2.06117947\n",
      "Iteration 4, loss = 2.02971000\n",
      "Iteration 5, loss = 1.99855045\n",
      "Iteration 6, loss = 1.96772280\n",
      "Iteration 7, loss = 1.93723647\n",
      "Iteration 8, loss = 1.90711296\n",
      "Iteration 9, loss = 1.87736513\n",
      "Iteration 10, loss = 1.84800415\n",
      "Iteration 11, loss = 1.81904406\n",
      "Iteration 12, loss = 1.79049159\n",
      "Iteration 13, loss = 1.76237010\n",
      "Iteration 14, loss = 1.73468317\n",
      "Iteration 15, loss = 1.70744398\n",
      "Iteration 16, loss = 1.68068301\n",
      "Iteration 17, loss = 1.65440262\n",
      "Iteration 18, loss = 1.62858414\n",
      "Iteration 19, loss = 1.60325735\n",
      "Iteration 20, loss = 1.57844333\n",
      "Iteration 21, loss = 1.55414875\n",
      "Iteration 22, loss = 1.53040610\n",
      "Iteration 23, loss = 1.50720252\n",
      "Iteration 24, loss = 1.48453851\n",
      "Iteration 25, loss = 1.46245469\n",
      "Iteration 26, loss = 1.44094520\n",
      "Iteration 27, loss = 1.42002064\n",
      "Iteration 28, loss = 1.39968862\n",
      "Iteration 29, loss = 1.37995094\n",
      "Iteration 30, loss = 1.36080262\n",
      "Iteration 31, loss = 1.34223061\n",
      "Iteration 32, loss = 1.32423374\n",
      "Iteration 33, loss = 1.30684586\n",
      "Iteration 34, loss = 1.29005473\n",
      "Iteration 35, loss = 1.27380647\n",
      "Iteration 36, loss = 1.25814243\n",
      "Iteration 37, loss = 1.24308085\n",
      "Iteration 38, loss = 1.22861692\n",
      "Iteration 39, loss = 1.21471901\n",
      "Iteration 40, loss = 1.20135254\n",
      "Iteration 41, loss = 1.18853179\n",
      "Iteration 42, loss = 1.17623204\n",
      "Iteration 43, loss = 1.16443885\n",
      "Iteration 44, loss = 1.15316341\n",
      "Iteration 45, loss = 1.14241200\n",
      "Iteration 46, loss = 1.13216573\n",
      "Iteration 47, loss = 1.12240465\n",
      "Iteration 48, loss = 1.11310796\n",
      "Iteration 49, loss = 1.10425343\n",
      "Iteration 50, loss = 1.09581096\n",
      "Iteration 51, loss = 1.08774408\n",
      "Iteration 52, loss = 1.08004141\n",
      "Iteration 53, loss = 1.07269867\n",
      "Iteration 54, loss = 1.06571337\n",
      "Iteration 55, loss = 1.05907580\n",
      "Iteration 56, loss = 1.05274058\n",
      "Iteration 57, loss = 1.04672019\n",
      "Iteration 58, loss = 1.04099481\n",
      "Iteration 59, loss = 1.03554432\n",
      "Iteration 60, loss = 1.03033626\n",
      "Iteration 61, loss = 1.02533746\n",
      "Iteration 62, loss = 1.02053018\n",
      "Iteration 63, loss = 1.01590865\n",
      "Iteration 64, loss = 1.01146048\n",
      "Iteration 65, loss = 1.00716554\n",
      "Iteration 66, loss = 1.00301543\n",
      "Iteration 67, loss = 0.99899438\n",
      "Iteration 68, loss = 0.99508493\n",
      "Iteration 69, loss = 0.99126750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.98753563\n",
      "Iteration 71, loss = 0.98386987\n",
      "Iteration 72, loss = 0.98026838\n",
      "Iteration 73, loss = 0.97671459\n",
      "Iteration 74, loss = 0.97320431\n",
      "Iteration 75, loss = 0.96973153\n",
      "Iteration 76, loss = 0.96628476\n",
      "Iteration 77, loss = 0.96286029\n",
      "Iteration 78, loss = 0.95945494\n",
      "Iteration 79, loss = 0.95606417\n",
      "Iteration 80, loss = 0.95268422\n",
      "Iteration 81, loss = 0.94931194\n",
      "Iteration 82, loss = 0.94594144\n",
      "Iteration 83, loss = 0.94257316\n",
      "Iteration 84, loss = 0.93920676\n",
      "Iteration 85, loss = 0.93584128\n",
      "Iteration 86, loss = 0.93247611\n",
      "Iteration 87, loss = 0.92911099\n",
      "Iteration 88, loss = 0.92574593\n",
      "Iteration 89, loss = 0.92238119\n",
      "Iteration 90, loss = 0.91901720\n",
      "Iteration 91, loss = 0.91565457\n",
      "Iteration 92, loss = 0.91229401\n",
      "Iteration 93, loss = 0.90896850\n",
      "Iteration 94, loss = 0.90564987\n",
      "Iteration 95, loss = 0.90233840\n",
      "Iteration 96, loss = 0.89903474\n",
      "Iteration 97, loss = 0.89573958\n",
      "Iteration 98, loss = 0.89245361\n",
      "Iteration 99, loss = 0.88917748\n",
      "Iteration 100, loss = 0.88591187\n",
      "Iteration 101, loss = 0.88265872\n",
      "Iteration 102, loss = 0.87947479\n",
      "Iteration 103, loss = 0.87631720\n",
      "Iteration 104, loss = 0.87320540\n",
      "Iteration 105, loss = 0.87011474\n",
      "Iteration 106, loss = 0.86704341\n",
      "Iteration 107, loss = 0.86399121\n",
      "Iteration 108, loss = 0.86095793\n",
      "Iteration 109, loss = 0.85794340\n",
      "Iteration 110, loss = 0.85495902\n",
      "Iteration 111, loss = 0.85206294\n",
      "Iteration 112, loss = 0.84927194\n",
      "Iteration 113, loss = 0.84654986\n",
      "Iteration 114, loss = 0.84389110\n",
      "Iteration 115, loss = 0.84132894\n",
      "Iteration 116, loss = 0.83885657\n",
      "Iteration 117, loss = 0.83646768\n",
      "Iteration 118, loss = 0.83409892\n",
      "Iteration 119, loss = 0.83174578\n",
      "Iteration 120, loss = 0.82942531\n",
      "Iteration 121, loss = 0.82714381\n",
      "Iteration 122, loss = 0.82488708\n",
      "Iteration 123, loss = 0.82263663\n",
      "Iteration 124, loss = 0.82039325\n",
      "Iteration 125, loss = 0.81815767\n",
      "Iteration 126, loss = 0.81593055\n",
      "Iteration 127, loss = 0.81371247\n",
      "Iteration 128, loss = 0.81150357\n",
      "Iteration 129, loss = 0.80930322\n",
      "Iteration 130, loss = 0.80711327\n",
      "Iteration 131, loss = 0.80493500\n",
      "Iteration 132, loss = 0.80276744\n",
      "Iteration 133, loss = 0.80061032\n",
      "Iteration 134, loss = 0.79846410\n",
      "Iteration 135, loss = 0.79632926\n",
      "Iteration 136, loss = 0.79420793\n",
      "Iteration 137, loss = 0.79209915\n",
      "Iteration 138, loss = 0.79000301\n",
      "Iteration 139, loss = 0.78791956\n",
      "Iteration 140, loss = 0.78584884\n",
      "Iteration 141, loss = 0.78379085\n",
      "Iteration 142, loss = 0.78174558\n",
      "Iteration 143, loss = 0.77971300\n",
      "Iteration 144, loss = 0.77769306\n",
      "Iteration 145, loss = 0.77568570\n",
      "Iteration 146, loss = 0.77369085\n",
      "Iteration 147, loss = 0.77170843\n",
      "Iteration 148, loss = 0.76973835\n",
      "Iteration 149, loss = 0.76778051\n",
      "Iteration 150, loss = 0.76583483\n",
      "Iteration 151, loss = 0.76390118\n",
      "Iteration 152, loss = 0.76197948\n",
      "Iteration 153, loss = 0.76006962\n",
      "Iteration 154, loss = 0.75817150\n",
      "Iteration 155, loss = 0.75628500\n",
      "Iteration 156, loss = 0.75441004\n",
      "Iteration 157, loss = 0.75254675\n",
      "Iteration 158, loss = 0.75069864\n",
      "Iteration 159, loss = 0.74886292\n",
      "Iteration 160, loss = 0.74703970\n",
      "Iteration 161, loss = 0.74522773\n",
      "Iteration 162, loss = 0.74342596\n",
      "Iteration 163, loss = 0.74163440\n",
      "Iteration 164, loss = 0.73985305\n",
      "Iteration 165, loss = 0.73808192\n",
      "Iteration 166, loss = 0.73632102\n",
      "Iteration 167, loss = 0.73457037\n",
      "Iteration 168, loss = 0.73283041\n",
      "Iteration 169, loss = 0.73110072\n",
      "Iteration 170, loss = 0.72938130\n",
      "Iteration 171, loss = 0.72767214\n",
      "Iteration 172, loss = 0.72597323\n",
      "Iteration 173, loss = 0.72428455\n",
      "Iteration 174, loss = 0.72260608\n",
      "Iteration 175, loss = 0.72093779\n",
      "Iteration 176, loss = 0.71928017\n",
      "Iteration 177, loss = 0.71763352\n",
      "Iteration 178, loss = 0.71599693\n",
      "Iteration 179, loss = 0.71437034\n",
      "Iteration 180, loss = 0.71275371\n",
      "Iteration 181, loss = 0.71114723\n",
      "Iteration 182, loss = 0.70955047\n",
      "Iteration 183, loss = 0.70796264\n",
      "Iteration 184, loss = 0.70638454\n",
      "Iteration 185, loss = 0.70481631\n",
      "Iteration 186, loss = 0.70325758\n",
      "Iteration 187, loss = 0.70170831\n",
      "Iteration 188, loss = 0.70016842\n",
      "Iteration 189, loss = 0.69863785\n",
      "Iteration 190, loss = 0.69711677\n",
      "Iteration 191, loss = 0.69560536\n",
      "Iteration 192, loss = 0.69410304\n",
      "Iteration 193, loss = 0.69260942\n",
      "Iteration 194, loss = 0.69112464\n",
      "Iteration 195, loss = 0.68964881\n",
      "Iteration 196, loss = 0.68818189\n",
      "Iteration 197, loss = 0.68672383\n",
      "Iteration 198, loss = 0.68527461\n",
      "Iteration 199, loss = 0.68383416\n",
      "Iteration 200, loss = 0.68240245\n",
      "Iteration 201, loss = 0.68097942\n",
      "Iteration 202, loss = 0.67956503\n",
      "Iteration 203, loss = 0.67815933\n",
      "Iteration 204, loss = 0.67676255\n",
      "Iteration 205, loss = 0.67537428\n",
      "Iteration 206, loss = 0.67399445\n",
      "Iteration 207, loss = 0.67262302\n",
      "Iteration 208, loss = 0.67125991\n",
      "Iteration 209, loss = 0.66990506\n",
      "Iteration 210, loss = 0.66855841\n",
      "Iteration 211, loss = 0.66721990\n",
      "Iteration 212, loss = 0.66588947\n",
      "Iteration 213, loss = 0.66456727\n",
      "Iteration 214, loss = 0.66325319\n",
      "Iteration 215, loss = 0.66194703\n",
      "Iteration 216, loss = 0.66064895\n",
      "Iteration 217, loss = 0.65935872\n",
      "Iteration 218, loss = 0.65807623\n",
      "Iteration 219, loss = 0.65680141\n",
      "Iteration 220, loss = 0.65553418\n",
      "Iteration 221, loss = 0.65427449\n",
      "Iteration 222, loss = 0.65302253\n",
      "Iteration 223, loss = 0.65177808\n",
      "Iteration 224, loss = 0.65054099\n",
      "Iteration 225, loss = 0.64931126\n",
      "Iteration 226, loss = 0.64808897\n",
      "Iteration 227, loss = 0.64687386\n",
      "Iteration 228, loss = 0.64566587\n",
      "Iteration 229, loss = 0.64446492\n",
      "Iteration 230, loss = 0.64327095\n",
      "Iteration 231, loss = 0.64208390\n",
      "Iteration 232, loss = 0.64090370\n",
      "Iteration 233, loss = 0.63973029\n",
      "Iteration 234, loss = 0.63856360\n",
      "Iteration 235, loss = 0.63740358\n",
      "Iteration 236, loss = 0.63625017\n",
      "Iteration 237, loss = 0.63510329\n",
      "Iteration 238, loss = 0.63396289\n",
      "Iteration 239, loss = 0.63282891\n",
      "Iteration 240, loss = 0.63170128\n",
      "Iteration 241, loss = 0.63057996\n",
      "Iteration 242, loss = 0.62946491\n",
      "Iteration 243, loss = 0.62835631\n",
      "Iteration 244, loss = 0.62725386\n",
      "Iteration 245, loss = 0.62615749\n",
      "Iteration 246, loss = 0.62506714\n",
      "Iteration 247, loss = 0.62398296\n",
      "Iteration 248, loss = 0.62290512\n",
      "Iteration 249, loss = 0.62183319\n",
      "Iteration 250, loss = 0.62076707\n",
      "Iteration 251, loss = 0.61970672\n",
      "Iteration 252, loss = 0.61865207\n",
      "Iteration 253, loss = 0.61760307\n",
      "Iteration 254, loss = 0.61655964\n",
      "Iteration 255, loss = 0.61552175\n",
      "Iteration 256, loss = 0.61448931\n",
      "Iteration 257, loss = 0.61346229\n",
      "Iteration 258, loss = 0.61244107\n",
      "Iteration 259, loss = 0.61142579\n",
      "Iteration 260, loss = 0.61041619\n",
      "Iteration 261, loss = 0.60941184\n",
      "Iteration 262, loss = 0.60841264\n",
      "Iteration 263, loss = 0.60741854\n",
      "Iteration 264, loss = 0.60642949\n",
      "Iteration 265, loss = 0.60544546\n",
      "Iteration 266, loss = 0.60446646\n",
      "Iteration 267, loss = 0.60349234\n",
      "Iteration 268, loss = 0.60252306\n",
      "Iteration 269, loss = 0.60155861\n",
      "Iteration 270, loss = 0.60059894\n",
      "Iteration 271, loss = 0.59964393\n",
      "Iteration 272, loss = 0.59869354\n",
      "Iteration 273, loss = 0.59774771\n",
      "Iteration 274, loss = 0.59680640\n",
      "Iteration 275, loss = 0.59586955\n",
      "Iteration 276, loss = 0.59493710\n",
      "Iteration 277, loss = 0.59400902\n",
      "Iteration 278, loss = 0.59308524\n",
      "Iteration 279, loss = 0.59216571\n",
      "Iteration 280, loss = 0.59125038\n",
      "Iteration 281, loss = 0.59033921\n",
      "Iteration 282, loss = 0.58943214\n",
      "Iteration 283, loss = 0.58852934\n",
      "Iteration 284, loss = 0.58763332\n",
      "Iteration 285, loss = 0.58674179\n",
      "Iteration 286, loss = 0.58585746\n",
      "Iteration 287, loss = 0.58497723\n",
      "Iteration 288, loss = 0.58410108\n",
      "Iteration 289, loss = 0.58322975\n",
      "Iteration 290, loss = 0.58236384\n",
      "Iteration 291, loss = 0.58150182\n",
      "Iteration 292, loss = 0.58064370\n",
      "Iteration 293, loss = 0.57978946\n",
      "Iteration 294, loss = 0.57893912\n",
      "Iteration 295, loss = 0.57809272\n",
      "Iteration 296, loss = 0.57725023\n",
      "Iteration 297, loss = 0.57641162\n",
      "Iteration 298, loss = 0.57557689\n",
      "Iteration 299, loss = 0.57474602\n",
      "Iteration 300, loss = 0.57391897\n",
      "Iteration 301, loss = 0.57309573\n",
      "Iteration 302, loss = 0.57227628\n",
      "Iteration 303, loss = 0.57146056\n",
      "Iteration 304, loss = 0.57064856\n",
      "Iteration 305, loss = 0.56984023\n",
      "Iteration 306, loss = 0.56903557\n",
      "Iteration 307, loss = 0.56823453\n",
      "Iteration 308, loss = 0.56743703\n",
      "Iteration 309, loss = 0.56664306\n",
      "Iteration 310, loss = 0.56585257\n",
      "Iteration 311, loss = 0.56506548\n",
      "Iteration 312, loss = 0.56428178\n",
      "Iteration 313, loss = 0.56350230\n",
      "Iteration 314, loss = 0.56272793\n",
      "Iteration 315, loss = 0.56195613\n",
      "Iteration 316, loss = 0.56118705\n",
      "Iteration 317, loss = 0.56042094\n",
      "Iteration 318, loss = 0.55965859\n",
      "Iteration 319, loss = 0.55889972\n",
      "Iteration 320, loss = 0.55814474\n",
      "Iteration 321, loss = 0.55739308\n",
      "Iteration 322, loss = 0.55664446\n",
      "Iteration 323, loss = 0.55589911\n",
      "Iteration 324, loss = 0.55515693\n",
      "Iteration 325, loss = 0.55441793\n",
      "Iteration 326, loss = 0.55368228\n",
      "Iteration 327, loss = 0.55294972\n",
      "Iteration 328, loss = 0.55222022\n",
      "Iteration 329, loss = 0.55149378\n",
      "Iteration 330, loss = 0.55077038\n",
      "Iteration 331, loss = 0.55004998\n",
      "Iteration 332, loss = 0.54933257\n",
      "Iteration 333, loss = 0.54861812\n",
      "Iteration 334, loss = 0.54790659\n",
      "Iteration 335, loss = 0.54719796\n",
      "Iteration 336, loss = 0.54649219\n",
      "Iteration 337, loss = 0.54578925\n",
      "Iteration 338, loss = 0.54508910\n",
      "Iteration 339, loss = 0.54439170\n",
      "Iteration 340, loss = 0.54369701\n",
      "Iteration 341, loss = 0.54300501\n",
      "Iteration 342, loss = 0.54231565\n",
      "Iteration 343, loss = 0.54162889\n",
      "Iteration 344, loss = 0.54094470\n",
      "Iteration 345, loss = 0.54026303\n",
      "Iteration 346, loss = 0.53958385\n",
      "Iteration 347, loss = 0.53890713\n",
      "Iteration 348, loss = 0.53823281\n",
      "Iteration 349, loss = 0.53756087\n",
      "Iteration 350, loss = 0.53689192\n",
      "Iteration 351, loss = 0.53622537\n",
      "Iteration 352, loss = 0.53556117\n",
      "Iteration 353, loss = 0.53489931\n",
      "Iteration 354, loss = 0.53423975\n",
      "Iteration 355, loss = 0.53358259\n",
      "Iteration 356, loss = 0.53292784\n",
      "Iteration 357, loss = 0.53227525\n",
      "Iteration 358, loss = 0.53162483\n",
      "Iteration 359, loss = 0.53097659\n",
      "Iteration 360, loss = 0.53033055\n",
      "Iteration 361, loss = 0.52968696\n",
      "Iteration 362, loss = 0.52904554\n",
      "Iteration 363, loss = 0.52840624\n",
      "Iteration 364, loss = 0.52776903\n",
      "Iteration 365, loss = 0.52713388\n",
      "Iteration 366, loss = 0.52650079\n",
      "Iteration 367, loss = 0.52587006\n",
      "Iteration 368, loss = 0.52524137\n",
      "Iteration 369, loss = 0.52461473\n",
      "Iteration 370, loss = 0.52399016\n",
      "Iteration 371, loss = 0.52336760\n",
      "Iteration 372, loss = 0.52274702\n",
      "Iteration 373, loss = 0.52212841\n",
      "Iteration 374, loss = 0.52151174\n",
      "Iteration 375, loss = 0.52089700\n",
      "Iteration 376, loss = 0.52028416\n",
      "Iteration 377, loss = 0.51967319\n",
      "Iteration 378, loss = 0.51906407\n",
      "Iteration 379, loss = 0.51845678\n",
      "Iteration 380, loss = 0.51785132\n",
      "Iteration 381, loss = 0.51724776\n",
      "Iteration 382, loss = 0.51664598\n",
      "Iteration 383, loss = 0.51604597\n",
      "Iteration 384, loss = 0.51544770\n",
      "Iteration 385, loss = 0.51485117\n",
      "Iteration 386, loss = 0.51425634\n",
      "Iteration 387, loss = 0.51366321\n",
      "Iteration 388, loss = 0.51307175\n",
      "Iteration 389, loss = 0.51248196\n",
      "Iteration 390, loss = 0.51189380\n",
      "Iteration 391, loss = 0.51130726\n",
      "Iteration 392, loss = 0.51072232\n",
      "Iteration 393, loss = 0.51013896\n",
      "Iteration 394, loss = 0.50955716\n",
      "Iteration 395, loss = 0.50897691\n",
      "Iteration 396, loss = 0.50839829\n",
      "Iteration 397, loss = 0.50782119\n",
      "Iteration 398, loss = 0.50724559\n",
      "Iteration 399, loss = 0.50667148\n",
      "Iteration 400, loss = 0.50609882\n",
      "Iteration 401, loss = 0.50552760\n",
      "Iteration 402, loss = 0.50495780\n",
      "Iteration 403, loss = 0.50438939\n",
      "Iteration 404, loss = 0.50382235\n",
      "Iteration 405, loss = 0.50325666\n",
      "Iteration 406, loss = 0.50269230\n",
      "Iteration 407, loss = 0.50212924\n",
      "Iteration 408, loss = 0.50156746\n",
      "Iteration 409, loss = 0.50100694\n",
      "Iteration 410, loss = 0.50044766\n",
      "Iteration 411, loss = 0.49988959\n",
      "Iteration 412, loss = 0.49933271\n",
      "Iteration 413, loss = 0.49877701\n",
      "Iteration 414, loss = 0.49822258\n",
      "Iteration 415, loss = 0.49766930\n",
      "Iteration 416, loss = 0.49711720\n",
      "Iteration 417, loss = 0.49656624\n",
      "Iteration 418, loss = 0.49601636\n",
      "Iteration 419, loss = 0.49546755\n",
      "Iteration 420, loss = 0.49491978\n",
      "Iteration 421, loss = 0.49437320\n",
      "Iteration 422, loss = 0.49382861\n",
      "Iteration 423, loss = 0.49328509\n",
      "Iteration 424, loss = 0.49274262\n",
      "Iteration 425, loss = 0.49220120\n",
      "Iteration 426, loss = 0.49166081\n",
      "Iteration 427, loss = 0.49112144\n",
      "Iteration 428, loss = 0.49058309\n",
      "Iteration 429, loss = 0.49004573\n",
      "Iteration 430, loss = 0.48950935\n",
      "Iteration 431, loss = 0.48897395\n",
      "Iteration 432, loss = 0.48843950\n",
      "Iteration 433, loss = 0.48790599\n",
      "Iteration 434, loss = 0.48737344\n",
      "Iteration 435, loss = 0.48684189\n",
      "Iteration 436, loss = 0.48631128\n",
      "Iteration 437, loss = 0.48578161\n",
      "Iteration 438, loss = 0.48525281\n",
      "Iteration 439, loss = 0.48472488\n",
      "Iteration 440, loss = 0.48419780\n",
      "Iteration 441, loss = 0.48367154\n",
      "Iteration 442, loss = 0.48314609\n",
      "Iteration 443, loss = 0.48262144\n",
      "Iteration 444, loss = 0.48209756\n",
      "Iteration 445, loss = 0.48157444\n",
      "Iteration 446, loss = 0.48105207\n",
      "Iteration 447, loss = 0.48053042\n",
      "Iteration 448, loss = 0.48000948\n",
      "Iteration 449, loss = 0.47949000\n",
      "Iteration 450, loss = 0.47897161\n",
      "Iteration 451, loss = 0.47845395\n",
      "Iteration 452, loss = 0.47793708\n",
      "Iteration 453, loss = 0.47742096\n",
      "Iteration 454, loss = 0.47690557\n",
      "Iteration 455, loss = 0.47639089\n",
      "Iteration 456, loss = 0.47587691\n",
      "Iteration 457, loss = 0.47536404\n",
      "Iteration 458, loss = 0.47485194\n",
      "Iteration 459, loss = 0.47434045\n",
      "Iteration 460, loss = 0.47382957\n",
      "Iteration 461, loss = 0.47331933\n",
      "Iteration 462, loss = 0.47280973\n",
      "Iteration 463, loss = 0.47230078\n",
      "Iteration 464, loss = 0.47179250\n",
      "Iteration 465, loss = 0.47128487\n",
      "Iteration 466, loss = 0.47077792\n",
      "Iteration 467, loss = 0.47027163\n",
      "Iteration 468, loss = 0.46976617\n",
      "Iteration 469, loss = 0.46926139\n",
      "Iteration 470, loss = 0.46875735\n",
      "Iteration 471, loss = 0.46825404\n",
      "Iteration 472, loss = 0.46775125\n",
      "Iteration 473, loss = 0.46724897\n",
      "Iteration 474, loss = 0.46674736\n",
      "Iteration 475, loss = 0.46624640\n",
      "Iteration 476, loss = 0.46574606\n",
      "Iteration 477, loss = 0.46524644\n",
      "Iteration 478, loss = 0.46474743\n",
      "Iteration 479, loss = 0.46424903\n",
      "Iteration 480, loss = 0.46375120\n",
      "Iteration 481, loss = 0.46325391\n",
      "Iteration 482, loss = 0.46275716\n",
      "Iteration 483, loss = 0.46226093\n",
      "Iteration 484, loss = 0.46176522\n",
      "Iteration 485, loss = 0.46127001\n",
      "Iteration 486, loss = 0.46077529\n",
      "Iteration 487, loss = 0.46028105\n",
      "Iteration 488, loss = 0.45978727\n",
      "Iteration 489, loss = 0.45929394\n",
      "Iteration 490, loss = 0.45880106\n",
      "Iteration 491, loss = 0.45830860\n",
      "Iteration 492, loss = 0.45781655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 493, loss = 0.45732491\n",
      "Iteration 494, loss = 0.45683365\n",
      "Iteration 495, loss = 0.45634280\n",
      "Iteration 496, loss = 0.45585237\n",
      "Iteration 497, loss = 0.45536231\n",
      "Iteration 498, loss = 0.45487260\n",
      "Iteration 499, loss = 0.45438322\n",
      "Iteration 500, loss = 0.45389418\n",
      "Iteration 501, loss = 0.45340544\n",
      "Iteration 502, loss = 0.45291705\n",
      "Iteration 503, loss = 0.45242912\n",
      "Iteration 504, loss = 0.45194160\n",
      "Iteration 505, loss = 0.45145438\n",
      "Iteration 506, loss = 0.45096745\n",
      "Iteration 507, loss = 0.45048079\n",
      "Iteration 508, loss = 0.44999440\n",
      "Iteration 509, loss = 0.44950826\n",
      "Iteration 510, loss = 0.44902236\n",
      "Iteration 511, loss = 0.44853669\n",
      "Iteration 512, loss = 0.44805124\n",
      "Iteration 513, loss = 0.44756600\n",
      "Iteration 514, loss = 0.44708096\n",
      "Iteration 515, loss = 0.44659612\n",
      "Iteration 516, loss = 0.44611145\n",
      "Iteration 517, loss = 0.44562695\n",
      "Iteration 518, loss = 0.44514262\n",
      "Iteration 519, loss = 0.44465843\n",
      "Iteration 520, loss = 0.44417439\n",
      "Iteration 521, loss = 0.44369049\n",
      "Iteration 522, loss = 0.44320671\n",
      "Iteration 523, loss = 0.44272304\n",
      "Iteration 524, loss = 0.44223949\n",
      "Iteration 525, loss = 0.44175603\n",
      "Iteration 526, loss = 0.44127267\n",
      "Iteration 527, loss = 0.44078939\n",
      "Iteration 528, loss = 0.44030618\n",
      "Iteration 529, loss = 0.43982305\n",
      "Iteration 530, loss = 0.43933997\n",
      "Iteration 531, loss = 0.43885694\n",
      "Iteration 532, loss = 0.43837396\n",
      "Iteration 533, loss = 0.43789102\n",
      "Iteration 534, loss = 0.43740810\n",
      "Iteration 535, loss = 0.43692521\n",
      "Iteration 536, loss = 0.43644233\n",
      "Iteration 537, loss = 0.43595947\n",
      "Iteration 538, loss = 0.43547660\n",
      "Iteration 539, loss = 0.43499373\n",
      "Iteration 540, loss = 0.43451084\n",
      "Iteration 541, loss = 0.43402805\n",
      "Iteration 542, loss = 0.43354538\n",
      "Iteration 543, loss = 0.43306269\n",
      "Iteration 544, loss = 0.43257998\n",
      "Iteration 545, loss = 0.43209725\n",
      "Iteration 546, loss = 0.43161876\n",
      "Iteration 547, loss = 0.43114144\n",
      "Iteration 548, loss = 0.43066381\n",
      "Iteration 549, loss = 0.43018592\n",
      "Iteration 550, loss = 0.42970781\n",
      "Iteration 551, loss = 0.42922954\n",
      "Iteration 552, loss = 0.42875113\n",
      "Iteration 553, loss = 0.42827264\n",
      "Iteration 554, loss = 0.42779408\n",
      "Iteration 555, loss = 0.42731549\n",
      "Iteration 556, loss = 0.42683690\n",
      "Iteration 557, loss = 0.42635833\n",
      "Iteration 558, loss = 0.42587998\n",
      "Iteration 559, loss = 0.42540171\n",
      "Iteration 560, loss = 0.42492351\n",
      "Iteration 561, loss = 0.42444541\n",
      "Iteration 562, loss = 0.42396741\n",
      "Iteration 563, loss = 0.42348951\n",
      "Iteration 564, loss = 0.42301172\n",
      "Iteration 565, loss = 0.42253403\n",
      "Iteration 566, loss = 0.42205646\n",
      "Iteration 567, loss = 0.42157900\n",
      "Iteration 568, loss = 0.42110165\n",
      "Iteration 569, loss = 0.42062440\n",
      "Iteration 570, loss = 0.42014725\n",
      "Iteration 571, loss = 0.41967020\n",
      "Iteration 572, loss = 0.41919332\n",
      "Iteration 573, loss = 0.41871693\n",
      "Iteration 574, loss = 0.41824036\n",
      "Iteration 575, loss = 0.41776364\n",
      "Iteration 576, loss = 0.41728668\n",
      "Iteration 577, loss = 0.41680981\n",
      "Iteration 578, loss = 0.41633323\n",
      "Iteration 579, loss = 0.41585665\n",
      "Iteration 580, loss = 0.41538007\n",
      "Iteration 581, loss = 0.41490348\n",
      "Iteration 582, loss = 0.41442689\n",
      "Iteration 583, loss = 0.41395029\n",
      "Iteration 584, loss = 0.41347367\n",
      "Iteration 585, loss = 0.41299703\n",
      "Iteration 586, loss = 0.41252038\n",
      "Iteration 587, loss = 0.41204370\n",
      "Iteration 588, loss = 0.41156699\n",
      "Iteration 589, loss = 0.41109025\n",
      "Iteration 590, loss = 0.41061363\n",
      "Iteration 591, loss = 0.41013704\n",
      "Iteration 592, loss = 0.40966041\n",
      "Iteration 593, loss = 0.40918375\n",
      "Iteration 594, loss = 0.40870706\n",
      "Iteration 595, loss = 0.40823031\n",
      "Iteration 596, loss = 0.40775352\n",
      "Iteration 597, loss = 0.40727666\n",
      "Iteration 598, loss = 0.40679974\n",
      "Iteration 599, loss = 0.40632276\n",
      "Iteration 600, loss = 0.40584569\n",
      "Iteration 601, loss = 0.40536855\n",
      "Iteration 602, loss = 0.40489133\n",
      "Iteration 603, loss = 0.40441401\n",
      "Iteration 604, loss = 0.40393660\n",
      "Iteration 605, loss = 0.40345909\n",
      "Iteration 606, loss = 0.40298147\n",
      "Iteration 607, loss = 0.40250375\n",
      "Iteration 608, loss = 0.40202592\n",
      "Iteration 609, loss = 0.40154797\n",
      "Iteration 610, loss = 0.40106989\n",
      "Iteration 611, loss = 0.40059170\n",
      "Iteration 612, loss = 0.40011337\n",
      "Iteration 613, loss = 0.39963492\n",
      "Iteration 614, loss = 0.39915633\n",
      "Iteration 615, loss = 0.39867760\n",
      "Iteration 616, loss = 0.39819873\n",
      "Iteration 617, loss = 0.39771971\n",
      "Iteration 618, loss = 0.39724055\n",
      "Iteration 619, loss = 0.39676123\n",
      "Iteration 620, loss = 0.39628177\n",
      "Iteration 621, loss = 0.39580214\n",
      "Iteration 622, loss = 0.39532236\n",
      "Iteration 623, loss = 0.39484242\n",
      "Iteration 624, loss = 0.39436231\n",
      "Iteration 625, loss = 0.39388204\n",
      "Iteration 626, loss = 0.39340159\n",
      "Iteration 627, loss = 0.39292098\n",
      "Iteration 628, loss = 0.39244020\n",
      "Iteration 629, loss = 0.39195924\n",
      "Iteration 630, loss = 0.39147811\n",
      "Iteration 631, loss = 0.39099723\n",
      "Iteration 632, loss = 0.39051653\n",
      "Iteration 633, loss = 0.39003570\n",
      "Iteration 634, loss = 0.38955475\n",
      "Iteration 635, loss = 0.38907366\n",
      "Iteration 636, loss = 0.38859244\n",
      "Iteration 637, loss = 0.38811108\n",
      "Iteration 638, loss = 0.38762970\n",
      "Iteration 639, loss = 0.38714837\n",
      "Iteration 640, loss = 0.38666691\n",
      "Iteration 641, loss = 0.38618531\n",
      "Iteration 642, loss = 0.38570358\n",
      "Iteration 643, loss = 0.38522190\n",
      "Iteration 644, loss = 0.38474009\n",
      "Iteration 645, loss = 0.38425814\n",
      "Iteration 646, loss = 0.38377604\n",
      "Iteration 647, loss = 0.38329379\n",
      "Iteration 648, loss = 0.38281171\n",
      "Iteration 649, loss = 0.38233072\n",
      "Iteration 650, loss = 0.38184955\n",
      "Iteration 651, loss = 0.38136821\n",
      "Iteration 652, loss = 0.38088671\n",
      "Iteration 653, loss = 0.38040504\n",
      "Iteration 654, loss = 0.37992322\n",
      "Iteration 655, loss = 0.37944125\n",
      "Iteration 656, loss = 0.37895912\n",
      "Iteration 657, loss = 0.37847685\n",
      "Iteration 658, loss = 0.37799444\n",
      "Iteration 659, loss = 0.37751189\n",
      "Iteration 660, loss = 0.37702920\n",
      "Iteration 661, loss = 0.37654637\n",
      "Iteration 662, loss = 0.37606341\n",
      "Iteration 663, loss = 0.37558032\n",
      "Iteration 664, loss = 0.37509709\n",
      "Iteration 665, loss = 0.37461374\n",
      "Iteration 666, loss = 0.37413025\n",
      "Iteration 667, loss = 0.37364663\n",
      "Iteration 668, loss = 0.37316289\n",
      "Iteration 669, loss = 0.37267901\n",
      "Iteration 670, loss = 0.37219501\n",
      "Iteration 671, loss = 0.37171087\n",
      "Iteration 672, loss = 0.37122661\n",
      "Iteration 673, loss = 0.37074222\n",
      "Iteration 674, loss = 0.37025769\n",
      "Iteration 675, loss = 0.36977304\n",
      "Iteration 676, loss = 0.36928825\n",
      "Iteration 677, loss = 0.36880333\n",
      "Iteration 678, loss = 0.36831829\n",
      "Iteration 679, loss = 0.36783311\n",
      "Iteration 680, loss = 0.36734779\n",
      "Iteration 681, loss = 0.36686235\n",
      "Iteration 682, loss = 0.36637677\n",
      "Iteration 683, loss = 0.36589106\n",
      "Iteration 684, loss = 0.36540522\n",
      "Iteration 685, loss = 0.36491924\n",
      "Iteration 686, loss = 0.36443313\n",
      "Iteration 687, loss = 0.36394688\n",
      "Iteration 688, loss = 0.36346051\n",
      "Iteration 689, loss = 0.36297400\n",
      "Iteration 690, loss = 0.36248736\n",
      "Iteration 691, loss = 0.36200058\n",
      "Iteration 692, loss = 0.36151368\n",
      "Iteration 693, loss = 0.36102664\n",
      "Iteration 694, loss = 0.36053947\n",
      "Iteration 695, loss = 0.36005217\n",
      "Iteration 696, loss = 0.35956474\n",
      "Iteration 697, loss = 0.35907718\n",
      "Iteration 698, loss = 0.35858950\n",
      "Iteration 699, loss = 0.35810168\n",
      "Iteration 700, loss = 0.35761374\n",
      "Iteration 701, loss = 0.35712568\n",
      "Iteration 702, loss = 0.35663749\n",
      "Iteration 703, loss = 0.35614918\n",
      "Iteration 704, loss = 0.35566129\n",
      "Iteration 705, loss = 0.35517335\n",
      "Iteration 706, loss = 0.35468519\n",
      "Iteration 707, loss = 0.35419683\n",
      "Iteration 708, loss = 0.35370828\n",
      "Iteration 709, loss = 0.35321957\n",
      "Iteration 710, loss = 0.35273071\n",
      "Iteration 711, loss = 0.35224170\n",
      "Iteration 712, loss = 0.35175257\n",
      "Iteration 713, loss = 0.35126332\n",
      "Iteration 714, loss = 0.35077447\n",
      "Iteration 715, loss = 0.35028551\n",
      "Iteration 716, loss = 0.34979641\n",
      "Iteration 717, loss = 0.34930715\n",
      "Iteration 718, loss = 0.34881775\n",
      "Iteration 719, loss = 0.34832821\n",
      "Iteration 720, loss = 0.34783854\n",
      "Iteration 721, loss = 0.34734874\n",
      "Iteration 722, loss = 0.34685888\n",
      "Iteration 723, loss = 0.34636927\n",
      "Iteration 724, loss = 0.34587955\n",
      "Iteration 725, loss = 0.34538971\n",
      "Iteration 726, loss = 0.34489977\n",
      "Iteration 727, loss = 0.34440974\n",
      "Iteration 728, loss = 0.34391962\n",
      "Iteration 729, loss = 0.34342943\n",
      "Iteration 730, loss = 0.34293946\n",
      "Iteration 731, loss = 0.34244937\n",
      "Iteration 732, loss = 0.34195913\n",
      "Iteration 733, loss = 0.34146875\n",
      "Iteration 734, loss = 0.34097828\n",
      "Iteration 735, loss = 0.34048798\n",
      "Iteration 736, loss = 0.33999762\n",
      "Iteration 737, loss = 0.33950719\n",
      "Iteration 738, loss = 0.33901671\n",
      "Iteration 739, loss = 0.33852619\n",
      "Iteration 740, loss = 0.33803566\n",
      "Iteration 741, loss = 0.33754513\n",
      "Iteration 742, loss = 0.33705457\n",
      "Iteration 743, loss = 0.33656402\n",
      "Iteration 744, loss = 0.33607343\n",
      "Iteration 745, loss = 0.33558281\n",
      "Iteration 746, loss = 0.33509216\n",
      "Iteration 747, loss = 0.33460163\n",
      "Iteration 748, loss = 0.33411099\n",
      "Iteration 749, loss = 0.33362031\n",
      "Iteration 750, loss = 0.33312971\n",
      "Iteration 751, loss = 0.33263911\n",
      "Iteration 752, loss = 0.33214849\n",
      "Iteration 753, loss = 0.33165788\n",
      "Iteration 754, loss = 0.33116726\n",
      "Iteration 755, loss = 0.33067666\n",
      "Iteration 756, loss = 0.33018623\n",
      "Iteration 757, loss = 0.32969567\n",
      "Iteration 758, loss = 0.32920508\n",
      "Iteration 759, loss = 0.32871463\n",
      "Iteration 760, loss = 0.32822420\n",
      "Iteration 761, loss = 0.32773379\n",
      "Iteration 762, loss = 0.32724341\n",
      "Iteration 763, loss = 0.32675307\n",
      "Iteration 764, loss = 0.32626276\n",
      "Iteration 765, loss = 0.32577250\n",
      "Iteration 766, loss = 0.32528229\n",
      "Iteration 767, loss = 0.32479222\n",
      "Iteration 768, loss = 0.32430208\n",
      "Iteration 769, loss = 0.32381207\n",
      "Iteration 770, loss = 0.32332214\n",
      "Iteration 771, loss = 0.32283227\n",
      "Iteration 772, loss = 0.32234246\n",
      "Iteration 773, loss = 0.32185272\n",
      "Iteration 774, loss = 0.32136305\n",
      "Iteration 775, loss = 0.32087347\n",
      "Iteration 776, loss = 0.32038396\n",
      "Iteration 777, loss = 0.31989454\n",
      "Iteration 778, loss = 0.31940520\n",
      "Iteration 779, loss = 0.31891597\n",
      "Iteration 780, loss = 0.31842683\n",
      "Iteration 781, loss = 0.31793779\n",
      "Iteration 782, loss = 0.31744886\n",
      "Iteration 783, loss = 0.31696004\n",
      "Iteration 784, loss = 0.31647133\n",
      "Iteration 785, loss = 0.31598278\n",
      "Iteration 786, loss = 0.31549429\n",
      "Iteration 787, loss = 0.31500595\n",
      "Iteration 788, loss = 0.31451775\n",
      "Iteration 789, loss = 0.31402967\n",
      "Iteration 790, loss = 0.31354173\n",
      "Iteration 791, loss = 0.31305392\n",
      "Iteration 792, loss = 0.31256626\n",
      "Iteration 793, loss = 0.31207875\n",
      "Iteration 794, loss = 0.31159138\n",
      "Iteration 795, loss = 0.31110417\n",
      "Iteration 796, loss = 0.31061712\n",
      "Iteration 797, loss = 0.31013022\n",
      "Iteration 798, loss = 0.30964350\n",
      "Iteration 799, loss = 0.30915694\n",
      "Iteration 800, loss = 0.30867056\n",
      "Iteration 801, loss = 0.30818435\n",
      "Iteration 802, loss = 0.30769835\n",
      "Iteration 803, loss = 0.30721249\n",
      "Iteration 804, loss = 0.30672684\n",
      "Iteration 805, loss = 0.30624138\n",
      "Iteration 806, loss = 0.30575611\n",
      "Iteration 807, loss = 0.30527105\n",
      "Iteration 808, loss = 0.30478619\n",
      "Iteration 809, loss = 0.30430153\n",
      "Iteration 810, loss = 0.30381709\n",
      "Iteration 811, loss = 0.30333287\n",
      "Iteration 812, loss = 0.30284886\n",
      "Iteration 813, loss = 0.30236508\n",
      "Iteration 814, loss = 0.30188152\n",
      "Iteration 815, loss = 0.30139820\n",
      "Iteration 816, loss = 0.30091511\n",
      "Iteration 817, loss = 0.30043226\n",
      "Iteration 818, loss = 0.29994965\n",
      "Iteration 819, loss = 0.29946730\n",
      "Iteration 820, loss = 0.29898519\n",
      "Iteration 821, loss = 0.29850334\n",
      "Iteration 822, loss = 0.29802175\n",
      "Iteration 823, loss = 0.29754042\n",
      "Iteration 824, loss = 0.29705936\n",
      "Iteration 825, loss = 0.29657857\n",
      "Iteration 826, loss = 0.29609806\n",
      "Iteration 827, loss = 0.29561782\n",
      "Iteration 828, loss = 0.29513787\n",
      "Iteration 829, loss = 0.29465821\n",
      "Iteration 830, loss = 0.29417884\n",
      "Iteration 831, loss = 0.29369977\n",
      "Iteration 832, loss = 0.29322099\n",
      "Iteration 833, loss = 0.29274252\n",
      "Iteration 834, loss = 0.29226436\n",
      "Iteration 835, loss = 0.29178650\n",
      "Iteration 836, loss = 0.29130897\n",
      "Iteration 837, loss = 0.29083175\n",
      "Iteration 838, loss = 0.29035486\n",
      "Iteration 839, loss = 0.28987829\n",
      "Iteration 840, loss = 0.28940206\n",
      "Iteration 841, loss = 0.28892616\n",
      "Iteration 842, loss = 0.28845060\n",
      "Iteration 843, loss = 0.28797539\n",
      "Iteration 844, loss = 0.28750052\n",
      "Iteration 845, loss = 0.28702600\n",
      "Iteration 846, loss = 0.28655184\n",
      "Iteration 847, loss = 0.28607804\n",
      "Iteration 848, loss = 0.28560461\n",
      "Iteration 849, loss = 0.28513154\n",
      "Iteration 850, loss = 0.28465884\n",
      "Iteration 851, loss = 0.28418652\n",
      "Iteration 852, loss = 0.28371458\n",
      "Iteration 853, loss = 0.28324302\n",
      "Iteration 854, loss = 0.28277185\n",
      "Iteration 855, loss = 0.28230107\n",
      "Iteration 856, loss = 0.28183069\n",
      "Iteration 857, loss = 0.28136070\n",
      "Iteration 858, loss = 0.28089112\n",
      "Iteration 859, loss = 0.28042195\n",
      "Iteration 860, loss = 0.27995318\n",
      "Iteration 861, loss = 0.27948483\n",
      "Iteration 862, loss = 0.27901690\n",
      "Iteration 863, loss = 0.27854939\n",
      "Iteration 864, loss = 0.27808231\n",
      "Iteration 865, loss = 0.27761566\n",
      "Iteration 866, loss = 0.27714944\n",
      "Iteration 867, loss = 0.27668366\n",
      "Iteration 868, loss = 0.27621832\n",
      "Iteration 869, loss = 0.27575343\n",
      "Iteration 870, loss = 0.27528899\n",
      "Iteration 871, loss = 0.27482500\n",
      "Iteration 872, loss = 0.27436146\n",
      "Iteration 873, loss = 0.27389839\n",
      "Iteration 874, loss = 0.27343578\n",
      "Iteration 875, loss = 0.27297364\n",
      "Iteration 876, loss = 0.27251197\n",
      "Iteration 877, loss = 0.27205078\n",
      "Iteration 878, loss = 0.27159006\n",
      "Iteration 879, loss = 0.27112983\n",
      "Iteration 880, loss = 0.27067008\n",
      "Iteration 881, loss = 0.27021083\n",
      "Iteration 882, loss = 0.26975209\n",
      "Iteration 883, loss = 0.26929384\n",
      "Iteration 884, loss = 0.26883609\n",
      "Iteration 885, loss = 0.26837885\n",
      "Iteration 886, loss = 0.26792211\n",
      "Iteration 887, loss = 0.26746589\n",
      "Iteration 888, loss = 0.26701018\n",
      "Iteration 889, loss = 0.26655500\n",
      "Iteration 890, loss = 0.26610033\n",
      "Iteration 891, loss = 0.26564619\n",
      "Iteration 892, loss = 0.26519258\n",
      "Iteration 893, loss = 0.26473950\n",
      "Iteration 894, loss = 0.26428695\n",
      "Iteration 895, loss = 0.26383495\n",
      "Iteration 896, loss = 0.26338349\n",
      "Iteration 897, loss = 0.26293257\n",
      "Iteration 898, loss = 0.26248220\n",
      "Iteration 899, loss = 0.26203238\n",
      "Iteration 900, loss = 0.26158312\n",
      "Iteration 901, loss = 0.26113442\n",
      "Iteration 902, loss = 0.26068628\n",
      "Iteration 903, loss = 0.26023870\n",
      "Iteration 904, loss = 0.25979170\n",
      "Iteration 905, loss = 0.25934526\n",
      "Iteration 906, loss = 0.25889940\n",
      "Iteration 907, loss = 0.25845412\n",
      "Iteration 908, loss = 0.25800941\n",
      "Iteration 909, loss = 0.25756529\n",
      "Iteration 910, loss = 0.25712176\n",
      "Iteration 911, loss = 0.25667882\n",
      "Iteration 912, loss = 0.25623647\n",
      "Iteration 913, loss = 0.25579472\n",
      "Iteration 914, loss = 0.25535356\n",
      "Iteration 915, loss = 0.25491301\n",
      "Iteration 916, loss = 0.25447306\n",
      "Iteration 917, loss = 0.25403372\n",
      "Iteration 918, loss = 0.25359499\n",
      "Iteration 919, loss = 0.25315688\n",
      "Iteration 920, loss = 0.25271938\n",
      "Iteration 921, loss = 0.25228250\n",
      "Iteration 922, loss = 0.25184624\n",
      "Iteration 923, loss = 0.25141060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 924, loss = 0.25097560\n",
      "Iteration 925, loss = 0.25054122\n",
      "Iteration 926, loss = 0.25010748\n",
      "Iteration 927, loss = 0.24967437\n",
      "Iteration 928, loss = 0.24924190\n",
      "Iteration 929, loss = 0.24881007\n",
      "Iteration 930, loss = 0.24837889\n",
      "Iteration 931, loss = 0.24794835\n",
      "Iteration 932, loss = 0.24751846\n",
      "Iteration 933, loss = 0.24708922\n",
      "Iteration 934, loss = 0.24666063\n",
      "Iteration 935, loss = 0.24623270\n",
      "Iteration 936, loss = 0.24580543\n",
      "Iteration 937, loss = 0.24537882\n",
      "Iteration 938, loss = 0.24495288\n",
      "Iteration 939, loss = 0.24452760\n",
      "Iteration 940, loss = 0.24410298\n",
      "Iteration 941, loss = 0.24367904\n",
      "Iteration 942, loss = 0.24325578\n",
      "Iteration 943, loss = 0.24283318\n",
      "Iteration 944, loss = 0.24241127\n",
      "Iteration 945, loss = 0.24199003\n",
      "Iteration 946, loss = 0.24156948\n",
      "Iteration 947, loss = 0.24114961\n",
      "Iteration 948, loss = 0.24073043\n",
      "Iteration 949, loss = 0.24031193\n",
      "Iteration 950, loss = 0.23989413\n",
      "Iteration 951, loss = 0.23947702\n",
      "Iteration 952, loss = 0.23906060\n",
      "Iteration 953, loss = 0.23864488\n",
      "Iteration 954, loss = 0.23822986\n",
      "Iteration 955, loss = 0.23781554\n",
      "Iteration 956, loss = 0.23740192\n",
      "Iteration 957, loss = 0.23698901\n",
      "Iteration 958, loss = 0.23657681\n",
      "Iteration 959, loss = 0.23616531\n",
      "Iteration 960, loss = 0.23575452\n",
      "Iteration 961, loss = 0.23534445\n",
      "Iteration 962, loss = 0.23493509\n",
      "Iteration 963, loss = 0.23452644\n",
      "Iteration 964, loss = 0.23411852\n",
      "Iteration 965, loss = 0.23371131\n",
      "Iteration 966, loss = 0.23330483\n",
      "Iteration 967, loss = 0.23289906\n",
      "Iteration 968, loss = 0.23249402\n",
      "Iteration 969, loss = 0.23208971\n",
      "Iteration 970, loss = 0.23168613\n",
      "Iteration 971, loss = 0.23128327\n",
      "Iteration 972, loss = 0.23088115\n",
      "Iteration 973, loss = 0.23047976\n",
      "Iteration 974, loss = 0.23007910\n",
      "Iteration 975, loss = 0.22967918\n",
      "Iteration 976, loss = 0.22927999\n",
      "Iteration 977, loss = 0.22888154\n",
      "Iteration 978, loss = 0.22848384\n",
      "Iteration 979, loss = 0.22808687\n",
      "Iteration 980, loss = 0.22769065\n",
      "Iteration 981, loss = 0.22729517\n",
      "Iteration 982, loss = 0.22690043\n",
      "Iteration 983, loss = 0.22650645\n",
      "Iteration 984, loss = 0.22611321\n",
      "Iteration 985, loss = 0.22572072\n",
      "Iteration 986, loss = 0.22532898\n",
      "Iteration 987, loss = 0.22493799\n",
      "Iteration 988, loss = 0.22454775\n",
      "Iteration 989, loss = 0.22415827\n",
      "Iteration 990, loss = 0.22376954\n",
      "Iteration 991, loss = 0.22338157\n",
      "Iteration 992, loss = 0.22299436\n",
      "Iteration 993, loss = 0.22260790\n",
      "Iteration 994, loss = 0.22222220\n",
      "Iteration 995, loss = 0.22183727\n",
      "Iteration 996, loss = 0.22145309\n",
      "Iteration 997, loss = 0.22106968\n",
      "Iteration 998, loss = 0.22068703\n",
      "Iteration 999, loss = 0.22030514\n",
      "Iteration 1000, loss = 0.21992402\n",
      "Iteration 1, loss = 2.10484323\n",
      "Iteration 2, loss = 2.07321804\n",
      "Iteration 3, loss = 2.04187298\n",
      "Iteration 4, loss = 2.01083043\n",
      "Iteration 5, loss = 1.98010077\n",
      "Iteration 6, loss = 1.94970460\n",
      "Iteration 7, loss = 1.91965043\n",
      "Iteration 8, loss = 1.88996137\n",
      "Iteration 9, loss = 1.86064891\n",
      "Iteration 10, loss = 1.83171542\n",
      "Iteration 11, loss = 1.80318368\n",
      "Iteration 12, loss = 1.77504880\n",
      "Iteration 13, loss = 1.74734591\n",
      "Iteration 14, loss = 1.72007563\n",
      "Iteration 15, loss = 1.69325128\n",
      "Iteration 16, loss = 1.66689470\n",
      "Iteration 17, loss = 1.64100817\n",
      "Iteration 18, loss = 1.61557942\n",
      "Iteration 19, loss = 1.59064518\n",
      "Iteration 20, loss = 1.56623165\n",
      "Iteration 21, loss = 1.54233450\n",
      "Iteration 22, loss = 1.51899005\n",
      "Iteration 23, loss = 1.49616291\n",
      "Iteration 24, loss = 1.47387636\n",
      "Iteration 25, loss = 1.45216584\n",
      "Iteration 26, loss = 1.43103661\n",
      "Iteration 27, loss = 1.41048372\n",
      "Iteration 28, loss = 1.39052160\n",
      "Iteration 29, loss = 1.37115100\n",
      "Iteration 30, loss = 1.35238145\n",
      "Iteration 31, loss = 1.33422392\n",
      "Iteration 32, loss = 1.31664998\n",
      "Iteration 33, loss = 1.29968226\n",
      "Iteration 34, loss = 1.28332352\n",
      "Iteration 35, loss = 1.26757078\n",
      "Iteration 36, loss = 1.25241974\n",
      "Iteration 37, loss = 1.23786489\n",
      "Iteration 38, loss = 1.22389940\n",
      "Iteration 39, loss = 1.21049759\n",
      "Iteration 40, loss = 1.19761335\n",
      "Iteration 41, loss = 1.18526273\n",
      "Iteration 42, loss = 1.17343146\n",
      "Iteration 43, loss = 1.16207615\n",
      "Iteration 44, loss = 1.15121226\n",
      "Iteration 45, loss = 1.14083708\n",
      "Iteration 46, loss = 1.13095763\n",
      "Iteration 47, loss = 1.12155893\n",
      "Iteration 48, loss = 1.11262469\n",
      "Iteration 49, loss = 1.10411852\n",
      "Iteration 50, loss = 1.09603263\n",
      "Iteration 51, loss = 1.08829698\n",
      "Iteration 52, loss = 1.08093197\n",
      "Iteration 53, loss = 1.07390271\n",
      "Iteration 54, loss = 1.06721241\n",
      "Iteration 55, loss = 1.06084257\n",
      "Iteration 56, loss = 1.05477495\n",
      "Iteration 57, loss = 1.04895901\n",
      "Iteration 58, loss = 1.04339120\n",
      "Iteration 59, loss = 1.03807443\n",
      "Iteration 60, loss = 1.03297616\n",
      "Iteration 61, loss = 1.02808945\n",
      "Iteration 62, loss = 1.02337320\n",
      "Iteration 63, loss = 1.01881863\n",
      "Iteration 64, loss = 1.01441052\n",
      "Iteration 65, loss = 1.01014093\n",
      "Iteration 66, loss = 1.00600118\n",
      "Iteration 67, loss = 1.00197701\n",
      "Iteration 68, loss = 0.99805489\n",
      "Iteration 69, loss = 0.99422216\n",
      "Iteration 70, loss = 0.99046699\n",
      "Iteration 71, loss = 0.98677457\n",
      "Iteration 72, loss = 0.98313467\n",
      "Iteration 73, loss = 0.97954275\n",
      "Iteration 74, loss = 0.97598356\n",
      "Iteration 75, loss = 0.97245689\n",
      "Iteration 76, loss = 0.96895707\n",
      "Iteration 77, loss = 0.96547273\n",
      "Iteration 78, loss = 0.96200516\n",
      "Iteration 79, loss = 0.95855037\n",
      "Iteration 80, loss = 0.95510031\n",
      "Iteration 81, loss = 0.95165356\n",
      "Iteration 82, loss = 0.94820630\n",
      "Iteration 83, loss = 0.94476311\n",
      "Iteration 84, loss = 0.94132243\n",
      "Iteration 85, loss = 0.93788018\n",
      "Iteration 86, loss = 0.93443988\n",
      "Iteration 87, loss = 0.93100143\n",
      "Iteration 88, loss = 0.92756493\n",
      "Iteration 89, loss = 0.92412865\n",
      "Iteration 90, loss = 0.92069406\n",
      "Iteration 91, loss = 0.91726273\n",
      "Iteration 92, loss = 0.91383532\n",
      "Iteration 93, loss = 0.91041259\n",
      "Iteration 94, loss = 0.90701819\n",
      "Iteration 95, loss = 0.90364201\n",
      "Iteration 96, loss = 0.90027542\n",
      "Iteration 97, loss = 0.89691898\n",
      "Iteration 98, loss = 0.89357325\n",
      "Iteration 99, loss = 0.89023881\n",
      "Iteration 100, loss = 0.88691622\n",
      "Iteration 101, loss = 0.88360602\n",
      "Iteration 102, loss = 0.88032258\n",
      "Iteration 103, loss = 0.87709638\n",
      "Iteration 104, loss = 0.87390092\n",
      "Iteration 105, loss = 0.87073799\n",
      "Iteration 106, loss = 0.86761175\n",
      "Iteration 107, loss = 0.86450612\n",
      "Iteration 108, loss = 0.86142085\n",
      "Iteration 109, loss = 0.85835569\n",
      "Iteration 110, loss = 0.85531044\n",
      "Iteration 111, loss = 0.85228313\n",
      "Iteration 112, loss = 0.84930087\n",
      "Iteration 113, loss = 0.84640455\n",
      "Iteration 114, loss = 0.84357605\n",
      "Iteration 115, loss = 0.84082361\n",
      "Iteration 116, loss = 0.83818039\n",
      "Iteration 117, loss = 0.83568358\n",
      "Iteration 118, loss = 0.83325326\n",
      "Iteration 119, loss = 0.83087126\n",
      "Iteration 120, loss = 0.82850460\n",
      "Iteration 121, loss = 0.82614765\n",
      "Iteration 122, loss = 0.82380817\n",
      "Iteration 123, loss = 0.82148473\n",
      "Iteration 124, loss = 0.81918275\n",
      "Iteration 125, loss = 0.81689643\n",
      "Iteration 126, loss = 0.81461574\n",
      "Iteration 127, loss = 0.81233925\n",
      "Iteration 128, loss = 0.81006832\n",
      "Iteration 129, loss = 0.80780415\n",
      "Iteration 130, loss = 0.80554782\n",
      "Iteration 131, loss = 0.80330030\n",
      "Iteration 132, loss = 0.80106240\n",
      "Iteration 133, loss = 0.79883486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 134, loss = 0.79661685\n",
      "Iteration 135, loss = 0.79440984\n",
      "Iteration 136, loss = 0.79221471\n",
      "Iteration 137, loss = 0.79003185\n",
      "Iteration 138, loss = 0.78786415\n",
      "Iteration 139, loss = 0.78571630\n",
      "Iteration 140, loss = 0.78358757\n",
      "Iteration 141, loss = 0.78147518\n",
      "Iteration 142, loss = 0.77937690\n",
      "Iteration 143, loss = 0.77729254\n",
      "Iteration 144, loss = 0.77522194\n",
      "Iteration 145, loss = 0.77316490\n",
      "Iteration 146, loss = 0.77112127\n",
      "Iteration 147, loss = 0.76909088\n",
      "Iteration 148, loss = 0.76707358\n",
      "Iteration 149, loss = 0.76506920\n",
      "Iteration 150, loss = 0.76307761\n",
      "Iteration 151, loss = 0.76110029\n",
      "Iteration 152, loss = 0.75913621\n",
      "Iteration 153, loss = 0.75718434\n",
      "Iteration 154, loss = 0.75524454\n",
      "Iteration 155, loss = 0.75331672\n",
      "Iteration 156, loss = 0.75140077\n",
      "Iteration 157, loss = 0.74949662\n",
      "Iteration 158, loss = 0.74760416\n",
      "Iteration 159, loss = 0.74572331\n",
      "Iteration 160, loss = 0.74385400\n",
      "Iteration 161, loss = 0.74199616\n",
      "Iteration 162, loss = 0.74014970\n",
      "Iteration 163, loss = 0.73831467\n",
      "Iteration 164, loss = 0.73649125\n",
      "Iteration 165, loss = 0.73467904\n",
      "Iteration 166, loss = 0.73287797\n",
      "Iteration 167, loss = 0.73108797\n",
      "Iteration 168, loss = 0.72930896\n",
      "Iteration 169, loss = 0.72754088\n",
      "Iteration 170, loss = 0.72578366\n",
      "Iteration 171, loss = 0.72403723\n",
      "Iteration 172, loss = 0.72230151\n",
      "Iteration 173, loss = 0.72057643\n",
      "Iteration 174, loss = 0.71886193\n",
      "Iteration 175, loss = 0.71715792\n",
      "Iteration 176, loss = 0.71546435\n",
      "Iteration 177, loss = 0.71378113\n",
      "Iteration 178, loss = 0.71210819\n",
      "Iteration 179, loss = 0.71044546\n",
      "Iteration 180, loss = 0.70879287\n",
      "Iteration 181, loss = 0.70715034\n",
      "Iteration 182, loss = 0.70551780\n",
      "Iteration 183, loss = 0.70389517\n",
      "Iteration 184, loss = 0.70228238\n",
      "Iteration 185, loss = 0.70067935\n",
      "Iteration 186, loss = 0.69908601\n",
      "Iteration 187, loss = 0.69750228\n",
      "Iteration 188, loss = 0.69592809\n",
      "Iteration 189, loss = 0.69436335\n",
      "Iteration 190, loss = 0.69280801\n",
      "Iteration 191, loss = 0.69126197\n",
      "Iteration 192, loss = 0.68972517\n",
      "Iteration 193, loss = 0.68819820\n",
      "Iteration 194, loss = 0.68668069\n",
      "Iteration 195, loss = 0.68517227\n",
      "Iteration 196, loss = 0.68367287\n",
      "Iteration 197, loss = 0.68218243\n",
      "Iteration 198, loss = 0.68070088\n",
      "Iteration 199, loss = 0.67922815\n",
      "Iteration 200, loss = 0.67776418\n",
      "Iteration 201, loss = 0.67630890\n",
      "Iteration 202, loss = 0.67486224\n",
      "Iteration 203, loss = 0.67342412\n",
      "Iteration 204, loss = 0.67199448\n",
      "Iteration 205, loss = 0.67057329\n",
      "Iteration 206, loss = 0.66916091\n",
      "Iteration 207, loss = 0.66775718\n",
      "Iteration 208, loss = 0.66636259\n",
      "Iteration 209, loss = 0.66497679\n",
      "Iteration 210, loss = 0.66359915\n",
      "Iteration 211, loss = 0.66222961\n",
      "Iteration 212, loss = 0.66086813\n",
      "Iteration 213, loss = 0.65951463\n",
      "Iteration 214, loss = 0.65816908\n",
      "Iteration 215, loss = 0.65683172\n",
      "Iteration 216, loss = 0.65550231\n",
      "Iteration 217, loss = 0.65418069\n",
      "Iteration 218, loss = 0.65286699\n",
      "Iteration 219, loss = 0.65156109\n",
      "Iteration 220, loss = 0.65026345\n",
      "Iteration 221, loss = 0.64897349\n",
      "Iteration 222, loss = 0.64769101\n",
      "Iteration 223, loss = 0.64641599\n",
      "Iteration 224, loss = 0.64514837\n",
      "Iteration 225, loss = 0.64388810\n",
      "Iteration 226, loss = 0.64263515\n",
      "Iteration 227, loss = 0.64138945\n",
      "Iteration 228, loss = 0.64015096\n",
      "Iteration 229, loss = 0.63891961\n",
      "Iteration 230, loss = 0.63769541\n",
      "Iteration 231, loss = 0.63647820\n",
      "Iteration 232, loss = 0.63526861\n",
      "Iteration 233, loss = 0.63406609\n",
      "Iteration 234, loss = 0.63287070\n",
      "Iteration 235, loss = 0.63168215\n",
      "Iteration 236, loss = 0.63050036\n",
      "Iteration 237, loss = 0.62932534\n",
      "Iteration 238, loss = 0.62815736\n",
      "Iteration 239, loss = 0.62699595\n",
      "Iteration 240, loss = 0.62584105\n",
      "Iteration 241, loss = 0.62469261\n",
      "Iteration 242, loss = 0.62355067\n",
      "Iteration 243, loss = 0.62241510\n",
      "Iteration 244, loss = 0.62128577\n",
      "Iteration 245, loss = 0.62016277\n",
      "Iteration 246, loss = 0.61904596\n",
      "Iteration 247, loss = 0.61793550\n",
      "Iteration 248, loss = 0.61683214\n",
      "Iteration 249, loss = 0.61573525\n",
      "Iteration 250, loss = 0.61464458\n",
      "Iteration 251, loss = 0.61355987\n",
      "Iteration 252, loss = 0.61248106\n",
      "Iteration 253, loss = 0.61140808\n",
      "Iteration 254, loss = 0.61034090\n",
      "Iteration 255, loss = 0.60927959\n",
      "Iteration 256, loss = 0.60822392\n",
      "Iteration 257, loss = 0.60717389\n",
      "Iteration 258, loss = 0.60612963\n",
      "Iteration 259, loss = 0.60509083\n",
      "Iteration 260, loss = 0.60405744\n",
      "Iteration 261, loss = 0.60302939\n",
      "Iteration 262, loss = 0.60200670\n",
      "Iteration 263, loss = 0.60098930\n",
      "Iteration 264, loss = 0.59997706\n",
      "Iteration 265, loss = 0.59896993\n",
      "Iteration 266, loss = 0.59796785\n",
      "Iteration 267, loss = 0.59697075\n",
      "Iteration 268, loss = 0.59598047\n",
      "Iteration 269, loss = 0.59499574\n",
      "Iteration 270, loss = 0.59401572\n",
      "Iteration 271, loss = 0.59304041\n",
      "Iteration 272, loss = 0.59206985\n",
      "Iteration 273, loss = 0.59110405\n",
      "Iteration 274, loss = 0.59014299\n",
      "Iteration 275, loss = 0.58918669\n",
      "Iteration 276, loss = 0.58823514\n",
      "Iteration 277, loss = 0.58728831\n",
      "Iteration 278, loss = 0.58634619\n",
      "Iteration 279, loss = 0.58540876\n",
      "Iteration 280, loss = 0.58447599\n",
      "Iteration 281, loss = 0.58354785\n",
      "Iteration 282, loss = 0.58262431\n",
      "Iteration 283, loss = 0.58170532\n",
      "Iteration 284, loss = 0.58079112\n",
      "Iteration 285, loss = 0.57988138\n",
      "Iteration 286, loss = 0.57897585\n",
      "Iteration 287, loss = 0.57807447\n",
      "Iteration 288, loss = 0.57717722\n",
      "Iteration 289, loss = 0.57628457\n",
      "Iteration 290, loss = 0.57539611\n",
      "Iteration 291, loss = 0.57451180\n",
      "Iteration 292, loss = 0.57363161\n",
      "Iteration 293, loss = 0.57275548\n",
      "Iteration 294, loss = 0.57188342\n",
      "Iteration 295, loss = 0.57101534\n",
      "Iteration 296, loss = 0.57015120\n",
      "Iteration 297, loss = 0.56929094\n",
      "Iteration 298, loss = 0.56843454\n",
      "Iteration 299, loss = 0.56758262\n",
      "Iteration 300, loss = 0.56673463\n",
      "Iteration 301, loss = 0.56589034\n",
      "Iteration 302, loss = 0.56504974\n",
      "Iteration 303, loss = 0.56421282\n",
      "Iteration 304, loss = 0.56337957\n",
      "Iteration 305, loss = 0.56254998\n",
      "Iteration 306, loss = 0.56172402\n",
      "Iteration 307, loss = 0.56090168\n",
      "Iteration 308, loss = 0.56008292\n",
      "Iteration 309, loss = 0.55926772\n",
      "Iteration 310, loss = 0.55845604\n",
      "Iteration 311, loss = 0.55764786\n",
      "Iteration 312, loss = 0.55684314\n",
      "Iteration 313, loss = 0.55604184\n",
      "Iteration 314, loss = 0.55524396\n",
      "Iteration 315, loss = 0.55444944\n",
      "Iteration 316, loss = 0.55365823\n",
      "Iteration 317, loss = 0.55287030\n",
      "Iteration 318, loss = 0.55208561\n",
      "Iteration 319, loss = 0.55130411\n",
      "Iteration 320, loss = 0.55052576\n",
      "Iteration 321, loss = 0.54975053\n",
      "Iteration 322, loss = 0.54897838\n",
      "Iteration 323, loss = 0.54820925\n",
      "Iteration 324, loss = 0.54744312\n",
      "Iteration 325, loss = 0.54667994\n",
      "Iteration 326, loss = 0.54591967\n",
      "Iteration 327, loss = 0.54516226\n",
      "Iteration 328, loss = 0.54440769\n",
      "Iteration 329, loss = 0.54365591\n",
      "Iteration 330, loss = 0.54290688\n",
      "Iteration 331, loss = 0.54216057\n",
      "Iteration 332, loss = 0.54141693\n",
      "Iteration 333, loss = 0.54067592\n",
      "Iteration 334, loss = 0.53993802\n",
      "Iteration 335, loss = 0.53920328\n",
      "Iteration 336, loss = 0.53847116\n",
      "Iteration 337, loss = 0.53774162\n",
      "Iteration 338, loss = 0.53701465\n",
      "Iteration 339, loss = 0.53629023\n",
      "Iteration 340, loss = 0.53556833\n",
      "Iteration 341, loss = 0.53484898\n",
      "Iteration 342, loss = 0.53413212\n",
      "Iteration 343, loss = 0.53341777\n",
      "Iteration 344, loss = 0.53270587\n",
      "Iteration 345, loss = 0.53199637\n",
      "Iteration 346, loss = 0.53128925\n",
      "Iteration 347, loss = 0.53058448\n",
      "Iteration 348, loss = 0.52988203\n",
      "Iteration 349, loss = 0.52918187\n",
      "Iteration 350, loss = 0.52848398\n",
      "Iteration 351, loss = 0.52778831\n",
      "Iteration 352, loss = 0.52709485\n",
      "Iteration 353, loss = 0.52640356\n",
      "Iteration 354, loss = 0.52571441\n",
      "Iteration 355, loss = 0.52502740\n",
      "Iteration 356, loss = 0.52434317\n",
      "Iteration 357, loss = 0.52366104\n",
      "Iteration 358, loss = 0.52298099\n",
      "Iteration 359, loss = 0.52230300\n",
      "Iteration 360, loss = 0.52162707\n",
      "Iteration 361, loss = 0.52095317\n",
      "Iteration 362, loss = 0.52028130\n",
      "Iteration 363, loss = 0.51961142\n",
      "Iteration 364, loss = 0.51894353\n",
      "Iteration 365, loss = 0.51827760\n",
      "Iteration 366, loss = 0.51761360\n",
      "Iteration 367, loss = 0.51695152\n",
      "Iteration 368, loss = 0.51629133\n",
      "Iteration 369, loss = 0.51563300\n",
      "Iteration 370, loss = 0.51497652\n",
      "Iteration 371, loss = 0.51432190\n",
      "Iteration 372, loss = 0.51366909\n",
      "Iteration 373, loss = 0.51301805\n",
      "Iteration 374, loss = 0.51236876\n",
      "Iteration 375, loss = 0.51172118\n",
      "Iteration 376, loss = 0.51107529\n",
      "Iteration 377, loss = 0.51043107\n",
      "Iteration 378, loss = 0.50978849\n",
      "Iteration 379, loss = 0.50914752\n",
      "Iteration 380, loss = 0.50850815\n",
      "Iteration 381, loss = 0.50787043\n",
      "Iteration 382, loss = 0.50723426\n",
      "Iteration 383, loss = 0.50659960\n",
      "Iteration 384, loss = 0.50596644\n",
      "Iteration 385, loss = 0.50533475\n",
      "Iteration 386, loss = 0.50470450\n",
      "Iteration 387, loss = 0.50407567\n",
      "Iteration 388, loss = 0.50344823\n",
      "Iteration 389, loss = 0.50282217\n",
      "Iteration 390, loss = 0.50219745\n",
      "Iteration 391, loss = 0.50157405\n",
      "Iteration 392, loss = 0.50095195\n",
      "Iteration 393, loss = 0.50033113\n",
      "Iteration 394, loss = 0.49971164\n",
      "Iteration 395, loss = 0.49909342\n",
      "Iteration 396, loss = 0.49847739\n",
      "Iteration 397, loss = 0.49786321\n",
      "Iteration 398, loss = 0.49725033\n",
      "Iteration 399, loss = 0.49663872\n",
      "Iteration 400, loss = 0.49602838\n",
      "Iteration 401, loss = 0.49541927\n",
      "Iteration 402, loss = 0.49481202\n",
      "Iteration 403, loss = 0.49420644\n",
      "Iteration 404, loss = 0.49360199\n",
      "Iteration 405, loss = 0.49299868\n",
      "Iteration 406, loss = 0.49239652\n",
      "Iteration 407, loss = 0.49179552\n",
      "Iteration 408, loss = 0.49119571\n",
      "Iteration 409, loss = 0.49059707\n",
      "Iteration 410, loss = 0.48999960\n",
      "Iteration 411, loss = 0.48940327\n",
      "Iteration 412, loss = 0.48880810\n",
      "Iteration 413, loss = 0.48821406\n",
      "Iteration 414, loss = 0.48762130\n",
      "Iteration 415, loss = 0.48702968\n",
      "Iteration 416, loss = 0.48643919\n",
      "Iteration 417, loss = 0.48584980\n",
      "Iteration 418, loss = 0.48526151\n",
      "Iteration 419, loss = 0.48467430\n",
      "Iteration 420, loss = 0.48408815\n",
      "Iteration 421, loss = 0.48350305\n",
      "Iteration 422, loss = 0.48291898\n",
      "Iteration 423, loss = 0.48233592\n",
      "Iteration 424, loss = 0.48175385\n",
      "Iteration 425, loss = 0.48117276\n",
      "Iteration 426, loss = 0.48059263\n",
      "Iteration 427, loss = 0.48001344\n",
      "Iteration 428, loss = 0.47943531\n",
      "Iteration 429, loss = 0.47885810\n",
      "Iteration 430, loss = 0.47828179\n",
      "Iteration 431, loss = 0.47770636\n",
      "Iteration 432, loss = 0.47713188\n",
      "Iteration 433, loss = 0.47655826\n",
      "Iteration 434, loss = 0.47598546\n",
      "Iteration 435, loss = 0.47541348\n",
      "Iteration 436, loss = 0.47484229\n",
      "Iteration 437, loss = 0.47427187\n",
      "Iteration 438, loss = 0.47370221\n",
      "Iteration 439, loss = 0.47313330\n",
      "Iteration 440, loss = 0.47256510\n",
      "Iteration 441, loss = 0.47199761\n",
      "Iteration 442, loss = 0.47143081\n",
      "Iteration 443, loss = 0.47086469\n",
      "Iteration 444, loss = 0.47029932\n",
      "Iteration 445, loss = 0.46973463\n",
      "Iteration 446, loss = 0.46917058\n",
      "Iteration 447, loss = 0.46860714\n",
      "Iteration 448, loss = 0.46804432\n",
      "Iteration 449, loss = 0.46748208\n",
      "Iteration 450, loss = 0.46692041\n",
      "Iteration 451, loss = 0.46635931\n",
      "Iteration 452, loss = 0.46579875\n",
      "Iteration 453, loss = 0.46523872\n",
      "Iteration 454, loss = 0.46467922\n",
      "Iteration 455, loss = 0.46412033\n",
      "Iteration 456, loss = 0.46356194\n",
      "Iteration 457, loss = 0.46300462\n",
      "Iteration 458, loss = 0.46244917\n",
      "Iteration 459, loss = 0.46189427\n",
      "Iteration 460, loss = 0.46133993\n",
      "Iteration 461, loss = 0.46078612\n",
      "Iteration 462, loss = 0.46023284\n",
      "Iteration 463, loss = 0.45968009\n",
      "Iteration 464, loss = 0.45912785\n",
      "Iteration 465, loss = 0.45857612\n",
      "Iteration 466, loss = 0.45802489\n",
      "Iteration 467, loss = 0.45747415\n",
      "Iteration 468, loss = 0.45692410\n",
      "Iteration 469, loss = 0.45637456\n",
      "Iteration 470, loss = 0.45582549\n",
      "Iteration 471, loss = 0.45527690\n",
      "Iteration 472, loss = 0.45472877\n",
      "Iteration 473, loss = 0.45418109\n",
      "Iteration 474, loss = 0.45363384\n",
      "Iteration 475, loss = 0.45308701\n",
      "Iteration 476, loss = 0.45254070\n",
      "Iteration 477, loss = 0.45199480\n",
      "Iteration 478, loss = 0.45144930\n",
      "Iteration 479, loss = 0.45090423\n",
      "Iteration 480, loss = 0.45035957\n",
      "Iteration 481, loss = 0.44981527\n",
      "Iteration 482, loss = 0.44927133\n",
      "Iteration 483, loss = 0.44872888\n",
      "Iteration 484, loss = 0.44818722\n",
      "Iteration 485, loss = 0.44764651\n",
      "Iteration 486, loss = 0.44710629\n",
      "Iteration 487, loss = 0.44656601\n",
      "Iteration 488, loss = 0.44602581\n",
      "Iteration 489, loss = 0.44548575\n",
      "Iteration 490, loss = 0.44494584\n",
      "Iteration 491, loss = 0.44440611\n",
      "Iteration 492, loss = 0.44386695\n",
      "Iteration 493, loss = 0.44332884\n",
      "Iteration 494, loss = 0.44279125\n",
      "Iteration 495, loss = 0.44225409\n",
      "Iteration 496, loss = 0.44171791\n",
      "Iteration 497, loss = 0.44118176\n",
      "Iteration 498, loss = 0.44064562\n",
      "Iteration 499, loss = 0.44010947\n",
      "Iteration 500, loss = 0.43957365\n",
      "Iteration 501, loss = 0.43903849\n",
      "Iteration 502, loss = 0.43850363\n",
      "Iteration 503, loss = 0.43796927\n",
      "Iteration 504, loss = 0.43743551\n",
      "Iteration 505, loss = 0.43690189\n",
      "Iteration 506, loss = 0.43636842\n",
      "Iteration 507, loss = 0.43583514\n",
      "Iteration 508, loss = 0.43530206\n",
      "Iteration 509, loss = 0.43476938\n",
      "Iteration 510, loss = 0.43423723\n",
      "Iteration 511, loss = 0.43370530\n",
      "Iteration 512, loss = 0.43317357\n",
      "Iteration 513, loss = 0.43264203\n",
      "Iteration 514, loss = 0.43211068\n",
      "Iteration 515, loss = 0.43157954\n",
      "Iteration 516, loss = 0.43104888\n",
      "Iteration 517, loss = 0.43051837\n",
      "Iteration 518, loss = 0.42998804\n",
      "Iteration 519, loss = 0.42945790\n",
      "Iteration 520, loss = 0.42892813\n",
      "Iteration 521, loss = 0.42839851\n",
      "Iteration 522, loss = 0.42786897\n",
      "Iteration 523, loss = 0.42733971\n",
      "Iteration 524, loss = 0.42681070\n",
      "Iteration 525, loss = 0.42628183\n",
      "Iteration 526, loss = 0.42575338\n",
      "Iteration 527, loss = 0.42522512\n",
      "Iteration 528, loss = 0.42469704\n",
      "Iteration 529, loss = 0.42416915\n",
      "Iteration 530, loss = 0.42364143\n",
      "Iteration 531, loss = 0.42311404\n",
      "Iteration 532, loss = 0.42258666\n",
      "Iteration 533, loss = 0.42205940\n",
      "Iteration 534, loss = 0.42153238\n",
      "Iteration 535, loss = 0.42100550\n",
      "Iteration 536, loss = 0.42047874\n",
      "Iteration 537, loss = 0.41995211\n",
      "Iteration 538, loss = 0.41942560\n",
      "Iteration 539, loss = 0.41889920\n",
      "Iteration 540, loss = 0.41837292\n",
      "Iteration 541, loss = 0.41784674\n",
      "Iteration 542, loss = 0.41732067\n",
      "Iteration 543, loss = 0.41679468\n",
      "Iteration 544, loss = 0.41626879\n",
      "Iteration 545, loss = 0.41574298\n",
      "Iteration 546, loss = 0.41521725\n",
      "Iteration 547, loss = 0.41469159\n",
      "Iteration 548, loss = 0.41416600\n",
      "Iteration 549, loss = 0.41364046\n",
      "Iteration 550, loss = 0.41311498\n",
      "Iteration 551, loss = 0.41258954\n",
      "Iteration 552, loss = 0.41206414\n",
      "Iteration 553, loss = 0.41153877\n",
      "Iteration 554, loss = 0.41101344\n",
      "Iteration 555, loss = 0.41048812\n",
      "Iteration 556, loss = 0.40996283\n",
      "Iteration 557, loss = 0.40943754\n",
      "Iteration 558, loss = 0.40891225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 559, loss = 0.40838697\n",
      "Iteration 560, loss = 0.40786168\n",
      "Iteration 561, loss = 0.40733637\n",
      "Iteration 562, loss = 0.40681105\n",
      "Iteration 563, loss = 0.40628570\n",
      "Iteration 564, loss = 0.40576033\n",
      "Iteration 565, loss = 0.40523492\n",
      "Iteration 566, loss = 0.40470948\n",
      "Iteration 567, loss = 0.40418399\n",
      "Iteration 568, loss = 0.40365845\n",
      "Iteration 569, loss = 0.40313286\n",
      "Iteration 570, loss = 0.40260726\n",
      "Iteration 571, loss = 0.40208184\n",
      "Iteration 572, loss = 0.40155637\n",
      "Iteration 573, loss = 0.40103085\n",
      "Iteration 574, loss = 0.40050528\n",
      "Iteration 575, loss = 0.39997968\n",
      "Iteration 576, loss = 0.39945396\n",
      "Iteration 577, loss = 0.39892819\n",
      "Iteration 578, loss = 0.39840235\n",
      "Iteration 579, loss = 0.39787643\n",
      "Iteration 580, loss = 0.39735043\n",
      "Iteration 581, loss = 0.39682433\n",
      "Iteration 582, loss = 0.39629815\n",
      "Iteration 583, loss = 0.39577187\n",
      "Iteration 584, loss = 0.39524548\n",
      "Iteration 585, loss = 0.39471900\n",
      "Iteration 586, loss = 0.39419240\n",
      "Iteration 587, loss = 0.39366570\n",
      "Iteration 588, loss = 0.39313888\n",
      "Iteration 589, loss = 0.39261195\n",
      "Iteration 590, loss = 0.39208564\n",
      "Iteration 591, loss = 0.39155963\n",
      "Iteration 592, loss = 0.39103347\n",
      "Iteration 593, loss = 0.39050715\n",
      "Iteration 594, loss = 0.38998068\n",
      "Iteration 595, loss = 0.38945408\n",
      "Iteration 596, loss = 0.38892736\n",
      "Iteration 597, loss = 0.38840051\n",
      "Iteration 598, loss = 0.38787356\n",
      "Iteration 599, loss = 0.38734650\n",
      "Iteration 600, loss = 0.38681933\n",
      "Iteration 601, loss = 0.38629207\n",
      "Iteration 602, loss = 0.38576471\n",
      "Iteration 603, loss = 0.38523726\n",
      "Iteration 604, loss = 0.38470973\n",
      "Iteration 605, loss = 0.38418210\n",
      "Iteration 606, loss = 0.38365439\n",
      "Iteration 607, loss = 0.38312712\n",
      "Iteration 608, loss = 0.38260014\n",
      "Iteration 609, loss = 0.38207314\n",
      "Iteration 610, loss = 0.38154612\n",
      "Iteration 611, loss = 0.38101907\n",
      "Iteration 612, loss = 0.38049199\n",
      "Iteration 613, loss = 0.37996487\n",
      "Iteration 614, loss = 0.37943770\n",
      "Iteration 615, loss = 0.37891049\n",
      "Iteration 616, loss = 0.37838323\n",
      "Iteration 617, loss = 0.37785591\n",
      "Iteration 618, loss = 0.37732852\n",
      "Iteration 619, loss = 0.37680110\n",
      "Iteration 620, loss = 0.37627384\n",
      "Iteration 621, loss = 0.37574653\n",
      "Iteration 622, loss = 0.37521916\n",
      "Iteration 623, loss = 0.37469172\n",
      "Iteration 624, loss = 0.37416422\n",
      "Iteration 625, loss = 0.37363664\n",
      "Iteration 626, loss = 0.37310899\n",
      "Iteration 627, loss = 0.37258126\n",
      "Iteration 628, loss = 0.37205345\n",
      "Iteration 629, loss = 0.37152555\n",
      "Iteration 630, loss = 0.37099756\n",
      "Iteration 631, loss = 0.37046948\n",
      "Iteration 632, loss = 0.36994131\n",
      "Iteration 633, loss = 0.36941304\n",
      "Iteration 634, loss = 0.36888468\n",
      "Iteration 635, loss = 0.36835621\n",
      "Iteration 636, loss = 0.36782765\n",
      "Iteration 637, loss = 0.36729898\n",
      "Iteration 638, loss = 0.36677021\n",
      "Iteration 639, loss = 0.36624134\n",
      "Iteration 640, loss = 0.36571237\n",
      "Iteration 641, loss = 0.36518328\n",
      "Iteration 642, loss = 0.36465409\n",
      "Iteration 643, loss = 0.36412480\n",
      "Iteration 644, loss = 0.36359540\n",
      "Iteration 645, loss = 0.36306589\n",
      "Iteration 646, loss = 0.36253627\n",
      "Iteration 647, loss = 0.36200655\n",
      "Iteration 648, loss = 0.36147672\n",
      "Iteration 649, loss = 0.36094678\n",
      "Iteration 650, loss = 0.36041674\n",
      "Iteration 651, loss = 0.35988659\n",
      "Iteration 652, loss = 0.35935633\n",
      "Iteration 653, loss = 0.35882596\n",
      "Iteration 654, loss = 0.35829550\n",
      "Iteration 655, loss = 0.35776492\n",
      "Iteration 656, loss = 0.35723425\n",
      "Iteration 657, loss = 0.35670347\n",
      "Iteration 658, loss = 0.35617259\n",
      "Iteration 659, loss = 0.35564161\n",
      "Iteration 660, loss = 0.35511052\n",
      "Iteration 661, loss = 0.35457934\n",
      "Iteration 662, loss = 0.35404807\n",
      "Iteration 663, loss = 0.35351669\n",
      "Iteration 664, loss = 0.35298522\n",
      "Iteration 665, loss = 0.35245366\n",
      "Iteration 666, loss = 0.35192200\n",
      "Iteration 667, loss = 0.35139026\n",
      "Iteration 668, loss = 0.35085842\n",
      "Iteration 669, loss = 0.35032650\n",
      "Iteration 670, loss = 0.34979449\n",
      "Iteration 671, loss = 0.34926240\n",
      "Iteration 672, loss = 0.34873023\n",
      "Iteration 673, loss = 0.34819797\n",
      "Iteration 674, loss = 0.34766564\n",
      "Iteration 675, loss = 0.34713323\n",
      "Iteration 676, loss = 0.34660075\n",
      "Iteration 677, loss = 0.34606819\n",
      "Iteration 678, loss = 0.34553557\n",
      "Iteration 679, loss = 0.34500287\n",
      "Iteration 680, loss = 0.34447012\n",
      "Iteration 681, loss = 0.34393729\n",
      "Iteration 682, loss = 0.34340441\n",
      "Iteration 683, loss = 0.34287147\n",
      "Iteration 684, loss = 0.34233847\n",
      "Iteration 685, loss = 0.34180542\n",
      "Iteration 686, loss = 0.34127232\n",
      "Iteration 687, loss = 0.34073916\n",
      "Iteration 688, loss = 0.34020597\n",
      "Iteration 689, loss = 0.33967273\n",
      "Iteration 690, loss = 0.33913945\n",
      "Iteration 691, loss = 0.33860613\n",
      "Iteration 692, loss = 0.33807362\n",
      "Iteration 693, loss = 0.33754132\n",
      "Iteration 694, loss = 0.33700890\n",
      "Iteration 695, loss = 0.33647639\n",
      "Iteration 696, loss = 0.33594379\n",
      "Iteration 697, loss = 0.33541112\n",
      "Iteration 698, loss = 0.33487840\n",
      "Iteration 699, loss = 0.33434563\n",
      "Iteration 700, loss = 0.33381283\n",
      "Iteration 701, loss = 0.33328001\n",
      "Iteration 702, loss = 0.33274717\n",
      "Iteration 703, loss = 0.33221434\n",
      "Iteration 704, loss = 0.33168151\n",
      "Iteration 705, loss = 0.33114869\n",
      "Iteration 706, loss = 0.33061590\n",
      "Iteration 707, loss = 0.33008314\n",
      "Iteration 708, loss = 0.32955042\n",
      "Iteration 709, loss = 0.32901773\n",
      "Iteration 710, loss = 0.32848510\n",
      "Iteration 711, loss = 0.32795306\n",
      "Iteration 712, loss = 0.32742095\n",
      "Iteration 713, loss = 0.32688876\n",
      "Iteration 714, loss = 0.32635649\n",
      "Iteration 715, loss = 0.32582416\n",
      "Iteration 716, loss = 0.32529178\n",
      "Iteration 717, loss = 0.32475939\n",
      "Iteration 718, loss = 0.32422750\n",
      "Iteration 719, loss = 0.32369565\n",
      "Iteration 720, loss = 0.32316386\n",
      "Iteration 721, loss = 0.32263212\n",
      "Iteration 722, loss = 0.32210046\n",
      "Iteration 723, loss = 0.32156886\n",
      "Iteration 724, loss = 0.32103734\n",
      "Iteration 725, loss = 0.32050591\n",
      "Iteration 726, loss = 0.31997457\n",
      "Iteration 727, loss = 0.31944333\n",
      "Iteration 728, loss = 0.31891218\n",
      "Iteration 729, loss = 0.31838115\n",
      "Iteration 730, loss = 0.31785025\n",
      "Iteration 731, loss = 0.31731951\n",
      "Iteration 732, loss = 0.31678884\n",
      "Iteration 733, loss = 0.31625833\n",
      "Iteration 734, loss = 0.31572794\n",
      "Iteration 735, loss = 0.31519768\n",
      "Iteration 736, loss = 0.31466755\n",
      "Iteration 737, loss = 0.31413755\n",
      "Iteration 738, loss = 0.31360769\n",
      "Iteration 739, loss = 0.31307797\n",
      "Iteration 740, loss = 0.31254841\n",
      "Iteration 741, loss = 0.31201901\n",
      "Iteration 742, loss = 0.31148978\n",
      "Iteration 743, loss = 0.31096070\n",
      "Iteration 744, loss = 0.31043178\n",
      "Iteration 745, loss = 0.30990303\n",
      "Iteration 746, loss = 0.30937445\n",
      "Iteration 747, loss = 0.30884605\n",
      "Iteration 748, loss = 0.30831783\n",
      "Iteration 749, loss = 0.30778987\n",
      "Iteration 750, loss = 0.30726197\n",
      "Iteration 751, loss = 0.30673433\n",
      "Iteration 752, loss = 0.30620688\n",
      "Iteration 753, loss = 0.30567964\n",
      "Iteration 754, loss = 0.30515260\n",
      "Iteration 755, loss = 0.30462577\n",
      "Iteration 756, loss = 0.30409915\n",
      "Iteration 757, loss = 0.30357275\n",
      "Iteration 758, loss = 0.30304657\n",
      "Iteration 759, loss = 0.30252062\n",
      "Iteration 760, loss = 0.30199490\n",
      "Iteration 761, loss = 0.30146941\n",
      "Iteration 762, loss = 0.30094417\n",
      "Iteration 763, loss = 0.30041916\n",
      "Iteration 764, loss = 0.29989441\n",
      "Iteration 765, loss = 0.29936990\n",
      "Iteration 766, loss = 0.29884566\n",
      "Iteration 767, loss = 0.29832171\n",
      "Iteration 768, loss = 0.29779796\n",
      "Iteration 769, loss = 0.29727451\n",
      "Iteration 770, loss = 0.29675133\n",
      "Iteration 771, loss = 0.29622843\n",
      "Iteration 772, loss = 0.29570581\n",
      "Iteration 773, loss = 0.29518347\n",
      "Iteration 774, loss = 0.29466143\n",
      "Iteration 775, loss = 0.29413968\n",
      "Iteration 776, loss = 0.29361823\n",
      "Iteration 777, loss = 0.29309708\n",
      "Iteration 778, loss = 0.29257624\n",
      "Iteration 779, loss = 0.29205571\n",
      "Iteration 780, loss = 0.29153550\n",
      "Iteration 781, loss = 0.29101561\n",
      "Iteration 782, loss = 0.29049604\n",
      "Iteration 783, loss = 0.28997680\n",
      "Iteration 784, loss = 0.28945790\n",
      "Iteration 785, loss = 0.28893933\n",
      "Iteration 786, loss = 0.28842111\n",
      "Iteration 787, loss = 0.28790323\n",
      "Iteration 788, loss = 0.28738570\n",
      "Iteration 789, loss = 0.28686853\n",
      "Iteration 790, loss = 0.28635172\n",
      "Iteration 791, loss = 0.28583527\n",
      "Iteration 792, loss = 0.28531919\n",
      "Iteration 793, loss = 0.28480349\n",
      "Iteration 794, loss = 0.28428816\n",
      "Iteration 795, loss = 0.28377321\n",
      "Iteration 796, loss = 0.28325865\n",
      "Iteration 797, loss = 0.28274448\n",
      "Iteration 798, loss = 0.28223070\n",
      "Iteration 799, loss = 0.28171732\n",
      "Iteration 800, loss = 0.28120434\n",
      "Iteration 801, loss = 0.28069178\n",
      "Iteration 802, loss = 0.28017962\n",
      "Iteration 803, loss = 0.27966788\n",
      "Iteration 804, loss = 0.27915656\n",
      "Iteration 805, loss = 0.27864566\n",
      "Iteration 806, loss = 0.27813520\n",
      "Iteration 807, loss = 0.27762516\n",
      "Iteration 808, loss = 0.27711557\n",
      "Iteration 809, loss = 0.27660641\n",
      "Iteration 810, loss = 0.27609770\n",
      "Iteration 811, loss = 0.27558944\n",
      "Iteration 812, loss = 0.27508164\n",
      "Iteration 813, loss = 0.27457430\n",
      "Iteration 814, loss = 0.27406741\n",
      "Iteration 815, loss = 0.27356100\n",
      "Iteration 816, loss = 0.27305506\n",
      "Iteration 817, loss = 0.27254959\n",
      "Iteration 818, loss = 0.27204460\n",
      "Iteration 819, loss = 0.27154010\n",
      "Iteration 820, loss = 0.27103608\n",
      "Iteration 821, loss = 0.27053256\n",
      "Iteration 822, loss = 0.27002953\n",
      "Iteration 823, loss = 0.26952700\n",
      "Iteration 824, loss = 0.26902498\n",
      "Iteration 825, loss = 0.26852347\n",
      "Iteration 826, loss = 0.26802247\n",
      "Iteration 827, loss = 0.26752199\n",
      "Iteration 828, loss = 0.26702202\n",
      "Iteration 829, loss = 0.26652259\n",
      "Iteration 830, loss = 0.26602368\n",
      "Iteration 831, loss = 0.26552530\n",
      "Iteration 832, loss = 0.26502747\n",
      "Iteration 833, loss = 0.26453017\n",
      "Iteration 834, loss = 0.26403342\n",
      "Iteration 835, loss = 0.26353722\n",
      "Iteration 836, loss = 0.26304157\n",
      "Iteration 837, loss = 0.26254648\n",
      "Iteration 838, loss = 0.26205195\n",
      "Iteration 839, loss = 0.26155798\n",
      "Iteration 840, loss = 0.26106458\n",
      "Iteration 841, loss = 0.26057176\n",
      "Iteration 842, loss = 0.26007951\n",
      "Iteration 843, loss = 0.25958784\n",
      "Iteration 844, loss = 0.25909675\n",
      "Iteration 845, loss = 0.25860625\n",
      "Iteration 846, loss = 0.25811635\n",
      "Iteration 847, loss = 0.25762703\n",
      "Iteration 848, loss = 0.25713832\n",
      "Iteration 849, loss = 0.25665021\n",
      "Iteration 850, loss = 0.25616271\n",
      "Iteration 851, loss = 0.25567581\n",
      "Iteration 852, loss = 0.25518953\n",
      "Iteration 853, loss = 0.25470387\n",
      "Iteration 854, loss = 0.25421882\n",
      "Iteration 855, loss = 0.25373440\n",
      "Iteration 856, loss = 0.25325061\n",
      "Iteration 857, loss = 0.25276745\n",
      "Iteration 858, loss = 0.25228493\n",
      "Iteration 859, loss = 0.25180304\n",
      "Iteration 860, loss = 0.25132179\n",
      "Iteration 861, loss = 0.25084119\n",
      "Iteration 862, loss = 0.25036124\n",
      "Iteration 863, loss = 0.24988194\n",
      "Iteration 864, loss = 0.24940329\n",
      "Iteration 865, loss = 0.24892530\n",
      "Iteration 866, loss = 0.24844798\n",
      "Iteration 867, loss = 0.24797132\n",
      "Iteration 868, loss = 0.24749532\n",
      "Iteration 869, loss = 0.24702000\n",
      "Iteration 870, loss = 0.24654536\n",
      "Iteration 871, loss = 0.24607139\n",
      "Iteration 872, loss = 0.24559810\n",
      "Iteration 873, loss = 0.24512550\n",
      "Iteration 874, loss = 0.24465358\n",
      "Iteration 875, loss = 0.24418235\n",
      "Iteration 876, loss = 0.24371182\n",
      "Iteration 877, loss = 0.24324198\n",
      "Iteration 878, loss = 0.24277285\n",
      "Iteration 879, loss = 0.24230441\n",
      "Iteration 880, loss = 0.24183668\n",
      "Iteration 881, loss = 0.24136966\n",
      "Iteration 882, loss = 0.24090335\n",
      "Iteration 883, loss = 0.24043775\n",
      "Iteration 884, loss = 0.23997288\n",
      "Iteration 885, loss = 0.23950872\n",
      "Iteration 886, loss = 0.23904528\n",
      "Iteration 887, loss = 0.23858256\n",
      "Iteration 888, loss = 0.23812058\n",
      "Iteration 889, loss = 0.23765933\n",
      "Iteration 890, loss = 0.23719881\n",
      "Iteration 891, loss = 0.23673902\n",
      "Iteration 892, loss = 0.23627997\n",
      "Iteration 893, loss = 0.23582167\n",
      "Iteration 894, loss = 0.23536411\n",
      "Iteration 895, loss = 0.23490729\n",
      "Iteration 896, loss = 0.23445122\n",
      "Iteration 897, loss = 0.23399591\n",
      "Iteration 898, loss = 0.23354134\n",
      "Iteration 899, loss = 0.23308754\n",
      "Iteration 900, loss = 0.23263449\n",
      "Iteration 901, loss = 0.23218220\n",
      "Iteration 902, loss = 0.23173068\n",
      "Iteration 903, loss = 0.23127992\n",
      "Iteration 904, loss = 0.23082993\n",
      "Iteration 905, loss = 0.23038071\n",
      "Iteration 906, loss = 0.22993226\n",
      "Iteration 907, loss = 0.22948459\n",
      "Iteration 908, loss = 0.22903769\n",
      "Iteration 909, loss = 0.22859157\n",
      "Iteration 910, loss = 0.22814623\n",
      "Iteration 911, loss = 0.22770168\n",
      "Iteration 912, loss = 0.22725791\n",
      "Iteration 913, loss = 0.22681493\n",
      "Iteration 914, loss = 0.22637273\n",
      "Iteration 915, loss = 0.22593133\n",
      "Iteration 916, loss = 0.22549072\n",
      "Iteration 917, loss = 0.22505090\n",
      "Iteration 918, loss = 0.22461189\n",
      "Iteration 919, loss = 0.22417367\n",
      "Iteration 920, loss = 0.22373625\n",
      "Iteration 921, loss = 0.22329963\n",
      "Iteration 922, loss = 0.22286382\n",
      "Iteration 923, loss = 0.22242881\n",
      "Iteration 924, loss = 0.22199461\n",
      "Iteration 925, loss = 0.22156122\n",
      "Iteration 926, loss = 0.22112864\n",
      "Iteration 927, loss = 0.22069687\n",
      "Iteration 928, loss = 0.22026592\n",
      "Iteration 929, loss = 0.21983578\n",
      "Iteration 930, loss = 0.21940646\n",
      "Iteration 931, loss = 0.21897796\n",
      "Iteration 932, loss = 0.21855028\n",
      "Iteration 933, loss = 0.21812342\n",
      "Iteration 934, loss = 0.21769739\n",
      "Iteration 935, loss = 0.21727218\n",
      "Iteration 936, loss = 0.21684779\n",
      "Iteration 937, loss = 0.21642423\n",
      "Iteration 938, loss = 0.21600151\n",
      "Iteration 939, loss = 0.21557961\n",
      "Iteration 940, loss = 0.21515854\n",
      "Iteration 941, loss = 0.21473830\n",
      "Iteration 942, loss = 0.21431890\n",
      "Iteration 943, loss = 0.21390034\n",
      "Iteration 944, loss = 0.21348261\n",
      "Iteration 945, loss = 0.21306571\n",
      "Iteration 946, loss = 0.21264966\n",
      "Iteration 947, loss = 0.21223444\n",
      "Iteration 948, loss = 0.21182007\n",
      "Iteration 949, loss = 0.21140654\n",
      "Iteration 950, loss = 0.21099385\n",
      "Iteration 951, loss = 0.21058200\n",
      "Iteration 952, loss = 0.21017100\n",
      "Iteration 953, loss = 0.20976084\n",
      "Iteration 954, loss = 0.20935153\n",
      "Iteration 955, loss = 0.20894307\n",
      "Iteration 956, loss = 0.20853546\n",
      "Iteration 957, loss = 0.20812869\n",
      "Iteration 958, loss = 0.20772278\n",
      "Iteration 959, loss = 0.20731771\n",
      "Iteration 960, loss = 0.20691350\n",
      "Iteration 961, loss = 0.20651014\n",
      "Iteration 962, loss = 0.20610763\n",
      "Iteration 963, loss = 0.20570597\n",
      "Iteration 964, loss = 0.20530517\n",
      "Iteration 965, loss = 0.20490522\n",
      "Iteration 966, loss = 0.20450613\n",
      "Iteration 967, loss = 0.20410790\n",
      "Iteration 968, loss = 0.20371052\n",
      "Iteration 969, loss = 0.20331400\n",
      "Iteration 970, loss = 0.20291833\n",
      "Iteration 971, loss = 0.20252353\n",
      "Iteration 972, loss = 0.20212958\n",
      "Iteration 973, loss = 0.20173649\n",
      "Iteration 974, loss = 0.20134426\n",
      "Iteration 975, loss = 0.20095289\n",
      "Iteration 976, loss = 0.20056238\n",
      "Iteration 977, loss = 0.20017273\n",
      "Iteration 978, loss = 0.19978394\n",
      "Iteration 979, loss = 0.19939602\n",
      "Iteration 980, loss = 0.19900895\n",
      "Iteration 981, loss = 0.19862274\n",
      "Iteration 982, loss = 0.19823740\n",
      "Iteration 983, loss = 0.19785292\n",
      "Iteration 984, loss = 0.19746930\n",
      "Iteration 985, loss = 0.19708655\n",
      "Iteration 986, loss = 0.19670465\n",
      "Iteration 987, loss = 0.19632362\n",
      "Iteration 988, loss = 0.19594345\n",
      "Iteration 989, loss = 0.19556415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 990, loss = 0.19518570\n",
      "Iteration 991, loss = 0.19480812\n",
      "Iteration 992, loss = 0.19443140\n",
      "Iteration 993, loss = 0.19405555\n",
      "Iteration 994, loss = 0.19368056\n",
      "Iteration 995, loss = 0.19330643\n",
      "Iteration 996, loss = 0.19293316\n",
      "Iteration 997, loss = 0.19256076\n",
      "Iteration 998, loss = 0.19218922\n",
      "Iteration 999, loss = 0.19181854\n",
      "Iteration 1000, loss = 0.19144872\n",
      "Iteration 1, loss = 2.11317117\n",
      "Iteration 2, loss = 2.08117414\n",
      "Iteration 3, loss = 2.04945789\n",
      "Iteration 4, loss = 2.01804557\n",
      "Iteration 5, loss = 1.98695256\n",
      "Iteration 6, loss = 1.95619455\n",
      "Iteration 7, loss = 1.92578755\n",
      "Iteration 8, loss = 1.89574782\n",
      "Iteration 9, loss = 1.86608435\n",
      "Iteration 10, loss = 1.83680858\n",
      "Iteration 11, loss = 1.80793953\n",
      "Iteration 12, loss = 1.77947208\n",
      "Iteration 13, loss = 1.75144585\n",
      "Iteration 14, loss = 1.72387502\n",
      "Iteration 15, loss = 1.69675923\n",
      "Iteration 16, loss = 1.67011932\n",
      "Iteration 17, loss = 1.64394653\n",
      "Iteration 18, loss = 1.61823652\n",
      "Iteration 19, loss = 1.59302386\n",
      "Iteration 20, loss = 1.56831390\n",
      "Iteration 21, loss = 1.54413746\n",
      "Iteration 22, loss = 1.52052521\n",
      "Iteration 23, loss = 1.49744057\n",
      "Iteration 24, loss = 1.47492399\n",
      "Iteration 25, loss = 1.45299072\n",
      "Iteration 26, loss = 1.43163565\n",
      "Iteration 27, loss = 1.41087625\n",
      "Iteration 28, loss = 1.39070924\n",
      "Iteration 29, loss = 1.37115075\n",
      "Iteration 30, loss = 1.35218541\n",
      "Iteration 31, loss = 1.33379606\n",
      "Iteration 32, loss = 1.31598630\n",
      "Iteration 33, loss = 1.29879258\n",
      "Iteration 34, loss = 1.28219450\n",
      "Iteration 35, loss = 1.26614060\n",
      "Iteration 36, loss = 1.25067448\n",
      "Iteration 37, loss = 1.23581081\n",
      "Iteration 38, loss = 1.22154419\n",
      "Iteration 39, loss = 1.20786511\n",
      "Iteration 40, loss = 1.19474246\n",
      "Iteration 41, loss = 1.18217589\n",
      "Iteration 42, loss = 1.17015107\n",
      "Iteration 43, loss = 1.15865827\n",
      "Iteration 44, loss = 1.14763953\n",
      "Iteration 45, loss = 1.13712276\n",
      "Iteration 46, loss = 1.12710614\n",
      "Iteration 47, loss = 1.11756160\n",
      "Iteration 48, loss = 1.10848057\n",
      "Iteration 49, loss = 1.09982213\n",
      "Iteration 50, loss = 1.09158621\n",
      "Iteration 51, loss = 1.08374037\n",
      "Iteration 52, loss = 1.07626599\n",
      "Iteration 53, loss = 1.06912394\n",
      "Iteration 54, loss = 1.06232664\n",
      "Iteration 55, loss = 1.05586651\n",
      "Iteration 56, loss = 1.04971438\n",
      "Iteration 57, loss = 1.04382774\n",
      "Iteration 58, loss = 1.03821275\n",
      "Iteration 59, loss = 1.03285014\n",
      "Iteration 60, loss = 1.02772020\n",
      "Iteration 61, loss = 1.02279499\n",
      "Iteration 62, loss = 1.01805340\n",
      "Iteration 63, loss = 1.01348721\n",
      "Iteration 64, loss = 1.00907830\n",
      "Iteration 65, loss = 1.00481760\n",
      "Iteration 66, loss = 1.00068216\n",
      "Iteration 67, loss = 0.99666678\n",
      "Iteration 68, loss = 0.99275743\n",
      "Iteration 69, loss = 0.98894090\n",
      "Iteration 70, loss = 0.98519960\n",
      "Iteration 71, loss = 0.98152403\n",
      "Iteration 72, loss = 0.97790546\n",
      "Iteration 73, loss = 0.97433000\n",
      "Iteration 74, loss = 0.97079625\n",
      "Iteration 75, loss = 0.96729322\n",
      "Iteration 76, loss = 0.96381461\n",
      "Iteration 77, loss = 0.96035841\n",
      "Iteration 78, loss = 0.95691401\n",
      "Iteration 79, loss = 0.95347796\n",
      "Iteration 80, loss = 0.95004544\n",
      "Iteration 81, loss = 0.94662069\n",
      "Iteration 82, loss = 0.94319874\n",
      "Iteration 83, loss = 0.93977873\n",
      "Iteration 84, loss = 0.93636165\n",
      "Iteration 85, loss = 0.93294674\n",
      "Iteration 86, loss = 0.92953130\n",
      "Iteration 87, loss = 0.92611603\n",
      "Iteration 88, loss = 0.92270236\n",
      "Iteration 89, loss = 0.91929057\n",
      "Iteration 90, loss = 0.91588112\n",
      "Iteration 91, loss = 0.91247457\n",
      "Iteration 92, loss = 0.90907160\n",
      "Iteration 93, loss = 0.90569623\n",
      "Iteration 94, loss = 0.90233791\n",
      "Iteration 95, loss = 0.89898805\n",
      "Iteration 96, loss = 0.89564725\n",
      "Iteration 97, loss = 0.89231611\n",
      "Iteration 98, loss = 0.88899526\n",
      "Iteration 99, loss = 0.88568376\n",
      "Iteration 100, loss = 0.88238299\n",
      "Iteration 101, loss = 0.87911123\n",
      "Iteration 102, loss = 0.87590961\n",
      "Iteration 103, loss = 0.87275888\n",
      "Iteration 104, loss = 0.86964528\n",
      "Iteration 105, loss = 0.86656666\n",
      "Iteration 106, loss = 0.86350802\n",
      "Iteration 107, loss = 0.86046997\n",
      "Iteration 108, loss = 0.85745276\n",
      "Iteration 109, loss = 0.85445607\n",
      "Iteration 110, loss = 0.85147964\n",
      "Iteration 111, loss = 0.84852321\n",
      "Iteration 112, loss = 0.84560701\n",
      "Iteration 113, loss = 0.84275954\n",
      "Iteration 114, loss = 0.83995819\n",
      "Iteration 115, loss = 0.83726764\n",
      "Iteration 116, loss = 0.83470527\n",
      "Iteration 117, loss = 0.83224009\n",
      "Iteration 118, loss = 0.82985807\n",
      "Iteration 119, loss = 0.82749207\n",
      "Iteration 120, loss = 0.82514424\n",
      "Iteration 121, loss = 0.82282542\n",
      "Iteration 122, loss = 0.82053465\n",
      "Iteration 123, loss = 0.81826068\n",
      "Iteration 124, loss = 0.81599379\n",
      "Iteration 125, loss = 0.81373473\n",
      "Iteration 126, loss = 0.81148419\n",
      "Iteration 127, loss = 0.80924281\n",
      "Iteration 128, loss = 0.80701119\n",
      "Iteration 129, loss = 0.80478986\n",
      "Iteration 130, loss = 0.80257929\n",
      "Iteration 131, loss = 0.80037990\n",
      "Iteration 132, loss = 0.79819208\n",
      "Iteration 133, loss = 0.79601616\n",
      "Iteration 134, loss = 0.79385241\n",
      "Iteration 135, loss = 0.79170107\n",
      "Iteration 136, loss = 0.78956235\n",
      "Iteration 137, loss = 0.78743638\n",
      "Iteration 138, loss = 0.78532330\n",
      "Iteration 139, loss = 0.78322320\n",
      "Iteration 140, loss = 0.78113612\n",
      "Iteration 141, loss = 0.77906211\n",
      "Iteration 142, loss = 0.77700116\n",
      "Iteration 143, loss = 0.77495325\n",
      "Iteration 144, loss = 0.77291836\n",
      "Iteration 145, loss = 0.77089642\n",
      "Iteration 146, loss = 0.76888738\n",
      "Iteration 147, loss = 0.76689116\n",
      "Iteration 148, loss = 0.76490766\n",
      "Iteration 149, loss = 0.76293679\n",
      "Iteration 150, loss = 0.76097846\n",
      "Iteration 151, loss = 0.75903256\n",
      "Iteration 152, loss = 0.75709897\n",
      "Iteration 153, loss = 0.75517759\n",
      "Iteration 154, loss = 0.75326963\n",
      "Iteration 155, loss = 0.75137590\n",
      "Iteration 156, loss = 0.74949479\n",
      "Iteration 157, loss = 0.74762831\n",
      "Iteration 158, loss = 0.74577321\n",
      "Iteration 159, loss = 0.74392940\n",
      "Iteration 160, loss = 0.74209678\n",
      "Iteration 161, loss = 0.74027528\n",
      "Iteration 162, loss = 0.73846482\n",
      "Iteration 163, loss = 0.73666534\n",
      "Iteration 164, loss = 0.73487676\n",
      "Iteration 165, loss = 0.73309903\n",
      "Iteration 166, loss = 0.73133209\n",
      "Iteration 167, loss = 0.72957588\n",
      "Iteration 168, loss = 0.72783036\n",
      "Iteration 169, loss = 0.72609545\n",
      "Iteration 170, loss = 0.72437111\n",
      "Iteration 171, loss = 0.72265729\n",
      "Iteration 172, loss = 0.72095392\n",
      "Iteration 173, loss = 0.71926096\n",
      "Iteration 174, loss = 0.71757834\n",
      "Iteration 175, loss = 0.71590601\n",
      "Iteration 176, loss = 0.71424391\n",
      "Iteration 177, loss = 0.71259197\n",
      "Iteration 178, loss = 0.71095015\n",
      "Iteration 179, loss = 0.70931838\n",
      "Iteration 180, loss = 0.70769659\n",
      "Iteration 181, loss = 0.70608472\n",
      "Iteration 182, loss = 0.70448271\n",
      "Iteration 183, loss = 0.70289049\n",
      "Iteration 184, loss = 0.70130800\n",
      "Iteration 185, loss = 0.69973518\n",
      "Iteration 186, loss = 0.69817194\n",
      "Iteration 187, loss = 0.69661823\n",
      "Iteration 188, loss = 0.69507398\n",
      "Iteration 189, loss = 0.69353913\n",
      "Iteration 190, loss = 0.69201359\n",
      "Iteration 191, loss = 0.69049673\n",
      "Iteration 192, loss = 0.68898903\n",
      "Iteration 193, loss = 0.68749042\n",
      "Iteration 194, loss = 0.68600086\n",
      "Iteration 195, loss = 0.68452026\n",
      "Iteration 196, loss = 0.68304856\n",
      "Iteration 197, loss = 0.68158571\n",
      "Iteration 198, loss = 0.68013163\n",
      "Iteration 199, loss = 0.67868665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 200, loss = 0.67725049\n",
      "Iteration 201, loss = 0.67582295\n",
      "Iteration 202, loss = 0.67440396\n",
      "Iteration 203, loss = 0.67299345\n",
      "Iteration 204, loss = 0.67159135\n",
      "Iteration 205, loss = 0.67019758\n",
      "Iteration 206, loss = 0.66881209\n",
      "Iteration 207, loss = 0.66743512\n",
      "Iteration 208, loss = 0.66606675\n",
      "Iteration 209, loss = 0.66470661\n",
      "Iteration 210, loss = 0.66335452\n",
      "Iteration 211, loss = 0.66201063\n",
      "Iteration 212, loss = 0.66067476\n",
      "Iteration 213, loss = 0.65934674\n",
      "Iteration 214, loss = 0.65802649\n",
      "Iteration 215, loss = 0.65671394\n",
      "Iteration 216, loss = 0.65540910\n",
      "Iteration 217, loss = 0.65411214\n",
      "Iteration 218, loss = 0.65282267\n",
      "Iteration 219, loss = 0.65154065\n",
      "Iteration 220, loss = 0.65026598\n",
      "Iteration 221, loss = 0.64899860\n",
      "Iteration 222, loss = 0.64773873\n",
      "Iteration 223, loss = 0.64648612\n",
      "Iteration 224, loss = 0.64524061\n",
      "Iteration 225, loss = 0.64400213\n",
      "Iteration 226, loss = 0.64277061\n",
      "Iteration 227, loss = 0.64154598\n",
      "Iteration 228, loss = 0.64032816\n",
      "Iteration 229, loss = 0.63911710\n",
      "Iteration 230, loss = 0.63791271\n",
      "Iteration 231, loss = 0.63671493\n",
      "Iteration 232, loss = 0.63552370\n",
      "Iteration 233, loss = 0.63433894\n",
      "Iteration 234, loss = 0.63316059\n",
      "Iteration 235, loss = 0.63198858\n",
      "Iteration 236, loss = 0.63082297\n",
      "Iteration 237, loss = 0.62966507\n",
      "Iteration 238, loss = 0.62851438\n",
      "Iteration 239, loss = 0.62736995\n",
      "Iteration 240, loss = 0.62623171\n",
      "Iteration 241, loss = 0.62510018\n",
      "Iteration 242, loss = 0.62397486\n",
      "Iteration 243, loss = 0.62285604\n",
      "Iteration 244, loss = 0.62174438\n",
      "Iteration 245, loss = 0.62063868\n",
      "Iteration 246, loss = 0.61953889\n",
      "Iteration 247, loss = 0.61844497\n",
      "Iteration 248, loss = 0.61735687\n",
      "Iteration 249, loss = 0.61627454\n",
      "Iteration 250, loss = 0.61519793\n",
      "Iteration 251, loss = 0.61412700\n",
      "Iteration 252, loss = 0.61306200\n",
      "Iteration 253, loss = 0.61200319\n",
      "Iteration 254, loss = 0.61095113\n",
      "Iteration 255, loss = 0.60990474\n",
      "Iteration 256, loss = 0.60886495\n",
      "Iteration 257, loss = 0.60783041\n",
      "Iteration 258, loss = 0.60680114\n",
      "Iteration 259, loss = 0.60577733\n",
      "Iteration 260, loss = 0.60475885\n",
      "Iteration 261, loss = 0.60374592\n",
      "Iteration 262, loss = 0.60273843\n",
      "Iteration 263, loss = 0.60173624\n",
      "Iteration 264, loss = 0.60073935\n",
      "Iteration 265, loss = 0.59974793\n",
      "Iteration 266, loss = 0.59876178\n",
      "Iteration 267, loss = 0.59778084\n",
      "Iteration 268, loss = 0.59680507\n",
      "Iteration 269, loss = 0.59583444\n",
      "Iteration 270, loss = 0.59486890\n",
      "Iteration 271, loss = 0.59390840\n",
      "Iteration 272, loss = 0.59295290\n",
      "Iteration 273, loss = 0.59200245\n",
      "Iteration 274, loss = 0.59105688\n",
      "Iteration 275, loss = 0.59011616\n",
      "Iteration 276, loss = 0.58918022\n",
      "Iteration 277, loss = 0.58824905\n",
      "Iteration 278, loss = 0.58732240\n",
      "Iteration 279, loss = 0.58640048\n",
      "Iteration 280, loss = 0.58548310\n",
      "Iteration 281, loss = 0.58457022\n",
      "Iteration 282, loss = 0.58366181\n",
      "Iteration 283, loss = 0.58275780\n",
      "Iteration 284, loss = 0.58185811\n",
      "Iteration 285, loss = 0.58096269\n",
      "Iteration 286, loss = 0.58007150\n",
      "Iteration 287, loss = 0.57918448\n",
      "Iteration 288, loss = 0.57830158\n",
      "Iteration 289, loss = 0.57742275\n",
      "Iteration 290, loss = 0.57654794\n",
      "Iteration 291, loss = 0.57567710\n",
      "Iteration 292, loss = 0.57481020\n",
      "Iteration 293, loss = 0.57394716\n",
      "Iteration 294, loss = 0.57308794\n",
      "Iteration 295, loss = 0.57223249\n",
      "Iteration 296, loss = 0.57138076\n",
      "Iteration 297, loss = 0.57053270\n",
      "Iteration 298, loss = 0.56968827\n",
      "Iteration 299, loss = 0.56884742\n",
      "Iteration 300, loss = 0.56801009\n",
      "Iteration 301, loss = 0.56717988\n",
      "Iteration 302, loss = 0.56635405\n",
      "Iteration 303, loss = 0.56553190\n",
      "Iteration 304, loss = 0.56471249\n",
      "Iteration 305, loss = 0.56389559\n",
      "Iteration 306, loss = 0.56308137\n",
      "Iteration 307, loss = 0.56226997\n",
      "Iteration 308, loss = 0.56146230\n",
      "Iteration 309, loss = 0.56065909\n",
      "Iteration 310, loss = 0.55986094\n",
      "Iteration 311, loss = 0.55906621\n",
      "Iteration 312, loss = 0.55827476\n",
      "Iteration 313, loss = 0.55748650\n",
      "Iteration 314, loss = 0.55670139\n",
      "Iteration 315, loss = 0.55591938\n",
      "Iteration 316, loss = 0.55514041\n",
      "Iteration 317, loss = 0.55436443\n",
      "Iteration 318, loss = 0.55359141\n",
      "Iteration 319, loss = 0.55282129\n",
      "Iteration 320, loss = 0.55205411\n",
      "Iteration 321, loss = 0.55129187\n",
      "Iteration 322, loss = 0.55053282\n",
      "Iteration 323, loss = 0.54977666\n",
      "Iteration 324, loss = 0.54902310\n",
      "Iteration 325, loss = 0.54827221\n",
      "Iteration 326, loss = 0.54752402\n",
      "Iteration 327, loss = 0.54677899\n",
      "Iteration 328, loss = 0.54603707\n",
      "Iteration 329, loss = 0.54529813\n",
      "Iteration 330, loss = 0.54456234\n",
      "Iteration 331, loss = 0.54382949\n",
      "Iteration 332, loss = 0.54309930\n",
      "Iteration 333, loss = 0.54237174\n",
      "Iteration 334, loss = 0.54164674\n",
      "Iteration 335, loss = 0.54092437\n",
      "Iteration 336, loss = 0.54020495\n",
      "Iteration 337, loss = 0.53948817\n",
      "Iteration 338, loss = 0.53877399\n",
      "Iteration 339, loss = 0.53806238\n",
      "Iteration 340, loss = 0.53735331\n",
      "Iteration 341, loss = 0.53664689\n",
      "Iteration 342, loss = 0.53594310\n",
      "Iteration 343, loss = 0.53524172\n",
      "Iteration 344, loss = 0.53454276\n",
      "Iteration 345, loss = 0.53384622\n",
      "Iteration 346, loss = 0.53315207\n",
      "Iteration 347, loss = 0.53246032\n",
      "Iteration 348, loss = 0.53177094\n",
      "Iteration 349, loss = 0.53108394\n",
      "Iteration 350, loss = 0.53039932\n",
      "Iteration 351, loss = 0.52971691\n",
      "Iteration 352, loss = 0.52903683\n",
      "Iteration 353, loss = 0.52835900\n",
      "Iteration 354, loss = 0.52768339\n",
      "Iteration 355, loss = 0.52700997\n",
      "Iteration 356, loss = 0.52633873\n",
      "Iteration 357, loss = 0.52566968\n",
      "Iteration 358, loss = 0.52500279\n",
      "Iteration 359, loss = 0.52433801\n",
      "Iteration 360, loss = 0.52367529\n",
      "Iteration 361, loss = 0.52301463\n",
      "Iteration 362, loss = 0.52235598\n",
      "Iteration 363, loss = 0.52169932\n",
      "Iteration 364, loss = 0.52104475\n",
      "Iteration 365, loss = 0.52039249\n",
      "Iteration 366, loss = 0.51974218\n",
      "Iteration 367, loss = 0.51909377\n",
      "Iteration 368, loss = 0.51844727\n",
      "Iteration 369, loss = 0.51780268\n",
      "Iteration 370, loss = 0.51715997\n",
      "Iteration 371, loss = 0.51651914\n",
      "Iteration 372, loss = 0.51588017\n",
      "Iteration 373, loss = 0.51524305\n",
      "Iteration 374, loss = 0.51460775\n",
      "Iteration 375, loss = 0.51397427\n",
      "Iteration 376, loss = 0.51334258\n",
      "Iteration 377, loss = 0.51271265\n",
      "Iteration 378, loss = 0.51208448\n",
      "Iteration 379, loss = 0.51145802\n",
      "Iteration 380, loss = 0.51083326\n",
      "Iteration 381, loss = 0.51021018\n",
      "Iteration 382, loss = 0.50958875\n",
      "Iteration 383, loss = 0.50896895\n",
      "Iteration 384, loss = 0.50835074\n",
      "Iteration 385, loss = 0.50773417\n",
      "Iteration 386, loss = 0.50711921\n",
      "Iteration 387, loss = 0.50650579\n",
      "Iteration 388, loss = 0.50589387\n",
      "Iteration 389, loss = 0.50528345\n",
      "Iteration 390, loss = 0.50467448\n",
      "Iteration 391, loss = 0.50406695\n",
      "Iteration 392, loss = 0.50346082\n",
      "Iteration 393, loss = 0.50285609\n",
      "Iteration 394, loss = 0.50225271\n",
      "Iteration 395, loss = 0.50165067\n",
      "Iteration 396, loss = 0.50104994\n",
      "Iteration 397, loss = 0.50045052\n",
      "Iteration 398, loss = 0.49985240\n",
      "Iteration 399, loss = 0.49925551\n",
      "Iteration 400, loss = 0.49865985\n",
      "Iteration 401, loss = 0.49806539\n",
      "Iteration 402, loss = 0.49747211\n",
      "Iteration 403, loss = 0.49687999\n",
      "Iteration 404, loss = 0.49628904\n",
      "Iteration 405, loss = 0.49569929\n",
      "Iteration 406, loss = 0.49511167\n",
      "Iteration 407, loss = 0.49452539\n",
      "Iteration 408, loss = 0.49394025\n",
      "Iteration 409, loss = 0.49335626\n",
      "Iteration 410, loss = 0.49277338\n",
      "Iteration 411, loss = 0.49219161\n",
      "Iteration 412, loss = 0.49161094\n",
      "Iteration 413, loss = 0.49103136\n",
      "Iteration 414, loss = 0.49045284\n",
      "Iteration 415, loss = 0.48987537\n",
      "Iteration 416, loss = 0.48929894\n",
      "Iteration 417, loss = 0.48872354\n",
      "Iteration 418, loss = 0.48814914\n",
      "Iteration 419, loss = 0.48757572\n",
      "Iteration 420, loss = 0.48700328\n",
      "Iteration 421, loss = 0.48643180\n",
      "Iteration 422, loss = 0.48586125\n",
      "Iteration 423, loss = 0.48529166\n",
      "Iteration 424, loss = 0.48472304\n",
      "Iteration 425, loss = 0.48415532\n",
      "Iteration 426, loss = 0.48358849\n",
      "Iteration 427, loss = 0.48302251\n",
      "Iteration 428, loss = 0.48245737\n",
      "Iteration 429, loss = 0.48189305\n",
      "Iteration 430, loss = 0.48133015\n",
      "Iteration 431, loss = 0.48076849\n",
      "Iteration 432, loss = 0.48020763\n",
      "Iteration 433, loss = 0.47964759\n",
      "Iteration 434, loss = 0.47908835\n",
      "Iteration 435, loss = 0.47852992\n",
      "Iteration 436, loss = 0.47797229\n",
      "Iteration 437, loss = 0.47741544\n",
      "Iteration 438, loss = 0.47685938\n",
      "Iteration 439, loss = 0.47630409\n",
      "Iteration 440, loss = 0.47574957\n",
      "Iteration 441, loss = 0.47519580\n",
      "Iteration 442, loss = 0.47464277\n",
      "Iteration 443, loss = 0.47409054\n",
      "Iteration 444, loss = 0.47353911\n",
      "Iteration 445, loss = 0.47298839\n",
      "Iteration 446, loss = 0.47243837\n",
      "Iteration 447, loss = 0.47188923\n",
      "Iteration 448, loss = 0.47134080\n",
      "Iteration 449, loss = 0.47079305\n",
      "Iteration 450, loss = 0.47024595\n",
      "Iteration 451, loss = 0.46969951\n",
      "Iteration 452, loss = 0.46915369\n",
      "Iteration 453, loss = 0.46860849\n",
      "Iteration 454, loss = 0.46806390\n",
      "Iteration 455, loss = 0.46751999\n",
      "Iteration 456, loss = 0.46697680\n",
      "Iteration 457, loss = 0.46643427\n",
      "Iteration 458, loss = 0.46589303\n",
      "Iteration 459, loss = 0.46535250\n",
      "Iteration 460, loss = 0.46481254\n",
      "Iteration 461, loss = 0.46427314\n",
      "Iteration 462, loss = 0.46373430\n",
      "Iteration 463, loss = 0.46319603\n",
      "Iteration 464, loss = 0.46265831\n",
      "Iteration 465, loss = 0.46212115\n",
      "Iteration 466, loss = 0.46158452\n",
      "Iteration 467, loss = 0.46104843\n",
      "Iteration 468, loss = 0.46051291\n",
      "Iteration 469, loss = 0.45997794\n",
      "Iteration 470, loss = 0.45944350\n",
      "Iteration 471, loss = 0.45890956\n",
      "Iteration 472, loss = 0.45837611\n",
      "Iteration 473, loss = 0.45784315\n",
      "Iteration 474, loss = 0.45731072\n",
      "Iteration 475, loss = 0.45677888\n",
      "Iteration 476, loss = 0.45624755\n",
      "Iteration 477, loss = 0.45571667\n",
      "Iteration 478, loss = 0.45518624\n",
      "Iteration 479, loss = 0.45465626\n",
      "Iteration 480, loss = 0.45412701\n",
      "Iteration 481, loss = 0.45359820\n",
      "Iteration 482, loss = 0.45306980\n",
      "Iteration 483, loss = 0.45254181\n",
      "Iteration 484, loss = 0.45201420\n",
      "Iteration 485, loss = 0.45148698\n",
      "Iteration 486, loss = 0.45096011\n",
      "Iteration 487, loss = 0.45043360\n",
      "Iteration 488, loss = 0.44990743\n",
      "Iteration 489, loss = 0.44938159\n",
      "Iteration 490, loss = 0.44885606\n",
      "Iteration 491, loss = 0.44833083\n",
      "Iteration 492, loss = 0.44780589\n",
      "Iteration 493, loss = 0.44728123\n",
      "Iteration 494, loss = 0.44675684\n",
      "Iteration 495, loss = 0.44623270\n",
      "Iteration 496, loss = 0.44570881\n",
      "Iteration 497, loss = 0.44518515\n",
      "Iteration 498, loss = 0.44466172\n",
      "Iteration 499, loss = 0.44413849\n",
      "Iteration 500, loss = 0.44361547\n",
      "Iteration 501, loss = 0.44309264\n",
      "Iteration 502, loss = 0.44256999\n",
      "Iteration 503, loss = 0.44204751\n",
      "Iteration 504, loss = 0.44152519\n",
      "Iteration 505, loss = 0.44100303\n",
      "Iteration 506, loss = 0.44048101\n",
      "Iteration 507, loss = 0.43995912\n",
      "Iteration 508, loss = 0.43943735\n",
      "Iteration 509, loss = 0.43891570\n",
      "Iteration 510, loss = 0.43839416\n",
      "Iteration 511, loss = 0.43787272\n",
      "Iteration 512, loss = 0.43735136\n",
      "Iteration 513, loss = 0.43683126\n",
      "Iteration 514, loss = 0.43631185\n",
      "Iteration 515, loss = 0.43579258\n",
      "Iteration 516, loss = 0.43527342\n",
      "Iteration 517, loss = 0.43475439\n",
      "Iteration 518, loss = 0.43423549\n",
      "Iteration 519, loss = 0.43371670\n",
      "Iteration 520, loss = 0.43319803\n",
      "Iteration 521, loss = 0.43267965\n",
      "Iteration 522, loss = 0.43216147\n",
      "Iteration 523, loss = 0.43164343\n",
      "Iteration 524, loss = 0.43112551\n",
      "Iteration 525, loss = 0.43060771\n",
      "Iteration 526, loss = 0.43009002\n",
      "Iteration 527, loss = 0.42957244\n",
      "Iteration 528, loss = 0.42905496\n",
      "Iteration 529, loss = 0.42853757\n",
      "Iteration 530, loss = 0.42802027\n",
      "Iteration 531, loss = 0.42750305\n",
      "Iteration 532, loss = 0.42698590\n",
      "Iteration 533, loss = 0.42646881\n",
      "Iteration 534, loss = 0.42595178\n",
      "Iteration 535, loss = 0.42543480\n",
      "Iteration 536, loss = 0.42491786\n",
      "Iteration 537, loss = 0.42440099\n",
      "Iteration 538, loss = 0.42388432\n",
      "Iteration 539, loss = 0.42336769\n",
      "Iteration 540, loss = 0.42285108\n",
      "Iteration 541, loss = 0.42233450\n",
      "Iteration 542, loss = 0.42181793\n",
      "Iteration 543, loss = 0.42130137\n",
      "Iteration 544, loss = 0.42078599\n",
      "Iteration 545, loss = 0.42027070\n",
      "Iteration 546, loss = 0.41975536\n",
      "Iteration 547, loss = 0.41923997\n",
      "Iteration 548, loss = 0.41872456\n",
      "Iteration 549, loss = 0.41820914\n",
      "Iteration 550, loss = 0.41769371\n",
      "Iteration 551, loss = 0.41717827\n",
      "Iteration 552, loss = 0.41666284\n",
      "Iteration 553, loss = 0.41614743\n",
      "Iteration 554, loss = 0.41563202\n",
      "Iteration 555, loss = 0.41511664\n",
      "Iteration 556, loss = 0.41460127\n",
      "Iteration 557, loss = 0.41408592\n",
      "Iteration 558, loss = 0.41357059\n",
      "Iteration 559, loss = 0.41305527\n",
      "Iteration 560, loss = 0.41253996\n",
      "Iteration 561, loss = 0.41202467\n",
      "Iteration 562, loss = 0.41150938\n",
      "Iteration 563, loss = 0.41099410\n",
      "Iteration 564, loss = 0.41047882\n",
      "Iteration 565, loss = 0.40996353\n",
      "Iteration 566, loss = 0.40944823\n",
      "Iteration 567, loss = 0.40893292\n",
      "Iteration 568, loss = 0.40841758\n",
      "Iteration 569, loss = 0.40790223\n",
      "Iteration 570, loss = 0.40738684\n",
      "Iteration 571, loss = 0.40687141\n",
      "Iteration 572, loss = 0.40635595\n",
      "Iteration 573, loss = 0.40584044\n",
      "Iteration 574, loss = 0.40532487\n",
      "Iteration 575, loss = 0.40480928\n",
      "Iteration 576, loss = 0.40429356\n",
      "Iteration 577, loss = 0.40377781\n",
      "Iteration 578, loss = 0.40326198\n",
      "Iteration 579, loss = 0.40274607\n",
      "Iteration 580, loss = 0.40223008\n",
      "Iteration 581, loss = 0.40171400\n",
      "Iteration 582, loss = 0.40119783\n",
      "Iteration 583, loss = 0.40068156\n",
      "Iteration 584, loss = 0.40016518\n",
      "Iteration 585, loss = 0.39964870\n",
      "Iteration 586, loss = 0.39913210\n",
      "Iteration 587, loss = 0.39861539\n",
      "Iteration 588, loss = 0.39809856\n",
      "Iteration 589, loss = 0.39758161\n",
      "Iteration 590, loss = 0.39706453\n",
      "Iteration 591, loss = 0.39654731\n",
      "Iteration 592, loss = 0.39602996\n",
      "Iteration 593, loss = 0.39551247\n",
      "Iteration 594, loss = 0.39499484\n",
      "Iteration 595, loss = 0.39447707\n",
      "Iteration 596, loss = 0.39395914\n",
      "Iteration 597, loss = 0.39344107\n",
      "Iteration 598, loss = 0.39292284\n",
      "Iteration 599, loss = 0.39240445\n",
      "Iteration 600, loss = 0.39188590\n",
      "Iteration 601, loss = 0.39136719\n",
      "Iteration 602, loss = 0.39084832\n",
      "Iteration 603, loss = 0.39032928\n",
      "Iteration 604, loss = 0.38981007\n",
      "Iteration 605, loss = 0.38929068\n",
      "Iteration 606, loss = 0.38877113\n",
      "Iteration 607, loss = 0.38825140\n",
      "Iteration 608, loss = 0.38773149\n",
      "Iteration 609, loss = 0.38721193\n",
      "Iteration 610, loss = 0.38669268\n",
      "Iteration 611, loss = 0.38617427\n",
      "Iteration 612, loss = 0.38565571\n",
      "Iteration 613, loss = 0.38513699\n",
      "Iteration 614, loss = 0.38461813\n",
      "Iteration 615, loss = 0.38409913\n",
      "Iteration 616, loss = 0.38358016\n",
      "Iteration 617, loss = 0.38306127\n",
      "Iteration 618, loss = 0.38254228\n",
      "Iteration 619, loss = 0.38202318\n",
      "Iteration 620, loss = 0.38150399\n",
      "Iteration 621, loss = 0.38098475\n",
      "Iteration 622, loss = 0.38046560\n",
      "Iteration 623, loss = 0.37994636\n",
      "Iteration 624, loss = 0.37942704\n",
      "Iteration 625, loss = 0.37890763\n",
      "Iteration 626, loss = 0.37838812\n",
      "Iteration 627, loss = 0.37786853\n",
      "Iteration 628, loss = 0.37734883\n",
      "Iteration 629, loss = 0.37682904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 630, loss = 0.37630915\n",
      "Iteration 631, loss = 0.37578915\n",
      "Iteration 632, loss = 0.37526904\n",
      "Iteration 633, loss = 0.37474883\n",
      "Iteration 634, loss = 0.37422850\n",
      "Iteration 635, loss = 0.37370806\n",
      "Iteration 636, loss = 0.37318750\n",
      "Iteration 637, loss = 0.37266683\n",
      "Iteration 638, loss = 0.37214604\n",
      "Iteration 639, loss = 0.37162512\n",
      "Iteration 640, loss = 0.37110409\n",
      "Iteration 641, loss = 0.37058292\n",
      "Iteration 642, loss = 0.37006164\n",
      "Iteration 643, loss = 0.36954022\n",
      "Iteration 644, loss = 0.36901868\n",
      "Iteration 645, loss = 0.36849700\n",
      "Iteration 646, loss = 0.36797520\n",
      "Iteration 647, loss = 0.36745326\n",
      "Iteration 648, loss = 0.36693119\n",
      "Iteration 649, loss = 0.36640898\n",
      "Iteration 650, loss = 0.36588664\n",
      "Iteration 651, loss = 0.36536416\n",
      "Iteration 652, loss = 0.36484155\n",
      "Iteration 653, loss = 0.36431880\n",
      "Iteration 654, loss = 0.36379591\n",
      "Iteration 655, loss = 0.36327289\n",
      "Iteration 656, loss = 0.36274972\n",
      "Iteration 657, loss = 0.36222642\n",
      "Iteration 658, loss = 0.36170299\n",
      "Iteration 659, loss = 0.36117941\n",
      "Iteration 660, loss = 0.36065570\n",
      "Iteration 661, loss = 0.36013185\n",
      "Iteration 662, loss = 0.35960786\n",
      "Iteration 663, loss = 0.35908374\n",
      "Iteration 664, loss = 0.35855948\n",
      "Iteration 665, loss = 0.35803508\n",
      "Iteration 666, loss = 0.35751056\n",
      "Iteration 667, loss = 0.35698589\n",
      "Iteration 668, loss = 0.35646110\n",
      "Iteration 669, loss = 0.35593617\n",
      "Iteration 670, loss = 0.35541112\n",
      "Iteration 671, loss = 0.35488593\n",
      "Iteration 672, loss = 0.35436061\n",
      "Iteration 673, loss = 0.35383517\n",
      "Iteration 674, loss = 0.35330960\n",
      "Iteration 675, loss = 0.35278390\n",
      "Iteration 676, loss = 0.35225809\n",
      "Iteration 677, loss = 0.35173215\n",
      "Iteration 678, loss = 0.35120609\n",
      "Iteration 679, loss = 0.35067991\n",
      "Iteration 680, loss = 0.35015361\n",
      "Iteration 681, loss = 0.34962720\n",
      "Iteration 682, loss = 0.34910068\n",
      "Iteration 683, loss = 0.34857404\n",
      "Iteration 684, loss = 0.34804730\n",
      "Iteration 685, loss = 0.34752045\n",
      "Iteration 686, loss = 0.34699349\n",
      "Iteration 687, loss = 0.34646725\n",
      "Iteration 688, loss = 0.34594086\n",
      "Iteration 689, loss = 0.34541427\n",
      "Iteration 690, loss = 0.34488751\n",
      "Iteration 691, loss = 0.34436059\n",
      "Iteration 692, loss = 0.34383352\n",
      "Iteration 693, loss = 0.34330633\n",
      "Iteration 694, loss = 0.34277901\n",
      "Iteration 695, loss = 0.34225159\n",
      "Iteration 696, loss = 0.34172408\n",
      "Iteration 697, loss = 0.34119648\n",
      "Iteration 698, loss = 0.34066881\n",
      "Iteration 699, loss = 0.34014108\n",
      "Iteration 700, loss = 0.33961374\n",
      "Iteration 701, loss = 0.33908638\n",
      "Iteration 702, loss = 0.33855889\n",
      "Iteration 703, loss = 0.33803128\n",
      "Iteration 704, loss = 0.33750354\n",
      "Iteration 705, loss = 0.33697568\n",
      "Iteration 706, loss = 0.33644771\n",
      "Iteration 707, loss = 0.33591965\n",
      "Iteration 708, loss = 0.33539188\n",
      "Iteration 709, loss = 0.33486416\n",
      "Iteration 710, loss = 0.33433637\n",
      "Iteration 711, loss = 0.33380853\n",
      "Iteration 712, loss = 0.33328063\n",
      "Iteration 713, loss = 0.33275269\n",
      "Iteration 714, loss = 0.33222472\n",
      "Iteration 715, loss = 0.33169672\n",
      "Iteration 716, loss = 0.33116870\n",
      "Iteration 717, loss = 0.33064068\n",
      "Iteration 718, loss = 0.33011265\n",
      "Iteration 719, loss = 0.32958470\n",
      "Iteration 720, loss = 0.32905688\n",
      "Iteration 721, loss = 0.32852895\n",
      "Iteration 722, loss = 0.32800092\n",
      "Iteration 723, loss = 0.32747314\n",
      "Iteration 724, loss = 0.32694535\n",
      "Iteration 725, loss = 0.32641757\n",
      "Iteration 726, loss = 0.32588979\n",
      "Iteration 727, loss = 0.32536204\n",
      "Iteration 728, loss = 0.32483431\n",
      "Iteration 729, loss = 0.32430660\n",
      "Iteration 730, loss = 0.32377894\n",
      "Iteration 731, loss = 0.32325131\n",
      "Iteration 732, loss = 0.32272373\n",
      "Iteration 733, loss = 0.32219620\n",
      "Iteration 734, loss = 0.32166890\n",
      "Iteration 735, loss = 0.32114157\n",
      "Iteration 736, loss = 0.32061415\n",
      "Iteration 737, loss = 0.32008694\n",
      "Iteration 738, loss = 0.31955983\n",
      "Iteration 739, loss = 0.31903278\n",
      "Iteration 740, loss = 0.31850581\n",
      "Iteration 741, loss = 0.31797891\n",
      "Iteration 742, loss = 0.31745209\n",
      "Iteration 743, loss = 0.31692536\n",
      "Iteration 744, loss = 0.31639871\n",
      "Iteration 745, loss = 0.31587217\n",
      "Iteration 746, loss = 0.31534573\n",
      "Iteration 747, loss = 0.31481939\n",
      "Iteration 748, loss = 0.31429316\n",
      "Iteration 749, loss = 0.31376705\n",
      "Iteration 750, loss = 0.31324106\n",
      "Iteration 751, loss = 0.31271520\n",
      "Iteration 752, loss = 0.31218962\n",
      "Iteration 753, loss = 0.31166400\n",
      "Iteration 754, loss = 0.31113846\n",
      "Iteration 755, loss = 0.31061317\n",
      "Iteration 756, loss = 0.31008802\n",
      "Iteration 757, loss = 0.30956302\n",
      "Iteration 758, loss = 0.30903816\n",
      "Iteration 759, loss = 0.30851346\n",
      "Iteration 760, loss = 0.30798892\n",
      "Iteration 761, loss = 0.30746454\n",
      "Iteration 762, loss = 0.30694033\n",
      "Iteration 763, loss = 0.30641629\n",
      "Iteration 764, loss = 0.30589243\n",
      "Iteration 765, loss = 0.30536875\n",
      "Iteration 766, loss = 0.30484526\n",
      "Iteration 767, loss = 0.30432196\n",
      "Iteration 768, loss = 0.30379886\n",
      "Iteration 769, loss = 0.30327595\n",
      "Iteration 770, loss = 0.30275325\n",
      "Iteration 771, loss = 0.30223076\n",
      "Iteration 772, loss = 0.30170848\n",
      "Iteration 773, loss = 0.30118642\n",
      "Iteration 774, loss = 0.30066458\n",
      "Iteration 775, loss = 0.30014297\n",
      "Iteration 776, loss = 0.29962159\n",
      "Iteration 777, loss = 0.29910047\n",
      "Iteration 778, loss = 0.29857954\n",
      "Iteration 779, loss = 0.29805887\n",
      "Iteration 780, loss = 0.29753846\n",
      "Iteration 781, loss = 0.29701829\n",
      "Iteration 782, loss = 0.29649838\n",
      "Iteration 783, loss = 0.29597874\n",
      "Iteration 784, loss = 0.29545935\n",
      "Iteration 785, loss = 0.29494024\n",
      "Iteration 786, loss = 0.29442140\n",
      "Iteration 787, loss = 0.29390284\n",
      "Iteration 788, loss = 0.29338456\n",
      "Iteration 789, loss = 0.29286657\n",
      "Iteration 790, loss = 0.29234887\n",
      "Iteration 791, loss = 0.29183147\n",
      "Iteration 792, loss = 0.29131437\n",
      "Iteration 793, loss = 0.29079757\n",
      "Iteration 794, loss = 0.29028108\n",
      "Iteration 795, loss = 0.28976491\n",
      "Iteration 796, loss = 0.28924906\n",
      "Iteration 797, loss = 0.28873353\n",
      "Iteration 798, loss = 0.28821832\n",
      "Iteration 799, loss = 0.28770345\n",
      "Iteration 800, loss = 0.28718892\n",
      "Iteration 801, loss = 0.28667473\n",
      "Iteration 802, loss = 0.28616088\n",
      "Iteration 803, loss = 0.28564738\n",
      "Iteration 804, loss = 0.28513423\n",
      "Iteration 805, loss = 0.28462145\n",
      "Iteration 806, loss = 0.28410903\n",
      "Iteration 807, loss = 0.28359697\n",
      "Iteration 808, loss = 0.28308529\n",
      "Iteration 809, loss = 0.28257399\n",
      "Iteration 810, loss = 0.28206306\n",
      "Iteration 811, loss = 0.28155252\n",
      "Iteration 812, loss = 0.28104237\n",
      "Iteration 813, loss = 0.28053262\n",
      "Iteration 814, loss = 0.28002326\n",
      "Iteration 815, loss = 0.27951431\n",
      "Iteration 816, loss = 0.27900576\n",
      "Iteration 817, loss = 0.27849763\n",
      "Iteration 818, loss = 0.27798991\n",
      "Iteration 819, loss = 0.27748262\n",
      "Iteration 820, loss = 0.27697575\n",
      "Iteration 821, loss = 0.27646930\n",
      "Iteration 822, loss = 0.27596329\n",
      "Iteration 823, loss = 0.27545772\n",
      "Iteration 824, loss = 0.27495260\n",
      "Iteration 825, loss = 0.27444791\n",
      "Iteration 826, loss = 0.27394368\n",
      "Iteration 827, loss = 0.27343991\n",
      "Iteration 828, loss = 0.27293659\n",
      "Iteration 829, loss = 0.27243374\n",
      "Iteration 830, loss = 0.27193136\n",
      "Iteration 831, loss = 0.27142945\n",
      "Iteration 832, loss = 0.27092801\n",
      "Iteration 833, loss = 0.27042706\n",
      "Iteration 834, loss = 0.26992659\n",
      "Iteration 835, loss = 0.26942661\n",
      "Iteration 836, loss = 0.26892713\n",
      "Iteration 837, loss = 0.26842814\n",
      "Iteration 838, loss = 0.26792965\n",
      "Iteration 839, loss = 0.26743167\n",
      "Iteration 840, loss = 0.26693420\n",
      "Iteration 841, loss = 0.26643725\n",
      "Iteration 842, loss = 0.26594081\n",
      "Iteration 843, loss = 0.26544490\n",
      "Iteration 844, loss = 0.26494951\n",
      "Iteration 845, loss = 0.26445465\n",
      "Iteration 846, loss = 0.26396033\n",
      "Iteration 847, loss = 0.26346654\n",
      "Iteration 848, loss = 0.26297330\n",
      "Iteration 849, loss = 0.26248060\n",
      "Iteration 850, loss = 0.26198846\n",
      "Iteration 851, loss = 0.26149686\n",
      "Iteration 852, loss = 0.26100583\n",
      "Iteration 853, loss = 0.26051536\n",
      "Iteration 854, loss = 0.26002545\n",
      "Iteration 855, loss = 0.25953612\n",
      "Iteration 856, loss = 0.25904736\n",
      "Iteration 857, loss = 0.25855917\n",
      "Iteration 858, loss = 0.25807157\n",
      "Iteration 859, loss = 0.25758455\n",
      "Iteration 860, loss = 0.25709812\n",
      "Iteration 861, loss = 0.25661229\n",
      "Iteration 862, loss = 0.25612705\n",
      "Iteration 863, loss = 0.25564241\n",
      "Iteration 864, loss = 0.25515837\n",
      "Iteration 865, loss = 0.25467494\n",
      "Iteration 866, loss = 0.25419212\n",
      "Iteration 867, loss = 0.25370992\n",
      "Iteration 868, loss = 0.25322833\n",
      "Iteration 869, loss = 0.25274736\n",
      "Iteration 870, loss = 0.25226702\n",
      "Iteration 871, loss = 0.25178731\n",
      "Iteration 872, loss = 0.25130823\n",
      "Iteration 873, loss = 0.25082979\n",
      "Iteration 874, loss = 0.25035198\n",
      "Iteration 875, loss = 0.24987482\n",
      "Iteration 876, loss = 0.24939830\n",
      "Iteration 877, loss = 0.24892243\n",
      "Iteration 878, loss = 0.24844722\n",
      "Iteration 879, loss = 0.24797265\n",
      "Iteration 880, loss = 0.24749875\n",
      "Iteration 881, loss = 0.24702551\n",
      "Iteration 882, loss = 0.24655294\n",
      "Iteration 883, loss = 0.24608103\n",
      "Iteration 884, loss = 0.24560980\n",
      "Iteration 885, loss = 0.24513924\n",
      "Iteration 886, loss = 0.24466935\n",
      "Iteration 887, loss = 0.24420015\n",
      "Iteration 888, loss = 0.24373163\n",
      "Iteration 889, loss = 0.24326380\n",
      "Iteration 890, loss = 0.24279666\n",
      "Iteration 891, loss = 0.24233022\n",
      "Iteration 892, loss = 0.24186447\n",
      "Iteration 893, loss = 0.24139941\n",
      "Iteration 894, loss = 0.24093506\n",
      "Iteration 895, loss = 0.24047142\n",
      "Iteration 896, loss = 0.24000848\n",
      "Iteration 897, loss = 0.23954625\n",
      "Iteration 898, loss = 0.23908474\n",
      "Iteration 899, loss = 0.23862394\n",
      "Iteration 900, loss = 0.23816386\n",
      "Iteration 901, loss = 0.23770450\n",
      "Iteration 902, loss = 0.23724587\n",
      "Iteration 903, loss = 0.23678796\n",
      "Iteration 904, loss = 0.23633078\n",
      "Iteration 905, loss = 0.23587433\n",
      "Iteration 906, loss = 0.23541862\n",
      "Iteration 907, loss = 0.23496364\n",
      "Iteration 908, loss = 0.23450941\n",
      "Iteration 909, loss = 0.23405591\n",
      "Iteration 910, loss = 0.23360317\n",
      "Iteration 911, loss = 0.23315118\n",
      "Iteration 912, loss = 0.23269994\n",
      "Iteration 913, loss = 0.23224946\n",
      "Iteration 914, loss = 0.23179973\n",
      "Iteration 915, loss = 0.23135076\n",
      "Iteration 916, loss = 0.23090254\n",
      "Iteration 917, loss = 0.23045509\n",
      "Iteration 918, loss = 0.23000840\n",
      "Iteration 919, loss = 0.22956248\n",
      "Iteration 920, loss = 0.22911732\n",
      "Iteration 921, loss = 0.22867294\n",
      "Iteration 922, loss = 0.22822932\n",
      "Iteration 923, loss = 0.22778648\n",
      "Iteration 924, loss = 0.22734442\n",
      "Iteration 925, loss = 0.22690313\n",
      "Iteration 926, loss = 0.22646262\n",
      "Iteration 927, loss = 0.22602290\n",
      "Iteration 928, loss = 0.22558395\n",
      "Iteration 929, loss = 0.22514580\n",
      "Iteration 930, loss = 0.22470843\n",
      "Iteration 931, loss = 0.22427185\n",
      "Iteration 932, loss = 0.22383606\n",
      "Iteration 933, loss = 0.22340106\n",
      "Iteration 934, loss = 0.22296686\n",
      "Iteration 935, loss = 0.22253345\n",
      "Iteration 936, loss = 0.22210085\n",
      "Iteration 937, loss = 0.22166904\n",
      "Iteration 938, loss = 0.22123803\n",
      "Iteration 939, loss = 0.22080782\n",
      "Iteration 940, loss = 0.22037842\n",
      "Iteration 941, loss = 0.21994983\n",
      "Iteration 942, loss = 0.21952204\n",
      "Iteration 943, loss = 0.21909507\n",
      "Iteration 944, loss = 0.21866890\n",
      "Iteration 945, loss = 0.21824354\n",
      "Iteration 946, loss = 0.21781900\n",
      "Iteration 947, loss = 0.21739527\n",
      "Iteration 948, loss = 0.21697236\n",
      "Iteration 949, loss = 0.21655027\n",
      "Iteration 950, loss = 0.21612899\n",
      "Iteration 951, loss = 0.21570853\n",
      "Iteration 952, loss = 0.21528890\n",
      "Iteration 953, loss = 0.21487009\n",
      "Iteration 954, loss = 0.21445210\n",
      "Iteration 955, loss = 0.21403493\n",
      "Iteration 956, loss = 0.21361860\n",
      "Iteration 957, loss = 0.21320309\n",
      "Iteration 958, loss = 0.21278840\n",
      "Iteration 959, loss = 0.21237455\n",
      "Iteration 960, loss = 0.21196153\n",
      "Iteration 961, loss = 0.21154934\n",
      "Iteration 962, loss = 0.21113798\n",
      "Iteration 963, loss = 0.21072745\n",
      "Iteration 964, loss = 0.21031776\n",
      "Iteration 965, loss = 0.20990890\n",
      "Iteration 966, loss = 0.20950088\n",
      "Iteration 967, loss = 0.20909370\n",
      "Iteration 968, loss = 0.20868735\n",
      "Iteration 969, loss = 0.20828184\n",
      "Iteration 970, loss = 0.20787717\n",
      "Iteration 971, loss = 0.20747335\n",
      "Iteration 972, loss = 0.20707036\n",
      "Iteration 973, loss = 0.20666821\n",
      "Iteration 974, loss = 0.20626691\n",
      "Iteration 975, loss = 0.20586645\n",
      "Iteration 976, loss = 0.20546683\n",
      "Iteration 977, loss = 0.20506805\n",
      "Iteration 978, loss = 0.20467012\n",
      "Iteration 979, loss = 0.20427304\n",
      "Iteration 980, loss = 0.20387680\n",
      "Iteration 981, loss = 0.20348141\n",
      "Iteration 982, loss = 0.20308686\n",
      "Iteration 983, loss = 0.20269316\n",
      "Iteration 984, loss = 0.20230031\n",
      "Iteration 985, loss = 0.20190831\n",
      "Iteration 986, loss = 0.20151715\n",
      "Iteration 987, loss = 0.20112685\n",
      "Iteration 988, loss = 0.20073739\n",
      "Iteration 989, loss = 0.20034878\n",
      "Iteration 990, loss = 0.19996102\n",
      "Iteration 991, loss = 0.19957411\n",
      "Iteration 992, loss = 0.19918806\n",
      "Iteration 993, loss = 0.19880285\n",
      "Iteration 994, loss = 0.19841849\n",
      "Iteration 995, loss = 0.19803498\n",
      "Iteration 996, loss = 0.19765233\n",
      "Iteration 997, loss = 0.19727052\n",
      "Iteration 998, loss = 0.19688957\n",
      "Iteration 999, loss = 0.19650947\n",
      "Iteration 1000, loss = 0.19613022\n",
      "Iteration 1, loss = 2.11924132\n",
      "Iteration 2, loss = 2.08691723\n",
      "Iteration 3, loss = 2.05487142\n",
      "Iteration 4, loss = 2.02312692\n",
      "Iteration 5, loss = 1.99169333\n",
      "Iteration 6, loss = 1.96059150\n",
      "Iteration 7, loss = 1.92983093\n",
      "Iteration 8, loss = 1.89943443\n",
      "Iteration 9, loss = 1.86941548\n",
      "Iteration 10, loss = 1.83977998\n",
      "Iteration 11, loss = 1.81054774\n",
      "Iteration 12, loss = 1.78171722\n",
      "Iteration 13, loss = 1.75332010\n",
      "Iteration 14, loss = 1.72535347\n",
      "Iteration 15, loss = 1.69783681\n",
      "Iteration 16, loss = 1.67079653\n",
      "Iteration 17, loss = 1.64423657\n",
      "Iteration 18, loss = 1.61817099\n",
      "Iteration 19, loss = 1.59262303\n",
      "Iteration 20, loss = 1.56757578\n",
      "Iteration 21, loss = 1.54305361\n",
      "Iteration 22, loss = 1.51909233\n",
      "Iteration 23, loss = 1.49563947\n",
      "Iteration 24, loss = 1.47276330\n",
      "Iteration 25, loss = 1.45047110\n",
      "Iteration 26, loss = 1.42875760\n",
      "Iteration 27, loss = 1.40762610\n",
      "Iteration 28, loss = 1.38708464\n",
      "Iteration 29, loss = 1.36715769\n",
      "Iteration 30, loss = 1.34783293\n",
      "Iteration 31, loss = 1.32909128\n",
      "Iteration 32, loss = 1.31096776\n",
      "Iteration 33, loss = 1.29346875\n",
      "Iteration 34, loss = 1.27657394\n",
      "Iteration 35, loss = 1.26023020\n",
      "Iteration 36, loss = 1.24448578\n",
      "Iteration 37, loss = 1.22935367\n",
      "Iteration 38, loss = 1.21482882\n",
      "Iteration 39, loss = 1.20088428\n",
      "Iteration 40, loss = 1.18747810\n",
      "Iteration 41, loss = 1.17464137\n",
      "Iteration 42, loss = 1.16235131\n",
      "Iteration 43, loss = 1.15056770\n",
      "Iteration 44, loss = 1.13927222\n",
      "Iteration 45, loss = 1.12848885\n",
      "Iteration 46, loss = 1.11821930\n",
      "Iteration 47, loss = 1.10843623\n",
      "Iteration 48, loss = 1.09913398\n",
      "Iteration 49, loss = 1.09029027\n",
      "Iteration 50, loss = 1.08187438\n",
      "Iteration 51, loss = 1.07383517\n",
      "Iteration 52, loss = 1.06618696\n",
      "Iteration 53, loss = 1.05891630\n",
      "Iteration 54, loss = 1.05200205\n",
      "Iteration 55, loss = 1.04544159\n",
      "Iteration 56, loss = 1.03917710\n",
      "Iteration 57, loss = 1.03319437\n",
      "Iteration 58, loss = 1.02750026\n",
      "Iteration 59, loss = 1.02206487\n",
      "Iteration 60, loss = 1.01687846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 61, loss = 1.01191159\n",
      "Iteration 62, loss = 1.00714797\n",
      "Iteration 63, loss = 1.00256388\n",
      "Iteration 64, loss = 0.99815086\n",
      "Iteration 65, loss = 0.99388528\n",
      "Iteration 66, loss = 0.98975863\n",
      "Iteration 67, loss = 0.98575736\n",
      "Iteration 68, loss = 0.98186666\n",
      "Iteration 69, loss = 0.97807264\n",
      "Iteration 70, loss = 0.97436236\n",
      "Iteration 71, loss = 0.97072391\n",
      "Iteration 72, loss = 0.96714640\n",
      "Iteration 73, loss = 0.96361997\n",
      "Iteration 74, loss = 0.96013146\n",
      "Iteration 75, loss = 0.95667426\n",
      "Iteration 76, loss = 0.95324365\n",
      "Iteration 77, loss = 0.94982924\n",
      "Iteration 78, loss = 0.94642390\n",
      "Iteration 79, loss = 0.94302749\n",
      "Iteration 80, loss = 0.93964064\n",
      "Iteration 81, loss = 0.93625568\n",
      "Iteration 82, loss = 0.93287575\n",
      "Iteration 83, loss = 0.92949921\n",
      "Iteration 84, loss = 0.92612488\n",
      "Iteration 85, loss = 0.92275197\n",
      "Iteration 86, loss = 0.91938004\n",
      "Iteration 87, loss = 0.91600895\n",
      "Iteration 88, loss = 0.91263881\n",
      "Iteration 89, loss = 0.90926995\n",
      "Iteration 90, loss = 0.90590289\n",
      "Iteration 91, loss = 0.90253825\n",
      "Iteration 92, loss = 0.89917678\n",
      "Iteration 93, loss = 0.89581930\n",
      "Iteration 94, loss = 0.89246669\n",
      "Iteration 95, loss = 0.88911984\n",
      "Iteration 96, loss = 0.88577966\n",
      "Iteration 97, loss = 0.88244705\n",
      "Iteration 98, loss = 0.87912265\n",
      "Iteration 99, loss = 0.87580567\n",
      "Iteration 100, loss = 0.87249873\n",
      "Iteration 101, loss = 0.86920909\n",
      "Iteration 102, loss = 0.86598213\n",
      "Iteration 103, loss = 0.86282404\n",
      "Iteration 104, loss = 0.85968587\n",
      "Iteration 105, loss = 0.85656744\n",
      "Iteration 106, loss = 0.85346691\n",
      "Iteration 107, loss = 0.85038535\n",
      "Iteration 108, loss = 0.84732298\n",
      "Iteration 109, loss = 0.84427969\n",
      "Iteration 110, loss = 0.84125539\n",
      "Iteration 111, loss = 0.83827958\n",
      "Iteration 112, loss = 0.83540387\n",
      "Iteration 113, loss = 0.83257737\n",
      "Iteration 114, loss = 0.82984621\n",
      "Iteration 115, loss = 0.82722316\n",
      "Iteration 116, loss = 0.82469391\n",
      "Iteration 117, loss = 0.82223993\n",
      "Iteration 118, loss = 0.81984719\n",
      "Iteration 119, loss = 0.81747117\n",
      "Iteration 120, loss = 0.81512038\n",
      "Iteration 121, loss = 0.81280382\n",
      "Iteration 122, loss = 0.81052204\n",
      "Iteration 123, loss = 0.80824930\n",
      "Iteration 124, loss = 0.80598667\n",
      "Iteration 125, loss = 0.80373456\n",
      "Iteration 126, loss = 0.80149336\n",
      "Iteration 127, loss = 0.79926341\n",
      "Iteration 128, loss = 0.79704505\n",
      "Iteration 129, loss = 0.79484623\n",
      "Iteration 130, loss = 0.79265883\n",
      "Iteration 131, loss = 0.79048090\n",
      "Iteration 132, loss = 0.78831302\n",
      "Iteration 133, loss = 0.78615395\n",
      "Iteration 134, loss = 0.78400594\n",
      "Iteration 135, loss = 0.78186944\n",
      "Iteration 136, loss = 0.77974485\n",
      "Iteration 137, loss = 0.77763248\n",
      "Iteration 138, loss = 0.77553260\n",
      "Iteration 139, loss = 0.77344883\n",
      "Iteration 140, loss = 0.77137913\n",
      "Iteration 141, loss = 0.76932307\n",
      "Iteration 142, loss = 0.76728328\n",
      "Iteration 143, loss = 0.76525738\n",
      "Iteration 144, loss = 0.76324508\n",
      "Iteration 145, loss = 0.76124624\n",
      "Iteration 146, loss = 0.75926074\n",
      "Iteration 147, loss = 0.75728845\n",
      "Iteration 148, loss = 0.75532925\n",
      "Iteration 149, loss = 0.75338299\n",
      "Iteration 150, loss = 0.75144957\n",
      "Iteration 151, loss = 0.74952886\n",
      "Iteration 152, loss = 0.74762074\n",
      "Iteration 153, loss = 0.74572509\n",
      "Iteration 154, loss = 0.74384181\n",
      "Iteration 155, loss = 0.74197078\n",
      "Iteration 156, loss = 0.74011190\n",
      "Iteration 157, loss = 0.73826506\n",
      "Iteration 158, loss = 0.73643016\n",
      "Iteration 159, loss = 0.73460711\n",
      "Iteration 160, loss = 0.73279582\n",
      "Iteration 161, loss = 0.73099619\n",
      "Iteration 162, loss = 0.72920813\n",
      "Iteration 163, loss = 0.72743156\n",
      "Iteration 164, loss = 0.72566640\n",
      "Iteration 165, loss = 0.72391256\n",
      "Iteration 166, loss = 0.72216996\n",
      "Iteration 167, loss = 0.72043852\n",
      "Iteration 168, loss = 0.71871832\n",
      "Iteration 169, loss = 0.71700939\n",
      "Iteration 170, loss = 0.71531142\n",
      "Iteration 171, loss = 0.71362434\n",
      "Iteration 172, loss = 0.71194806\n",
      "Iteration 173, loss = 0.71028251\n",
      "Iteration 174, loss = 0.70862807\n",
      "Iteration 175, loss = 0.70698497\n",
      "Iteration 176, loss = 0.70535236\n",
      "Iteration 177, loss = 0.70373057\n",
      "Iteration 178, loss = 0.70211964\n",
      "Iteration 179, loss = 0.70051872\n",
      "Iteration 180, loss = 0.69892781\n",
      "Iteration 181, loss = 0.69734690\n",
      "Iteration 182, loss = 0.69577597\n",
      "Iteration 183, loss = 0.69421500\n",
      "Iteration 184, loss = 0.69266397\n",
      "Iteration 185, loss = 0.69112284\n",
      "Iteration 186, loss = 0.68959159\n",
      "Iteration 187, loss = 0.68807040\n",
      "Iteration 188, loss = 0.68655946\n",
      "Iteration 189, loss = 0.68505823\n",
      "Iteration 190, loss = 0.68356662\n",
      "Iteration 191, loss = 0.68208457\n",
      "Iteration 192, loss = 0.68061199\n",
      "Iteration 193, loss = 0.67914881\n",
      "Iteration 194, loss = 0.67769496\n",
      "Iteration 195, loss = 0.67624998\n",
      "Iteration 196, loss = 0.67481403\n",
      "Iteration 197, loss = 0.67338763\n",
      "Iteration 198, loss = 0.67197042\n",
      "Iteration 199, loss = 0.67056219\n",
      "Iteration 200, loss = 0.66916292\n",
      "Iteration 201, loss = 0.66777254\n",
      "Iteration 202, loss = 0.66639101\n",
      "Iteration 203, loss = 0.66501827\n",
      "Iteration 204, loss = 0.66365427\n",
      "Iteration 205, loss = 0.66229901\n",
      "Iteration 206, loss = 0.66095278\n",
      "Iteration 207, loss = 0.65961515\n",
      "Iteration 208, loss = 0.65828605\n",
      "Iteration 209, loss = 0.65696540\n",
      "Iteration 210, loss = 0.65565315\n",
      "Iteration 211, loss = 0.65434923\n",
      "Iteration 212, loss = 0.65305356\n",
      "Iteration 213, loss = 0.65176609\n",
      "Iteration 214, loss = 0.65048698\n",
      "Iteration 215, loss = 0.64921610\n",
      "Iteration 216, loss = 0.64795323\n",
      "Iteration 217, loss = 0.64669830\n",
      "Iteration 218, loss = 0.64545145\n",
      "Iteration 219, loss = 0.64421248\n",
      "Iteration 220, loss = 0.64298125\n",
      "Iteration 221, loss = 0.64175770\n",
      "Iteration 222, loss = 0.64054174\n",
      "Iteration 223, loss = 0.63933332\n",
      "Iteration 224, loss = 0.63813236\n",
      "Iteration 225, loss = 0.63693878\n",
      "Iteration 226, loss = 0.63575252\n",
      "Iteration 227, loss = 0.63457351\n",
      "Iteration 228, loss = 0.63340186\n",
      "Iteration 229, loss = 0.63223741\n",
      "Iteration 230, loss = 0.63108002\n",
      "Iteration 231, loss = 0.62992982\n",
      "Iteration 232, loss = 0.62878671\n",
      "Iteration 233, loss = 0.62765047\n",
      "Iteration 234, loss = 0.62652105\n",
      "Iteration 235, loss = 0.62539836\n",
      "Iteration 236, loss = 0.62428235\n",
      "Iteration 237, loss = 0.62317294\n",
      "Iteration 238, loss = 0.62207007\n",
      "Iteration 239, loss = 0.62097368\n",
      "Iteration 240, loss = 0.61988369\n",
      "Iteration 241, loss = 0.61880005\n",
      "Iteration 242, loss = 0.61772269\n",
      "Iteration 243, loss = 0.61665155\n",
      "Iteration 244, loss = 0.61558655\n",
      "Iteration 245, loss = 0.61452765\n",
      "Iteration 246, loss = 0.61347497\n",
      "Iteration 247, loss = 0.61242835\n",
      "Iteration 248, loss = 0.61138765\n",
      "Iteration 249, loss = 0.61035281\n",
      "Iteration 250, loss = 0.60932377\n",
      "Iteration 251, loss = 0.60830047\n",
      "Iteration 252, loss = 0.60728306\n",
      "Iteration 253, loss = 0.60627137\n",
      "Iteration 254, loss = 0.60526525\n",
      "Iteration 255, loss = 0.60426466\n",
      "Iteration 256, loss = 0.60326951\n",
      "Iteration 257, loss = 0.60227976\n",
      "Iteration 258, loss = 0.60129534\n",
      "Iteration 259, loss = 0.60031619\n",
      "Iteration 260, loss = 0.59934226\n",
      "Iteration 261, loss = 0.59837348\n",
      "Iteration 262, loss = 0.59740980\n",
      "Iteration 263, loss = 0.59645158\n",
      "Iteration 264, loss = 0.59549927\n",
      "Iteration 265, loss = 0.59455209\n",
      "Iteration 266, loss = 0.59360982\n",
      "Iteration 267, loss = 0.59267239\n",
      "Iteration 268, loss = 0.59173982\n",
      "Iteration 269, loss = 0.59081206\n",
      "Iteration 270, loss = 0.58988898\n",
      "Iteration 271, loss = 0.58897057\n",
      "Iteration 272, loss = 0.58805678\n",
      "Iteration 273, loss = 0.58714749\n",
      "Iteration 274, loss = 0.58624266\n",
      "Iteration 275, loss = 0.58534223\n",
      "Iteration 276, loss = 0.58444615\n",
      "Iteration 277, loss = 0.58355436\n",
      "Iteration 278, loss = 0.58266681\n",
      "Iteration 279, loss = 0.58178345\n",
      "Iteration 280, loss = 0.58090423\n",
      "Iteration 281, loss = 0.58003210\n",
      "Iteration 282, loss = 0.57916641\n",
      "Iteration 283, loss = 0.57830679\n",
      "Iteration 284, loss = 0.57745304\n",
      "Iteration 285, loss = 0.57660329\n",
      "Iteration 286, loss = 0.57575758\n",
      "Iteration 287, loss = 0.57491765\n",
      "Iteration 288, loss = 0.57408137\n",
      "Iteration 289, loss = 0.57324883\n",
      "Iteration 290, loss = 0.57242008\n",
      "Iteration 291, loss = 0.57159519\n",
      "Iteration 292, loss = 0.57077419\n",
      "Iteration 293, loss = 0.56995713\n",
      "Iteration 294, loss = 0.56914402\n",
      "Iteration 295, loss = 0.56833488\n",
      "Iteration 296, loss = 0.56752972\n",
      "Iteration 297, loss = 0.56672859\n",
      "Iteration 298, loss = 0.56593144\n",
      "Iteration 299, loss = 0.56513825\n",
      "Iteration 300, loss = 0.56434901\n",
      "Iteration 301, loss = 0.56356370\n",
      "Iteration 302, loss = 0.56278231\n",
      "Iteration 303, loss = 0.56200481\n",
      "Iteration 304, loss = 0.56123114\n",
      "Iteration 305, loss = 0.56046126\n",
      "Iteration 306, loss = 0.55969514\n",
      "Iteration 307, loss = 0.55893274\n",
      "Iteration 308, loss = 0.55817402\n",
      "Iteration 309, loss = 0.55741892\n",
      "Iteration 310, loss = 0.55666740\n",
      "Iteration 311, loss = 0.55591943\n",
      "Iteration 312, loss = 0.55517494\n",
      "Iteration 313, loss = 0.55443390\n",
      "Iteration 314, loss = 0.55369625\n",
      "Iteration 315, loss = 0.55296196\n",
      "Iteration 316, loss = 0.55223096\n",
      "Iteration 317, loss = 0.55150323\n",
      "Iteration 318, loss = 0.55077870\n",
      "Iteration 319, loss = 0.55005734\n",
      "Iteration 320, loss = 0.54933909\n",
      "Iteration 321, loss = 0.54862392\n",
      "Iteration 322, loss = 0.54791178\n",
      "Iteration 323, loss = 0.54720263\n",
      "Iteration 324, loss = 0.54649641\n",
      "Iteration 325, loss = 0.54579309\n",
      "Iteration 326, loss = 0.54509263\n",
      "Iteration 327, loss = 0.54439500\n",
      "Iteration 328, loss = 0.54370015\n",
      "Iteration 329, loss = 0.54300804\n",
      "Iteration 330, loss = 0.54232017\n",
      "Iteration 331, loss = 0.54163525\n",
      "Iteration 332, loss = 0.54095309\n",
      "Iteration 333, loss = 0.54027473\n",
      "Iteration 334, loss = 0.53959965\n",
      "Iteration 335, loss = 0.53892712\n",
      "Iteration 336, loss = 0.53825715\n",
      "Iteration 337, loss = 0.53758978\n",
      "Iteration 338, loss = 0.53692501\n",
      "Iteration 339, loss = 0.53626286\n",
      "Iteration 340, loss = 0.53560332\n",
      "Iteration 341, loss = 0.53494640\n",
      "Iteration 342, loss = 0.53429208\n",
      "Iteration 343, loss = 0.53364037\n",
      "Iteration 344, loss = 0.53299125\n",
      "Iteration 345, loss = 0.53234471\n",
      "Iteration 346, loss = 0.53170072\n",
      "Iteration 347, loss = 0.53105926\n",
      "Iteration 348, loss = 0.53042032\n",
      "Iteration 349, loss = 0.52978387\n",
      "Iteration 350, loss = 0.52914987\n",
      "Iteration 351, loss = 0.52851831\n",
      "Iteration 352, loss = 0.52788916\n",
      "Iteration 353, loss = 0.52726237\n",
      "Iteration 354, loss = 0.52663793\n",
      "Iteration 355, loss = 0.52601580\n",
      "Iteration 356, loss = 0.52539595\n",
      "Iteration 357, loss = 0.52477835\n",
      "Iteration 358, loss = 0.52416296\n",
      "Iteration 359, loss = 0.52354976\n",
      "Iteration 360, loss = 0.52293870\n",
      "Iteration 361, loss = 0.52232976\n",
      "Iteration 362, loss = 0.52172294\n",
      "Iteration 363, loss = 0.52111818\n",
      "Iteration 364, loss = 0.52051545\n",
      "Iteration 365, loss = 0.51991471\n",
      "Iteration 366, loss = 0.51931593\n",
      "Iteration 367, loss = 0.51871908\n",
      "Iteration 368, loss = 0.51812413\n",
      "Iteration 369, loss = 0.51753106\n",
      "Iteration 370, loss = 0.51693986\n",
      "Iteration 371, loss = 0.51635047\n",
      "Iteration 372, loss = 0.51576338\n",
      "Iteration 373, loss = 0.51517856\n",
      "Iteration 374, loss = 0.51459556\n",
      "Iteration 375, loss = 0.51401437\n",
      "Iteration 376, loss = 0.51343496\n",
      "Iteration 377, loss = 0.51285733\n",
      "Iteration 378, loss = 0.51228146\n",
      "Iteration 379, loss = 0.51170733\n",
      "Iteration 380, loss = 0.51113492\n",
      "Iteration 381, loss = 0.51056420\n",
      "Iteration 382, loss = 0.50999517\n",
      "Iteration 383, loss = 0.50942780\n",
      "Iteration 384, loss = 0.50886206\n",
      "Iteration 385, loss = 0.50829794\n",
      "Iteration 386, loss = 0.50773541\n",
      "Iteration 387, loss = 0.50717445\n",
      "Iteration 388, loss = 0.50661504\n",
      "Iteration 389, loss = 0.50605715\n",
      "Iteration 390, loss = 0.50550076\n",
      "Iteration 391, loss = 0.50494585\n",
      "Iteration 392, loss = 0.50439239\n",
      "Iteration 393, loss = 0.50384040\n",
      "Iteration 394, loss = 0.50328985\n",
      "Iteration 395, loss = 0.50274072\n",
      "Iteration 396, loss = 0.50219296\n",
      "Iteration 397, loss = 0.50164655\n",
      "Iteration 398, loss = 0.50110146\n",
      "Iteration 399, loss = 0.50055767\n",
      "Iteration 400, loss = 0.50001516\n",
      "Iteration 401, loss = 0.49947391\n",
      "Iteration 402, loss = 0.49893388\n",
      "Iteration 403, loss = 0.49839507\n",
      "Iteration 404, loss = 0.49785745\n",
      "Iteration 405, loss = 0.49732100\n",
      "Iteration 406, loss = 0.49678569\n",
      "Iteration 407, loss = 0.49625150\n",
      "Iteration 408, loss = 0.49571931\n",
      "Iteration 409, loss = 0.49518911\n",
      "Iteration 410, loss = 0.49466011\n",
      "Iteration 411, loss = 0.49413234\n",
      "Iteration 412, loss = 0.49360573\n",
      "Iteration 413, loss = 0.49308028\n",
      "Iteration 414, loss = 0.49255597\n",
      "Iteration 415, loss = 0.49203339\n",
      "Iteration 416, loss = 0.49151244\n",
      "Iteration 417, loss = 0.49099255\n",
      "Iteration 418, loss = 0.49047374\n",
      "Iteration 419, loss = 0.48995600\n",
      "Iteration 420, loss = 0.48943935\n",
      "Iteration 421, loss = 0.48892379\n",
      "Iteration 422, loss = 0.48840931\n",
      "Iteration 423, loss = 0.48789592\n",
      "Iteration 424, loss = 0.48738361\n",
      "Iteration 425, loss = 0.48687237\n",
      "Iteration 426, loss = 0.48636219\n",
      "Iteration 427, loss = 0.48585308\n",
      "Iteration 428, loss = 0.48534500\n",
      "Iteration 429, loss = 0.48483796\n",
      "Iteration 430, loss = 0.48433195\n",
      "Iteration 431, loss = 0.48382693\n",
      "Iteration 432, loss = 0.48332292\n",
      "Iteration 433, loss = 0.48282024\n",
      "Iteration 434, loss = 0.48231848\n",
      "Iteration 435, loss = 0.48181767\n",
      "Iteration 436, loss = 0.48131780\n",
      "Iteration 437, loss = 0.48081887\n",
      "Iteration 438, loss = 0.48032088\n",
      "Iteration 439, loss = 0.47982385\n",
      "Iteration 440, loss = 0.47932776\n",
      "Iteration 441, loss = 0.47883261\n",
      "Iteration 442, loss = 0.47833845\n",
      "Iteration 443, loss = 0.47784527\n",
      "Iteration 444, loss = 0.47735290\n",
      "Iteration 445, loss = 0.47686145\n",
      "Iteration 446, loss = 0.47637093\n",
      "Iteration 447, loss = 0.47588136\n",
      "Iteration 448, loss = 0.47539264\n",
      "Iteration 449, loss = 0.47490478\n",
      "Iteration 450, loss = 0.47441776\n",
      "Iteration 451, loss = 0.47393157\n",
      "Iteration 452, loss = 0.47344620\n",
      "Iteration 453, loss = 0.47296171\n",
      "Iteration 454, loss = 0.47247806\n",
      "Iteration 455, loss = 0.47199520\n",
      "Iteration 456, loss = 0.47151311\n",
      "Iteration 457, loss = 0.47103178\n",
      "Iteration 458, loss = 0.47055119\n",
      "Iteration 459, loss = 0.47007134\n",
      "Iteration 460, loss = 0.46959220\n",
      "Iteration 461, loss = 0.46911376\n",
      "Iteration 462, loss = 0.46863603\n",
      "Iteration 463, loss = 0.46815906\n",
      "Iteration 464, loss = 0.46768274\n",
      "Iteration 465, loss = 0.46720708\n",
      "Iteration 466, loss = 0.46673205\n",
      "Iteration 467, loss = 0.46625764\n",
      "Iteration 468, loss = 0.46578383\n",
      "Iteration 469, loss = 0.46531061\n",
      "Iteration 470, loss = 0.46483797\n",
      "Iteration 471, loss = 0.46436589\n",
      "Iteration 472, loss = 0.46389436\n",
      "Iteration 473, loss = 0.46342336\n",
      "Iteration 474, loss = 0.46295288\n",
      "Iteration 475, loss = 0.46248291\n",
      "Iteration 476, loss = 0.46201343\n",
      "Iteration 477, loss = 0.46154444\n",
      "Iteration 478, loss = 0.46107591\n",
      "Iteration 479, loss = 0.46060783\n",
      "Iteration 480, loss = 0.46014020\n",
      "Iteration 481, loss = 0.45967311\n",
      "Iteration 482, loss = 0.45920644\n",
      "Iteration 483, loss = 0.45874018\n",
      "Iteration 484, loss = 0.45827433\n",
      "Iteration 485, loss = 0.45780887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 486, loss = 0.45734379\n",
      "Iteration 487, loss = 0.45687908\n",
      "Iteration 488, loss = 0.45641472\n",
      "Iteration 489, loss = 0.45595071\n",
      "Iteration 490, loss = 0.45548704\n",
      "Iteration 491, loss = 0.45502369\n",
      "Iteration 492, loss = 0.45456067\n",
      "Iteration 493, loss = 0.45409803\n",
      "Iteration 494, loss = 0.45363569\n",
      "Iteration 495, loss = 0.45317364\n",
      "Iteration 496, loss = 0.45271348\n",
      "Iteration 497, loss = 0.45225406\n",
      "Iteration 498, loss = 0.45179499\n",
      "Iteration 499, loss = 0.45133627\n",
      "Iteration 500, loss = 0.45087789\n",
      "Iteration 501, loss = 0.45041986\n",
      "Iteration 502, loss = 0.44996215\n",
      "Iteration 503, loss = 0.44950478\n",
      "Iteration 504, loss = 0.44904790\n",
      "Iteration 505, loss = 0.44859134\n",
      "Iteration 506, loss = 0.44813511\n",
      "Iteration 507, loss = 0.44767921\n",
      "Iteration 508, loss = 0.44722361\n",
      "Iteration 509, loss = 0.44676832\n",
      "Iteration 510, loss = 0.44631339\n",
      "Iteration 511, loss = 0.44585888\n",
      "Iteration 512, loss = 0.44540465\n",
      "Iteration 513, loss = 0.44495072\n",
      "Iteration 514, loss = 0.44449706\n",
      "Iteration 515, loss = 0.44404367\n",
      "Iteration 516, loss = 0.44359054\n",
      "Iteration 517, loss = 0.44313770\n",
      "Iteration 518, loss = 0.44268512\n",
      "Iteration 519, loss = 0.44223277\n",
      "Iteration 520, loss = 0.44178065\n",
      "Iteration 521, loss = 0.44132875\n",
      "Iteration 522, loss = 0.44087812\n",
      "Iteration 523, loss = 0.44042820\n",
      "Iteration 524, loss = 0.43997845\n",
      "Iteration 525, loss = 0.43952889\n",
      "Iteration 526, loss = 0.43907953\n",
      "Iteration 527, loss = 0.43863038\n",
      "Iteration 528, loss = 0.43818145\n",
      "Iteration 529, loss = 0.43773282\n",
      "Iteration 530, loss = 0.43728442\n",
      "Iteration 531, loss = 0.43683626\n",
      "Iteration 532, loss = 0.43638841\n",
      "Iteration 533, loss = 0.43594083\n",
      "Iteration 534, loss = 0.43549350\n",
      "Iteration 535, loss = 0.43504642\n",
      "Iteration 536, loss = 0.43459958\n",
      "Iteration 537, loss = 0.43415302\n",
      "Iteration 538, loss = 0.43370673\n",
      "Iteration 539, loss = 0.43326067\n",
      "Iteration 540, loss = 0.43281496\n",
      "Iteration 541, loss = 0.43236958\n",
      "Iteration 542, loss = 0.43192443\n",
      "Iteration 543, loss = 0.43147951\n",
      "Iteration 544, loss = 0.43103480\n",
      "Iteration 545, loss = 0.43059030\n",
      "Iteration 546, loss = 0.43014600\n",
      "Iteration 547, loss = 0.42970189\n",
      "Iteration 548, loss = 0.42925796\n",
      "Iteration 549, loss = 0.42881421\n",
      "Iteration 550, loss = 0.42837062\n",
      "Iteration 551, loss = 0.42792719\n",
      "Iteration 552, loss = 0.42748391\n",
      "Iteration 553, loss = 0.42704076\n",
      "Iteration 554, loss = 0.42659775\n",
      "Iteration 555, loss = 0.42615486\n",
      "Iteration 556, loss = 0.42571209\n",
      "Iteration 557, loss = 0.42526942\n",
      "Iteration 558, loss = 0.42482686\n",
      "Iteration 559, loss = 0.42438438\n",
      "Iteration 560, loss = 0.42394200\n",
      "Iteration 561, loss = 0.42349969\n",
      "Iteration 562, loss = 0.42305744\n",
      "Iteration 563, loss = 0.42261527\n",
      "Iteration 564, loss = 0.42217315\n",
      "Iteration 565, loss = 0.42173108\n",
      "Iteration 566, loss = 0.42128905\n",
      "Iteration 567, loss = 0.42084706\n",
      "Iteration 568, loss = 0.42040510\n",
      "Iteration 569, loss = 0.41996316\n",
      "Iteration 570, loss = 0.41952124\n",
      "Iteration 571, loss = 0.41907933\n",
      "Iteration 572, loss = 0.41863743\n",
      "Iteration 573, loss = 0.41819553\n",
      "Iteration 574, loss = 0.41775363\n",
      "Iteration 575, loss = 0.41731174\n",
      "Iteration 576, loss = 0.41686981\n",
      "Iteration 577, loss = 0.41642908\n",
      "Iteration 578, loss = 0.41598831\n",
      "Iteration 579, loss = 0.41554750\n",
      "Iteration 580, loss = 0.41510667\n",
      "Iteration 581, loss = 0.41466582\n",
      "Iteration 582, loss = 0.41422496\n",
      "Iteration 583, loss = 0.41378409\n",
      "Iteration 584, loss = 0.41334322\n",
      "Iteration 585, loss = 0.41290236\n",
      "Iteration 586, loss = 0.41246150\n",
      "Iteration 587, loss = 0.41202065\n",
      "Iteration 588, loss = 0.41157981\n",
      "Iteration 589, loss = 0.41113898\n",
      "Iteration 590, loss = 0.41069816\n",
      "Iteration 591, loss = 0.41025734\n",
      "Iteration 592, loss = 0.40981653\n",
      "Iteration 593, loss = 0.40937573\n",
      "Iteration 594, loss = 0.40893492\n",
      "Iteration 595, loss = 0.40849412\n",
      "Iteration 596, loss = 0.40805344\n",
      "Iteration 597, loss = 0.40761280\n",
      "Iteration 598, loss = 0.40717217\n",
      "Iteration 599, loss = 0.40673155\n",
      "Iteration 600, loss = 0.40629091\n",
      "Iteration 601, loss = 0.40585027\n",
      "Iteration 602, loss = 0.40540962\n",
      "Iteration 603, loss = 0.40496894\n",
      "Iteration 604, loss = 0.40452824\n",
      "Iteration 605, loss = 0.40408752\n",
      "Iteration 606, loss = 0.40364676\n",
      "Iteration 607, loss = 0.40320596\n",
      "Iteration 608, loss = 0.40276512\n",
      "Iteration 609, loss = 0.40232423\n",
      "Iteration 610, loss = 0.40188329\n",
      "Iteration 611, loss = 0.40144230\n",
      "Iteration 612, loss = 0.40100124\n",
      "Iteration 613, loss = 0.40056012\n",
      "Iteration 614, loss = 0.40011893\n",
      "Iteration 615, loss = 0.39967767\n",
      "Iteration 616, loss = 0.39923633\n",
      "Iteration 617, loss = 0.39879491\n",
      "Iteration 618, loss = 0.39835341\n",
      "Iteration 619, loss = 0.39791181\n",
      "Iteration 620, loss = 0.39747013\n",
      "Iteration 621, loss = 0.39702835\n",
      "Iteration 622, loss = 0.39658647\n",
      "Iteration 623, loss = 0.39614449\n",
      "Iteration 624, loss = 0.39570241\n",
      "Iteration 625, loss = 0.39526022\n",
      "Iteration 626, loss = 0.39481796\n",
      "Iteration 627, loss = 0.39437575\n",
      "Iteration 628, loss = 0.39393345\n",
      "Iteration 629, loss = 0.39349104\n",
      "Iteration 630, loss = 0.39304853\n",
      "Iteration 631, loss = 0.39260590\n",
      "Iteration 632, loss = 0.39216316\n",
      "Iteration 633, loss = 0.39172030\n",
      "Iteration 634, loss = 0.39127733\n",
      "Iteration 635, loss = 0.39083497\n",
      "Iteration 636, loss = 0.39039271\n",
      "Iteration 637, loss = 0.38995026\n",
      "Iteration 638, loss = 0.38950763\n",
      "Iteration 639, loss = 0.38906484\n",
      "Iteration 640, loss = 0.38862189\n",
      "Iteration 641, loss = 0.38817880\n",
      "Iteration 642, loss = 0.38773558\n",
      "Iteration 643, loss = 0.38729222\n",
      "Iteration 644, loss = 0.38684874\n",
      "Iteration 645, loss = 0.38640514\n",
      "Iteration 646, loss = 0.38596143\n",
      "Iteration 647, loss = 0.38551762\n",
      "Iteration 648, loss = 0.38507369\n",
      "Iteration 649, loss = 0.38462967\n",
      "Iteration 650, loss = 0.38418554\n",
      "Iteration 651, loss = 0.38374132\n",
      "Iteration 652, loss = 0.38329700\n",
      "Iteration 653, loss = 0.38285269\n",
      "Iteration 654, loss = 0.38240844\n",
      "Iteration 655, loss = 0.38196392\n",
      "Iteration 656, loss = 0.38151912\n",
      "Iteration 657, loss = 0.38107431\n",
      "Iteration 658, loss = 0.38062949\n",
      "Iteration 659, loss = 0.38018455\n",
      "Iteration 660, loss = 0.37973948\n",
      "Iteration 661, loss = 0.37929428\n",
      "Iteration 662, loss = 0.37884896\n",
      "Iteration 663, loss = 0.37840353\n",
      "Iteration 664, loss = 0.37795797\n",
      "Iteration 665, loss = 0.37751229\n",
      "Iteration 666, loss = 0.37706650\n",
      "Iteration 667, loss = 0.37662058\n",
      "Iteration 668, loss = 0.37617455\n",
      "Iteration 669, loss = 0.37572840\n",
      "Iteration 670, loss = 0.37528213\n",
      "Iteration 671, loss = 0.37483574\n",
      "Iteration 672, loss = 0.37438924\n",
      "Iteration 673, loss = 0.37394261\n",
      "Iteration 674, loss = 0.37349587\n",
      "Iteration 675, loss = 0.37304900\n",
      "Iteration 676, loss = 0.37260202\n",
      "Iteration 677, loss = 0.37215497\n",
      "Iteration 678, loss = 0.37170808\n",
      "Iteration 679, loss = 0.37126108\n",
      "Iteration 680, loss = 0.37081399\n",
      "Iteration 681, loss = 0.37036680\n",
      "Iteration 682, loss = 0.36991950\n",
      "Iteration 683, loss = 0.36947209\n",
      "Iteration 684, loss = 0.36902458\n",
      "Iteration 685, loss = 0.36857695\n",
      "Iteration 686, loss = 0.36812921\n",
      "Iteration 687, loss = 0.36768136\n",
      "Iteration 688, loss = 0.36723339\n",
      "Iteration 689, loss = 0.36678531\n",
      "Iteration 690, loss = 0.36633711\n",
      "Iteration 691, loss = 0.36588878\n",
      "Iteration 692, loss = 0.36544034\n",
      "Iteration 693, loss = 0.36499177\n",
      "Iteration 694, loss = 0.36454309\n",
      "Iteration 695, loss = 0.36409428\n",
      "Iteration 696, loss = 0.36364535\n",
      "Iteration 697, loss = 0.36319630\n",
      "Iteration 698, loss = 0.36274712\n",
      "Iteration 699, loss = 0.36229782\n",
      "Iteration 700, loss = 0.36184839\n",
      "Iteration 701, loss = 0.36139884\n",
      "Iteration 702, loss = 0.36094917\n",
      "Iteration 703, loss = 0.36049938\n",
      "Iteration 704, loss = 0.36004946\n",
      "Iteration 705, loss = 0.35959942\n",
      "Iteration 706, loss = 0.35914925\n",
      "Iteration 707, loss = 0.35869897\n",
      "Iteration 708, loss = 0.35824856\n",
      "Iteration 709, loss = 0.35779803\n",
      "Iteration 710, loss = 0.35734738\n",
      "Iteration 711, loss = 0.35689662\n",
      "Iteration 712, loss = 0.35644573\n",
      "Iteration 713, loss = 0.35599472\n",
      "Iteration 714, loss = 0.35554360\n",
      "Iteration 715, loss = 0.35509237\n",
      "Iteration 716, loss = 0.35464101\n",
      "Iteration 717, loss = 0.35418954\n",
      "Iteration 718, loss = 0.35373796\n",
      "Iteration 719, loss = 0.35328627\n",
      "Iteration 720, loss = 0.35283447\n",
      "Iteration 721, loss = 0.35238255\n",
      "Iteration 722, loss = 0.35193053\n",
      "Iteration 723, loss = 0.35147840\n",
      "Iteration 724, loss = 0.35102617\n",
      "Iteration 725, loss = 0.35057383\n",
      "Iteration 726, loss = 0.35012138\n",
      "Iteration 727, loss = 0.34966884\n",
      "Iteration 728, loss = 0.34921619\n",
      "Iteration 729, loss = 0.34876345\n",
      "Iteration 730, loss = 0.34831061\n",
      "Iteration 731, loss = 0.34785767\n",
      "Iteration 732, loss = 0.34740464\n",
      "Iteration 733, loss = 0.34695152\n",
      "Iteration 734, loss = 0.34649865\n",
      "Iteration 735, loss = 0.34604603\n",
      "Iteration 736, loss = 0.34559319\n",
      "Iteration 737, loss = 0.34514015\n",
      "Iteration 738, loss = 0.34468692\n",
      "Iteration 739, loss = 0.34423352\n",
      "Iteration 740, loss = 0.34377998\n",
      "Iteration 741, loss = 0.34332629\n",
      "Iteration 742, loss = 0.34287249\n",
      "Iteration 743, loss = 0.34241920\n",
      "Iteration 744, loss = 0.34196581\n",
      "Iteration 745, loss = 0.34151230\n",
      "Iteration 746, loss = 0.34105867\n",
      "Iteration 747, loss = 0.34060494\n",
      "Iteration 748, loss = 0.34015110\n",
      "Iteration 749, loss = 0.33969717\n",
      "Iteration 750, loss = 0.33924314\n",
      "Iteration 751, loss = 0.33878903\n",
      "Iteration 752, loss = 0.33833483\n",
      "Iteration 753, loss = 0.33788085\n",
      "Iteration 754, loss = 0.33742696\n",
      "Iteration 755, loss = 0.33697295\n",
      "Iteration 756, loss = 0.33651882\n",
      "Iteration 757, loss = 0.33606460\n",
      "Iteration 758, loss = 0.33561028\n",
      "Iteration 759, loss = 0.33515589\n",
      "Iteration 760, loss = 0.33470174\n",
      "Iteration 761, loss = 0.33424762\n",
      "Iteration 762, loss = 0.33379339\n",
      "Iteration 763, loss = 0.33333908\n",
      "Iteration 764, loss = 0.33288468\n",
      "Iteration 765, loss = 0.33243021\n",
      "Iteration 766, loss = 0.33197587\n",
      "Iteration 767, loss = 0.33152164\n",
      "Iteration 768, loss = 0.33106734\n",
      "Iteration 769, loss = 0.33061297\n",
      "Iteration 770, loss = 0.33015856\n",
      "Iteration 771, loss = 0.32970410\n",
      "Iteration 772, loss = 0.32924988\n",
      "Iteration 773, loss = 0.32879564\n",
      "Iteration 774, loss = 0.32834133\n",
      "Iteration 775, loss = 0.32788695\n",
      "Iteration 776, loss = 0.32743251\n",
      "Iteration 777, loss = 0.32697840\n",
      "Iteration 778, loss = 0.32652426\n",
      "Iteration 779, loss = 0.32607009\n",
      "Iteration 780, loss = 0.32561589\n",
      "Iteration 781, loss = 0.32516168\n",
      "Iteration 782, loss = 0.32470746\n",
      "Iteration 783, loss = 0.32425324\n",
      "Iteration 784, loss = 0.32379929\n",
      "Iteration 785, loss = 0.32334539\n",
      "Iteration 786, loss = 0.32289142\n",
      "Iteration 787, loss = 0.32243741\n",
      "Iteration 788, loss = 0.32198337\n",
      "Iteration 789, loss = 0.32152962\n",
      "Iteration 790, loss = 0.32107587\n",
      "Iteration 791, loss = 0.32062215\n",
      "Iteration 792, loss = 0.32016845\n",
      "Iteration 793, loss = 0.31971497\n",
      "Iteration 794, loss = 0.31926149\n",
      "Iteration 795, loss = 0.31880797\n",
      "Iteration 796, loss = 0.31835460\n",
      "Iteration 797, loss = 0.31790136\n",
      "Iteration 798, loss = 0.31744817\n",
      "Iteration 799, loss = 0.31699502\n",
      "Iteration 800, loss = 0.31654193\n",
      "Iteration 801, loss = 0.31608894\n",
      "Iteration 802, loss = 0.31563609\n",
      "Iteration 803, loss = 0.31518331\n",
      "Iteration 804, loss = 0.31473065\n",
      "Iteration 805, loss = 0.31427807\n",
      "Iteration 806, loss = 0.31382564\n",
      "Iteration 807, loss = 0.31337325\n",
      "Iteration 808, loss = 0.31292105\n",
      "Iteration 809, loss = 0.31246895\n",
      "Iteration 810, loss = 0.31201694\n",
      "Iteration 811, loss = 0.31156503\n",
      "Iteration 812, loss = 0.31111321\n",
      "Iteration 813, loss = 0.31066166\n",
      "Iteration 814, loss = 0.31021017\n",
      "Iteration 815, loss = 0.30975869\n",
      "Iteration 816, loss = 0.30930750\n",
      "Iteration 817, loss = 0.30885647\n",
      "Iteration 818, loss = 0.30840554\n",
      "Iteration 819, loss = 0.30795473\n",
      "Iteration 820, loss = 0.30750404\n",
      "Iteration 821, loss = 0.30705349\n",
      "Iteration 822, loss = 0.30660307\n",
      "Iteration 823, loss = 0.30615297\n",
      "Iteration 824, loss = 0.30570300\n",
      "Iteration 825, loss = 0.30525306\n",
      "Iteration 826, loss = 0.30480332\n",
      "Iteration 827, loss = 0.30435386\n",
      "Iteration 828, loss = 0.30390454\n",
      "Iteration 829, loss = 0.30345538\n",
      "Iteration 830, loss = 0.30300638\n",
      "Iteration 831, loss = 0.30255755\n",
      "Iteration 832, loss = 0.30210895\n",
      "Iteration 833, loss = 0.30166056\n",
      "Iteration 834, loss = 0.30121232\n",
      "Iteration 835, loss = 0.30076434\n",
      "Iteration 836, loss = 0.30031653\n",
      "Iteration 837, loss = 0.29986892\n",
      "Iteration 838, loss = 0.29942157\n",
      "Iteration 839, loss = 0.29897437\n",
      "Iteration 840, loss = 0.29852745\n",
      "Iteration 841, loss = 0.29808076\n",
      "Iteration 842, loss = 0.29763426\n",
      "Iteration 843, loss = 0.29718798\n",
      "Iteration 844, loss = 0.29674190\n",
      "Iteration 845, loss = 0.29629604\n",
      "Iteration 846, loss = 0.29585060\n",
      "Iteration 847, loss = 0.29540529\n",
      "Iteration 848, loss = 0.29496009\n",
      "Iteration 849, loss = 0.29451529\n",
      "Iteration 850, loss = 0.29407077\n",
      "Iteration 851, loss = 0.29362648\n",
      "Iteration 852, loss = 0.29318242\n",
      "Iteration 853, loss = 0.29273861\n",
      "Iteration 854, loss = 0.29229504\n",
      "Iteration 855, loss = 0.29185172\n",
      "Iteration 856, loss = 0.29140867\n",
      "Iteration 857, loss = 0.29096603\n",
      "Iteration 858, loss = 0.29052363\n",
      "Iteration 859, loss = 0.29008139\n",
      "Iteration 860, loss = 0.28963951\n",
      "Iteration 861, loss = 0.28919798\n",
      "Iteration 862, loss = 0.28875673\n",
      "Iteration 863, loss = 0.28831575\n",
      "Iteration 864, loss = 0.28787504\n",
      "Iteration 865, loss = 0.28743463\n",
      "Iteration 866, loss = 0.28699451\n",
      "Iteration 867, loss = 0.28655476\n",
      "Iteration 868, loss = 0.28611534\n",
      "Iteration 869, loss = 0.28567613\n",
      "Iteration 870, loss = 0.28523734\n",
      "Iteration 871, loss = 0.28479885\n",
      "Iteration 872, loss = 0.28436067\n",
      "Iteration 873, loss = 0.28392294\n",
      "Iteration 874, loss = 0.28348542\n",
      "Iteration 875, loss = 0.28304824\n",
      "Iteration 876, loss = 0.28261146\n",
      "Iteration 877, loss = 0.28217501\n",
      "Iteration 878, loss = 0.28173888\n",
      "Iteration 879, loss = 0.28130309\n",
      "Iteration 880, loss = 0.28086764\n",
      "Iteration 881, loss = 0.28043273\n",
      "Iteration 882, loss = 0.27999805\n",
      "Iteration 883, loss = 0.27956362\n",
      "Iteration 884, loss = 0.27912968\n",
      "Iteration 885, loss = 0.27869615\n",
      "Iteration 886, loss = 0.27826298\n",
      "Iteration 887, loss = 0.27783015\n",
      "Iteration 888, loss = 0.27739769\n",
      "Iteration 889, loss = 0.27696560\n",
      "Iteration 890, loss = 0.27653387\n",
      "Iteration 891, loss = 0.27610252\n",
      "Iteration 892, loss = 0.27567176\n",
      "Iteration 893, loss = 0.27524131\n",
      "Iteration 894, loss = 0.27481115\n",
      "Iteration 895, loss = 0.27438136\n",
      "Iteration 896, loss = 0.27395213\n",
      "Iteration 897, loss = 0.27352328\n",
      "Iteration 898, loss = 0.27309482\n",
      "Iteration 899, loss = 0.27266676\n",
      "Iteration 900, loss = 0.27223910\n",
      "Iteration 901, loss = 0.27181197\n",
      "Iteration 902, loss = 0.27138522\n",
      "Iteration 903, loss = 0.27095878\n",
      "Iteration 904, loss = 0.27053296\n",
      "Iteration 905, loss = 0.27010755\n",
      "Iteration 906, loss = 0.26968255\n",
      "Iteration 907, loss = 0.26925796\n",
      "Iteration 908, loss = 0.26883380\n",
      "Iteration 909, loss = 0.26841007\n",
      "Iteration 910, loss = 0.26798676\n",
      "Iteration 911, loss = 0.26756393\n",
      "Iteration 912, loss = 0.26714163\n",
      "Iteration 913, loss = 0.26671971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 914, loss = 0.26629832\n",
      "Iteration 915, loss = 0.26587741\n",
      "Iteration 916, loss = 0.26545695\n",
      "Iteration 917, loss = 0.26503698\n",
      "Iteration 918, loss = 0.26461745\n",
      "Iteration 919, loss = 0.26419849\n",
      "Iteration 920, loss = 0.26377992\n",
      "Iteration 921, loss = 0.26336184\n",
      "Iteration 922, loss = 0.26294430\n",
      "Iteration 923, loss = 0.26252721\n",
      "Iteration 924, loss = 0.26211060\n",
      "Iteration 925, loss = 0.26169448\n",
      "Iteration 926, loss = 0.26127887\n",
      "Iteration 927, loss = 0.26086381\n",
      "Iteration 928, loss = 0.26044924\n",
      "Iteration 929, loss = 0.26003515\n",
      "Iteration 930, loss = 0.25962153\n",
      "Iteration 931, loss = 0.25920846\n",
      "Iteration 932, loss = 0.25879590\n",
      "Iteration 933, loss = 0.25838386\n",
      "Iteration 934, loss = 0.25797236\n",
      "Iteration 935, loss = 0.25756135\n",
      "Iteration 936, loss = 0.25715085\n",
      "Iteration 937, loss = 0.25674091\n",
      "Iteration 938, loss = 0.25633149\n",
      "Iteration 939, loss = 0.25592262\n",
      "Iteration 940, loss = 0.25551428\n",
      "Iteration 941, loss = 0.25510644\n",
      "Iteration 942, loss = 0.25469925\n",
      "Iteration 943, loss = 0.25429252\n",
      "Iteration 944, loss = 0.25388626\n",
      "Iteration 945, loss = 0.25348070\n",
      "Iteration 946, loss = 0.25307567\n",
      "Iteration 947, loss = 0.25267115\n",
      "Iteration 948, loss = 0.25226715\n",
      "Iteration 949, loss = 0.25186368\n",
      "Iteration 950, loss = 0.25146074\n",
      "Iteration 951, loss = 0.25105851\n",
      "Iteration 952, loss = 0.25065683\n",
      "Iteration 953, loss = 0.25025563\n",
      "Iteration 954, loss = 0.24985494\n",
      "Iteration 955, loss = 0.24945491\n",
      "Iteration 956, loss = 0.24905551\n",
      "Iteration 957, loss = 0.24865665\n",
      "Iteration 958, loss = 0.24825832\n",
      "Iteration 959, loss = 0.24786054\n",
      "Iteration 960, loss = 0.24746331\n",
      "Iteration 961, loss = 0.24706688\n",
      "Iteration 962, loss = 0.24667096\n",
      "Iteration 963, loss = 0.24627556\n",
      "Iteration 964, loss = 0.24588069\n",
      "Iteration 965, loss = 0.24548635\n",
      "Iteration 966, loss = 0.24509279\n",
      "Iteration 967, loss = 0.24469977\n",
      "Iteration 968, loss = 0.24430732\n",
      "Iteration 969, loss = 0.24391551\n",
      "Iteration 970, loss = 0.24352431\n",
      "Iteration 971, loss = 0.24313364\n",
      "Iteration 972, loss = 0.24274370\n",
      "Iteration 973, loss = 0.24235433\n",
      "Iteration 974, loss = 0.24196553\n",
      "Iteration 975, loss = 0.24157730\n",
      "Iteration 976, loss = 0.24118972\n",
      "Iteration 977, loss = 0.24080280\n",
      "Iteration 978, loss = 0.24041644\n",
      "Iteration 979, loss = 0.24003070\n",
      "Iteration 980, loss = 0.23964562\n",
      "Iteration 981, loss = 0.23926112\n",
      "Iteration 982, loss = 0.23887731\n",
      "Iteration 983, loss = 0.23849410\n",
      "Iteration 984, loss = 0.23811147\n",
      "Iteration 985, loss = 0.23772951\n",
      "Iteration 986, loss = 0.23734820\n",
      "Iteration 987, loss = 0.23696747\n",
      "Iteration 988, loss = 0.23658737\n",
      "Iteration 989, loss = 0.23620794\n",
      "Iteration 990, loss = 0.23582912\n",
      "Iteration 991, loss = 0.23545097\n",
      "Iteration 992, loss = 0.23507345\n",
      "Iteration 993, loss = 0.23469656\n",
      "Iteration 994, loss = 0.23432034\n",
      "Iteration 995, loss = 0.23394473\n",
      "Iteration 996, loss = 0.23356984\n",
      "Iteration 997, loss = 0.23319556\n",
      "Iteration 998, loss = 0.23282189\n",
      "Iteration 999, loss = 0.23244883\n",
      "Iteration 1000, loss = 0.23207650\n",
      "Iteration 1, loss = 2.10901821\n",
      "Iteration 2, loss = 2.04600424\n",
      "Iteration 3, loss = 1.95912762\n",
      "Iteration 4, loss = 1.85404741\n",
      "Iteration 5, loss = 1.73681090\n",
      "Iteration 6, loss = 1.61374326\n",
      "Iteration 7, loss = 1.49166328\n",
      "Iteration 8, loss = 1.37747868\n",
      "Iteration 9, loss = 1.27697269\n",
      "Iteration 10, loss = 1.19437197\n",
      "Iteration 11, loss = 1.13232529\n",
      "Iteration 12, loss = 1.09199410\n",
      "Iteration 13, loss = 1.07102761\n",
      "Iteration 14, loss = 1.06496888\n",
      "Iteration 15, loss = 1.06855862\n",
      "Iteration 16, loss = 1.07697764\n",
      "Iteration 17, loss = 1.08621422\n",
      "Iteration 18, loss = 1.09352467\n",
      "Iteration 19, loss = 1.09736450\n",
      "Iteration 20, loss = 1.09713359\n",
      "Iteration 21, loss = 1.09290122\n",
      "Iteration 22, loss = 1.08520405\n",
      "Iteration 23, loss = 1.07485043\n",
      "Iteration 24, loss = 1.06275931\n",
      "Iteration 25, loss = 1.04983385\n",
      "Iteration 26, loss = 1.03686718\n",
      "Iteration 27, loss = 1.02446040\n",
      "Iteration 28, loss = 1.01307772\n",
      "Iteration 29, loss = 1.00296211\n",
      "Iteration 30, loss = 0.99418207\n",
      "Iteration 31, loss = 0.98666644\n",
      "Iteration 32, loss = 0.98024602\n",
      "Iteration 33, loss = 0.97469531\n",
      "Iteration 34, loss = 0.96976967\n",
      "Iteration 35, loss = 0.96523494\n",
      "Iteration 36, loss = 0.96089547\n",
      "Iteration 37, loss = 0.95658699\n",
      "Iteration 38, loss = 0.95220537\n",
      "Iteration 39, loss = 0.94768720\n",
      "Iteration 40, loss = 0.94300554\n",
      "Iteration 41, loss = 0.93816904\n",
      "Iteration 42, loss = 0.93321909\n",
      "Iteration 43, loss = 0.92820768\n",
      "Iteration 44, loss = 0.92319181\n",
      "Iteration 45, loss = 0.91822568\n",
      "Iteration 46, loss = 0.91335488\n",
      "Iteration 47, loss = 0.90861276\n",
      "Iteration 48, loss = 0.90401888\n",
      "Iteration 49, loss = 0.89957937\n",
      "Iteration 50, loss = 0.89528870\n",
      "Iteration 51, loss = 0.89113241\n",
      "Iteration 52, loss = 0.88708226\n",
      "Iteration 53, loss = 0.88312054\n",
      "Iteration 54, loss = 0.87922857\n",
      "Iteration 55, loss = 0.87538623\n",
      "Iteration 56, loss = 0.87157712\n",
      "Iteration 57, loss = 0.86778936\n",
      "Iteration 58, loss = 0.86401575\n",
      "Iteration 59, loss = 0.86025333\n",
      "Iteration 60, loss = 0.85650266\n",
      "Iteration 61, loss = 0.85276684\n",
      "Iteration 62, loss = 0.84905055\n",
      "Iteration 63, loss = 0.84535855\n",
      "Iteration 64, loss = 0.84169651\n",
      "Iteration 65, loss = 0.83806892\n",
      "Iteration 66, loss = 0.83447917\n",
      "Iteration 67, loss = 0.83092943\n",
      "Iteration 68, loss = 0.82742069\n",
      "Iteration 69, loss = 0.82395282\n",
      "Iteration 70, loss = 0.82052487\n",
      "Iteration 71, loss = 0.81713527\n",
      "Iteration 72, loss = 0.81378209\n",
      "Iteration 73, loss = 0.81046332\n",
      "Iteration 74, loss = 0.80717698\n",
      "Iteration 75, loss = 0.80392132\n",
      "Iteration 76, loss = 0.80069488\n",
      "Iteration 77, loss = 0.79749650\n",
      "Iteration 78, loss = 0.79435120\n",
      "Iteration 79, loss = 0.79124169\n",
      "Iteration 80, loss = 0.78815929\n",
      "Iteration 81, loss = 0.78510373\n",
      "Iteration 82, loss = 0.78207496\n",
      "Iteration 83, loss = 0.77907285\n",
      "Iteration 84, loss = 0.77610052\n",
      "Iteration 85, loss = 0.77318816\n",
      "Iteration 86, loss = 0.77033572\n",
      "Iteration 87, loss = 0.76752521\n",
      "Iteration 88, loss = 0.76474223\n",
      "Iteration 89, loss = 0.76198645\n",
      "Iteration 90, loss = 0.75925751\n",
      "Iteration 91, loss = 0.75655497\n",
      "Iteration 92, loss = 0.75387837\n",
      "Iteration 93, loss = 0.75122723\n",
      "Iteration 94, loss = 0.74861835\n",
      "Iteration 95, loss = 0.74604252\n",
      "Iteration 96, loss = 0.74349179\n",
      "Iteration 97, loss = 0.74096564\n",
      "Iteration 98, loss = 0.73846356\n",
      "Iteration 99, loss = 0.73598500\n",
      "Iteration 100, loss = 0.73354328\n",
      "Iteration 101, loss = 0.73113094\n",
      "Iteration 102, loss = 0.72874168\n",
      "Iteration 103, loss = 0.72637496\n",
      "Iteration 104, loss = 0.72403023\n",
      "Iteration 105, loss = 0.72170695\n",
      "Iteration 106, loss = 0.71940458\n",
      "Iteration 107, loss = 0.71712260\n",
      "Iteration 108, loss = 0.71486051\n",
      "Iteration 109, loss = 0.71261781\n",
      "Iteration 110, loss = 0.71039405\n",
      "Iteration 111, loss = 0.70819633\n",
      "Iteration 112, loss = 0.70602575\n",
      "Iteration 113, loss = 0.70387387\n",
      "Iteration 114, loss = 0.70174027\n",
      "Iteration 115, loss = 0.69962419\n",
      "Iteration 116, loss = 0.69752533\n",
      "Iteration 117, loss = 0.69545269\n",
      "Iteration 118, loss = 0.69340166\n",
      "Iteration 119, loss = 0.69136777\n",
      "Iteration 120, loss = 0.68935069\n",
      "Iteration 121, loss = 0.68735561\n",
      "Iteration 122, loss = 0.68539635\n",
      "Iteration 123, loss = 0.68345694\n",
      "Iteration 124, loss = 0.68153365\n",
      "Iteration 125, loss = 0.67962604\n",
      "Iteration 126, loss = 0.67773430\n",
      "Iteration 127, loss = 0.67585818\n",
      "Iteration 128, loss = 0.67400526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1027, in fit\n",
      "    return self._fit(X, y, incremental=(self.warm_start and\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 321, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 427, in _validate_hyperparameters\n",
      "    raise ValueError(\"The solver %s is not supported. \"\n",
      "ValueError: The solver lfbs is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 129, loss = 0.67217771\n",
      "Iteration 130, loss = 0.67037010\n",
      "Iteration 131, loss = 0.66857899\n",
      "Iteration 132, loss = 0.66680239\n",
      "Iteration 133, loss = 0.66504030\n",
      "Iteration 134, loss = 0.66329232\n",
      "Iteration 135, loss = 0.66155821\n",
      "Iteration 136, loss = 0.65983775\n",
      "Iteration 137, loss = 0.65813329\n",
      "Iteration 138, loss = 0.65644449\n",
      "Iteration 139, loss = 0.65476922\n",
      "Iteration 140, loss = 0.65310733\n",
      "Iteration 141, loss = 0.65145865\n",
      "Iteration 142, loss = 0.64982298\n",
      "Iteration 143, loss = 0.64820007\n",
      "Iteration 144, loss = 0.64658970\n",
      "Iteration 145, loss = 0.64499159\n",
      "Iteration 146, loss = 0.64340551\n",
      "Iteration 147, loss = 0.64183119\n",
      "Iteration 148, loss = 0.64026842\n",
      "Iteration 149, loss = 0.63871664\n",
      "Iteration 150, loss = 0.63717512\n",
      "Iteration 151, loss = 0.63564442\n",
      "Iteration 152, loss = 0.63412388\n",
      "Iteration 153, loss = 0.63261330\n",
      "Iteration 154, loss = 0.63111301\n",
      "Iteration 155, loss = 0.62962290\n",
      "Iteration 156, loss = 0.62814286\n",
      "Iteration 157, loss = 0.62667282\n",
      "Iteration 158, loss = 0.62521267\n",
      "Iteration 159, loss = 0.62376684\n",
      "Iteration 160, loss = 0.62233484\n",
      "Iteration 161, loss = 0.62091259\n",
      "Iteration 162, loss = 0.61950001\n",
      "Iteration 163, loss = 0.61809626\n",
      "Iteration 164, loss = 0.61670206\n",
      "Iteration 165, loss = 0.61531735\n",
      "Iteration 166, loss = 0.61394210\n",
      "Iteration 167, loss = 0.61257622\n",
      "Iteration 168, loss = 0.61121895\n",
      "Iteration 169, loss = 0.60987058\n",
      "Iteration 170, loss = 0.60853039\n",
      "Iteration 171, loss = 0.60719802\n",
      "Iteration 172, loss = 0.60587445\n",
      "Iteration 173, loss = 0.60455894\n",
      "Iteration 174, loss = 0.60325104\n",
      "Iteration 175, loss = 0.60194970\n",
      "Iteration 176, loss = 0.60065559\n",
      "Iteration 177, loss = 0.59936966\n",
      "Iteration 178, loss = 0.59809189\n",
      "Iteration 179, loss = 0.59682229\n",
      "Iteration 180, loss = 0.59556083\n",
      "Iteration 181, loss = 0.59430750\n",
      "Iteration 182, loss = 0.59306226\n",
      "Iteration 183, loss = 0.59182461\n",
      "Iteration 184, loss = 0.59059387\n",
      "Iteration 185, loss = 0.58936860\n",
      "Iteration 186, loss = 0.58815039\n",
      "Iteration 187, loss = 0.58693978\n",
      "Iteration 188, loss = 0.58573678\n",
      "Iteration 189, loss = 0.58454057\n",
      "Iteration 190, loss = 0.58335104\n",
      "Iteration 191, loss = 0.58216896\n",
      "Iteration 192, loss = 0.58099460\n",
      "Iteration 193, loss = 0.57982796\n",
      "Iteration 194, loss = 0.57866876\n",
      "Iteration 195, loss = 0.57751698\n",
      "Iteration 196, loss = 0.57637338\n",
      "Iteration 197, loss = 0.57523732\n",
      "Iteration 198, loss = 0.57410349\n",
      "Iteration 199, loss = 0.57297650\n",
      "Iteration 200, loss = 0.57185638\n",
      "Iteration 201, loss = 0.57074314\n",
      "Iteration 202, loss = 0.56963500\n",
      "Iteration 203, loss = 0.56853299\n",
      "Iteration 204, loss = 0.56743767\n",
      "Iteration 205, loss = 0.56634905\n",
      "Iteration 206, loss = 0.56526714\n",
      "Iteration 207, loss = 0.56419193\n",
      "Iteration 208, loss = 0.56312311\n",
      "Iteration 209, loss = 0.56205922\n",
      "Iteration 210, loss = 0.56100070\n",
      "Iteration 211, loss = 0.55994717\n",
      "Iteration 212, loss = 0.55889832\n",
      "Iteration 213, loss = 0.55785538\n",
      "Iteration 214, loss = 0.55681860\n",
      "Iteration 215, loss = 0.55578738\n",
      "Iteration 216, loss = 0.55476072\n",
      "Iteration 217, loss = 0.55373826\n",
      "Iteration 218, loss = 0.55272144\n",
      "Iteration 219, loss = 0.55171048\n",
      "Iteration 220, loss = 0.55070569\n",
      "Iteration 221, loss = 0.54970697\n",
      "Iteration 222, loss = 0.54871071\n",
      "Iteration 223, loss = 0.54771894\n",
      "Iteration 224, loss = 0.54673307\n",
      "Iteration 225, loss = 0.54575285\n",
      "Iteration 226, loss = 0.54477557\n",
      "Iteration 227, loss = 0.54380091\n",
      "Iteration 228, loss = 0.54282830\n",
      "Iteration 229, loss = 0.54186057\n",
      "Iteration 230, loss = 0.54089616\n",
      "Iteration 231, loss = 0.53993738\n",
      "Iteration 232, loss = 0.53898391\n",
      "Iteration 233, loss = 0.53803554\n",
      "Iteration 234, loss = 0.53709231\n",
      "Iteration 235, loss = 0.53615273\n",
      "Iteration 236, loss = 0.53521741\n",
      "Iteration 237, loss = 0.53428735\n",
      "Iteration 238, loss = 0.53336097\n",
      "Iteration 239, loss = 0.53243760\n",
      "Iteration 240, loss = 0.53151903\n",
      "Iteration 241, loss = 0.53060530\n",
      "Iteration 242, loss = 0.52969643\n",
      "Iteration 243, loss = 0.52879243\n",
      "Iteration 244, loss = 0.52789330\n",
      "Iteration 245, loss = 0.52699912\n",
      "Iteration 246, loss = 0.52611007\n",
      "Iteration 247, loss = 0.52522589\n",
      "Iteration 248, loss = 0.52434662\n",
      "Iteration 249, loss = 0.52347238\n",
      "Iteration 250, loss = 0.52260369\n",
      "Iteration 251, loss = 0.52174049\n",
      "Iteration 252, loss = 0.52088208\n",
      "Iteration 253, loss = 0.52002859\n",
      "Iteration 254, loss = 0.51917994\n",
      "Iteration 255, loss = 0.51833608\n",
      "Iteration 256, loss = 0.51749707\n",
      "Iteration 257, loss = 0.51666283\n",
      "Iteration 258, loss = 0.51583314\n",
      "Iteration 259, loss = 0.51500795\n",
      "Iteration 260, loss = 0.51418720\n",
      "Iteration 261, loss = 0.51337085\n",
      "Iteration 262, loss = 0.51255884\n",
      "Iteration 263, loss = 0.51175129\n",
      "Iteration 264, loss = 0.51094821\n",
      "Iteration 265, loss = 0.51014938\n",
      "Iteration 266, loss = 0.50935480\n",
      "Iteration 267, loss = 0.50856442\n",
      "Iteration 268, loss = 0.50777824\n",
      "Iteration 269, loss = 0.50699610\n",
      "Iteration 270, loss = 0.50621795\n",
      "Iteration 271, loss = 0.50544375\n",
      "Iteration 272, loss = 0.50467345\n",
      "Iteration 273, loss = 0.50390700\n",
      "Iteration 274, loss = 0.50314437\n",
      "Iteration 275, loss = 0.50238563\n",
      "Iteration 276, loss = 0.50163066\n",
      "Iteration 277, loss = 0.50087938\n",
      "Iteration 278, loss = 0.50013174\n",
      "Iteration 279, loss = 0.49938772\n",
      "Iteration 280, loss = 0.49864738\n",
      "Iteration 281, loss = 0.49791065\n",
      "Iteration 282, loss = 0.49717743\n",
      "Iteration 283, loss = 0.49644778\n",
      "Iteration 284, loss = 0.49572167\n",
      "Iteration 285, loss = 0.49499894\n",
      "Iteration 286, loss = 0.49427956\n",
      "Iteration 287, loss = 0.49356349\n",
      "Iteration 288, loss = 0.49285069\n",
      "Iteration 289, loss = 0.49214113\n",
      "Iteration 290, loss = 0.49143476\n",
      "Iteration 291, loss = 0.49073155\n",
      "Iteration 292, loss = 0.49003152\n",
      "Iteration 293, loss = 0.48933463\n",
      "Iteration 294, loss = 0.48864080\n",
      "Iteration 295, loss = 0.48795001\n",
      "Iteration 296, loss = 0.48726220\n",
      "Iteration 297, loss = 0.48657736\n",
      "Iteration 298, loss = 0.48589545\n",
      "Iteration 299, loss = 0.48521643\n",
      "Iteration 300, loss = 0.48454028\n",
      "Iteration 301, loss = 0.48386697\n",
      "Iteration 302, loss = 0.48319646\n",
      "Iteration 303, loss = 0.48252880\n",
      "Iteration 304, loss = 0.48186392\n",
      "Iteration 305, loss = 0.48120176\n",
      "Iteration 306, loss = 0.48054232\n",
      "Iteration 307, loss = 0.47988565\n",
      "Iteration 308, loss = 0.47923237\n",
      "Iteration 309, loss = 0.47858183\n",
      "Iteration 310, loss = 0.47793391\n",
      "Iteration 311, loss = 0.47728861\n",
      "Iteration 312, loss = 0.47664593\n",
      "Iteration 313, loss = 0.47600578\n",
      "Iteration 314, loss = 0.47536815\n",
      "Iteration 315, loss = 0.47473300\n",
      "Iteration 316, loss = 0.47410031\n",
      "Iteration 317, loss = 0.47347006\n",
      "Iteration 318, loss = 0.47284221\n",
      "Iteration 319, loss = 0.47221674\n",
      "Iteration 320, loss = 0.47159363\n",
      "Iteration 321, loss = 0.47097285\n",
      "Iteration 322, loss = 0.47035442\n",
      "Iteration 323, loss = 0.46973829\n",
      "Iteration 324, loss = 0.46912441\n",
      "Iteration 325, loss = 0.46851277\n",
      "Iteration 326, loss = 0.46790334\n",
      "Iteration 327, loss = 0.46729614\n",
      "Iteration 328, loss = 0.46669143\n",
      "Iteration 329, loss = 0.46608897\n",
      "Iteration 330, loss = 0.46548864\n",
      "Iteration 331, loss = 0.46489043\n",
      "Iteration 332, loss = 0.46429432\n",
      "Iteration 333, loss = 0.46370029\n",
      "Iteration 334, loss = 0.46310832\n",
      "Iteration 335, loss = 0.46251839\n",
      "Iteration 336, loss = 0.46193048\n",
      "Iteration 337, loss = 0.46134457\n",
      "Iteration 338, loss = 0.46076064\n",
      "Iteration 339, loss = 0.46017866\n",
      "Iteration 340, loss = 0.45959862\n",
      "Iteration 341, loss = 0.45902050\n",
      "Iteration 342, loss = 0.45844427\n",
      "Iteration 343, loss = 0.45786992\n",
      "Iteration 344, loss = 0.45729742\n",
      "Iteration 345, loss = 0.45672398\n",
      "Iteration 346, loss = 0.45615104\n",
      "Iteration 347, loss = 0.45557958\n",
      "Iteration 348, loss = 0.45500963\n",
      "Iteration 349, loss = 0.45444121\n",
      "Iteration 350, loss = 0.45387440\n",
      "Iteration 351, loss = 0.45330921\n",
      "Iteration 352, loss = 0.45274542\n",
      "Iteration 353, loss = 0.45218213\n",
      "Iteration 354, loss = 0.45162032\n",
      "Iteration 355, loss = 0.45105999\n",
      "Iteration 356, loss = 0.45050116\n",
      "Iteration 357, loss = 0.44994382\n",
      "Iteration 358, loss = 0.44938798\n",
      "Iteration 359, loss = 0.44883363\n",
      "Iteration 360, loss = 0.44828078\n",
      "Iteration 361, loss = 0.44772941\n",
      "Iteration 362, loss = 0.44717954\n",
      "Iteration 363, loss = 0.44663114\n",
      "Iteration 364, loss = 0.44608423\n",
      "Iteration 365, loss = 0.44553879\n",
      "Iteration 366, loss = 0.44499481\n",
      "Iteration 367, loss = 0.44445230\n",
      "Iteration 368, loss = 0.44391027\n",
      "Iteration 369, loss = 0.44336909\n",
      "Iteration 370, loss = 0.44282800\n",
      "Iteration 371, loss = 0.44228814\n",
      "Iteration 372, loss = 0.44174955\n",
      "Iteration 373, loss = 0.44121222\n",
      "Iteration 374, loss = 0.44067617\n",
      "Iteration 375, loss = 0.44014140\n",
      "Iteration 376, loss = 0.43960793\n",
      "Iteration 377, loss = 0.43907574\n",
      "Iteration 378, loss = 0.43854486\n",
      "Iteration 379, loss = 0.43801488\n",
      "Iteration 380, loss = 0.43748578\n",
      "Iteration 381, loss = 0.43695792\n",
      "Iteration 382, loss = 0.43643130\n",
      "Iteration 383, loss = 0.43590594\n",
      "Iteration 384, loss = 0.43538181\n",
      "Iteration 385, loss = 0.43485894\n",
      "Iteration 386, loss = 0.43433731\n",
      "Iteration 387, loss = 0.43381696\n",
      "Iteration 388, loss = 0.43329788\n",
      "Iteration 389, loss = 0.43278005\n",
      "Iteration 390, loss = 0.43226346\n",
      "Iteration 391, loss = 0.43174809\n",
      "Iteration 392, loss = 0.43123400\n",
      "Iteration 393, loss = 0.43072118\n",
      "Iteration 394, loss = 0.43020958\n",
      "Iteration 395, loss = 0.42969919\n",
      "Iteration 396, loss = 0.42919001\n",
      "Iteration 397, loss = 0.42868203\n",
      "Iteration 398, loss = 0.42817524\n",
      "Iteration 399, loss = 0.42766896\n",
      "Iteration 400, loss = 0.42716380\n",
      "Iteration 401, loss = 0.42665974\n",
      "Iteration 402, loss = 0.42615623\n",
      "Iteration 403, loss = 0.42565278\n",
      "Iteration 404, loss = 0.42514948\n",
      "Iteration 405, loss = 0.42464710\n",
      "Iteration 406, loss = 0.42414567\n",
      "Iteration 407, loss = 0.42364520\n",
      "Iteration 408, loss = 0.42314571\n",
      "Iteration 409, loss = 0.42264668\n",
      "Iteration 410, loss = 0.42214819\n",
      "Iteration 411, loss = 0.42165064\n",
      "Iteration 412, loss = 0.42115407\n",
      "Iteration 413, loss = 0.42065849\n",
      "Iteration 414, loss = 0.42016324\n",
      "Iteration 415, loss = 0.41966733\n",
      "Iteration 416, loss = 0.41917228\n",
      "Iteration 417, loss = 0.41867757\n",
      "Iteration 418, loss = 0.41818295\n",
      "Iteration 419, loss = 0.41768822\n",
      "Iteration 420, loss = 0.41719428\n",
      "Iteration 421, loss = 0.41670116\n",
      "Iteration 422, loss = 0.41620888\n",
      "Iteration 423, loss = 0.41571708\n",
      "Iteration 424, loss = 0.41522108\n",
      "Iteration 425, loss = 0.41472182\n",
      "Iteration 426, loss = 0.41422074\n",
      "Iteration 427, loss = 0.41371853\n",
      "Iteration 428, loss = 0.41321462\n",
      "Iteration 429, loss = 0.41271082\n",
      "Iteration 430, loss = 0.41220717\n",
      "Iteration 431, loss = 0.41170289\n",
      "Iteration 432, loss = 0.41119831\n",
      "Iteration 433, loss = 0.41069340\n",
      "Iteration 434, loss = 0.41018874\n",
      "Iteration 435, loss = 0.40968122\n",
      "Iteration 436, loss = 0.40917142\n",
      "Iteration 437, loss = 0.40866178\n",
      "Iteration 438, loss = 0.40815242\n",
      "Iteration 439, loss = 0.40764343\n",
      "Iteration 440, loss = 0.40713490\n",
      "Iteration 441, loss = 0.40662690\n",
      "Iteration 442, loss = 0.40611890\n",
      "Iteration 443, loss = 0.40561035\n",
      "Iteration 444, loss = 0.40510157\n",
      "Iteration 445, loss = 0.40459340\n",
      "Iteration 446, loss = 0.40408594\n",
      "Iteration 447, loss = 0.40357919\n",
      "Iteration 448, loss = 0.40307302\n",
      "Iteration 449, loss = 0.40256639\n",
      "Iteration 450, loss = 0.40205839\n",
      "Iteration 451, loss = 0.40155074\n",
      "Iteration 452, loss = 0.40104375\n",
      "Iteration 453, loss = 0.40053749\n",
      "Iteration 454, loss = 0.40003150\n",
      "Iteration 455, loss = 0.39952548\n",
      "Iteration 456, loss = 0.39902022\n",
      "Iteration 457, loss = 0.39851577\n",
      "Iteration 458, loss = 0.39801216\n",
      "Iteration 459, loss = 0.39750943\n",
      "Iteration 460, loss = 0.39700762\n",
      "Iteration 461, loss = 0.39650594\n",
      "Iteration 462, loss = 0.39600485\n",
      "Iteration 463, loss = 0.39550403\n",
      "Iteration 464, loss = 0.39500279\n",
      "Iteration 465, loss = 0.39450166\n",
      "Iteration 466, loss = 0.39399997\n",
      "Iteration 467, loss = 0.39349902\n",
      "Iteration 468, loss = 0.39299886\n",
      "Iteration 469, loss = 0.39249953\n",
      "Iteration 470, loss = 0.39200105\n",
      "Iteration 471, loss = 0.39150347\n",
      "Iteration 472, loss = 0.39100992\n",
      "Iteration 473, loss = 0.39051839\n",
      "Iteration 474, loss = 0.39002803\n",
      "Iteration 475, loss = 0.38953885\n",
      "Iteration 476, loss = 0.38905082\n",
      "Iteration 477, loss = 0.38856394\n",
      "Iteration 478, loss = 0.38807820\n",
      "Iteration 479, loss = 0.38759359\n",
      "Iteration 480, loss = 0.38711009\n",
      "Iteration 481, loss = 0.38662672\n",
      "Iteration 482, loss = 0.38614417\n",
      "Iteration 483, loss = 0.38566264\n",
      "Iteration 484, loss = 0.38518212\n",
      "Iteration 485, loss = 0.38470261\n",
      "Iteration 486, loss = 0.38422410\n",
      "Iteration 487, loss = 0.38374661\n",
      "Iteration 488, loss = 0.38327012\n",
      "Iteration 489, loss = 0.38279463\n",
      "Iteration 490, loss = 0.38232014\n",
      "Iteration 491, loss = 0.38184665\n",
      "Iteration 492, loss = 0.38137415\n",
      "Iteration 493, loss = 0.38090264\n",
      "Iteration 494, loss = 0.38043211\n",
      "Iteration 495, loss = 0.37996257\n",
      "Iteration 496, loss = 0.37949400\n",
      "Iteration 497, loss = 0.37902640\n",
      "Iteration 498, loss = 0.37855977\n",
      "Iteration 499, loss = 0.37809391\n",
      "Iteration 500, loss = 0.37762726\n",
      "Iteration 501, loss = 0.37716423\n",
      "Iteration 502, loss = 0.37671217\n",
      "Iteration 503, loss = 0.37626165\n",
      "Iteration 504, loss = 0.37581263\n",
      "Iteration 505, loss = 0.37536510\n",
      "Iteration 506, loss = 0.37491895\n",
      "Iteration 507, loss = 0.37447414\n",
      "Iteration 508, loss = 0.37403061\n",
      "Iteration 509, loss = 0.37358831\n",
      "Iteration 510, loss = 0.37314720\n",
      "Iteration 511, loss = 0.37270722\n",
      "Iteration 512, loss = 0.37226834\n",
      "Iteration 513, loss = 0.37183053\n",
      "Iteration 514, loss = 0.37139374\n",
      "Iteration 515, loss = 0.37095932\n",
      "Iteration 516, loss = 0.37052737\n",
      "Iteration 517, loss = 0.37009652\n",
      "Iteration 518, loss = 0.36966836\n",
      "Iteration 519, loss = 0.36924994\n",
      "Iteration 520, loss = 0.36883298\n",
      "Iteration 521, loss = 0.36841751\n",
      "Iteration 522, loss = 0.36800360\n",
      "Iteration 523, loss = 0.36759096\n",
      "Iteration 524, loss = 0.36717954\n",
      "Iteration 525, loss = 0.36677131\n",
      "Iteration 526, loss = 0.36636510\n",
      "Iteration 527, loss = 0.36596005\n",
      "Iteration 528, loss = 0.36555612\n",
      "Iteration 529, loss = 0.36515325\n",
      "Iteration 530, loss = 0.36475137\n",
      "Iteration 531, loss = 0.36435063\n",
      "Iteration 532, loss = 0.36395087\n",
      "Iteration 533, loss = 0.36355707\n",
      "Iteration 534, loss = 0.36316770\n",
      "Iteration 535, loss = 0.36277939\n",
      "Iteration 536, loss = 0.36239212\n",
      "Iteration 537, loss = 0.36200666\n",
      "Iteration 538, loss = 0.36163148\n",
      "Iteration 539, loss = 0.36125649\n",
      "Iteration 540, loss = 0.36088159\n",
      "Iteration 541, loss = 0.36050696\n",
      "Iteration 542, loss = 0.36013272\n",
      "Iteration 543, loss = 0.35975907\n",
      "Iteration 544, loss = 0.35938586\n",
      "Iteration 545, loss = 0.35901308\n",
      "Iteration 546, loss = 0.35864067\n",
      "Iteration 547, loss = 0.35826859\n",
      "Iteration 548, loss = 0.35789679\n",
      "Iteration 549, loss = 0.35752625\n",
      "Iteration 550, loss = 0.35715677\n",
      "Iteration 551, loss = 0.35678763\n",
      "Iteration 552, loss = 0.35641881\n",
      "Iteration 553, loss = 0.35605035\n",
      "Iteration 554, loss = 0.35568227\n",
      "Iteration 555, loss = 0.35531459\n",
      "Iteration 556, loss = 0.35494733\n",
      "Iteration 557, loss = 0.35458052\n",
      "Iteration 558, loss = 0.35421415\n",
      "Iteration 559, loss = 0.35384825\n",
      "Iteration 560, loss = 0.35348281\n",
      "Iteration 561, loss = 0.35311783\n",
      "Iteration 562, loss = 0.35275331\n",
      "Iteration 563, loss = 0.35238923\n",
      "Iteration 564, loss = 0.35202582\n",
      "Iteration 565, loss = 0.35166424\n",
      "Iteration 566, loss = 0.35130306\n",
      "Iteration 567, loss = 0.35094232\n",
      "Iteration 568, loss = 0.35058207\n",
      "Iteration 569, loss = 0.35022194\n",
      "Iteration 570, loss = 0.34986244\n",
      "Iteration 571, loss = 0.34950340\n",
      "Iteration 572, loss = 0.34914478\n",
      "Iteration 573, loss = 0.34878658\n",
      "Iteration 574, loss = 0.34842878\n",
      "Iteration 575, loss = 0.34807137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 576, loss = 0.34771435\n",
      "Iteration 577, loss = 0.34735773\n",
      "Iteration 578, loss = 0.34700150\n",
      "Iteration 579, loss = 0.34664610\n",
      "Iteration 580, loss = 0.34629065\n",
      "Iteration 581, loss = 0.34593598\n",
      "Iteration 582, loss = 0.34558178\n",
      "Iteration 583, loss = 0.34522801\n",
      "Iteration 584, loss = 0.34487466\n",
      "Iteration 585, loss = 0.34452172\n",
      "Iteration 586, loss = 0.34416948\n",
      "Iteration 587, loss = 0.34381730\n",
      "Iteration 588, loss = 0.34346588\n",
      "Iteration 589, loss = 0.34311470\n",
      "Iteration 590, loss = 0.34276395\n",
      "Iteration 591, loss = 0.34241374\n",
      "Iteration 592, loss = 0.34206383\n",
      "Iteration 593, loss = 0.34171430\n",
      "Iteration 594, loss = 0.34136544\n",
      "Iteration 595, loss = 0.34101661\n",
      "Iteration 596, loss = 0.34066851\n",
      "Iteration 597, loss = 0.34032063\n",
      "Iteration 598, loss = 0.33997316\n",
      "Iteration 599, loss = 0.33962629\n",
      "Iteration 600, loss = 0.33927960\n",
      "Iteration 601, loss = 0.33893339\n",
      "Iteration 602, loss = 0.33858775\n",
      "Iteration 603, loss = 0.33824235\n",
      "Iteration 604, loss = 0.33789732\n",
      "Iteration 605, loss = 0.33755288\n",
      "Iteration 606, loss = 0.33720863\n",
      "Iteration 607, loss = 0.33686492\n",
      "Iteration 608, loss = 0.33652163\n",
      "Iteration 609, loss = 0.33617867\n",
      "Iteration 610, loss = 0.33583607\n",
      "Iteration 611, loss = 0.33549412\n",
      "Iteration 612, loss = 0.33515221\n",
      "Iteration 613, loss = 0.33481096\n",
      "Iteration 614, loss = 0.33447003\n",
      "Iteration 615, loss = 0.33412947\n",
      "Iteration 616, loss = 0.33378927\n",
      "Iteration 617, loss = 0.33344967\n",
      "Iteration 618, loss = 0.33311030\n",
      "Iteration 619, loss = 0.33277127\n",
      "Iteration 620, loss = 0.33243291\n",
      "Iteration 621, loss = 0.33209458\n",
      "Iteration 622, loss = 0.33175716\n",
      "Iteration 623, loss = 0.33142001\n",
      "Iteration 624, loss = 0.33108322\n",
      "Iteration 625, loss = 0.33074683\n",
      "Iteration 626, loss = 0.33041098\n",
      "Iteration 627, loss = 0.33007539\n",
      "Iteration 628, loss = 0.32974042\n",
      "Iteration 629, loss = 0.32940555\n",
      "Iteration 630, loss = 0.32907137\n",
      "Iteration 631, loss = 0.32873738\n",
      "Iteration 632, loss = 0.32840383\n",
      "Iteration 633, loss = 0.32807086\n",
      "Iteration 634, loss = 0.32773827\n",
      "Iteration 635, loss = 0.32740700\n",
      "Iteration 636, loss = 0.32707649\n",
      "Iteration 637, loss = 0.32674640\n",
      "Iteration 638, loss = 0.32641753\n",
      "Iteration 639, loss = 0.32608903\n",
      "Iteration 640, loss = 0.32576100\n",
      "Iteration 641, loss = 0.32543395\n",
      "Iteration 642, loss = 0.32510727\n",
      "Iteration 643, loss = 0.32478120\n",
      "Iteration 644, loss = 0.32445519\n",
      "Iteration 645, loss = 0.32412986\n",
      "Iteration 646, loss = 0.32380472\n",
      "Iteration 647, loss = 0.32347990\n",
      "Iteration 648, loss = 0.32315565\n",
      "Iteration 649, loss = 0.32283154\n",
      "Iteration 650, loss = 0.32250812\n",
      "Iteration 651, loss = 0.32218477\n",
      "Iteration 652, loss = 0.32186203\n",
      "Iteration 653, loss = 0.32153959\n",
      "Iteration 654, loss = 0.32121747\n",
      "Iteration 655, loss = 0.32089593\n",
      "Iteration 656, loss = 0.32057452\n",
      "Iteration 657, loss = 0.32025371\n",
      "Iteration 658, loss = 0.31993315\n",
      "Iteration 659, loss = 0.31961298\n",
      "Iteration 660, loss = 0.31929336\n",
      "Iteration 661, loss = 0.31897394\n",
      "Iteration 662, loss = 0.31865494\n",
      "Iteration 663, loss = 0.31833638\n",
      "Iteration 664, loss = 0.31801812\n",
      "Iteration 665, loss = 0.31770039\n",
      "Iteration 666, loss = 0.31738287\n",
      "Iteration 667, loss = 0.31706594\n",
      "Iteration 668, loss = 0.31674914\n",
      "Iteration 669, loss = 0.31643300\n",
      "Iteration 670, loss = 0.31611705\n",
      "Iteration 671, loss = 0.31580143\n",
      "Iteration 672, loss = 0.31548641\n",
      "Iteration 673, loss = 0.31517155\n",
      "Iteration 674, loss = 0.31485736\n",
      "Iteration 675, loss = 0.31454327\n",
      "Iteration 676, loss = 0.31422969\n",
      "Iteration 677, loss = 0.31391639\n",
      "Iteration 678, loss = 0.31360365\n",
      "Iteration 679, loss = 0.31329107\n",
      "Iteration 680, loss = 0.31297913\n",
      "Iteration 681, loss = 0.31266731\n",
      "Iteration 682, loss = 0.31235610\n",
      "Iteration 683, loss = 0.31204509\n",
      "Iteration 684, loss = 0.31173458\n",
      "Iteration 685, loss = 0.31142441\n",
      "Iteration 686, loss = 0.31111457\n",
      "Iteration 687, loss = 0.31080526\n",
      "Iteration 688, loss = 0.31049613\n",
      "Iteration 689, loss = 0.31018761\n",
      "Iteration 690, loss = 0.30987924\n",
      "Iteration 691, loss = 0.30957140\n",
      "Iteration 692, loss = 0.30926381\n",
      "Iteration 693, loss = 0.30895679\n",
      "Iteration 694, loss = 0.30864994\n",
      "Iteration 695, loss = 0.30834366\n",
      "Iteration 696, loss = 0.30803758\n",
      "Iteration 697, loss = 0.30773202\n",
      "Iteration 698, loss = 0.30742669\n",
      "Iteration 699, loss = 0.30712197\n",
      "Iteration 700, loss = 0.30681740\n",
      "Iteration 701, loss = 0.30651332\n",
      "Iteration 702, loss = 0.30620950\n",
      "Iteration 703, loss = 0.30590631\n",
      "Iteration 704, loss = 0.30560324\n",
      "Iteration 705, loss = 0.30530071\n",
      "Iteration 706, loss = 0.30499840\n",
      "Iteration 707, loss = 0.30469668\n",
      "Iteration 708, loss = 0.30439513\n",
      "Iteration 709, loss = 0.30409410\n",
      "Iteration 710, loss = 0.30379329\n",
      "Iteration 711, loss = 0.30349313\n",
      "Iteration 712, loss = 0.30319310\n",
      "Iteration 713, loss = 0.30289347\n",
      "Iteration 714, loss = 0.30259430\n",
      "Iteration 715, loss = 0.30229547\n",
      "Iteration 716, loss = 0.30199702\n",
      "Iteration 717, loss = 0.30169894\n",
      "Iteration 718, loss = 0.30140127\n",
      "Iteration 719, loss = 0.30110389\n",
      "Iteration 720, loss = 0.30080705\n",
      "Iteration 721, loss = 0.30051040\n",
      "Iteration 722, loss = 0.30021430\n",
      "Iteration 723, loss = 0.29991837\n",
      "Iteration 724, loss = 0.29962306\n",
      "Iteration 725, loss = 0.29932794\n",
      "Iteration 726, loss = 0.29903318\n",
      "Iteration 727, loss = 0.29873893\n",
      "Iteration 728, loss = 0.29844490\n",
      "Iteration 729, loss = 0.29815146\n",
      "Iteration 730, loss = 0.29785819\n",
      "Iteration 731, loss = 0.29756544\n",
      "Iteration 732, loss = 0.29727289\n",
      "Iteration 733, loss = 0.29698095\n",
      "Iteration 734, loss = 0.29668914\n",
      "Iteration 735, loss = 0.29639789\n",
      "Iteration 736, loss = 0.29610692\n",
      "Iteration 737, loss = 0.29581629\n",
      "Iteration 738, loss = 0.29552625\n",
      "Iteration 739, loss = 0.29523628\n",
      "Iteration 740, loss = 0.29494698\n",
      "Iteration 741, loss = 0.29465787\n",
      "Iteration 742, loss = 0.29436909\n",
      "Iteration 743, loss = 0.29408082\n",
      "Iteration 744, loss = 0.29379277\n",
      "Iteration 745, loss = 0.29350529\n",
      "Iteration 746, loss = 0.29321799\n",
      "Iteration 747, loss = 0.29293118\n",
      "Iteration 748, loss = 0.29264476\n",
      "Iteration 749, loss = 0.29235862\n",
      "Iteration 750, loss = 0.29207292\n",
      "Iteration 751, loss = 0.29178757\n",
      "Iteration 752, loss = 0.29150254\n",
      "Iteration 753, loss = 0.29121809\n",
      "Iteration 754, loss = 0.29093371\n",
      "Iteration 755, loss = 0.29065000\n",
      "Iteration 756, loss = 0.29036650\n",
      "Iteration 757, loss = 0.29008332\n",
      "Iteration 758, loss = 0.28980054\n",
      "Iteration 759, loss = 0.28951821\n",
      "Iteration 760, loss = 0.28923615\n",
      "Iteration 761, loss = 0.28895470\n",
      "Iteration 762, loss = 0.28867364\n",
      "Iteration 763, loss = 0.28839322\n",
      "Iteration 764, loss = 0.28811303\n",
      "Iteration 765, loss = 0.28783324\n",
      "Iteration 766, loss = 0.28755393\n",
      "Iteration 767, loss = 0.28727491\n",
      "Iteration 768, loss = 0.28699627\n",
      "Iteration 769, loss = 0.28671808\n",
      "Iteration 770, loss = 0.28644022\n",
      "Iteration 771, loss = 0.28616272\n",
      "Iteration 772, loss = 0.28588562\n",
      "Iteration 773, loss = 0.28560894\n",
      "Iteration 774, loss = 0.28533257\n",
      "Iteration 775, loss = 0.28505655\n",
      "Iteration 776, loss = 0.28478107\n",
      "Iteration 777, loss = 0.28450573\n",
      "Iteration 778, loss = 0.28423086\n",
      "Iteration 779, loss = 0.28395647\n",
      "Iteration 780, loss = 0.28368235\n",
      "Iteration 781, loss = 0.28340858\n",
      "Iteration 782, loss = 0.28313516\n",
      "Iteration 783, loss = 0.28286226\n",
      "Iteration 784, loss = 0.28258953\n",
      "Iteration 785, loss = 0.28231727\n",
      "Iteration 786, loss = 0.28204544\n",
      "Iteration 787, loss = 0.28177391\n",
      "Iteration 788, loss = 0.28150272\n",
      "Iteration 789, loss = 0.28123188\n",
      "Iteration 790, loss = 0.28096161\n",
      "Iteration 791, loss = 0.28069141\n",
      "Iteration 792, loss = 0.28042177\n",
      "Iteration 793, loss = 0.28015248\n",
      "Iteration 794, loss = 0.27988352\n",
      "Iteration 795, loss = 0.27961490\n",
      "Iteration 796, loss = 0.27934664\n",
      "Iteration 797, loss = 0.27907892\n",
      "Iteration 798, loss = 0.27881130\n",
      "Iteration 799, loss = 0.27854418\n",
      "Iteration 800, loss = 0.27827750\n",
      "Iteration 801, loss = 0.27801110\n",
      "Iteration 802, loss = 0.27774505\n",
      "Iteration 803, loss = 0.27747935\n",
      "Iteration 804, loss = 0.27721405\n",
      "Iteration 805, loss = 0.27694914\n",
      "Iteration 806, loss = 0.27668454\n",
      "Iteration 807, loss = 0.27642030\n",
      "Iteration 808, loss = 0.27615657\n",
      "Iteration 809, loss = 0.27589300\n",
      "Iteration 810, loss = 0.27562987\n",
      "Iteration 811, loss = 0.27536712\n",
      "Iteration 812, loss = 0.27510478\n",
      "Iteration 813, loss = 0.27484275\n",
      "Iteration 814, loss = 0.27458106\n",
      "Iteration 815, loss = 0.27431972\n",
      "Iteration 816, loss = 0.27405879\n",
      "Iteration 817, loss = 0.27379822\n",
      "Iteration 818, loss = 0.27353799\n",
      "Iteration 819, loss = 0.27327811\n",
      "Iteration 820, loss = 0.27301864\n",
      "Iteration 821, loss = 0.27275964\n",
      "Iteration 822, loss = 0.27250102\n",
      "Iteration 823, loss = 0.27224276\n",
      "Iteration 824, loss = 0.27198486\n",
      "Iteration 825, loss = 0.27172746\n",
      "Iteration 826, loss = 0.27147023\n",
      "Iteration 827, loss = 0.27121345\n",
      "Iteration 828, loss = 0.27095702\n",
      "Iteration 829, loss = 0.27070101\n",
      "Iteration 830, loss = 0.27044534\n",
      "Iteration 831, loss = 0.27019002\n",
      "Iteration 832, loss = 0.26993505\n",
      "Iteration 833, loss = 0.26968044\n",
      "Iteration 834, loss = 0.26942619\n",
      "Iteration 835, loss = 0.26917231\n",
      "Iteration 836, loss = 0.26891883\n",
      "Iteration 837, loss = 0.26866567\n",
      "Iteration 838, loss = 0.26841287\n",
      "Iteration 839, loss = 0.26816042\n",
      "Iteration 840, loss = 0.26790833\n",
      "Iteration 841, loss = 0.26765661\n",
      "Iteration 842, loss = 0.26740528\n",
      "Iteration 843, loss = 0.26715428\n",
      "Iteration 844, loss = 0.26690363\n",
      "Iteration 845, loss = 0.26665333\n",
      "Iteration 846, loss = 0.26640338\n",
      "Iteration 847, loss = 0.26615379\n",
      "Iteration 848, loss = 0.26590469\n",
      "Iteration 849, loss = 0.26565573\n",
      "Iteration 850, loss = 0.26540723\n",
      "Iteration 851, loss = 0.26515907\n",
      "Iteration 852, loss = 0.26491127\n",
      "Iteration 853, loss = 0.26466382\n",
      "Iteration 854, loss = 0.26441678\n",
      "Iteration 855, loss = 0.26417006\n",
      "Iteration 856, loss = 0.26392368\n",
      "Iteration 857, loss = 0.26367765\n",
      "Iteration 858, loss = 0.26343197\n",
      "Iteration 859, loss = 0.26318663\n",
      "Iteration 860, loss = 0.26294165\n",
      "Iteration 861, loss = 0.26269707\n",
      "Iteration 862, loss = 0.26245281\n",
      "Iteration 863, loss = 0.26220890\n",
      "Iteration 864, loss = 0.26196535\n",
      "Iteration 865, loss = 0.26172214\n",
      "Iteration 866, loss = 0.26147928\n",
      "Iteration 867, loss = 0.26123677\n",
      "Iteration 868, loss = 0.26099462\n",
      "Iteration 869, loss = 0.26075285\n",
      "Iteration 870, loss = 0.26051141\n",
      "Iteration 871, loss = 0.26027031\n",
      "Iteration 872, loss = 0.26002956\n",
      "Iteration 873, loss = 0.25978937\n",
      "Iteration 874, loss = 0.25954969\n",
      "Iteration 875, loss = 0.25931048\n",
      "Iteration 876, loss = 0.25907162\n",
      "Iteration 877, loss = 0.25883309\n",
      "Iteration 878, loss = 0.25859494\n",
      "Iteration 879, loss = 0.25835711\n",
      "Iteration 880, loss = 0.25811962\n",
      "Iteration 881, loss = 0.25788248\n",
      "Iteration 882, loss = 0.25764569\n",
      "Iteration 883, loss = 0.25740924\n",
      "Iteration 884, loss = 0.25717314\n",
      "Iteration 885, loss = 0.25693740\n",
      "Iteration 886, loss = 0.25670200\n",
      "Iteration 887, loss = 0.25646696\n",
      "Iteration 888, loss = 0.25623226\n",
      "Iteration 889, loss = 0.25599792\n",
      "Iteration 890, loss = 0.25576402\n",
      "Iteration 891, loss = 0.25553044\n",
      "Iteration 892, loss = 0.25529719\n",
      "Iteration 893, loss = 0.25506428\n",
      "Iteration 894, loss = 0.25483169\n",
      "Iteration 895, loss = 0.25459950\n",
      "Iteration 896, loss = 0.25436766\n",
      "Iteration 897, loss = 0.25413616\n",
      "Iteration 898, loss = 0.25390501\n",
      "Iteration 899, loss = 0.25367421\n",
      "Iteration 900, loss = 0.25344375\n",
      "Iteration 901, loss = 0.25321366\n",
      "Iteration 902, loss = 0.25298389\n",
      "Iteration 903, loss = 0.25275450\n",
      "Iteration 904, loss = 0.25252543\n",
      "Iteration 905, loss = 0.25229671\n",
      "Iteration 906, loss = 0.25206836\n",
      "Iteration 907, loss = 0.25184032\n",
      "Iteration 908, loss = 0.25161264\n",
      "Iteration 909, loss = 0.25138531\n",
      "Iteration 910, loss = 0.25115832\n",
      "Iteration 911, loss = 0.25093167\n",
      "Iteration 912, loss = 0.25070536\n",
      "Iteration 913, loss = 0.25047939\n",
      "Iteration 914, loss = 0.25025378\n",
      "Iteration 915, loss = 0.25002851\n",
      "Iteration 916, loss = 0.24980356\n",
      "Iteration 917, loss = 0.24957896\n",
      "Iteration 918, loss = 0.24935471\n",
      "Iteration 919, loss = 0.24913079\n",
      "Iteration 920, loss = 0.24890720\n",
      "Iteration 921, loss = 0.24868395\n",
      "Iteration 922, loss = 0.24846107\n",
      "Iteration 923, loss = 0.24823848\n",
      "Iteration 924, loss = 0.24801625\n",
      "Iteration 925, loss = 0.24779437\n",
      "Iteration 926, loss = 0.24757281\n",
      "Iteration 927, loss = 0.24735160\n",
      "Iteration 928, loss = 0.24713071\n",
      "Iteration 929, loss = 0.24691017\n",
      "Iteration 930, loss = 0.24668995\n",
      "Iteration 931, loss = 0.24647009\n",
      "Iteration 932, loss = 0.24625055\n",
      "Iteration 933, loss = 0.24603134\n",
      "Iteration 934, loss = 0.24581247\n",
      "Iteration 935, loss = 0.24559393\n",
      "Iteration 936, loss = 0.24537574\n",
      "Iteration 937, loss = 0.24515786\n",
      "Iteration 938, loss = 0.24494033\n",
      "Iteration 939, loss = 0.24472312\n",
      "Iteration 940, loss = 0.24450625\n",
      "Iteration 941, loss = 0.24428971\n",
      "Iteration 942, loss = 0.24407351\n",
      "Iteration 943, loss = 0.24385762\n",
      "Iteration 944, loss = 0.24364208\n",
      "Iteration 945, loss = 0.24342686\n",
      "Iteration 946, loss = 0.24321197\n",
      "Iteration 947, loss = 0.24299742\n",
      "Iteration 948, loss = 0.24278320\n",
      "Iteration 949, loss = 0.24256930\n",
      "Iteration 950, loss = 0.24235573\n",
      "Iteration 951, loss = 0.24214249\n",
      "Iteration 952, loss = 0.24193010\n",
      "Iteration 953, loss = 0.24171847\n",
      "Iteration 954, loss = 0.24150721\n",
      "Iteration 955, loss = 0.24129633\n",
      "Iteration 956, loss = 0.24108582\n",
      "Iteration 957, loss = 0.24087567\n",
      "Iteration 958, loss = 0.24066589\n",
      "Iteration 959, loss = 0.24045647\n",
      "Iteration 960, loss = 0.24024742\n",
      "Iteration 961, loss = 0.24003872\n",
      "Iteration 962, loss = 0.23983035\n",
      "Iteration 963, loss = 0.23962233\n",
      "Iteration 964, loss = 0.23941467\n",
      "Iteration 965, loss = 0.23920732\n",
      "Iteration 966, loss = 0.23900031\n",
      "Iteration 967, loss = 0.23879365\n",
      "Iteration 968, loss = 0.23858732\n",
      "Iteration 969, loss = 0.23838130\n",
      "Iteration 970, loss = 0.23817637\n",
      "Iteration 971, loss = 0.23797233\n",
      "Iteration 972, loss = 0.23776859\n",
      "Iteration 973, loss = 0.23756522\n",
      "Iteration 974, loss = 0.23736226\n",
      "Iteration 975, loss = 0.23715961\n",
      "Iteration 976, loss = 0.23695729\n",
      "Iteration 977, loss = 0.23675534\n",
      "Iteration 978, loss = 0.23655371\n",
      "Iteration 979, loss = 0.23635245\n",
      "Iteration 980, loss = 0.23615150\n",
      "Iteration 981, loss = 0.23595088\n",
      "Iteration 982, loss = 0.23575063\n",
      "Iteration 983, loss = 0.23555066\n",
      "Iteration 984, loss = 0.23535104\n",
      "Iteration 985, loss = 0.23515171\n",
      "Iteration 986, loss = 0.23495276\n",
      "Iteration 987, loss = 0.23475407\n",
      "Iteration 988, loss = 0.23455578\n",
      "Iteration 989, loss = 0.23435774\n",
      "Iteration 990, loss = 0.23416000\n",
      "Iteration 991, loss = 0.23396267\n",
      "Iteration 992, loss = 0.23376557\n",
      "Iteration 993, loss = 0.23356878\n",
      "Iteration 994, loss = 0.23337235\n",
      "Iteration 995, loss = 0.23317621\n",
      "Iteration 996, loss = 0.23298038\n",
      "Iteration 997, loss = 0.23278490\n",
      "Iteration 998, loss = 0.23258968\n",
      "Iteration 999, loss = 0.23239481\n",
      "Iteration 1000, loss = 0.23220021\n",
      "Iteration 1, loss = 2.12500615\n",
      "Iteration 2, loss = 2.06110204\n",
      "Iteration 3, loss = 1.97295593\n",
      "Iteration 4, loss = 1.86627277\n",
      "Iteration 5, loss = 1.74715888\n",
      "Iteration 6, loss = 1.62201683\n",
      "Iteration 7, loss = 1.49782438\n",
      "Iteration 8, loss = 1.38162390\n",
      "Iteration 9, loss = 1.27922789\n",
      "Iteration 10, loss = 1.19537615\n",
      "Iteration 11, loss = 1.13254127\n",
      "Iteration 12, loss = 1.09232197\n",
      "Iteration 13, loss = 1.07253913\n",
      "Iteration 14, loss = 1.06830787\n",
      "Iteration 15, loss = 1.07369284\n",
      "Iteration 16, loss = 1.08348195\n",
      "Iteration 17, loss = 1.09355113\n",
      "Iteration 18, loss = 1.10115978\n",
      "Iteration 19, loss = 1.10487388\n",
      "Iteration 20, loss = 1.10417797\n",
      "Iteration 21, loss = 1.09926348\n",
      "Iteration 22, loss = 1.09078638\n",
      "Iteration 23, loss = 1.07966110\n",
      "Iteration 24, loss = 1.06689233\n",
      "Iteration 25, loss = 1.05344401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 26, loss = 1.04014517\n",
      "Iteration 27, loss = 1.02762713\n",
      "Iteration 28, loss = 1.01628841\n",
      "Iteration 29, loss = 1.00636480\n",
      "Iteration 30, loss = 0.99788288\n",
      "Iteration 31, loss = 0.99072619\n",
      "Iteration 32, loss = 0.98468278\n",
      "Iteration 33, loss = 0.97949142\n",
      "Iteration 34, loss = 0.97488135\n",
      "Iteration 35, loss = 0.97060279\n",
      "Iteration 36, loss = 0.96645499\n",
      "Iteration 37, loss = 0.96227747\n",
      "Iteration 38, loss = 0.95796568\n",
      "Iteration 39, loss = 0.95347064\n",
      "Iteration 40, loss = 0.94878744\n",
      "Iteration 41, loss = 0.94394480\n",
      "Iteration 42, loss = 0.93899374\n",
      "Iteration 43, loss = 0.93399641\n",
      "Iteration 44, loss = 0.92901626\n",
      "Iteration 45, loss = 0.92411016\n",
      "Iteration 46, loss = 0.91932298\n",
      "Iteration 47, loss = 0.91468459\n",
      "Iteration 48, loss = 0.91020923\n",
      "Iteration 49, loss = 0.90589676\n",
      "Iteration 50, loss = 0.90172322\n",
      "Iteration 51, loss = 0.89767986\n",
      "Iteration 52, loss = 0.89374386\n",
      "Iteration 53, loss = 0.88988960\n",
      "Iteration 54, loss = 0.88609302\n",
      "Iteration 55, loss = 0.88233368\n",
      "Iteration 56, loss = 0.87859607\n",
      "Iteration 57, loss = 0.87487001\n",
      "Iteration 58, loss = 0.87115042\n",
      "Iteration 59, loss = 0.86743661\n",
      "Iteration 60, loss = 0.86373123\n",
      "Iteration 61, loss = 0.86003911\n",
      "Iteration 62, loss = 0.85636618\n",
      "Iteration 63, loss = 0.85271845\n",
      "Iteration 64, loss = 0.84910133\n",
      "Iteration 65, loss = 0.84551912\n",
      "Iteration 66, loss = 0.84197473\n",
      "Iteration 67, loss = 0.83846971\n",
      "Iteration 68, loss = 0.83500430\n",
      "Iteration 69, loss = 0.83157771\n",
      "Iteration 70, loss = 0.82818839\n",
      "Iteration 71, loss = 0.82483432\n",
      "Iteration 72, loss = 0.82151332\n",
      "Iteration 73, loss = 0.81822323\n",
      "Iteration 74, loss = 0.81496211\n",
      "Iteration 75, loss = 0.81172834\n",
      "Iteration 76, loss = 0.80852064\n",
      "Iteration 77, loss = 0.80533812\n",
      "Iteration 78, loss = 0.80218019\n",
      "Iteration 79, loss = 0.79906953\n",
      "Iteration 80, loss = 0.79599513\n",
      "Iteration 81, loss = 0.79294601\n",
      "Iteration 82, loss = 0.78992201\n",
      "Iteration 83, loss = 0.78692302\n",
      "Iteration 84, loss = 0.78394894\n",
      "Iteration 85, loss = 0.78100006\n",
      "Iteration 86, loss = 0.77811022\n",
      "Iteration 87, loss = 0.77526926\n",
      "Iteration 88, loss = 0.77245392\n",
      "Iteration 89, loss = 0.76966378\n",
      "Iteration 90, loss = 0.76689836\n",
      "Iteration 91, loss = 0.76415720\n",
      "Iteration 92, loss = 0.76143984\n",
      "Iteration 93, loss = 0.75874581\n",
      "Iteration 94, loss = 0.75607467\n",
      "Iteration 95, loss = 0.75342601\n",
      "Iteration 96, loss = 0.75079943\n",
      "Iteration 97, loss = 0.74819455\n",
      "Iteration 98, loss = 0.74561103\n",
      "Iteration 99, loss = 0.74304852\n",
      "Iteration 100, loss = 0.74050672\n",
      "Iteration 101, loss = 0.73798533\n",
      "Iteration 102, loss = 0.73548405\n",
      "Iteration 103, loss = 0.73300261\n",
      "Iteration 104, loss = 0.73054075\n",
      "Iteration 105, loss = 0.72809820\n",
      "Iteration 106, loss = 0.72567473\n",
      "Iteration 107, loss = 0.72327009\n",
      "Iteration 108, loss = 0.72088407\n",
      "Iteration 109, loss = 0.71851646\n",
      "Iteration 110, loss = 0.71616706\n",
      "Iteration 111, loss = 0.71383896\n",
      "Iteration 112, loss = 0.71155729\n",
      "Iteration 113, loss = 0.70930618\n",
      "Iteration 114, loss = 0.70707435\n",
      "Iteration 115, loss = 0.70486447\n",
      "Iteration 116, loss = 0.70269097\n",
      "Iteration 117, loss = 0.70053633\n",
      "Iteration 118, loss = 0.69840022\n",
      "Iteration 119, loss = 0.69628243\n",
      "Iteration 120, loss = 0.69422061\n",
      "Iteration 121, loss = 0.69219450\n",
      "Iteration 122, loss = 0.69018823\n",
      "Iteration 123, loss = 0.68819934\n",
      "Iteration 124, loss = 0.68622794\n",
      "Iteration 125, loss = 0.68427405\n",
      "Iteration 126, loss = 0.68234701\n",
      "Iteration 127, loss = 0.68043741\n",
      "Iteration 128, loss = 0.67854494\n",
      "Iteration 129, loss = 0.67666951\n",
      "Iteration 130, loss = 0.67481091\n",
      "Iteration 131, loss = 0.67296887\n",
      "Iteration 132, loss = 0.67114303\n",
      "Iteration 133, loss = 0.66933302\n",
      "Iteration 134, loss = 0.66754023\n",
      "Iteration 135, loss = 0.66576597\n",
      "Iteration 136, loss = 0.66401193\n",
      "Iteration 137, loss = 0.66227448\n",
      "Iteration 138, loss = 0.66055149\n",
      "Iteration 139, loss = 0.65884268\n",
      "Iteration 140, loss = 0.65714775\n",
      "Iteration 141, loss = 0.65546636\n",
      "Iteration 142, loss = 0.65379817\n",
      "Iteration 143, loss = 0.65214281\n",
      "Iteration 144, loss = 0.65049994\n",
      "Iteration 145, loss = 0.64886921\n",
      "Iteration 146, loss = 0.64725031\n",
      "Iteration 147, loss = 0.64564294\n",
      "Iteration 148, loss = 0.64404680\n",
      "Iteration 149, loss = 0.64246329\n",
      "Iteration 150, loss = 0.64089401\n",
      "Iteration 151, loss = 0.63933750\n",
      "Iteration 152, loss = 0.63779498\n",
      "Iteration 153, loss = 0.63626316\n",
      "Iteration 154, loss = 0.63474192\n",
      "Iteration 155, loss = 0.63323114\n",
      "Iteration 156, loss = 0.63173068\n",
      "Iteration 157, loss = 0.63024042\n",
      "Iteration 158, loss = 0.62876021\n",
      "Iteration 159, loss = 0.62728992\n",
      "Iteration 160, loss = 0.62582943\n",
      "Iteration 161, loss = 0.62437860\n",
      "Iteration 162, loss = 0.62293732\n",
      "Iteration 163, loss = 0.62150547\n",
      "Iteration 164, loss = 0.62008294\n",
      "Iteration 165, loss = 0.61866962\n",
      "Iteration 166, loss = 0.61726542\n",
      "Iteration 167, loss = 0.61587025\n",
      "Iteration 168, loss = 0.61448400\n",
      "Iteration 169, loss = 0.61310661\n",
      "Iteration 170, loss = 0.61173800\n",
      "Iteration 171, loss = 0.61037808\n",
      "Iteration 172, loss = 0.60902680\n",
      "Iteration 173, loss = 0.60768407\n",
      "Iteration 174, loss = 0.60634985\n",
      "Iteration 175, loss = 0.60502406\n",
      "Iteration 176, loss = 0.60370664\n",
      "Iteration 177, loss = 0.60239753\n",
      "Iteration 178, loss = 0.60109668\n",
      "Iteration 179, loss = 0.59980401\n",
      "Iteration 180, loss = 0.59852064\n",
      "Iteration 181, loss = 0.59724636\n",
      "Iteration 182, loss = 0.59598008\n",
      "Iteration 183, loss = 0.59472274\n",
      "Iteration 184, loss = 0.59347359\n",
      "Iteration 185, loss = 0.59223204\n",
      "Iteration 186, loss = 0.59099809\n",
      "Iteration 187, loss = 0.58977174\n",
      "Iteration 188, loss = 0.58855298\n",
      "Iteration 189, loss = 0.58734179\n",
      "Iteration 190, loss = 0.58613815\n",
      "Iteration 191, loss = 0.58494204\n",
      "Iteration 192, loss = 0.58375355\n",
      "Iteration 193, loss = 0.58257299\n",
      "Iteration 194, loss = 0.58139985\n",
      "Iteration 195, loss = 0.58023408\n",
      "Iteration 196, loss = 0.57907562\n",
      "Iteration 197, loss = 0.57792440\n",
      "Iteration 198, loss = 0.57678036\n",
      "Iteration 199, loss = 0.57564344\n",
      "Iteration 200, loss = 0.57451350\n",
      "Iteration 201, loss = 0.57339054\n",
      "Iteration 202, loss = 0.57227448\n",
      "Iteration 203, loss = 0.57116526\n",
      "Iteration 204, loss = 0.57006280\n",
      "Iteration 205, loss = 0.56896706\n",
      "Iteration 206, loss = 0.56787796\n",
      "Iteration 207, loss = 0.56679543\n",
      "Iteration 208, loss = 0.56571942\n",
      "Iteration 209, loss = 0.56464988\n",
      "Iteration 210, loss = 0.56358673\n",
      "Iteration 211, loss = 0.56252992\n",
      "Iteration 212, loss = 0.56147939\n",
      "Iteration 213, loss = 0.56043509\n",
      "Iteration 214, loss = 0.55939697\n",
      "Iteration 215, loss = 0.55836496\n",
      "Iteration 216, loss = 0.55733936\n",
      "Iteration 217, loss = 0.55631997\n",
      "Iteration 218, loss = 0.55530656\n",
      "Iteration 219, loss = 0.55429909\n",
      "Iteration 220, loss = 0.55329748\n",
      "Iteration 221, loss = 0.55230187\n",
      "Iteration 222, loss = 0.55131239\n",
      "Iteration 223, loss = 0.55032899\n",
      "Iteration 224, loss = 0.54935128\n",
      "Iteration 225, loss = 0.54837920\n",
      "Iteration 226, loss = 0.54741270\n",
      "Iteration 227, loss = 0.54645170\n",
      "Iteration 228, loss = 0.54549617\n",
      "Iteration 229, loss = 0.54454667\n",
      "Iteration 230, loss = 0.54360299\n",
      "Iteration 231, loss = 0.54266466\n",
      "Iteration 232, loss = 0.54173161\n",
      "Iteration 233, loss = 0.54080379\n",
      "Iteration 234, loss = 0.53988115\n",
      "Iteration 235, loss = 0.53896402\n",
      "Iteration 236, loss = 0.53805197\n",
      "Iteration 237, loss = 0.53714493\n",
      "Iteration 238, loss = 0.53624286\n",
      "Iteration 239, loss = 0.53534570\n",
      "Iteration 240, loss = 0.53445339\n",
      "Iteration 241, loss = 0.53356589\n",
      "Iteration 242, loss = 0.53268334\n",
      "Iteration 243, loss = 0.53180571\n",
      "Iteration 244, loss = 0.53093275\n",
      "Iteration 245, loss = 0.53006469\n",
      "Iteration 246, loss = 0.52920170\n",
      "Iteration 247, loss = 0.52834473\n",
      "Iteration 248, loss = 0.52749232\n",
      "Iteration 249, loss = 0.52664466\n",
      "Iteration 250, loss = 0.52580185\n",
      "Iteration 251, loss = 0.52496383\n",
      "Iteration 252, loss = 0.52413074\n",
      "Iteration 253, loss = 0.52330221\n",
      "Iteration 254, loss = 0.52247794\n",
      "Iteration 255, loss = 0.52165787\n",
      "Iteration 256, loss = 0.52084196\n",
      "Iteration 257, loss = 0.52003015\n",
      "Iteration 258, loss = 0.51922248\n",
      "Iteration 259, loss = 0.51841895\n",
      "Iteration 260, loss = 0.51761939\n",
      "Iteration 261, loss = 0.51682385\n",
      "Iteration 262, loss = 0.51603248\n",
      "Iteration 263, loss = 0.51524505\n",
      "Iteration 264, loss = 0.51446142\n",
      "Iteration 265, loss = 0.51368156\n",
      "Iteration 266, loss = 0.51290541\n",
      "Iteration 267, loss = 0.51213294\n",
      "Iteration 268, loss = 0.51136411\n",
      "Iteration 269, loss = 0.51059891\n",
      "Iteration 270, loss = 0.50983742\n",
      "Iteration 271, loss = 0.50907946\n",
      "Iteration 272, loss = 0.50832499\n",
      "Iteration 273, loss = 0.50757397\n",
      "Iteration 274, loss = 0.50682638\n",
      "Iteration 275, loss = 0.50608233\n",
      "Iteration 276, loss = 0.50534168\n",
      "Iteration 277, loss = 0.50460442\n",
      "Iteration 278, loss = 0.50387045\n",
      "Iteration 279, loss = 0.50313987\n",
      "Iteration 280, loss = 0.50241309\n",
      "Iteration 281, loss = 0.50168956\n",
      "Iteration 282, loss = 0.50096921\n",
      "Iteration 283, loss = 0.50025199\n",
      "Iteration 284, loss = 0.49953788\n",
      "Iteration 285, loss = 0.49882686\n",
      "Iteration 286, loss = 0.49811891\n",
      "Iteration 287, loss = 0.49741403\n",
      "Iteration 288, loss = 0.49671221\n",
      "Iteration 289, loss = 0.49601338\n",
      "Iteration 290, loss = 0.49531750\n",
      "Iteration 291, loss = 0.49462455\n",
      "Iteration 292, loss = 0.49393449\n",
      "Iteration 293, loss = 0.49324729\n",
      "Iteration 294, loss = 0.49256293\n",
      "Iteration 295, loss = 0.49188138\n",
      "Iteration 296, loss = 0.49120260\n",
      "Iteration 297, loss = 0.49052658\n",
      "Iteration 298, loss = 0.48985327\n",
      "Iteration 299, loss = 0.48918271\n",
      "Iteration 300, loss = 0.48851489\n",
      "Iteration 301, loss = 0.48784970\n",
      "Iteration 302, loss = 0.48718713\n",
      "Iteration 303, loss = 0.48652742\n",
      "Iteration 304, loss = 0.48587042\n",
      "Iteration 305, loss = 0.48521600\n",
      "Iteration 306, loss = 0.48456419\n",
      "Iteration 307, loss = 0.48391490\n",
      "Iteration 308, loss = 0.48326818\n",
      "Iteration 309, loss = 0.48262396\n",
      "Iteration 310, loss = 0.48198219\n",
      "Iteration 311, loss = 0.48134285\n",
      "Iteration 312, loss = 0.48070593\n",
      "Iteration 313, loss = 0.48007141\n",
      "Iteration 314, loss = 0.47943930\n",
      "Iteration 315, loss = 0.47880952\n",
      "Iteration 316, loss = 0.47818206\n",
      "Iteration 317, loss = 0.47755692\n",
      "Iteration 318, loss = 0.47693408\n",
      "Iteration 319, loss = 0.47631350\n",
      "Iteration 320, loss = 0.47569514\n",
      "Iteration 321, loss = 0.47507897\n",
      "Iteration 322, loss = 0.47446498\n",
      "Iteration 323, loss = 0.47385315\n",
      "Iteration 324, loss = 0.47324344\n",
      "Iteration 325, loss = 0.47263585\n",
      "Iteration 326, loss = 0.47203034\n",
      "Iteration 327, loss = 0.47142689\n",
      "Iteration 328, loss = 0.47082548\n",
      "Iteration 329, loss = 0.47022610\n",
      "Iteration 330, loss = 0.46962872\n",
      "Iteration 331, loss = 0.46903336\n",
      "Iteration 332, loss = 0.46844006\n",
      "Iteration 333, loss = 0.46784870\n",
      "Iteration 334, loss = 0.46725928\n",
      "Iteration 335, loss = 0.46667177\n",
      "Iteration 336, loss = 0.46608616\n",
      "Iteration 337, loss = 0.46550241\n",
      "Iteration 338, loss = 0.46492053\n",
      "Iteration 339, loss = 0.46434048\n",
      "Iteration 340, loss = 0.46376225\n",
      "Iteration 341, loss = 0.46318583\n",
      "Iteration 342, loss = 0.46261119\n",
      "Iteration 343, loss = 0.46203733\n",
      "Iteration 344, loss = 0.46146286\n",
      "Iteration 345, loss = 0.46088987\n",
      "Iteration 346, loss = 0.46031837\n",
      "Iteration 347, loss = 0.45974837\n",
      "Iteration 348, loss = 0.45917999\n",
      "Iteration 349, loss = 0.45861315\n",
      "Iteration 350, loss = 0.45804769\n",
      "Iteration 351, loss = 0.45748274\n",
      "Iteration 352, loss = 0.45691920\n",
      "Iteration 353, loss = 0.45635709\n",
      "Iteration 354, loss = 0.45579642\n",
      "Iteration 355, loss = 0.45523718\n",
      "Iteration 356, loss = 0.45467939\n",
      "Iteration 357, loss = 0.45412305\n",
      "Iteration 358, loss = 0.45356817\n",
      "Iteration 359, loss = 0.45301474\n",
      "Iteration 360, loss = 0.45246276\n",
      "Iteration 361, loss = 0.45191223\n",
      "Iteration 362, loss = 0.45136315\n",
      "Iteration 363, loss = 0.45081552\n",
      "Iteration 364, loss = 0.45026932\n",
      "Iteration 365, loss = 0.44972455\n",
      "Iteration 366, loss = 0.44918114\n",
      "Iteration 367, loss = 0.44863766\n",
      "Iteration 368, loss = 0.44809405\n",
      "Iteration 369, loss = 0.44755138\n",
      "Iteration 370, loss = 0.44700988\n",
      "Iteration 371, loss = 0.44646958\n",
      "Iteration 372, loss = 0.44593048\n",
      "Iteration 373, loss = 0.44539260\n",
      "Iteration 374, loss = 0.44485594\n",
      "Iteration 375, loss = 0.44432053\n",
      "Iteration 376, loss = 0.44378634\n",
      "Iteration 377, loss = 0.44325340\n",
      "Iteration 378, loss = 0.44272170\n",
      "Iteration 379, loss = 0.44219125\n",
      "Iteration 380, loss = 0.44166203\n",
      "Iteration 381, loss = 0.44113404\n",
      "Iteration 382, loss = 0.44060730\n",
      "Iteration 383, loss = 0.44008178\n",
      "Iteration 384, loss = 0.43955749\n",
      "Iteration 385, loss = 0.43903446\n",
      "Iteration 386, loss = 0.43851267\n",
      "Iteration 387, loss = 0.43799210\n",
      "Iteration 388, loss = 0.43747274\n",
      "Iteration 389, loss = 0.43695458\n",
      "Iteration 390, loss = 0.43643762\n",
      "Iteration 391, loss = 0.43592191\n",
      "Iteration 392, loss = 0.43540742\n",
      "Iteration 393, loss = 0.43489410\n",
      "Iteration 394, loss = 0.43438196\n",
      "Iteration 395, loss = 0.43387098\n",
      "Iteration 396, loss = 0.43336115\n",
      "Iteration 397, loss = 0.43285246\n",
      "Iteration 398, loss = 0.43234492\n",
      "Iteration 399, loss = 0.43183795\n",
      "Iteration 400, loss = 0.43133126\n",
      "Iteration 401, loss = 0.43082521\n",
      "Iteration 402, loss = 0.43031808\n",
      "Iteration 403, loss = 0.42980953\n",
      "Iteration 404, loss = 0.42930099\n",
      "Iteration 405, loss = 0.42879311\n",
      "Iteration 406, loss = 0.42828596\n",
      "Iteration 407, loss = 0.42777957\n",
      "Iteration 408, loss = 0.42727397\n",
      "Iteration 409, loss = 0.42676919\n",
      "Iteration 410, loss = 0.42626526\n",
      "Iteration 411, loss = 0.42576218\n",
      "Iteration 412, loss = 0.42525998\n",
      "Iteration 413, loss = 0.42475867\n",
      "Iteration 414, loss = 0.42425825\n",
      "Iteration 415, loss = 0.42375812\n",
      "Iteration 416, loss = 0.42325800\n",
      "Iteration 417, loss = 0.42275870\n",
      "Iteration 418, loss = 0.42225996\n",
      "Iteration 419, loss = 0.42176154\n",
      "Iteration 420, loss = 0.42126278\n",
      "Iteration 421, loss = 0.42076475\n",
      "Iteration 422, loss = 0.42026747\n",
      "Iteration 423, loss = 0.41976973\n",
      "Iteration 424, loss = 0.41926981\n",
      "Iteration 425, loss = 0.41876847\n",
      "Iteration 426, loss = 0.41826334\n",
      "Iteration 427, loss = 0.41775531\n",
      "Iteration 428, loss = 0.41724489\n",
      "Iteration 429, loss = 0.41673333\n",
      "Iteration 430, loss = 0.41622126\n",
      "Iteration 431, loss = 0.41570830\n",
      "Iteration 432, loss = 0.41519542\n",
      "Iteration 433, loss = 0.41468274\n",
      "Iteration 434, loss = 0.41417011\n",
      "Iteration 435, loss = 0.41365601\n",
      "Iteration 436, loss = 0.41313968\n",
      "Iteration 437, loss = 0.41261568\n",
      "Iteration 438, loss = 0.41209018\n",
      "Iteration 439, loss = 0.41156437\n",
      "Iteration 440, loss = 0.41103772\n",
      "Iteration 441, loss = 0.41051033\n",
      "Iteration 442, loss = 0.40998278\n",
      "Iteration 443, loss = 0.40945337\n",
      "Iteration 444, loss = 0.40892407\n",
      "Iteration 445, loss = 0.40839496\n",
      "Iteration 446, loss = 0.40786617\n",
      "Iteration 447, loss = 0.40733783\n",
      "Iteration 448, loss = 0.40680947\n",
      "Iteration 449, loss = 0.40627974\n",
      "Iteration 450, loss = 0.40574531\n",
      "Iteration 451, loss = 0.40521058\n",
      "Iteration 452, loss = 0.40467479\n",
      "Iteration 453, loss = 0.40413854\n",
      "Iteration 454, loss = 0.40360094\n",
      "Iteration 455, loss = 0.40306137\n",
      "Iteration 456, loss = 0.40252163\n",
      "Iteration 457, loss = 0.40198214\n",
      "Iteration 458, loss = 0.40144304\n",
      "Iteration 459, loss = 0.40090381\n",
      "Iteration 460, loss = 0.40036419\n",
      "Iteration 461, loss = 0.39982415\n",
      "Iteration 462, loss = 0.39928348\n",
      "Iteration 463, loss = 0.39874156\n",
      "Iteration 464, loss = 0.39820010\n",
      "Iteration 465, loss = 0.39765922\n",
      "Iteration 466, loss = 0.39711896\n",
      "Iteration 467, loss = 0.39657941\n",
      "Iteration 468, loss = 0.39604064\n",
      "Iteration 469, loss = 0.39550269\n",
      "Iteration 470, loss = 0.39496562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 471, loss = 0.39442947\n",
      "Iteration 472, loss = 0.39389427\n",
      "Iteration 473, loss = 0.39336005\n",
      "Iteration 474, loss = 0.39282685\n",
      "Iteration 475, loss = 0.39229469\n",
      "Iteration 476, loss = 0.39176262\n",
      "Iteration 477, loss = 0.39123081\n",
      "Iteration 478, loss = 0.39069996\n",
      "Iteration 479, loss = 0.39017012\n",
      "Iteration 480, loss = 0.38964131\n",
      "Iteration 481, loss = 0.38911353\n",
      "Iteration 482, loss = 0.38858682\n",
      "Iteration 483, loss = 0.38806118\n",
      "Iteration 484, loss = 0.38753662\n",
      "Iteration 485, loss = 0.38701315\n",
      "Iteration 486, loss = 0.38649078\n",
      "Iteration 487, loss = 0.38596952\n",
      "Iteration 488, loss = 0.38544937\n",
      "Iteration 489, loss = 0.38493033\n",
      "Iteration 490, loss = 0.38441240\n",
      "Iteration 491, loss = 0.38389559\n",
      "Iteration 492, loss = 0.38338810\n",
      "Iteration 493, loss = 0.38288778\n",
      "Iteration 494, loss = 0.38238865\n",
      "Iteration 495, loss = 0.38189127\n",
      "Iteration 496, loss = 0.38139559\n",
      "Iteration 497, loss = 0.38090153\n",
      "Iteration 498, loss = 0.38040904\n",
      "Iteration 499, loss = 0.37991805\n",
      "Iteration 500, loss = 0.37942850\n",
      "Iteration 501, loss = 0.37894035\n",
      "Iteration 502, loss = 0.37845354\n",
      "Iteration 503, loss = 0.37796801\n",
      "Iteration 504, loss = 0.37748374\n",
      "Iteration 505, loss = 0.37700068\n",
      "Iteration 506, loss = 0.37651925\n",
      "Iteration 507, loss = 0.37605229\n",
      "Iteration 508, loss = 0.37558713\n",
      "Iteration 509, loss = 0.37512365\n",
      "Iteration 510, loss = 0.37466179\n",
      "Iteration 511, loss = 0.37420147\n",
      "Iteration 512, loss = 0.37374262\n",
      "Iteration 513, loss = 0.37328518\n",
      "Iteration 514, loss = 0.37282908\n",
      "Iteration 515, loss = 0.37237424\n",
      "Iteration 516, loss = 0.37192060\n",
      "Iteration 517, loss = 0.37146811\n",
      "Iteration 518, loss = 0.37101672\n",
      "Iteration 519, loss = 0.37057527\n",
      "Iteration 520, loss = 0.37013801\n",
      "Iteration 521, loss = 0.36970276\n",
      "Iteration 522, loss = 0.36927850\n",
      "Iteration 523, loss = 0.36884607\n",
      "Iteration 524, loss = 0.36841164\n",
      "Iteration 525, loss = 0.36797807\n",
      "Iteration 526, loss = 0.36754530\n",
      "Iteration 527, loss = 0.36711498\n",
      "Iteration 528, loss = 0.36669509\n",
      "Iteration 529, loss = 0.36627636\n",
      "Iteration 530, loss = 0.36586133\n",
      "Iteration 531, loss = 0.36545209\n",
      "Iteration 532, loss = 0.36504360\n",
      "Iteration 533, loss = 0.36463589\n",
      "Iteration 534, loss = 0.36422894\n",
      "Iteration 535, loss = 0.36382276\n",
      "Iteration 536, loss = 0.36341731\n",
      "Iteration 537, loss = 0.36301256\n",
      "Iteration 538, loss = 0.36260846\n",
      "Iteration 539, loss = 0.36220499\n",
      "Iteration 540, loss = 0.36180210\n",
      "Iteration 541, loss = 0.36140163\n",
      "Iteration 542, loss = 0.36100160\n",
      "Iteration 543, loss = 0.36060172\n",
      "Iteration 544, loss = 0.36020202\n",
      "Iteration 545, loss = 0.35980255\n",
      "Iteration 546, loss = 0.35940335\n",
      "Iteration 547, loss = 0.35900442\n",
      "Iteration 548, loss = 0.35860581\n",
      "Iteration 549, loss = 0.35820756\n",
      "Iteration 550, loss = 0.35780984\n",
      "Iteration 551, loss = 0.35741250\n",
      "Iteration 552, loss = 0.35701575\n",
      "Iteration 553, loss = 0.35662028\n",
      "Iteration 554, loss = 0.35622521\n",
      "Iteration 555, loss = 0.35583060\n",
      "Iteration 556, loss = 0.35543648\n",
      "Iteration 557, loss = 0.35504288\n",
      "Iteration 558, loss = 0.35464984\n",
      "Iteration 559, loss = 0.35425733\n",
      "Iteration 560, loss = 0.35386561\n",
      "Iteration 561, loss = 0.35347413\n",
      "Iteration 562, loss = 0.35308334\n",
      "Iteration 563, loss = 0.35269305\n",
      "Iteration 564, loss = 0.35230321\n",
      "Iteration 565, loss = 0.35191384\n",
      "Iteration 566, loss = 0.35152492\n",
      "Iteration 567, loss = 0.35113646\n",
      "Iteration 568, loss = 0.35074856\n",
      "Iteration 569, loss = 0.35036109\n",
      "Iteration 570, loss = 0.34997409\n",
      "Iteration 571, loss = 0.34958754\n",
      "Iteration 572, loss = 0.34920149\n",
      "Iteration 573, loss = 0.34881604\n",
      "Iteration 574, loss = 0.34843096\n",
      "Iteration 575, loss = 0.34804633\n",
      "Iteration 576, loss = 0.34766224\n",
      "Iteration 577, loss = 0.34727867\n",
      "Iteration 578, loss = 0.34689549\n",
      "Iteration 579, loss = 0.34651285\n",
      "Iteration 580, loss = 0.34613070\n",
      "Iteration 581, loss = 0.34574894\n",
      "Iteration 582, loss = 0.34536776\n",
      "Iteration 583, loss = 0.34498698\n",
      "Iteration 584, loss = 0.34460662\n",
      "Iteration 585, loss = 0.34422698\n",
      "Iteration 586, loss = 0.34384746\n",
      "Iteration 587, loss = 0.34346867\n",
      "Iteration 588, loss = 0.34309025\n",
      "Iteration 589, loss = 0.34271226\n",
      "Iteration 590, loss = 0.34233484\n",
      "Iteration 591, loss = 0.34195778\n",
      "Iteration 592, loss = 0.34158114\n",
      "Iteration 593, loss = 0.34120520\n",
      "Iteration 594, loss = 0.34082951\n",
      "Iteration 595, loss = 0.34045423\n",
      "Iteration 596, loss = 0.34007977\n",
      "Iteration 597, loss = 0.33970537\n",
      "Iteration 598, loss = 0.33933162\n",
      "Iteration 599, loss = 0.33895826\n",
      "Iteration 600, loss = 0.33858536\n",
      "Iteration 601, loss = 0.33821306\n",
      "Iteration 602, loss = 0.33784104\n",
      "Iteration 603, loss = 0.33746866\n",
      "Iteration 604, loss = 0.33709247\n",
      "Iteration 605, loss = 0.33671685\n",
      "Iteration 606, loss = 0.33634115\n",
      "Iteration 607, loss = 0.33596545\n",
      "Iteration 608, loss = 0.33558986\n",
      "Iteration 609, loss = 0.33521443\n",
      "Iteration 610, loss = 0.33483920\n",
      "Iteration 611, loss = 0.33446689\n",
      "Iteration 612, loss = 0.33409680\n",
      "Iteration 613, loss = 0.33372697\n",
      "Iteration 614, loss = 0.33335745\n",
      "Iteration 615, loss = 0.33298829\n",
      "Iteration 616, loss = 0.33261955\n",
      "Iteration 617, loss = 0.33225127\n",
      "Iteration 618, loss = 0.33188348\n",
      "Iteration 619, loss = 0.33151622\n",
      "Iteration 620, loss = 0.33114948\n",
      "Iteration 621, loss = 0.33078327\n",
      "Iteration 622, loss = 0.33041756\n",
      "Iteration 623, loss = 0.33005233\n",
      "Iteration 624, loss = 0.32968758\n",
      "Iteration 625, loss = 0.32932327\n",
      "Iteration 626, loss = 0.32895941\n",
      "Iteration 627, loss = 0.32859599\n",
      "Iteration 628, loss = 0.32823299\n",
      "Iteration 629, loss = 0.32787043\n",
      "Iteration 630, loss = 0.32750830\n",
      "Iteration 631, loss = 0.32714662\n",
      "Iteration 632, loss = 0.32678539\n",
      "Iteration 633, loss = 0.32642460\n",
      "Iteration 634, loss = 0.32606428\n",
      "Iteration 635, loss = 0.32570441\n",
      "Iteration 636, loss = 0.32534500\n",
      "Iteration 637, loss = 0.32498604\n",
      "Iteration 638, loss = 0.32462750\n",
      "Iteration 639, loss = 0.32426940\n",
      "Iteration 640, loss = 0.32391176\n",
      "Iteration 641, loss = 0.32355456\n",
      "Iteration 642, loss = 0.32319780\n",
      "Iteration 643, loss = 0.32284149\n",
      "Iteration 644, loss = 0.32248563\n",
      "Iteration 645, loss = 0.32213021\n",
      "Iteration 646, loss = 0.32177524\n",
      "Iteration 647, loss = 0.32142071\n",
      "Iteration 648, loss = 0.32106663\n",
      "Iteration 649, loss = 0.32071300\n",
      "Iteration 650, loss = 0.32035982\n",
      "Iteration 651, loss = 0.32000708\n",
      "Iteration 652, loss = 0.31965479\n",
      "Iteration 653, loss = 0.31930295\n",
      "Iteration 654, loss = 0.31895155\n",
      "Iteration 655, loss = 0.31860060\n",
      "Iteration 656, loss = 0.31825010\n",
      "Iteration 657, loss = 0.31790004\n",
      "Iteration 658, loss = 0.31755043\n",
      "Iteration 659, loss = 0.31720127\n",
      "Iteration 660, loss = 0.31685255\n",
      "Iteration 661, loss = 0.31650427\n",
      "Iteration 662, loss = 0.31615644\n",
      "Iteration 663, loss = 0.31580906\n",
      "Iteration 664, loss = 0.31546212\n",
      "Iteration 665, loss = 0.31511562\n",
      "Iteration 666, loss = 0.31476957\n",
      "Iteration 667, loss = 0.31442397\n",
      "Iteration 668, loss = 0.31407881\n",
      "Iteration 669, loss = 0.31373410\n",
      "Iteration 670, loss = 0.31338982\n",
      "Iteration 671, loss = 0.31304600\n",
      "Iteration 672, loss = 0.31270261\n",
      "Iteration 673, loss = 0.31235968\n",
      "Iteration 674, loss = 0.31201718\n",
      "Iteration 675, loss = 0.31167513\n",
      "Iteration 676, loss = 0.31133352\n",
      "Iteration 677, loss = 0.31099236\n",
      "Iteration 678, loss = 0.31065162\n",
      "Iteration 679, loss = 0.31031130\n",
      "Iteration 680, loss = 0.30997142\n",
      "Iteration 681, loss = 0.30963197\n",
      "Iteration 682, loss = 0.30929297\n",
      "Iteration 683, loss = 0.30895441\n",
      "Iteration 684, loss = 0.30861628\n",
      "Iteration 685, loss = 0.30827860\n",
      "Iteration 686, loss = 0.30794136\n",
      "Iteration 687, loss = 0.30760456\n",
      "Iteration 688, loss = 0.30726820\n",
      "Iteration 689, loss = 0.30693228\n",
      "Iteration 690, loss = 0.30659680\n",
      "Iteration 691, loss = 0.30626176\n",
      "Iteration 692, loss = 0.30592714\n",
      "Iteration 693, loss = 0.30559293\n",
      "Iteration 694, loss = 0.30525916\n",
      "Iteration 695, loss = 0.30492582\n",
      "Iteration 696, loss = 0.30459292\n",
      "Iteration 697, loss = 0.30426045\n",
      "Iteration 698, loss = 0.30392843\n",
      "Iteration 699, loss = 0.30359684\n",
      "Iteration 700, loss = 0.30326569\n",
      "Iteration 701, loss = 0.30293498\n",
      "Iteration 702, loss = 0.30260471\n",
      "Iteration 703, loss = 0.30227488\n",
      "Iteration 704, loss = 0.30194549\n",
      "Iteration 705, loss = 0.30161653\n",
      "Iteration 706, loss = 0.30128802\n",
      "Iteration 707, loss = 0.30095994\n",
      "Iteration 708, loss = 0.30063231\n",
      "Iteration 709, loss = 0.30030511\n",
      "Iteration 710, loss = 0.29997835\n",
      "Iteration 711, loss = 0.29965204\n",
      "Iteration 712, loss = 0.29932616\n",
      "Iteration 713, loss = 0.29900072\n",
      "Iteration 714, loss = 0.29867572\n",
      "Iteration 715, loss = 0.29835116\n",
      "Iteration 716, loss = 0.29802699\n",
      "Iteration 717, loss = 0.29770321\n",
      "Iteration 718, loss = 0.29737987\n",
      "Iteration 719, loss = 0.29705695\n",
      "Iteration 720, loss = 0.29673448\n",
      "Iteration 721, loss = 0.29641243\n",
      "Iteration 722, loss = 0.29609082\n",
      "Iteration 723, loss = 0.29576965\n",
      "Iteration 724, loss = 0.29544891\n",
      "Iteration 725, loss = 0.29512860\n",
      "Iteration 726, loss = 0.29480873\n",
      "Iteration 727, loss = 0.29448930\n",
      "Iteration 728, loss = 0.29417054\n",
      "Iteration 729, loss = 0.29385208\n",
      "Iteration 730, loss = 0.29353391\n",
      "Iteration 731, loss = 0.29321645\n",
      "Iteration 732, loss = 0.29289932\n",
      "Iteration 733, loss = 0.29258266\n",
      "Iteration 734, loss = 0.29226641\n",
      "Iteration 735, loss = 0.29195066\n",
      "Iteration 736, loss = 0.29163532\n",
      "Iteration 737, loss = 0.29132039\n",
      "Iteration 738, loss = 0.29100594\n",
      "Iteration 739, loss = 0.29069195\n",
      "Iteration 740, loss = 0.29037834\n",
      "Iteration 741, loss = 0.29006516\n",
      "Iteration 742, loss = 0.28975255\n",
      "Iteration 743, loss = 0.28944027\n",
      "Iteration 744, loss = 0.28912841\n",
      "Iteration 745, loss = 0.28881713\n",
      "Iteration 746, loss = 0.28850612\n",
      "Iteration 747, loss = 0.28819562\n",
      "Iteration 748, loss = 0.28788566\n",
      "Iteration 749, loss = 0.28757617\n",
      "Iteration 750, loss = 0.28726729\n",
      "Iteration 751, loss = 0.28695884\n",
      "Iteration 752, loss = 0.28665098\n",
      "Iteration 753, loss = 0.28634346\n",
      "Iteration 754, loss = 0.28603637\n",
      "Iteration 755, loss = 0.28572992\n",
      "Iteration 756, loss = 0.28542364\n",
      "Iteration 757, loss = 0.28511805\n",
      "Iteration 758, loss = 0.28481280\n",
      "Iteration 759, loss = 0.28450797\n",
      "Iteration 760, loss = 0.28420354\n",
      "Iteration 761, loss = 0.28389978\n",
      "Iteration 762, loss = 0.28359622\n",
      "Iteration 763, loss = 0.28329322\n",
      "Iteration 764, loss = 0.28299055\n",
      "Iteration 765, loss = 0.28268832\n",
      "Iteration 766, loss = 0.28238662\n",
      "Iteration 767, loss = 0.28208525\n",
      "Iteration 768, loss = 0.28178441\n",
      "Iteration 769, loss = 0.28148391\n",
      "Iteration 770, loss = 0.28118393\n",
      "Iteration 771, loss = 0.28088436\n",
      "Iteration 772, loss = 0.28058518\n",
      "Iteration 773, loss = 0.28028658\n",
      "Iteration 774, loss = 0.27998818\n",
      "Iteration 775, loss = 0.27969040\n",
      "Iteration 776, loss = 0.27939289\n",
      "Iteration 777, loss = 0.27909589\n",
      "Iteration 778, loss = 0.27879938\n",
      "Iteration 779, loss = 0.27850320\n",
      "Iteration 780, loss = 0.27820750\n",
      "Iteration 781, loss = 0.27791223\n",
      "Iteration 782, loss = 0.27761738\n",
      "Iteration 783, loss = 0.27732302\n",
      "Iteration 784, loss = 0.27702900\n",
      "Iteration 785, loss = 0.27673555\n",
      "Iteration 786, loss = 0.27644235\n",
      "Iteration 787, loss = 0.27614973\n",
      "Iteration 788, loss = 0.27585740\n",
      "Iteration 789, loss = 0.27556545\n",
      "Iteration 790, loss = 0.27527413\n",
      "Iteration 791, loss = 0.27498305\n",
      "Iteration 792, loss = 0.27469246\n",
      "Iteration 793, loss = 0.27440225\n",
      "Iteration 794, loss = 0.27411264\n",
      "Iteration 795, loss = 0.27382322\n",
      "Iteration 796, loss = 0.27353431\n",
      "Iteration 797, loss = 0.27324608\n",
      "Iteration 798, loss = 0.27295864\n",
      "Iteration 799, loss = 0.27267163\n",
      "Iteration 800, loss = 0.27238502\n",
      "Iteration 801, loss = 0.27209881\n",
      "Iteration 802, loss = 0.27181308\n",
      "Iteration 803, loss = 0.27152769\n",
      "Iteration 804, loss = 0.27124291\n",
      "Iteration 805, loss = 0.27095836\n",
      "Iteration 806, loss = 0.27067440\n",
      "Iteration 807, loss = 0.27039079\n",
      "Iteration 808, loss = 0.27010760\n",
      "Iteration 809, loss = 0.26982494\n",
      "Iteration 810, loss = 0.26954261\n",
      "Iteration 811, loss = 0.26926078\n",
      "Iteration 812, loss = 0.26897942\n",
      "Iteration 813, loss = 0.26869844\n",
      "Iteration 814, loss = 0.26841787\n",
      "Iteration 815, loss = 0.26813788\n",
      "Iteration 816, loss = 0.26785814\n",
      "Iteration 817, loss = 0.26757898\n",
      "Iteration 818, loss = 0.26730020\n",
      "Iteration 819, loss = 0.26702185\n",
      "Iteration 820, loss = 0.26674395\n",
      "Iteration 821, loss = 0.26646653\n",
      "Iteration 822, loss = 0.26618948\n",
      "Iteration 823, loss = 0.26591290\n",
      "Iteration 824, loss = 0.26563676\n",
      "Iteration 825, loss = 0.26536101\n",
      "Iteration 826, loss = 0.26508581\n",
      "Iteration 827, loss = 0.26481089\n",
      "Iteration 828, loss = 0.26453651\n",
      "Iteration 829, loss = 0.26426253\n",
      "Iteration 830, loss = 0.26398895\n",
      "Iteration 831, loss = 0.26371584\n",
      "Iteration 832, loss = 0.26344309\n",
      "Iteration 833, loss = 0.26317074\n",
      "Iteration 834, loss = 0.26289890\n",
      "Iteration 835, loss = 0.26262738\n",
      "Iteration 836, loss = 0.26235632\n",
      "Iteration 837, loss = 0.26208573\n",
      "Iteration 838, loss = 0.26181544\n",
      "Iteration 839, loss = 0.26154555\n",
      "Iteration 840, loss = 0.26127619\n",
      "Iteration 841, loss = 0.26100711\n",
      "Iteration 842, loss = 0.26073853\n",
      "Iteration 843, loss = 0.26047038\n",
      "Iteration 844, loss = 0.26020261\n",
      "Iteration 845, loss = 0.25993524\n",
      "Iteration 846, loss = 0.25966835\n",
      "Iteration 847, loss = 0.25940170\n",
      "Iteration 848, loss = 0.25913555\n",
      "Iteration 849, loss = 0.25886984\n",
      "Iteration 850, loss = 0.25860451\n",
      "Iteration 851, loss = 0.25833956\n",
      "Iteration 852, loss = 0.25807508\n",
      "Iteration 853, loss = 0.25781099\n",
      "Iteration 854, loss = 0.25754730\n",
      "Iteration 855, loss = 0.25728407\n",
      "Iteration 856, loss = 0.25702123\n",
      "Iteration 857, loss = 0.25675879\n",
      "Iteration 858, loss = 0.25649676\n",
      "Iteration 859, loss = 0.25623521\n",
      "Iteration 860, loss = 0.25597394\n",
      "Iteration 861, loss = 0.25571307\n",
      "Iteration 862, loss = 0.25545265\n",
      "Iteration 863, loss = 0.25519259\n",
      "Iteration 864, loss = 0.25493285\n",
      "Iteration 865, loss = 0.25467352\n",
      "Iteration 866, loss = 0.25441464\n",
      "Iteration 867, loss = 0.25415612\n",
      "Iteration 868, loss = 0.25389799\n",
      "Iteration 869, loss = 0.25364029\n",
      "Iteration 870, loss = 0.25338301\n",
      "Iteration 871, loss = 0.25312611\n",
      "Iteration 872, loss = 0.25286961\n",
      "Iteration 873, loss = 0.25261360\n",
      "Iteration 874, loss = 0.25235790\n",
      "Iteration 875, loss = 0.25210264\n",
      "Iteration 876, loss = 0.25184778\n",
      "Iteration 877, loss = 0.25159340\n",
      "Iteration 878, loss = 0.25133937\n",
      "Iteration 879, loss = 0.25108574\n",
      "Iteration 880, loss = 0.25083252\n",
      "Iteration 881, loss = 0.25057972\n",
      "Iteration 882, loss = 0.25032735\n",
      "Iteration 883, loss = 0.25007536\n",
      "Iteration 884, loss = 0.24982377\n",
      "Iteration 885, loss = 0.24957262\n",
      "Iteration 886, loss = 0.24932188\n",
      "Iteration 887, loss = 0.24907153\n",
      "Iteration 888, loss = 0.24882158\n",
      "Iteration 889, loss = 0.24857203\n",
      "Iteration 890, loss = 0.24832295\n",
      "Iteration 891, loss = 0.24807420\n",
      "Iteration 892, loss = 0.24782589\n",
      "Iteration 893, loss = 0.24757794\n",
      "Iteration 894, loss = 0.24733032\n",
      "Iteration 895, loss = 0.24708313\n",
      "Iteration 896, loss = 0.24683631\n",
      "Iteration 897, loss = 0.24658988\n",
      "Iteration 898, loss = 0.24634366\n",
      "Iteration 899, loss = 0.24609762\n",
      "Iteration 900, loss = 0.24585193\n",
      "Iteration 901, loss = 0.24560661\n",
      "Iteration 902, loss = 0.24536165\n",
      "Iteration 903, loss = 0.24511706\n",
      "Iteration 904, loss = 0.24487285\n",
      "Iteration 905, loss = 0.24462901\n",
      "Iteration 906, loss = 0.24438563\n",
      "Iteration 907, loss = 0.24414252\n",
      "Iteration 908, loss = 0.24389985\n",
      "Iteration 909, loss = 0.24365756\n",
      "Iteration 910, loss = 0.24341565\n",
      "Iteration 911, loss = 0.24317414\n",
      "Iteration 912, loss = 0.24293308\n",
      "Iteration 913, loss = 0.24269232\n",
      "Iteration 914, loss = 0.24245199\n",
      "Iteration 915, loss = 0.24221205\n",
      "Iteration 916, loss = 0.24197240\n",
      "Iteration 917, loss = 0.24173309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 918, loss = 0.24149417\n",
      "Iteration 919, loss = 0.24125567\n",
      "Iteration 920, loss = 0.24101752\n",
      "Iteration 921, loss = 0.24077977\n",
      "Iteration 922, loss = 0.24054241\n",
      "Iteration 923, loss = 0.24030543\n",
      "Iteration 924, loss = 0.24006884\n",
      "Iteration 925, loss = 0.23983265\n",
      "Iteration 926, loss = 0.23959684\n",
      "Iteration 927, loss = 0.23936142\n",
      "Iteration 928, loss = 0.23912645\n",
      "Iteration 929, loss = 0.23889179\n",
      "Iteration 930, loss = 0.23865756\n",
      "Iteration 931, loss = 0.23842372\n",
      "Iteration 932, loss = 0.23819028\n",
      "Iteration 933, loss = 0.23795722\n",
      "Iteration 934, loss = 0.23772455\n",
      "Iteration 935, loss = 0.23749226\n",
      "Iteration 936, loss = 0.23726031\n",
      "Iteration 937, loss = 0.23702875\n",
      "Iteration 938, loss = 0.23679757\n",
      "Iteration 939, loss = 0.23656681\n",
      "Iteration 940, loss = 0.23633641\n",
      "Iteration 941, loss = 0.23610640\n",
      "Iteration 942, loss = 0.23587676\n",
      "Iteration 943, loss = 0.23564741\n",
      "Iteration 944, loss = 0.23541844\n",
      "Iteration 945, loss = 0.23518985\n",
      "Iteration 946, loss = 0.23496163\n",
      "Iteration 947, loss = 0.23473380\n",
      "Iteration 948, loss = 0.23450636\n",
      "Iteration 949, loss = 0.23427925\n",
      "Iteration 950, loss = 0.23405245\n",
      "Iteration 951, loss = 0.23382603\n",
      "Iteration 952, loss = 0.23359999\n",
      "Iteration 953, loss = 0.23337432\n",
      "Iteration 954, loss = 0.23314903\n",
      "Iteration 955, loss = 0.23292412\n",
      "Iteration 956, loss = 0.23269959\n",
      "Iteration 957, loss = 0.23247544\n",
      "Iteration 958, loss = 0.23225167\n",
      "Iteration 959, loss = 0.23202828\n",
      "Iteration 960, loss = 0.23180526\n",
      "Iteration 961, loss = 0.23158263\n",
      "Iteration 962, loss = 0.23136038\n",
      "Iteration 963, loss = 0.23113850\n",
      "Iteration 964, loss = 0.23091701\n",
      "Iteration 965, loss = 0.23069589\n",
      "Iteration 966, loss = 0.23047516\n",
      "Iteration 967, loss = 0.23025480\n",
      "Iteration 968, loss = 0.23003483\n",
      "Iteration 969, loss = 0.22981523\n",
      "Iteration 970, loss = 0.22959597\n",
      "Iteration 971, loss = 0.22937703\n",
      "Iteration 972, loss = 0.22915845\n",
      "Iteration 973, loss = 0.22894024\n",
      "Iteration 974, loss = 0.22872241\n",
      "Iteration 975, loss = 0.22850495\n",
      "Iteration 976, loss = 0.22828776\n",
      "Iteration 977, loss = 0.22807088\n",
      "Iteration 978, loss = 0.22785436\n",
      "Iteration 979, loss = 0.22763820\n",
      "Iteration 980, loss = 0.22742240\n",
      "Iteration 981, loss = 0.22720697\n",
      "Iteration 982, loss = 0.22699190\n",
      "Iteration 983, loss = 0.22677720\n",
      "Iteration 984, loss = 0.22656286\n",
      "Iteration 985, loss = 0.22634889\n",
      "Iteration 986, loss = 0.22613528\n",
      "Iteration 987, loss = 0.22592204\n",
      "Iteration 988, loss = 0.22570917\n",
      "Iteration 989, loss = 0.22549669\n",
      "Iteration 990, loss = 0.22528459\n",
      "Iteration 991, loss = 0.22507285\n",
      "Iteration 992, loss = 0.22486195\n",
      "Iteration 993, loss = 0.22465162\n",
      "Iteration 994, loss = 0.22444170\n",
      "Iteration 995, loss = 0.22423218\n",
      "Iteration 996, loss = 0.22402305\n",
      "Iteration 997, loss = 0.22381422\n",
      "Iteration 998, loss = 0.22360577\n",
      "Iteration 999, loss = 0.22339757\n",
      "Iteration 1000, loss = 0.22318974\n",
      "Iteration 1, loss = 2.10484323\n",
      "Iteration 2, loss = 2.04274245\n",
      "Iteration 3, loss = 1.95710842\n",
      "Iteration 4, loss = 1.85349864\n",
      "Iteration 5, loss = 1.73784451\n",
      "Iteration 6, loss = 1.61634379\n",
      "Iteration 7, loss = 1.49566567\n",
      "Iteration 8, loss = 1.38276777\n",
      "Iteration 9, loss = 1.28376401\n",
      "Iteration 10, loss = 1.20210433\n",
      "Iteration 11, loss = 1.14073029\n",
      "Iteration 12, loss = 1.10008653\n",
      "Iteration 13, loss = 1.07832340\n",
      "Iteration 14, loss = 1.07135737\n",
      "Iteration 15, loss = 1.07416208\n",
      "Iteration 16, loss = 1.08199872\n",
      "Iteration 17, loss = 1.09087421\n",
      "Iteration 18, loss = 1.09802470\n",
      "Iteration 19, loss = 1.10183963\n",
      "Iteration 20, loss = 1.10164734\n",
      "Iteration 21, loss = 1.09747579\n",
      "Iteration 22, loss = 1.08982860\n",
      "Iteration 23, loss = 1.07949079\n",
      "Iteration 24, loss = 1.06736789\n",
      "Iteration 25, loss = 1.05435882\n",
      "Iteration 26, loss = 1.04126219\n",
      "Iteration 27, loss = 1.02871548\n",
      "Iteration 28, loss = 1.01716508\n",
      "Iteration 29, loss = 1.00686368\n",
      "Iteration 30, loss = 0.99788936\n",
      "Iteration 31, loss = 0.99017915\n",
      "Iteration 32, loss = 0.98357001\n",
      "Iteration 33, loss = 0.97784033\n",
      "Iteration 34, loss = 0.97274732\n",
      "Iteration 35, loss = 0.96805701\n",
      "Iteration 36, loss = 0.96357096\n",
      "Iteration 37, loss = 0.95912882\n",
      "Iteration 38, loss = 0.95461245\n",
      "Iteration 39, loss = 0.94995753\n",
      "Iteration 40, loss = 0.94514213\n",
      "Iteration 41, loss = 0.94017846\n",
      "Iteration 42, loss = 0.93510319\n",
      "Iteration 43, loss = 0.92996745\n",
      "Iteration 44, loss = 0.92482752\n",
      "Iteration 45, loss = 0.91973723\n",
      "Iteration 46, loss = 0.91474228\n",
      "Iteration 47, loss = 0.90987627\n",
      "Iteration 48, loss = 0.90515923\n",
      "Iteration 49, loss = 0.90059786\n",
      "Iteration 50, loss = 0.89618729\n",
      "Iteration 51, loss = 0.89191366\n",
      "Iteration 52, loss = 0.88775728\n",
      "Iteration 53, loss = 0.88369573\n",
      "Iteration 54, loss = 0.87970665\n",
      "Iteration 55, loss = 0.87576991\n",
      "Iteration 56, loss = 0.87186913\n",
      "Iteration 57, loss = 0.86799238\n",
      "Iteration 58, loss = 0.86413237\n",
      "Iteration 59, loss = 0.86028599\n",
      "Iteration 60, loss = 0.85645362\n",
      "Iteration 61, loss = 0.85263815\n",
      "Iteration 62, loss = 0.84884403\n",
      "Iteration 63, loss = 0.84507632\n",
      "Iteration 64, loss = 0.84133996\n",
      "Iteration 65, loss = 0.83763923\n",
      "Iteration 66, loss = 0.83397734\n",
      "Iteration 67, loss = 0.83035634\n",
      "Iteration 68, loss = 0.82677708\n",
      "Iteration 69, loss = 0.82323940\n",
      "Iteration 70, loss = 0.81974227\n",
      "Iteration 71, loss = 0.81628412\n",
      "Iteration 72, loss = 0.81286302\n",
      "Iteration 73, loss = 0.80947694\n",
      "Iteration 74, loss = 0.80612394\n",
      "Iteration 75, loss = 0.80280226\n",
      "Iteration 76, loss = 0.79951044\n",
      "Iteration 77, loss = 0.79624734\n",
      "Iteration 78, loss = 0.79301210\n",
      "Iteration 79, loss = 0.78980416\n",
      "Iteration 80, loss = 0.78662318\n",
      "Iteration 81, loss = 0.78346896\n",
      "Iteration 82, loss = 0.78034142\n",
      "Iteration 83, loss = 0.77724050\n",
      "Iteration 84, loss = 0.77417412\n",
      "Iteration 85, loss = 0.77117112\n",
      "Iteration 86, loss = 0.76823226\n",
      "Iteration 87, loss = 0.76533078\n",
      "Iteration 88, loss = 0.76245734\n",
      "Iteration 89, loss = 0.75961150\n",
      "Iteration 90, loss = 0.75679280\n",
      "Iteration 91, loss = 0.75400079\n",
      "Iteration 92, loss = 0.75123499\n",
      "Iteration 93, loss = 0.74851493\n",
      "Iteration 94, loss = 0.74582901\n",
      "Iteration 95, loss = 0.74316905\n",
      "Iteration 96, loss = 0.74053455\n",
      "Iteration 97, loss = 0.73792505\n",
      "Iteration 98, loss = 0.73534705\n",
      "Iteration 99, loss = 0.73280902\n",
      "Iteration 100, loss = 0.73029555\n",
      "Iteration 101, loss = 0.72780597\n",
      "Iteration 102, loss = 0.72533997\n",
      "Iteration 103, loss = 0.72289702\n",
      "Iteration 104, loss = 0.72047661\n",
      "Iteration 105, loss = 0.71807821\n",
      "Iteration 106, loss = 0.71570129\n",
      "Iteration 107, loss = 0.71334534\n",
      "Iteration 108, loss = 0.71101071\n",
      "Iteration 109, loss = 0.70871331\n",
      "Iteration 110, loss = 0.70643650\n",
      "Iteration 111, loss = 0.70417979\n",
      "Iteration 112, loss = 0.70194269\n",
      "Iteration 113, loss = 0.69973596\n",
      "Iteration 114, loss = 0.69756349\n",
      "Iteration 115, loss = 0.69541575\n",
      "Iteration 116, loss = 0.69328724\n",
      "Iteration 117, loss = 0.69117755\n",
      "Iteration 118, loss = 0.68909123\n",
      "Iteration 119, loss = 0.68704214\n",
      "Iteration 120, loss = 0.68501537\n",
      "Iteration 121, loss = 0.68300658\n",
      "Iteration 122, loss = 0.68101551\n",
      "Iteration 123, loss = 0.67904185\n",
      "Iteration 124, loss = 0.67708525\n",
      "Iteration 125, loss = 0.67514666\n",
      "Iteration 126, loss = 0.67324052\n",
      "Iteration 127, loss = 0.67135347\n",
      "Iteration 128, loss = 0.66948604\n",
      "Iteration 129, loss = 0.66763424\n",
      "Iteration 130, loss = 0.66579802\n",
      "Iteration 131, loss = 0.66397724\n",
      "Iteration 132, loss = 0.66217174\n",
      "Iteration 133, loss = 0.66038125\n",
      "Iteration 134, loss = 0.65860552\n",
      "Iteration 135, loss = 0.65684835\n",
      "Iteration 136, loss = 0.65510553\n",
      "Iteration 137, loss = 0.65337668\n",
      "Iteration 138, loss = 0.65166357\n",
      "Iteration 139, loss = 0.64996502\n",
      "Iteration 140, loss = 0.64827970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 141, loss = 0.64660745\n",
      "Iteration 142, loss = 0.64494806\n",
      "Iteration 143, loss = 0.64330130\n",
      "Iteration 144, loss = 0.64166692\n",
      "Iteration 145, loss = 0.64004467\n",
      "Iteration 146, loss = 0.63843428\n",
      "Iteration 147, loss = 0.63683550\n",
      "Iteration 148, loss = 0.63524809\n",
      "Iteration 149, loss = 0.63367180\n",
      "Iteration 150, loss = 0.63210642\n",
      "Iteration 151, loss = 0.63055174\n",
      "Iteration 152, loss = 0.62900756\n",
      "Iteration 153, loss = 0.62747374\n",
      "Iteration 154, loss = 0.62595010\n",
      "Iteration 155, loss = 0.62443650\n",
      "Iteration 156, loss = 0.62293284\n",
      "Iteration 157, loss = 0.62143898\n",
      "Iteration 158, loss = 0.61995485\n",
      "Iteration 159, loss = 0.61848033\n",
      "Iteration 160, loss = 0.61701537\n",
      "Iteration 161, loss = 0.61555987\n",
      "Iteration 162, loss = 0.61411458\n",
      "Iteration 163, loss = 0.61268208\n",
      "Iteration 164, loss = 0.61125990\n",
      "Iteration 165, loss = 0.60984796\n",
      "Iteration 166, loss = 0.60844504\n",
      "Iteration 167, loss = 0.60705109\n",
      "Iteration 168, loss = 0.60566610\n",
      "Iteration 169, loss = 0.60429002\n",
      "Iteration 170, loss = 0.60292281\n",
      "Iteration 171, loss = 0.60156443\n",
      "Iteration 172, loss = 0.60021481\n",
      "Iteration 173, loss = 0.59887392\n",
      "Iteration 174, loss = 0.59754168\n",
      "Iteration 175, loss = 0.59621804\n",
      "Iteration 176, loss = 0.59490279\n",
      "Iteration 177, loss = 0.59359584\n",
      "Iteration 178, loss = 0.59229718\n",
      "Iteration 179, loss = 0.59100656\n",
      "Iteration 180, loss = 0.58972414\n",
      "Iteration 181, loss = 0.58844985\n",
      "Iteration 182, loss = 0.58718364\n",
      "Iteration 183, loss = 0.58592542\n",
      "Iteration 184, loss = 0.58467514\n",
      "Iteration 185, loss = 0.58343272\n",
      "Iteration 186, loss = 0.58219818\n",
      "Iteration 187, loss = 0.58097193\n",
      "Iteration 188, loss = 0.57975338\n",
      "Iteration 189, loss = 0.57854223\n",
      "Iteration 190, loss = 0.57733835\n",
      "Iteration 191, loss = 0.57614193\n",
      "Iteration 192, loss = 0.57495289\n",
      "Iteration 193, loss = 0.57377118\n",
      "Iteration 194, loss = 0.57259674\n",
      "Iteration 195, loss = 0.57142950\n",
      "Iteration 196, loss = 0.57026942\n",
      "Iteration 197, loss = 0.56911641\n",
      "Iteration 198, loss = 0.56797043\n",
      "Iteration 199, loss = 0.56683142\n",
      "Iteration 200, loss = 0.56569932\n",
      "Iteration 201, loss = 0.56457407\n",
      "Iteration 202, loss = 0.56345560\n",
      "Iteration 203, loss = 0.56234359\n",
      "Iteration 204, loss = 0.56123807\n",
      "Iteration 205, loss = 0.56013903\n",
      "Iteration 206, loss = 0.55904628\n",
      "Iteration 207, loss = 0.55795997\n",
      "Iteration 208, loss = 0.55687971\n",
      "Iteration 209, loss = 0.55580570\n",
      "Iteration 210, loss = 0.55473794\n",
      "Iteration 211, loss = 0.55367640\n",
      "Iteration 212, loss = 0.55262119\n",
      "Iteration 213, loss = 0.55157238\n",
      "Iteration 214, loss = 0.55052929\n",
      "Iteration 215, loss = 0.54949261\n",
      "Iteration 216, loss = 0.54846226\n",
      "Iteration 217, loss = 0.54743791\n",
      "Iteration 218, loss = 0.54641944\n",
      "Iteration 219, loss = 0.54540680\n",
      "Iteration 220, loss = 0.54439992\n",
      "Iteration 221, loss = 0.54339875\n",
      "Iteration 222, loss = 0.54240348\n",
      "Iteration 223, loss = 0.54141353\n",
      "Iteration 224, loss = 0.54042897\n",
      "Iteration 225, loss = 0.53944988\n",
      "Iteration 226, loss = 0.53847589\n",
      "Iteration 227, loss = 0.53750763\n",
      "Iteration 228, loss = 0.53654577\n",
      "Iteration 229, loss = 0.53558921\n",
      "Iteration 230, loss = 0.53463785\n",
      "Iteration 231, loss = 0.53369167\n",
      "Iteration 232, loss = 0.53275007\n",
      "Iteration 233, loss = 0.53181291\n",
      "Iteration 234, loss = 0.53088073\n",
      "Iteration 235, loss = 0.52995376\n",
      "Iteration 236, loss = 0.52903187\n",
      "Iteration 237, loss = 0.52811493\n",
      "Iteration 238, loss = 0.52720312\n",
      "Iteration 239, loss = 0.52629663\n",
      "Iteration 240, loss = 0.52539577\n",
      "Iteration 241, loss = 0.52449976\n",
      "Iteration 242, loss = 0.52360895\n",
      "Iteration 243, loss = 0.52272324\n",
      "Iteration 244, loss = 0.52184309\n",
      "Iteration 245, loss = 0.52096762\n",
      "Iteration 246, loss = 0.52009677\n",
      "Iteration 247, loss = 0.51922943\n",
      "Iteration 248, loss = 0.51836603\n",
      "Iteration 249, loss = 0.51750690\n",
      "Iteration 250, loss = 0.51665203\n",
      "Iteration 251, loss = 0.51580132\n",
      "Iteration 252, loss = 0.51495373\n",
      "Iteration 253, loss = 0.51410935\n",
      "Iteration 254, loss = 0.51326921\n",
      "Iteration 255, loss = 0.51243311\n",
      "Iteration 256, loss = 0.51160091\n",
      "Iteration 257, loss = 0.51077260\n",
      "Iteration 258, loss = 0.50994816\n",
      "Iteration 259, loss = 0.50912756\n",
      "Iteration 260, loss = 0.50831079\n",
      "Iteration 261, loss = 0.50749781\n",
      "Iteration 262, loss = 0.50668878\n",
      "Iteration 263, loss = 0.50588348\n",
      "Iteration 264, loss = 0.50508057\n",
      "Iteration 265, loss = 0.50428096\n",
      "Iteration 266, loss = 0.50348425\n",
      "Iteration 267, loss = 0.50269116\n",
      "Iteration 268, loss = 0.50190162\n",
      "Iteration 269, loss = 0.50111483\n",
      "Iteration 270, loss = 0.50033035\n",
      "Iteration 271, loss = 0.49954881\n",
      "Iteration 272, loss = 0.49877052\n",
      "Iteration 273, loss = 0.49799548\n",
      "Iteration 274, loss = 0.49722369\n",
      "Iteration 275, loss = 0.49645515\n",
      "Iteration 276, loss = 0.49568985\n",
      "Iteration 277, loss = 0.49492777\n",
      "Iteration 278, loss = 0.49416764\n",
      "Iteration 279, loss = 0.49340687\n",
      "Iteration 280, loss = 0.49264619\n",
      "Iteration 281, loss = 0.49188799\n",
      "Iteration 282, loss = 0.49113233\n",
      "Iteration 283, loss = 0.49037927\n",
      "Iteration 284, loss = 0.48962768\n",
      "Iteration 285, loss = 0.48887797\n",
      "Iteration 286, loss = 0.48813084\n",
      "Iteration 287, loss = 0.48738633\n",
      "Iteration 288, loss = 0.48664448\n",
      "Iteration 289, loss = 0.48590505\n",
      "Iteration 290, loss = 0.48516763\n",
      "Iteration 291, loss = 0.48443292\n",
      "Iteration 292, loss = 0.48370099\n",
      "Iteration 293, loss = 0.48297059\n",
      "Iteration 294, loss = 0.48224203\n",
      "Iteration 295, loss = 0.48151476\n",
      "Iteration 296, loss = 0.48079004\n",
      "Iteration 297, loss = 0.48006788\n",
      "Iteration 298, loss = 0.47934830\n",
      "Iteration 299, loss = 0.47863132\n",
      "Iteration 300, loss = 0.47791576\n",
      "Iteration 301, loss = 0.47720203\n",
      "Iteration 302, loss = 0.47649080\n",
      "Iteration 303, loss = 0.47578207\n",
      "Iteration 304, loss = 0.47507587\n",
      "Iteration 305, loss = 0.47437169\n",
      "Iteration 306, loss = 0.47366898\n",
      "Iteration 307, loss = 0.47296871\n",
      "Iteration 308, loss = 0.47227090\n",
      "Iteration 309, loss = 0.47157554\n",
      "Iteration 310, loss = 0.47088266\n",
      "Iteration 311, loss = 0.47019112\n",
      "Iteration 312, loss = 0.46949865\n",
      "Iteration 313, loss = 0.46880830\n",
      "Iteration 314, loss = 0.46811964\n",
      "Iteration 315, loss = 0.46743252\n",
      "Iteration 316, loss = 0.46674761\n",
      "Iteration 317, loss = 0.46606467\n",
      "Iteration 318, loss = 0.46538302\n",
      "Iteration 319, loss = 0.46470351\n",
      "Iteration 320, loss = 0.46402399\n",
      "Iteration 321, loss = 0.46334489\n",
      "Iteration 322, loss = 0.46266770\n",
      "Iteration 323, loss = 0.46199251\n",
      "Iteration 324, loss = 0.46131927\n",
      "Iteration 325, loss = 0.46064800\n",
      "Iteration 326, loss = 0.45997714\n",
      "Iteration 327, loss = 0.45930819\n",
      "Iteration 328, loss = 0.45864118\n",
      "Iteration 329, loss = 0.45797613\n",
      "Iteration 330, loss = 0.45731306\n",
      "Iteration 331, loss = 0.45665198\n",
      "Iteration 332, loss = 0.45599289\n",
      "Iteration 333, loss = 0.45533580\n",
      "Iteration 334, loss = 0.45468072\n",
      "Iteration 335, loss = 0.45402763\n",
      "Iteration 336, loss = 0.45337655\n",
      "Iteration 337, loss = 0.45272745\n",
      "Iteration 338, loss = 0.45208035\n",
      "Iteration 339, loss = 0.45143524\n",
      "Iteration 340, loss = 0.45079126\n",
      "Iteration 341, loss = 0.45014832\n",
      "Iteration 342, loss = 0.44950516\n",
      "Iteration 343, loss = 0.44886370\n",
      "Iteration 344, loss = 0.44822394\n",
      "Iteration 345, loss = 0.44758592\n",
      "Iteration 346, loss = 0.44694963\n",
      "Iteration 347, loss = 0.44631510\n",
      "Iteration 348, loss = 0.44568232\n",
      "Iteration 349, loss = 0.44505103\n",
      "Iteration 350, loss = 0.44442065\n",
      "Iteration 351, loss = 0.44379194\n",
      "Iteration 352, loss = 0.44316493\n",
      "Iteration 353, loss = 0.44253962\n",
      "Iteration 354, loss = 0.44191601\n",
      "Iteration 355, loss = 0.44129410\n",
      "Iteration 356, loss = 0.44067389\n",
      "Iteration 357, loss = 0.44005537\n",
      "Iteration 358, loss = 0.43943854\n",
      "Iteration 359, loss = 0.43882340\n",
      "Iteration 360, loss = 0.43820994\n",
      "Iteration 361, loss = 0.43759815\n",
      "Iteration 362, loss = 0.43698802\n",
      "Iteration 363, loss = 0.43637955\n",
      "Iteration 364, loss = 0.43577273\n",
      "Iteration 365, loss = 0.43516754\n",
      "Iteration 366, loss = 0.43456397\n",
      "Iteration 367, loss = 0.43396202\n",
      "Iteration 368, loss = 0.43336167\n",
      "Iteration 369, loss = 0.43276292\n",
      "Iteration 370, loss = 0.43216574\n",
      "Iteration 371, loss = 0.43156862\n",
      "Iteration 372, loss = 0.43097032\n",
      "Iteration 373, loss = 0.43037136\n",
      "Iteration 374, loss = 0.42977351\n",
      "Iteration 375, loss = 0.42917680\n",
      "Iteration 376, loss = 0.42858129\n",
      "Iteration 377, loss = 0.42798691\n",
      "Iteration 378, loss = 0.42739243\n",
      "Iteration 379, loss = 0.42679912\n",
      "Iteration 380, loss = 0.42620701\n",
      "Iteration 381, loss = 0.42561618\n",
      "Iteration 382, loss = 0.42502624\n",
      "Iteration 383, loss = 0.42443554\n",
      "Iteration 384, loss = 0.42384502\n",
      "Iteration 385, loss = 0.42325434\n",
      "Iteration 386, loss = 0.42266373\n",
      "Iteration 387, loss = 0.42207306\n",
      "Iteration 388, loss = 0.42148295\n",
      "Iteration 389, loss = 0.42089379\n",
      "Iteration 390, loss = 0.42030571\n",
      "Iteration 391, loss = 0.41971763\n",
      "Iteration 392, loss = 0.41912798\n",
      "Iteration 393, loss = 0.41853662\n",
      "Iteration 394, loss = 0.41794447\n",
      "Iteration 395, loss = 0.41735050\n",
      "Iteration 396, loss = 0.41675706\n",
      "Iteration 397, loss = 0.41616427\n",
      "Iteration 398, loss = 0.41557222\n",
      "Iteration 399, loss = 0.41497974\n",
      "Iteration 400, loss = 0.41438760\n",
      "Iteration 401, loss = 0.41379627\n",
      "Iteration 402, loss = 0.41320371\n",
      "Iteration 403, loss = 0.41260327\n",
      "Iteration 404, loss = 0.41200169\n",
      "Iteration 405, loss = 0.41140032\n",
      "Iteration 406, loss = 0.41079886\n",
      "Iteration 407, loss = 0.41019687\n",
      "Iteration 408, loss = 0.40959500\n",
      "Iteration 409, loss = 0.40899113\n",
      "Iteration 410, loss = 0.40838715\n",
      "Iteration 411, loss = 0.40778372\n",
      "Iteration 412, loss = 0.40718094\n",
      "Iteration 413, loss = 0.40657892\n",
      "Iteration 414, loss = 0.40597686\n",
      "Iteration 415, loss = 0.40537369\n",
      "Iteration 416, loss = 0.40476558\n",
      "Iteration 417, loss = 0.40415774\n",
      "Iteration 418, loss = 0.40354891\n",
      "Iteration 419, loss = 0.40293972\n",
      "Iteration 420, loss = 0.40232972\n",
      "Iteration 421, loss = 0.40171794\n",
      "Iteration 422, loss = 0.40110667\n",
      "Iteration 423, loss = 0.40049604\n",
      "Iteration 424, loss = 0.39988613\n",
      "Iteration 425, loss = 0.39927593\n",
      "Iteration 426, loss = 0.39866566\n",
      "Iteration 427, loss = 0.39805565\n",
      "Iteration 428, loss = 0.39744651\n",
      "Iteration 429, loss = 0.39683752\n",
      "Iteration 430, loss = 0.39622815\n",
      "Iteration 431, loss = 0.39561970\n",
      "Iteration 432, loss = 0.39501224\n",
      "Iteration 433, loss = 0.39440582\n",
      "Iteration 434, loss = 0.39380051\n",
      "Iteration 435, loss = 0.39319635\n",
      "Iteration 436, loss = 0.39259338\n",
      "Iteration 437, loss = 0.39199164\n",
      "Iteration 438, loss = 0.39139118\n",
      "Iteration 439, loss = 0.39079201\n",
      "Iteration 440, loss = 0.39019415\n",
      "Iteration 441, loss = 0.38959761\n",
      "Iteration 442, loss = 0.38900115\n",
      "Iteration 443, loss = 0.38840537\n",
      "Iteration 444, loss = 0.38781083\n",
      "Iteration 445, loss = 0.38721754\n",
      "Iteration 446, loss = 0.38662553\n",
      "Iteration 447, loss = 0.38603482\n",
      "Iteration 448, loss = 0.38544540\n",
      "Iteration 449, loss = 0.38485730\n",
      "Iteration 450, loss = 0.38427053\n",
      "Iteration 451, loss = 0.38368508\n",
      "Iteration 452, loss = 0.38310097\n",
      "Iteration 453, loss = 0.38251819\n",
      "Iteration 454, loss = 0.38193675\n",
      "Iteration 455, loss = 0.38135665\n",
      "Iteration 456, loss = 0.38077788\n",
      "Iteration 457, loss = 0.38020045\n",
      "Iteration 458, loss = 0.37962435\n",
      "Iteration 459, loss = 0.37904958\n",
      "Iteration 460, loss = 0.37847813\n",
      "Iteration 461, loss = 0.37792148\n",
      "Iteration 462, loss = 0.37736695\n",
      "Iteration 463, loss = 0.37681447\n",
      "Iteration 464, loss = 0.37626396\n",
      "Iteration 465, loss = 0.37571534\n",
      "Iteration 466, loss = 0.37516855\n",
      "Iteration 467, loss = 0.37462351\n",
      "Iteration 468, loss = 0.37408016\n",
      "Iteration 469, loss = 0.37353843\n",
      "Iteration 470, loss = 0.37299826\n",
      "Iteration 471, loss = 0.37245959\n",
      "Iteration 472, loss = 0.37192237\n",
      "Iteration 473, loss = 0.37138656\n",
      "Iteration 474, loss = 0.37085211\n",
      "Iteration 475, loss = 0.37031900\n",
      "Iteration 476, loss = 0.36978718\n",
      "Iteration 477, loss = 0.36925663\n",
      "Iteration 478, loss = 0.36872733\n",
      "Iteration 479, loss = 0.36819925\n",
      "Iteration 480, loss = 0.36767238\n",
      "Iteration 481, loss = 0.36714697\n",
      "Iteration 482, loss = 0.36662291\n",
      "Iteration 483, loss = 0.36610001\n",
      "Iteration 484, loss = 0.36557826\n",
      "Iteration 485, loss = 0.36506087\n",
      "Iteration 486, loss = 0.36455566\n",
      "Iteration 487, loss = 0.36405216\n",
      "Iteration 488, loss = 0.36355031\n",
      "Iteration 489, loss = 0.36305006\n",
      "Iteration 490, loss = 0.36255392\n",
      "Iteration 491, loss = 0.36206805\n",
      "Iteration 492, loss = 0.36158390\n",
      "Iteration 493, loss = 0.36110141\n",
      "Iteration 494, loss = 0.36062058\n",
      "Iteration 495, loss = 0.36014853\n",
      "Iteration 496, loss = 0.35967723\n",
      "Iteration 497, loss = 0.35919822\n",
      "Iteration 498, loss = 0.35872144\n",
      "Iteration 499, loss = 0.35825390\n",
      "Iteration 500, loss = 0.35779024\n",
      "Iteration 501, loss = 0.35733158\n",
      "Iteration 502, loss = 0.35687379\n",
      "Iteration 503, loss = 0.35641684\n",
      "Iteration 504, loss = 0.35596071\n",
      "Iteration 505, loss = 0.35550537\n",
      "Iteration 506, loss = 0.35505078\n",
      "Iteration 507, loss = 0.35459690\n",
      "Iteration 508, loss = 0.35414369\n",
      "Iteration 509, loss = 0.35369113\n",
      "Iteration 510, loss = 0.35323921\n",
      "Iteration 511, loss = 0.35278790\n",
      "Iteration 512, loss = 0.35233722\n",
      "Iteration 513, loss = 0.35188714\n",
      "Iteration 514, loss = 0.35143769\n",
      "Iteration 515, loss = 0.35098887\n",
      "Iteration 516, loss = 0.35054095\n",
      "Iteration 517, loss = 0.35009385\n",
      "Iteration 518, loss = 0.34964741\n",
      "Iteration 519, loss = 0.34920162\n",
      "Iteration 520, loss = 0.34875648\n",
      "Iteration 521, loss = 0.34831198\n",
      "Iteration 522, loss = 0.34786812\n",
      "Iteration 523, loss = 0.34742496\n",
      "Iteration 524, loss = 0.34698259\n",
      "Iteration 525, loss = 0.34654085\n",
      "Iteration 526, loss = 0.34609974\n",
      "Iteration 527, loss = 0.34565926\n",
      "Iteration 528, loss = 0.34521940\n",
      "Iteration 529, loss = 0.34478017\n",
      "Iteration 530, loss = 0.34434155\n",
      "Iteration 531, loss = 0.34390354\n",
      "Iteration 532, loss = 0.34346615\n",
      "Iteration 533, loss = 0.34302937\n",
      "Iteration 534, loss = 0.34259320\n",
      "Iteration 535, loss = 0.34215763\n",
      "Iteration 536, loss = 0.34172267\n",
      "Iteration 537, loss = 0.34128831\n",
      "Iteration 538, loss = 0.34085455\n",
      "Iteration 539, loss = 0.34042172\n",
      "Iteration 540, loss = 0.33998954\n",
      "Iteration 541, loss = 0.33955798\n",
      "Iteration 542, loss = 0.33912702\n",
      "Iteration 543, loss = 0.33869667\n",
      "Iteration 544, loss = 0.33826692\n",
      "Iteration 545, loss = 0.33783778\n",
      "Iteration 546, loss = 0.33740923\n",
      "Iteration 547, loss = 0.33698128\n",
      "Iteration 548, loss = 0.33655392\n",
      "Iteration 549, loss = 0.33612575\n",
      "Iteration 550, loss = 0.33569472\n",
      "Iteration 551, loss = 0.33526322\n",
      "Iteration 552, loss = 0.33483145\n",
      "Iteration 553, loss = 0.33439959\n",
      "Iteration 554, loss = 0.33396776\n",
      "Iteration 555, loss = 0.33353608\n",
      "Iteration 556, loss = 0.33310460\n",
      "Iteration 557, loss = 0.33267337\n",
      "Iteration 558, loss = 0.33224501\n",
      "Iteration 559, loss = 0.33182037\n",
      "Iteration 560, loss = 0.33139615\n",
      "Iteration 561, loss = 0.33097239\n",
      "Iteration 562, loss = 0.33054913\n",
      "Iteration 563, loss = 0.33012643\n",
      "Iteration 564, loss = 0.32970433\n",
      "Iteration 565, loss = 0.32928289\n",
      "Iteration 566, loss = 0.32886212\n",
      "Iteration 567, loss = 0.32844204\n",
      "Iteration 568, loss = 0.32802264\n",
      "Iteration 569, loss = 0.32760420\n",
      "Iteration 570, loss = 0.32718690\n",
      "Iteration 571, loss = 0.32677007\n",
      "Iteration 572, loss = 0.32635373\n",
      "Iteration 573, loss = 0.32593790\n",
      "Iteration 574, loss = 0.32552256\n",
      "Iteration 575, loss = 0.32510795\n",
      "Iteration 576, loss = 0.32469389\n",
      "Iteration 577, loss = 0.32428029\n",
      "Iteration 578, loss = 0.32386735\n",
      "Iteration 579, loss = 0.32345470\n",
      "Iteration 580, loss = 0.32304282\n",
      "Iteration 581, loss = 0.32263121\n",
      "Iteration 582, loss = 0.32222043\n",
      "Iteration 583, loss = 0.32180995\n",
      "Iteration 584, loss = 0.32140034\n",
      "Iteration 585, loss = 0.32099100\n",
      "Iteration 586, loss = 0.32058245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 587, loss = 0.32017436\n",
      "Iteration 588, loss = 0.31976680\n",
      "Iteration 589, loss = 0.31936004\n",
      "Iteration 590, loss = 0.31895355\n",
      "Iteration 591, loss = 0.31854777\n",
      "Iteration 592, loss = 0.31814259\n",
      "Iteration 593, loss = 0.31773789\n",
      "Iteration 594, loss = 0.31733370\n",
      "Iteration 595, loss = 0.31693003\n",
      "Iteration 596, loss = 0.31652712\n",
      "Iteration 597, loss = 0.31612445\n",
      "Iteration 598, loss = 0.31572244\n",
      "Iteration 599, loss = 0.31532113\n",
      "Iteration 600, loss = 0.31492023\n",
      "Iteration 601, loss = 0.31451985\n",
      "Iteration 602, loss = 0.31411999\n",
      "Iteration 603, loss = 0.31372076\n",
      "Iteration 604, loss = 0.31332203\n",
      "Iteration 605, loss = 0.31292381\n",
      "Iteration 606, loss = 0.31252629\n",
      "Iteration 607, loss = 0.31212914\n",
      "Iteration 608, loss = 0.31173256\n",
      "Iteration 609, loss = 0.31133668\n",
      "Iteration 610, loss = 0.31094115\n",
      "Iteration 611, loss = 0.31054620\n",
      "Iteration 612, loss = 0.31015192\n",
      "Iteration 613, loss = 0.30975803\n",
      "Iteration 614, loss = 0.30936470\n",
      "Iteration 615, loss = 0.30897201\n",
      "Iteration 616, loss = 0.30857977\n",
      "Iteration 617, loss = 0.30818806\n",
      "Iteration 618, loss = 0.30779695\n",
      "Iteration 619, loss = 0.30740636\n",
      "Iteration 620, loss = 0.30701626\n",
      "Iteration 621, loss = 0.30662671\n",
      "Iteration 622, loss = 0.30623779\n",
      "Iteration 623, loss = 0.30584931\n",
      "Iteration 624, loss = 0.30546134\n",
      "Iteration 625, loss = 0.30507409\n",
      "Iteration 626, loss = 0.30468712\n",
      "Iteration 627, loss = 0.30430081\n",
      "Iteration 628, loss = 0.30391513\n",
      "Iteration 629, loss = 0.30352988\n",
      "Iteration 630, loss = 0.30314514\n",
      "Iteration 631, loss = 0.30276091\n",
      "Iteration 632, loss = 0.30237745\n",
      "Iteration 633, loss = 0.30199418\n",
      "Iteration 634, loss = 0.30161167\n",
      "Iteration 635, loss = 0.30122967\n",
      "Iteration 636, loss = 0.30084834\n",
      "Iteration 637, loss = 0.30046752\n",
      "Iteration 638, loss = 0.30008725\n",
      "Iteration 639, loss = 0.29970762\n",
      "Iteration 640, loss = 0.29932902\n",
      "Iteration 641, loss = 0.29895283\n",
      "Iteration 642, loss = 0.29857739\n",
      "Iteration 643, loss = 0.29820256\n",
      "Iteration 644, loss = 0.29782838\n",
      "Iteration 645, loss = 0.29745482\n",
      "Iteration 646, loss = 0.29708200\n",
      "Iteration 647, loss = 0.29670970\n",
      "Iteration 648, loss = 0.29633799\n",
      "Iteration 649, loss = 0.29596685\n",
      "Iteration 650, loss = 0.29559643\n",
      "Iteration 651, loss = 0.29522644\n",
      "Iteration 652, loss = 0.29485707\n",
      "Iteration 653, loss = 0.29448831\n",
      "Iteration 654, loss = 0.29412013\n",
      "Iteration 655, loss = 0.29375246\n",
      "Iteration 656, loss = 0.29338533\n",
      "Iteration 657, loss = 0.29301874\n",
      "Iteration 658, loss = 0.29265285\n",
      "Iteration 659, loss = 0.29228729\n",
      "Iteration 660, loss = 0.29192235\n",
      "Iteration 661, loss = 0.29155801\n",
      "Iteration 662, loss = 0.29119419\n",
      "Iteration 663, loss = 0.29083087\n",
      "Iteration 664, loss = 0.29046807\n",
      "Iteration 665, loss = 0.29010579\n",
      "Iteration 666, loss = 0.28974418\n",
      "Iteration 667, loss = 0.28938291\n",
      "Iteration 668, loss = 0.28902223\n",
      "Iteration 669, loss = 0.28866211\n",
      "Iteration 670, loss = 0.28830254\n",
      "Iteration 671, loss = 0.28794344\n",
      "Iteration 672, loss = 0.28758486\n",
      "Iteration 673, loss = 0.28722678\n",
      "Iteration 674, loss = 0.28686932\n",
      "Iteration 675, loss = 0.28651228\n",
      "Iteration 676, loss = 0.28615578\n",
      "Iteration 677, loss = 0.28579979\n",
      "Iteration 678, loss = 0.28544444\n",
      "Iteration 679, loss = 0.28508945\n",
      "Iteration 680, loss = 0.28473503\n",
      "Iteration 681, loss = 0.28438112\n",
      "Iteration 682, loss = 0.28402780\n",
      "Iteration 683, loss = 0.28367492\n",
      "Iteration 684, loss = 0.28332257\n",
      "Iteration 685, loss = 0.28297073\n",
      "Iteration 686, loss = 0.28261938\n",
      "Iteration 687, loss = 0.28226870\n",
      "Iteration 688, loss = 0.28191833\n",
      "Iteration 689, loss = 0.28156855\n",
      "Iteration 690, loss = 0.28121927\n",
      "Iteration 691, loss = 0.28087064\n",
      "Iteration 692, loss = 0.28052234\n",
      "Iteration 693, loss = 0.28017463\n",
      "Iteration 694, loss = 0.27982741\n",
      "Iteration 695, loss = 0.27948071\n",
      "Iteration 696, loss = 0.27913460\n",
      "Iteration 697, loss = 0.27878894\n",
      "Iteration 698, loss = 0.27844377\n",
      "Iteration 699, loss = 0.27809911\n",
      "Iteration 700, loss = 0.27775496\n",
      "Iteration 701, loss = 0.27741146\n",
      "Iteration 702, loss = 0.27706826\n",
      "Iteration 703, loss = 0.27672567\n",
      "Iteration 704, loss = 0.27638357\n",
      "Iteration 705, loss = 0.27604210\n",
      "Iteration 706, loss = 0.27570099\n",
      "Iteration 707, loss = 0.27536045\n",
      "Iteration 708, loss = 0.27502047\n",
      "Iteration 709, loss = 0.27468108\n",
      "Iteration 710, loss = 0.27434226\n",
      "Iteration 711, loss = 0.27400504\n",
      "Iteration 712, loss = 0.27366906\n",
      "Iteration 713, loss = 0.27333369\n",
      "Iteration 714, loss = 0.27299892\n",
      "Iteration 715, loss = 0.27266474\n",
      "Iteration 716, loss = 0.27233127\n",
      "Iteration 717, loss = 0.27199819\n",
      "Iteration 718, loss = 0.27166576\n",
      "Iteration 719, loss = 0.27133388\n",
      "Iteration 720, loss = 0.27100256\n",
      "Iteration 721, loss = 0.27067306\n",
      "Iteration 722, loss = 0.27034449\n",
      "Iteration 723, loss = 0.27001650\n",
      "Iteration 724, loss = 0.26968910\n",
      "Iteration 725, loss = 0.26936229\n",
      "Iteration 726, loss = 0.26903605\n",
      "Iteration 727, loss = 0.26871038\n",
      "Iteration 728, loss = 0.26838527\n",
      "Iteration 729, loss = 0.26806079\n",
      "Iteration 730, loss = 0.26773677\n",
      "Iteration 731, loss = 0.26741332\n",
      "Iteration 732, loss = 0.26709041\n",
      "Iteration 733, loss = 0.26676803\n",
      "Iteration 734, loss = 0.26644617\n",
      "Iteration 735, loss = 0.26612486\n",
      "Iteration 736, loss = 0.26580407\n",
      "Iteration 737, loss = 0.26548379\n",
      "Iteration 738, loss = 0.26516477\n",
      "Iteration 739, loss = 0.26484664\n",
      "Iteration 740, loss = 0.26452907\n",
      "Iteration 741, loss = 0.26421205\n",
      "Iteration 742, loss = 0.26389557\n",
      "Iteration 743, loss = 0.26357973\n",
      "Iteration 744, loss = 0.26326428\n",
      "Iteration 745, loss = 0.26294942\n",
      "Iteration 746, loss = 0.26263508\n",
      "Iteration 747, loss = 0.26232125\n",
      "Iteration 748, loss = 0.26200794\n",
      "Iteration 749, loss = 0.26169513\n",
      "Iteration 750, loss = 0.26138291\n",
      "Iteration 751, loss = 0.26107109\n",
      "Iteration 752, loss = 0.26075982\n",
      "Iteration 753, loss = 0.26044904\n",
      "Iteration 754, loss = 0.26013876\n",
      "Iteration 755, loss = 0.25982897\n",
      "Iteration 756, loss = 0.25951968\n",
      "Iteration 757, loss = 0.25921087\n",
      "Iteration 758, loss = 0.25890256\n",
      "Iteration 759, loss = 0.25859477\n",
      "Iteration 760, loss = 0.25828745\n",
      "Iteration 761, loss = 0.25798062\n",
      "Iteration 762, loss = 0.25767426\n",
      "Iteration 763, loss = 0.25736839\n",
      "Iteration 764, loss = 0.25706300\n",
      "Iteration 765, loss = 0.25675810\n",
      "Iteration 766, loss = 0.25645367\n",
      "Iteration 767, loss = 0.25614973\n",
      "Iteration 768, loss = 0.25584627\n",
      "Iteration 769, loss = 0.25554337\n",
      "Iteration 770, loss = 0.25524081\n",
      "Iteration 771, loss = 0.25493880\n",
      "Iteration 772, loss = 0.25463727\n",
      "Iteration 773, loss = 0.25433621\n",
      "Iteration 774, loss = 0.25403562\n",
      "Iteration 775, loss = 0.25373552\n",
      "Iteration 776, loss = 0.25343589\n",
      "Iteration 777, loss = 0.25313673\n",
      "Iteration 778, loss = 0.25283805\n",
      "Iteration 779, loss = 0.25253985\n",
      "Iteration 780, loss = 0.25224212\n",
      "Iteration 781, loss = 0.25194492\n",
      "Iteration 782, loss = 0.25164811\n",
      "Iteration 783, loss = 0.25135181\n",
      "Iteration 784, loss = 0.25105598\n",
      "Iteration 785, loss = 0.25076063\n",
      "Iteration 786, loss = 0.25046574\n",
      "Iteration 787, loss = 0.25017133\n",
      "Iteration 788, loss = 0.24987739\n",
      "Iteration 789, loss = 0.24958392\n",
      "Iteration 790, loss = 0.24929092\n",
      "Iteration 791, loss = 0.24899840\n",
      "Iteration 792, loss = 0.24870634\n",
      "Iteration 793, loss = 0.24841476\n",
      "Iteration 794, loss = 0.24812364\n",
      "Iteration 795, loss = 0.24783300\n",
      "Iteration 796, loss = 0.24754282\n",
      "Iteration 797, loss = 0.24725312\n",
      "Iteration 798, loss = 0.24696388\n",
      "Iteration 799, loss = 0.24667511\n",
      "Iteration 800, loss = 0.24638681\n",
      "Iteration 801, loss = 0.24609899\n",
      "Iteration 802, loss = 0.24581162\n",
      "Iteration 803, loss = 0.24552473\n",
      "Iteration 804, loss = 0.24523830\n",
      "Iteration 805, loss = 0.24495234\n",
      "Iteration 806, loss = 0.24466684\n",
      "Iteration 807, loss = 0.24438181\n",
      "Iteration 808, loss = 0.24409724\n",
      "Iteration 809, loss = 0.24381314\n",
      "Iteration 810, loss = 0.24352950\n",
      "Iteration 811, loss = 0.24324633\n",
      "Iteration 812, loss = 0.24296362\n",
      "Iteration 813, loss = 0.24268137\n",
      "Iteration 814, loss = 0.24239959\n",
      "Iteration 815, loss = 0.24211827\n",
      "Iteration 816, loss = 0.24183741\n",
      "Iteration 817, loss = 0.24155702\n",
      "Iteration 818, loss = 0.24127709\n",
      "Iteration 819, loss = 0.24099762\n",
      "Iteration 820, loss = 0.24071861\n",
      "Iteration 821, loss = 0.24044007\n",
      "Iteration 822, loss = 0.24016198\n",
      "Iteration 823, loss = 0.23988435\n",
      "Iteration 824, loss = 0.23960719\n",
      "Iteration 825, loss = 0.23933048\n",
      "Iteration 826, loss = 0.23905423\n",
      "Iteration 827, loss = 0.23877844\n",
      "Iteration 828, loss = 0.23850311\n",
      "Iteration 829, loss = 0.23822824\n",
      "Iteration 830, loss = 0.23795382\n",
      "Iteration 831, loss = 0.23767986\n",
      "Iteration 832, loss = 0.23740636\n",
      "Iteration 833, loss = 0.23713332\n",
      "Iteration 834, loss = 0.23686073\n",
      "Iteration 835, loss = 0.23658886\n",
      "Iteration 836, loss = 0.23631790\n",
      "Iteration 837, loss = 0.23604742\n",
      "Iteration 838, loss = 0.23577745\n",
      "Iteration 839, loss = 0.23550854\n",
      "Iteration 840, loss = 0.23524013\n",
      "Iteration 841, loss = 0.23497219\n",
      "Iteration 842, loss = 0.23470473\n",
      "Iteration 843, loss = 0.23443775\n",
      "Iteration 844, loss = 0.23417124\n",
      "Iteration 845, loss = 0.23390520\n",
      "Iteration 846, loss = 0.23363962\n",
      "Iteration 847, loss = 0.23337451\n",
      "Iteration 848, loss = 0.23310987\n",
      "Iteration 849, loss = 0.23284568\n",
      "Iteration 850, loss = 0.23258195\n",
      "Iteration 851, loss = 0.23231868\n",
      "Iteration 852, loss = 0.23205586\n",
      "Iteration 853, loss = 0.23179350\n",
      "Iteration 854, loss = 0.23153158\n",
      "Iteration 855, loss = 0.23127012\n",
      "Iteration 856, loss = 0.23100911\n",
      "Iteration 857, loss = 0.23074855\n",
      "Iteration 858, loss = 0.23048843\n",
      "Iteration 859, loss = 0.23022876\n",
      "Iteration 860, loss = 0.22996953\n",
      "Iteration 861, loss = 0.22971075\n",
      "Iteration 862, loss = 0.22945241\n",
      "Iteration 863, loss = 0.22919452\n",
      "Iteration 864, loss = 0.22893707\n",
      "Iteration 865, loss = 0.22868005\n",
      "Iteration 866, loss = 0.22842348\n",
      "Iteration 867, loss = 0.22816735\n",
      "Iteration 868, loss = 0.22791166\n",
      "Iteration 869, loss = 0.22765640\n",
      "Iteration 870, loss = 0.22740158\n",
      "Iteration 871, loss = 0.22714720\n",
      "Iteration 872, loss = 0.22689325\n",
      "Iteration 873, loss = 0.22663974\n",
      "Iteration 874, loss = 0.22638667\n",
      "Iteration 875, loss = 0.22613423\n",
      "Iteration 876, loss = 0.22588223\n",
      "Iteration 877, loss = 0.22563068\n",
      "Iteration 878, loss = 0.22537956\n",
      "Iteration 879, loss = 0.22512888\n",
      "Iteration 880, loss = 0.22487864\n",
      "Iteration 881, loss = 0.22462883\n",
      "Iteration 882, loss = 0.22437946\n",
      "Iteration 883, loss = 0.22413053\n",
      "Iteration 884, loss = 0.22388202\n",
      "Iteration 885, loss = 0.22363395\n",
      "Iteration 886, loss = 0.22338631\n",
      "Iteration 887, loss = 0.22313910\n",
      "Iteration 888, loss = 0.22289232\n",
      "Iteration 889, loss = 0.22264597\n",
      "Iteration 890, loss = 0.22240004\n",
      "Iteration 891, loss = 0.22215455\n",
      "Iteration 892, loss = 0.22190947\n",
      "Iteration 893, loss = 0.22166483\n",
      "Iteration 894, loss = 0.22142061\n",
      "Iteration 895, loss = 0.22117681\n",
      "Iteration 896, loss = 0.22093366\n",
      "Iteration 897, loss = 0.22069110\n",
      "Iteration 898, loss = 0.22044897\n",
      "Iteration 899, loss = 0.22020727\n",
      "Iteration 900, loss = 0.21996599\n",
      "Iteration 901, loss = 0.21972514\n",
      "Iteration 902, loss = 0.21948471\n",
      "Iteration 903, loss = 0.21924471\n",
      "Iteration 904, loss = 0.21900513\n",
      "Iteration 905, loss = 0.21876597\n",
      "Iteration 906, loss = 0.21852723\n",
      "Iteration 907, loss = 0.21828891\n",
      "Iteration 908, loss = 0.21805101\n",
      "Iteration 909, loss = 0.21781353\n",
      "Iteration 910, loss = 0.21757646\n",
      "Iteration 911, loss = 0.21733982\n",
      "Iteration 912, loss = 0.21710359\n",
      "Iteration 913, loss = 0.21686777\n",
      "Iteration 914, loss = 0.21663237\n",
      "Iteration 915, loss = 0.21639744\n",
      "Iteration 916, loss = 0.21616293\n",
      "Iteration 917, loss = 0.21592883\n",
      "Iteration 918, loss = 0.21569513\n",
      "Iteration 919, loss = 0.21546185\n",
      "Iteration 920, loss = 0.21522897\n",
      "Iteration 921, loss = 0.21499650\n",
      "Iteration 922, loss = 0.21476444\n",
      "Iteration 923, loss = 0.21453279\n",
      "Iteration 924, loss = 0.21430154\n",
      "Iteration 925, loss = 0.21407070\n",
      "Iteration 926, loss = 0.21384027\n",
      "Iteration 927, loss = 0.21361024\n",
      "Iteration 928, loss = 0.21338062\n",
      "Iteration 929, loss = 0.21315141\n",
      "Iteration 930, loss = 0.21292260\n",
      "Iteration 931, loss = 0.21269420\n",
      "Iteration 932, loss = 0.21246620\n",
      "Iteration 933, loss = 0.21223860\n",
      "Iteration 934, loss = 0.21201141\n",
      "Iteration 935, loss = 0.21178462\n",
      "Iteration 936, loss = 0.21155823\n",
      "Iteration 937, loss = 0.21133224\n",
      "Iteration 938, loss = 0.21110666\n",
      "Iteration 939, loss = 0.21088213\n",
      "Iteration 940, loss = 0.21065873\n",
      "Iteration 941, loss = 0.21043578\n",
      "Iteration 942, loss = 0.21021328\n",
      "Iteration 943, loss = 0.20999122\n",
      "Iteration 944, loss = 0.20976960\n",
      "Iteration 945, loss = 0.20954842\n",
      "Iteration 946, loss = 0.20932767\n",
      "Iteration 947, loss = 0.20910734\n",
      "Iteration 948, loss = 0.20888743\n",
      "Iteration 949, loss = 0.20866794\n",
      "Iteration 950, loss = 0.20844886\n",
      "Iteration 951, loss = 0.20823019\n",
      "Iteration 952, loss = 0.20801193\n",
      "Iteration 953, loss = 0.20779407\n",
      "Iteration 954, loss = 0.20757661\n",
      "Iteration 955, loss = 0.20735955\n",
      "Iteration 956, loss = 0.20714289\n",
      "Iteration 957, loss = 0.20692663\n",
      "Iteration 958, loss = 0.20671076\n",
      "Iteration 959, loss = 0.20649528\n",
      "Iteration 960, loss = 0.20628020\n",
      "Iteration 961, loss = 0.20606551\n",
      "Iteration 962, loss = 0.20585120\n",
      "Iteration 963, loss = 0.20563728\n",
      "Iteration 964, loss = 0.20542375\n",
      "Iteration 965, loss = 0.20521061\n",
      "Iteration 966, loss = 0.20499828\n",
      "Iteration 967, loss = 0.20478656\n",
      "Iteration 968, loss = 0.20457524\n",
      "Iteration 969, loss = 0.20436432\n",
      "Iteration 970, loss = 0.20415380\n",
      "Iteration 971, loss = 0.20394367\n",
      "Iteration 972, loss = 0.20373393\n",
      "Iteration 973, loss = 0.20352459\n",
      "Iteration 974, loss = 0.20331563\n",
      "Iteration 975, loss = 0.20310706\n",
      "Iteration 976, loss = 0.20289892\n",
      "Iteration 977, loss = 0.20269141\n",
      "Iteration 978, loss = 0.20248427\n",
      "Iteration 979, loss = 0.20227752\n",
      "Iteration 980, loss = 0.20207114\n",
      "Iteration 981, loss = 0.20186514\n",
      "Iteration 982, loss = 0.20165952\n",
      "Iteration 983, loss = 0.20145427\n",
      "Iteration 984, loss = 0.20124940\n",
      "Iteration 985, loss = 0.20104490\n",
      "Iteration 986, loss = 0.20084077\n",
      "Iteration 987, loss = 0.20063700\n",
      "Iteration 988, loss = 0.20043361\n",
      "Iteration 989, loss = 0.20023059\n",
      "Iteration 990, loss = 0.20002793\n",
      "Iteration 991, loss = 0.19982564\n",
      "Iteration 992, loss = 0.19962386\n",
      "Iteration 993, loss = 0.19942284\n",
      "Iteration 994, loss = 0.19922216\n",
      "Iteration 995, loss = 0.19902184\n",
      "Iteration 996, loss = 0.19882187\n",
      "Iteration 997, loss = 0.19862226\n",
      "Iteration 998, loss = 0.19842301\n",
      "Iteration 999, loss = 0.19822413\n",
      "Iteration 1000, loss = 0.19802561\n",
      "Iteration 1, loss = 2.11317117\n",
      "Iteration 2, loss = 2.04951034\n",
      "Iteration 3, loss = 1.96173287\n",
      "Iteration 4, loss = 1.85555198\n",
      "Iteration 5, loss = 1.73708129\n",
      "Iteration 6, loss = 1.61272471\n",
      "Iteration 7, loss = 1.48943150\n",
      "Iteration 8, loss = 1.37420671\n",
      "Iteration 9, loss = 1.27307354\n",
      "Iteration 10, loss = 1.19076281\n",
      "Iteration 11, loss = 1.12931608\n",
      "Iteration 12, loss = 1.08960941\n",
      "Iteration 13, loss = 1.06913338\n",
      "Iteration 14, loss = 1.06372569\n",
      "Iteration 15, loss = 1.06810569\n",
      "Iteration 16, loss = 1.07703201\n",
      "Iteration 17, loss = 1.08646632\n",
      "Iteration 18, loss = 1.09368321\n",
      "Iteration 19, loss = 1.09722542\n",
      "Iteration 20, loss = 1.09654783\n",
      "Iteration 21, loss = 1.09178870\n",
      "Iteration 22, loss = 1.08354684\n",
      "Iteration 23, loss = 1.07268216\n",
      "Iteration 24, loss = 1.06015244\n",
      "Iteration 25, loss = 1.04688566\n",
      "Iteration 26, loss = 1.03368757\n",
      "Iteration 27, loss = 1.02117040\n",
      "Iteration 28, loss = 1.00975502\n",
      "Iteration 29, loss = 0.99967824\n",
      "Iteration 30, loss = 0.99098467\n",
      "Iteration 31, loss = 0.98357875\n",
      "Iteration 32, loss = 0.97726925\n",
      "Iteration 33, loss = 0.97181311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.96695361\n",
      "Iteration 35, loss = 0.96245022\n",
      "Iteration 36, loss = 0.95809909\n",
      "Iteration 37, loss = 0.95374450\n",
      "Iteration 38, loss = 0.94928265\n",
      "Iteration 39, loss = 0.94465930\n",
      "Iteration 40, loss = 0.93986304\n",
      "Iteration 41, loss = 0.93491596\n",
      "Iteration 42, loss = 0.92986300\n",
      "Iteration 43, loss = 0.92476141\n",
      "Iteration 44, loss = 0.91967129\n",
      "Iteration 45, loss = 0.91464780\n",
      "Iteration 46, loss = 0.90973565\n",
      "Iteration 47, loss = 0.90496585\n",
      "Iteration 48, loss = 0.90035469\n",
      "Iteration 49, loss = 0.89590459\n",
      "Iteration 50, loss = 0.89159702\n",
      "Iteration 51, loss = 0.88741994\n",
      "Iteration 52, loss = 0.88335540\n",
      "Iteration 53, loss = 0.87937914\n",
      "Iteration 54, loss = 0.87546778\n",
      "Iteration 55, loss = 0.87160099\n",
      "Iteration 56, loss = 0.86776283\n",
      "Iteration 57, loss = 0.86394235\n",
      "Iteration 58, loss = 0.86013351\n",
      "Iteration 59, loss = 0.85633460\n",
      "Iteration 60, loss = 0.85254731\n",
      "Iteration 61, loss = 0.84877571\n",
      "Iteration 62, loss = 0.84502512\n",
      "Iteration 63, loss = 0.84130121\n",
      "Iteration 64, loss = 0.83760926\n",
      "Iteration 65, loss = 0.83395357\n",
      "Iteration 66, loss = 0.83033723\n",
      "Iteration 67, loss = 0.82676200\n",
      "Iteration 68, loss = 0.82322840\n",
      "Iteration 69, loss = 0.81973587\n",
      "Iteration 70, loss = 0.81628307\n",
      "Iteration 71, loss = 0.81286813\n",
      "Iteration 72, loss = 0.80948894\n",
      "Iteration 73, loss = 0.80614337\n",
      "Iteration 74, loss = 0.80282942\n",
      "Iteration 75, loss = 0.79954538\n",
      "Iteration 76, loss = 0.79628987\n",
      "Iteration 77, loss = 0.79306186\n",
      "Iteration 78, loss = 0.78986503\n",
      "Iteration 79, loss = 0.78672539\n",
      "Iteration 80, loss = 0.78361292\n",
      "Iteration 81, loss = 0.78052732\n",
      "Iteration 82, loss = 0.77746841\n",
      "Iteration 83, loss = 0.77443606\n",
      "Iteration 84, loss = 0.77143014\n",
      "Iteration 85, loss = 0.76845051\n",
      "Iteration 86, loss = 0.76549695\n",
      "Iteration 87, loss = 0.76258853\n",
      "Iteration 88, loss = 0.75971475\n",
      "Iteration 89, loss = 0.75686718\n",
      "Iteration 90, loss = 0.75404536\n",
      "Iteration 91, loss = 0.75124881\n",
      "Iteration 92, loss = 0.74847707\n",
      "Iteration 93, loss = 0.74572971\n",
      "Iteration 94, loss = 0.74302387\n",
      "Iteration 95, loss = 0.74035426\n",
      "Iteration 96, loss = 0.73770921\n",
      "Iteration 97, loss = 0.73508827\n",
      "Iteration 98, loss = 0.73249103\n",
      "Iteration 99, loss = 0.72993226\n",
      "Iteration 100, loss = 0.72740573\n",
      "Iteration 101, loss = 0.72490282\n",
      "Iteration 102, loss = 0.72242309\n",
      "Iteration 103, loss = 0.71996608\n",
      "Iteration 104, loss = 0.71753137\n",
      "Iteration 105, loss = 0.71511849\n",
      "Iteration 106, loss = 0.71272700\n",
      "Iteration 107, loss = 0.71035648\n",
      "Iteration 108, loss = 0.70801497\n",
      "Iteration 109, loss = 0.70570431\n",
      "Iteration 110, loss = 0.70341438\n",
      "Iteration 111, loss = 0.70114472\n",
      "Iteration 112, loss = 0.69890109\n",
      "Iteration 113, loss = 0.69669846\n",
      "Iteration 114, loss = 0.69452230\n",
      "Iteration 115, loss = 0.69236604\n",
      "Iteration 116, loss = 0.69022927\n",
      "Iteration 117, loss = 0.68811504\n",
      "Iteration 118, loss = 0.68603851\n",
      "Iteration 119, loss = 0.68398854\n",
      "Iteration 120, loss = 0.68195733\n",
      "Iteration 121, loss = 0.67994456\n",
      "Iteration 122, loss = 0.67794991\n",
      "Iteration 123, loss = 0.67597667\n",
      "Iteration 124, loss = 0.67404227\n",
      "Iteration 125, loss = 0.67212966\n",
      "Iteration 126, loss = 0.67023802\n",
      "Iteration 127, loss = 0.66836273\n",
      "Iteration 128, loss = 0.66650396\n",
      "Iteration 129, loss = 0.66466171\n",
      "Iteration 130, loss = 0.66283588\n",
      "Iteration 131, loss = 0.66102628\n",
      "Iteration 132, loss = 0.65923262\n",
      "Iteration 133, loss = 0.65745455\n",
      "Iteration 134, loss = 0.65569197\n",
      "Iteration 135, loss = 0.65394740\n",
      "Iteration 136, loss = 0.65221713\n",
      "Iteration 137, loss = 0.65050108\n",
      "Iteration 138, loss = 0.64879865\n",
      "Iteration 139, loss = 0.64711087\n",
      "Iteration 140, loss = 0.64543749\n",
      "Iteration 141, loss = 0.64377703\n",
      "Iteration 142, loss = 0.64212924\n",
      "Iteration 143, loss = 0.64049384\n",
      "Iteration 144, loss = 0.63887052\n",
      "Iteration 145, loss = 0.63725901\n",
      "Iteration 146, loss = 0.63565901\n",
      "Iteration 147, loss = 0.63407028\n",
      "Iteration 148, loss = 0.63249254\n",
      "Iteration 149, loss = 0.63092557\n",
      "Iteration 150, loss = 0.62936915\n",
      "Iteration 151, loss = 0.62782309\n",
      "Iteration 152, loss = 0.62628722\n",
      "Iteration 153, loss = 0.62476138\n",
      "Iteration 154, loss = 0.62324544\n",
      "Iteration 155, loss = 0.62173928\n",
      "Iteration 156, loss = 0.62024279\n",
      "Iteration 157, loss = 0.61875588\n",
      "Iteration 158, loss = 0.61727847\n",
      "Iteration 159, loss = 0.61581047\n",
      "Iteration 160, loss = 0.61435184\n",
      "Iteration 161, loss = 0.61290249\n",
      "Iteration 162, loss = 0.61146237\n",
      "Iteration 163, loss = 0.61003143\n",
      "Iteration 164, loss = 0.60860930\n",
      "Iteration 165, loss = 0.60719889\n",
      "Iteration 166, loss = 0.60579769\n",
      "Iteration 167, loss = 0.60440510\n",
      "Iteration 168, loss = 0.60302136\n",
      "Iteration 169, loss = 0.60164623\n",
      "Iteration 170, loss = 0.60027951\n",
      "Iteration 171, loss = 0.59892140\n",
      "Iteration 172, loss = 0.59757185\n",
      "Iteration 173, loss = 0.59623080\n",
      "Iteration 174, loss = 0.59489819\n",
      "Iteration 175, loss = 0.59357375\n",
      "Iteration 176, loss = 0.59225722\n",
      "Iteration 177, loss = 0.59094890\n",
      "Iteration 178, loss = 0.58964875\n",
      "Iteration 179, loss = 0.58835671\n",
      "Iteration 180, loss = 0.58707272\n",
      "Iteration 181, loss = 0.58579673\n",
      "Iteration 182, loss = 0.58452868\n",
      "Iteration 183, loss = 0.58326851\n",
      "Iteration 184, loss = 0.58201617\n",
      "Iteration 185, loss = 0.58077196\n",
      "Iteration 186, loss = 0.57953542\n",
      "Iteration 187, loss = 0.57830648\n",
      "Iteration 188, loss = 0.57708509\n",
      "Iteration 189, loss = 0.57587121\n",
      "Iteration 190, loss = 0.57466480\n",
      "Iteration 191, loss = 0.57346581\n",
      "Iteration 192, loss = 0.57227420\n",
      "Iteration 193, loss = 0.57108992\n",
      "Iteration 194, loss = 0.56991312\n",
      "Iteration 195, loss = 0.56874321\n",
      "Iteration 196, loss = 0.56757937\n",
      "Iteration 197, loss = 0.56642247\n",
      "Iteration 198, loss = 0.56527263\n",
      "Iteration 199, loss = 0.56412969\n",
      "Iteration 200, loss = 0.56299363\n",
      "Iteration 201, loss = 0.56186433\n",
      "Iteration 202, loss = 0.56074180\n",
      "Iteration 203, loss = 0.55962564\n",
      "Iteration 204, loss = 0.55851588\n",
      "Iteration 205, loss = 0.55741267\n",
      "Iteration 206, loss = 0.55631597\n",
      "Iteration 207, loss = 0.55522573\n",
      "Iteration 208, loss = 0.55414190\n",
      "Iteration 209, loss = 0.55306442\n",
      "Iteration 210, loss = 0.55199309\n",
      "Iteration 211, loss = 0.55092802\n",
      "Iteration 212, loss = 0.54986913\n",
      "Iteration 213, loss = 0.54881582\n",
      "Iteration 214, loss = 0.54776833\n",
      "Iteration 215, loss = 0.54672678\n",
      "Iteration 216, loss = 0.54569124\n",
      "Iteration 217, loss = 0.54466103\n",
      "Iteration 218, loss = 0.54363611\n",
      "Iteration 219, loss = 0.54261565\n",
      "Iteration 220, loss = 0.54160010\n",
      "Iteration 221, loss = 0.54059001\n",
      "Iteration 222, loss = 0.53958561\n",
      "Iteration 223, loss = 0.53858819\n",
      "Iteration 224, loss = 0.53759622\n",
      "Iteration 225, loss = 0.53660967\n",
      "Iteration 226, loss = 0.53562853\n",
      "Iteration 227, loss = 0.53465277\n",
      "Iteration 228, loss = 0.53368303\n",
      "Iteration 229, loss = 0.53271841\n",
      "Iteration 230, loss = 0.53175876\n",
      "Iteration 231, loss = 0.53080280\n",
      "Iteration 232, loss = 0.52985116\n",
      "Iteration 233, loss = 0.52890358\n",
      "Iteration 234, loss = 0.52796065\n",
      "Iteration 235, loss = 0.52702271\n",
      "Iteration 236, loss = 0.52608981\n",
      "Iteration 237, loss = 0.52516114\n",
      "Iteration 238, loss = 0.52423664\n",
      "Iteration 239, loss = 0.52331739\n",
      "Iteration 240, loss = 0.52240369\n",
      "Iteration 241, loss = 0.52149468\n",
      "Iteration 242, loss = 0.52059033\n",
      "Iteration 243, loss = 0.51969096\n",
      "Iteration 244, loss = 0.51879736\n",
      "Iteration 245, loss = 0.51790790\n",
      "Iteration 246, loss = 0.51702083\n",
      "Iteration 247, loss = 0.51613550\n",
      "Iteration 248, loss = 0.51525426\n",
      "Iteration 249, loss = 0.51437704\n",
      "Iteration 250, loss = 0.51350402\n",
      "Iteration 251, loss = 0.51263442\n",
      "Iteration 252, loss = 0.51176794\n",
      "Iteration 253, loss = 0.51090557\n",
      "Iteration 254, loss = 0.51004738\n",
      "Iteration 255, loss = 0.50919316\n",
      "Iteration 256, loss = 0.50834281\n",
      "Iteration 257, loss = 0.50749545\n",
      "Iteration 258, loss = 0.50665173\n",
      "Iteration 259, loss = 0.50581049\n",
      "Iteration 260, loss = 0.50497302\n",
      "Iteration 261, loss = 0.50413943\n",
      "Iteration 262, loss = 0.50330884\n",
      "Iteration 263, loss = 0.50248132\n",
      "Iteration 264, loss = 0.50165746\n",
      "Iteration 265, loss = 0.50083727\n",
      "Iteration 266, loss = 0.50001982\n",
      "Iteration 267, loss = 0.49920521\n",
      "Iteration 268, loss = 0.49839421\n",
      "Iteration 269, loss = 0.49758682\n",
      "Iteration 270, loss = 0.49678312\n",
      "Iteration 271, loss = 0.49598296\n",
      "Iteration 272, loss = 0.49518634\n",
      "Iteration 273, loss = 0.49439242\n",
      "Iteration 274, loss = 0.49359889\n",
      "Iteration 275, loss = 0.49280858\n",
      "Iteration 276, loss = 0.49202153\n",
      "Iteration 277, loss = 0.49123773\n",
      "Iteration 278, loss = 0.49045526\n",
      "Iteration 279, loss = 0.48967436\n",
      "Iteration 280, loss = 0.48889268\n",
      "Iteration 281, loss = 0.48811372\n",
      "Iteration 282, loss = 0.48733706\n",
      "Iteration 283, loss = 0.48656239\n",
      "Iteration 284, loss = 0.48579050\n",
      "Iteration 285, loss = 0.48502141\n",
      "Iteration 286, loss = 0.48425518\n",
      "Iteration 287, loss = 0.48349182\n",
      "Iteration 288, loss = 0.48273080\n",
      "Iteration 289, loss = 0.48197142\n",
      "Iteration 290, loss = 0.48121485\n",
      "Iteration 291, loss = 0.48046117\n",
      "Iteration 292, loss = 0.47970880\n",
      "Iteration 293, loss = 0.47895819\n",
      "Iteration 294, loss = 0.47821027\n",
      "Iteration 295, loss = 0.47746505\n",
      "Iteration 296, loss = 0.47672255\n",
      "Iteration 297, loss = 0.47598280\n",
      "Iteration 298, loss = 0.47524579\n",
      "Iteration 299, loss = 0.47451154\n",
      "Iteration 300, loss = 0.47378005\n",
      "Iteration 301, loss = 0.47305136\n",
      "Iteration 302, loss = 0.47232544\n",
      "Iteration 303, loss = 0.47160227\n",
      "Iteration 304, loss = 0.47088128\n",
      "Iteration 305, loss = 0.47016011\n",
      "Iteration 306, loss = 0.46943970\n",
      "Iteration 307, loss = 0.46872158\n",
      "Iteration 308, loss = 0.46800578\n",
      "Iteration 309, loss = 0.46729234\n",
      "Iteration 310, loss = 0.46658136\n",
      "Iteration 311, loss = 0.46587282\n",
      "Iteration 312, loss = 0.46516669\n",
      "Iteration 313, loss = 0.46446296\n",
      "Iteration 314, loss = 0.46376164\n",
      "Iteration 315, loss = 0.46306272\n",
      "Iteration 316, loss = 0.46236620\n",
      "Iteration 317, loss = 0.46167213\n",
      "Iteration 318, loss = 0.46098050\n",
      "Iteration 319, loss = 0.46029124\n",
      "Iteration 320, loss = 0.45960434\n",
      "Iteration 321, loss = 0.45891993\n",
      "Iteration 322, loss = 0.45823840\n",
      "Iteration 323, loss = 0.45755920\n",
      "Iteration 324, loss = 0.45688233\n",
      "Iteration 325, loss = 0.45620778\n",
      "Iteration 326, loss = 0.45553552\n",
      "Iteration 327, loss = 0.45486556\n",
      "Iteration 328, loss = 0.45419787\n",
      "Iteration 329, loss = 0.45353243\n",
      "Iteration 330, loss = 0.45286923\n",
      "Iteration 331, loss = 0.45220824\n",
      "Iteration 332, loss = 0.45154946\n",
      "Iteration 333, loss = 0.45089285\n",
      "Iteration 334, loss = 0.45023845\n",
      "Iteration 335, loss = 0.44958627\n",
      "Iteration 336, loss = 0.44893594\n",
      "Iteration 337, loss = 0.44828689\n",
      "Iteration 338, loss = 0.44763986\n",
      "Iteration 339, loss = 0.44699482\n",
      "Iteration 340, loss = 0.44635176\n",
      "Iteration 341, loss = 0.44571068\n",
      "Iteration 342, loss = 0.44507156\n",
      "Iteration 343, loss = 0.44443438\n",
      "Iteration 344, loss = 0.44379912\n",
      "Iteration 345, loss = 0.44316578\n",
      "Iteration 346, loss = 0.44253433\n",
      "Iteration 347, loss = 0.44190505\n",
      "Iteration 348, loss = 0.44127770\n",
      "Iteration 349, loss = 0.44065223\n",
      "Iteration 350, loss = 0.44002860\n",
      "Iteration 351, loss = 0.43940680\n",
      "Iteration 352, loss = 0.43878683\n",
      "Iteration 353, loss = 0.43816867\n",
      "Iteration 354, loss = 0.43755229\n",
      "Iteration 355, loss = 0.43693770\n",
      "Iteration 356, loss = 0.43632474\n",
      "Iteration 357, loss = 0.43571214\n",
      "Iteration 358, loss = 0.43510088\n",
      "Iteration 359, loss = 0.43448914\n",
      "Iteration 360, loss = 0.43387754\n",
      "Iteration 361, loss = 0.43326569\n",
      "Iteration 362, loss = 0.43265448\n",
      "Iteration 363, loss = 0.43204451\n",
      "Iteration 364, loss = 0.43143582\n",
      "Iteration 365, loss = 0.43082844\n",
      "Iteration 366, loss = 0.43022209\n",
      "Iteration 367, loss = 0.42961600\n",
      "Iteration 368, loss = 0.42901119\n",
      "Iteration 369, loss = 0.42840769\n",
      "Iteration 370, loss = 0.42780552\n",
      "Iteration 371, loss = 0.42720373\n",
      "Iteration 372, loss = 0.42660155\n",
      "Iteration 373, loss = 0.42600010\n",
      "Iteration 374, loss = 0.42539898\n",
      "Iteration 375, loss = 0.42479766\n",
      "Iteration 376, loss = 0.42419560\n",
      "Iteration 377, loss = 0.42359392\n",
      "Iteration 378, loss = 0.42299332\n",
      "Iteration 379, loss = 0.42239377\n",
      "Iteration 380, loss = 0.42179356\n",
      "Iteration 381, loss = 0.42118638\n",
      "Iteration 382, loss = 0.42057393\n",
      "Iteration 383, loss = 0.41995949\n",
      "Iteration 384, loss = 0.41934339\n",
      "Iteration 385, loss = 0.41872726\n",
      "Iteration 386, loss = 0.41811115\n",
      "Iteration 387, loss = 0.41749405\n",
      "Iteration 388, loss = 0.41687670\n",
      "Iteration 389, loss = 0.41625853\n",
      "Iteration 390, loss = 0.41563994\n",
      "Iteration 391, loss = 0.41501804\n",
      "Iteration 392, loss = 0.41439321\n",
      "Iteration 393, loss = 0.41376742\n",
      "Iteration 394, loss = 0.41314188\n",
      "Iteration 395, loss = 0.41251636\n",
      "Iteration 396, loss = 0.41189019\n",
      "Iteration 397, loss = 0.41126457\n",
      "Iteration 398, loss = 0.41063893\n",
      "Iteration 399, loss = 0.41001324\n",
      "Iteration 400, loss = 0.40938832\n",
      "Iteration 401, loss = 0.40876426\n",
      "Iteration 402, loss = 0.40814116\n",
      "Iteration 403, loss = 0.40751764\n",
      "Iteration 404, loss = 0.40689244\n",
      "Iteration 405, loss = 0.40626722\n",
      "Iteration 406, loss = 0.40564272\n",
      "Iteration 407, loss = 0.40501758\n",
      "Iteration 408, loss = 0.40439224\n",
      "Iteration 409, loss = 0.40376529\n",
      "Iteration 410, loss = 0.40313816\n",
      "Iteration 411, loss = 0.40251185\n",
      "Iteration 412, loss = 0.40188644\n",
      "Iteration 413, loss = 0.40126178\n",
      "Iteration 414, loss = 0.40063687\n",
      "Iteration 415, loss = 0.40001300\n",
      "Iteration 416, loss = 0.39938953\n",
      "Iteration 417, loss = 0.39876497\n",
      "Iteration 418, loss = 0.39813981\n",
      "Iteration 419, loss = 0.39751516\n",
      "Iteration 420, loss = 0.39689151\n",
      "Iteration 421, loss = 0.39626893\n",
      "Iteration 422, loss = 0.39564749\n",
      "Iteration 423, loss = 0.39502724\n",
      "Iteration 424, loss = 0.39440822\n",
      "Iteration 425, loss = 0.39379048\n",
      "Iteration 426, loss = 0.39317405\n",
      "Iteration 427, loss = 0.39255895\n",
      "Iteration 428, loss = 0.39194522\n",
      "Iteration 429, loss = 0.39133288\n",
      "Iteration 430, loss = 0.39072194\n",
      "Iteration 431, loss = 0.39011088\n",
      "Iteration 432, loss = 0.38950091\n",
      "Iteration 433, loss = 0.38889227\n",
      "Iteration 434, loss = 0.38828498\n",
      "Iteration 435, loss = 0.38767905\n",
      "Iteration 436, loss = 0.38707449\n",
      "Iteration 437, loss = 0.38647137\n",
      "Iteration 438, loss = 0.38586998\n",
      "Iteration 439, loss = 0.38527007\n",
      "Iteration 440, loss = 0.38467156\n",
      "Iteration 441, loss = 0.38407448\n",
      "Iteration 442, loss = 0.38347882\n",
      "Iteration 443, loss = 0.38288459\n",
      "Iteration 444, loss = 0.38229180\n",
      "Iteration 445, loss = 0.38170044\n",
      "Iteration 446, loss = 0.38111053\n",
      "Iteration 447, loss = 0.38052046\n",
      "Iteration 448, loss = 0.37993324\n",
      "Iteration 449, loss = 0.37936140\n",
      "Iteration 450, loss = 0.37879176\n",
      "Iteration 451, loss = 0.37822426\n",
      "Iteration 452, loss = 0.37765880\n",
      "Iteration 453, loss = 0.37709532\n",
      "Iteration 454, loss = 0.37653374\n",
      "Iteration 455, loss = 0.37597399\n",
      "Iteration 456, loss = 0.37541601\n",
      "Iteration 457, loss = 0.37485973\n",
      "Iteration 458, loss = 0.37430510\n",
      "Iteration 459, loss = 0.37375205\n",
      "Iteration 460, loss = 0.37320054\n",
      "Iteration 461, loss = 0.37265052\n",
      "Iteration 462, loss = 0.37210451\n",
      "Iteration 463, loss = 0.37157148\n",
      "Iteration 464, loss = 0.37104051\n",
      "Iteration 465, loss = 0.37051148\n",
      "Iteration 466, loss = 0.36998431\n",
      "Iteration 467, loss = 0.36945892\n",
      "Iteration 468, loss = 0.36893523\n",
      "Iteration 469, loss = 0.36841315\n",
      "Iteration 470, loss = 0.36789262\n",
      "Iteration 471, loss = 0.36737357\n",
      "Iteration 472, loss = 0.36685595\n",
      "Iteration 473, loss = 0.36634517\n",
      "Iteration 474, loss = 0.36584274\n",
      "Iteration 475, loss = 0.36534198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 476, loss = 0.36484284\n",
      "Iteration 477, loss = 0.36435545\n",
      "Iteration 478, loss = 0.36387562\n",
      "Iteration 479, loss = 0.36339648\n",
      "Iteration 480, loss = 0.36291820\n",
      "Iteration 481, loss = 0.36244285\n",
      "Iteration 482, loss = 0.36196937\n",
      "Iteration 483, loss = 0.36149604\n",
      "Iteration 484, loss = 0.36102302\n",
      "Iteration 485, loss = 0.36055038\n",
      "Iteration 486, loss = 0.36007813\n",
      "Iteration 487, loss = 0.35960620\n",
      "Iteration 488, loss = 0.35913455\n",
      "Iteration 489, loss = 0.35866311\n",
      "Iteration 490, loss = 0.35819186\n",
      "Iteration 491, loss = 0.35772077\n",
      "Iteration 492, loss = 0.35725014\n",
      "Iteration 493, loss = 0.35678007\n",
      "Iteration 494, loss = 0.35631145\n",
      "Iteration 495, loss = 0.35584415\n",
      "Iteration 496, loss = 0.35537741\n",
      "Iteration 497, loss = 0.35491138\n",
      "Iteration 498, loss = 0.35444614\n",
      "Iteration 499, loss = 0.35398177\n",
      "Iteration 500, loss = 0.35351832\n",
      "Iteration 501, loss = 0.35305576\n",
      "Iteration 502, loss = 0.35259408\n",
      "Iteration 503, loss = 0.35213323\n",
      "Iteration 504, loss = 0.35167316\n",
      "Iteration 505, loss = 0.35121381\n",
      "Iteration 506, loss = 0.35075515\n",
      "Iteration 507, loss = 0.35029713\n",
      "Iteration 508, loss = 0.34983973\n",
      "Iteration 509, loss = 0.34938296\n",
      "Iteration 510, loss = 0.34892680\n",
      "Iteration 511, loss = 0.34847127\n",
      "Iteration 512, loss = 0.34801638\n",
      "Iteration 513, loss = 0.34756214\n",
      "Iteration 514, loss = 0.34710855\n",
      "Iteration 515, loss = 0.34665563\n",
      "Iteration 516, loss = 0.34620337\n",
      "Iteration 517, loss = 0.34575178\n",
      "Iteration 518, loss = 0.34530085\n",
      "Iteration 519, loss = 0.34485066\n",
      "Iteration 520, loss = 0.34440127\n",
      "Iteration 521, loss = 0.34395255\n",
      "Iteration 522, loss = 0.34350448\n",
      "Iteration 523, loss = 0.34305705\n",
      "Iteration 524, loss = 0.34261025\n",
      "Iteration 525, loss = 0.34216410\n",
      "Iteration 526, loss = 0.34171857\n",
      "Iteration 527, loss = 0.34127368\n",
      "Iteration 528, loss = 0.34082941\n",
      "Iteration 529, loss = 0.34038577\n",
      "Iteration 530, loss = 0.33994276\n",
      "Iteration 531, loss = 0.33950037\n",
      "Iteration 532, loss = 0.33905861\n",
      "Iteration 533, loss = 0.33861747\n",
      "Iteration 534, loss = 0.33817695\n",
      "Iteration 535, loss = 0.33773723\n",
      "Iteration 536, loss = 0.33729782\n",
      "Iteration 537, loss = 0.33685918\n",
      "Iteration 538, loss = 0.33642115\n",
      "Iteration 539, loss = 0.33598373\n",
      "Iteration 540, loss = 0.33554690\n",
      "Iteration 541, loss = 0.33511068\n",
      "Iteration 542, loss = 0.33467506\n",
      "Iteration 543, loss = 0.33424013\n",
      "Iteration 544, loss = 0.33380570\n",
      "Iteration 545, loss = 0.33337190\n",
      "Iteration 546, loss = 0.33293870\n",
      "Iteration 547, loss = 0.33250609\n",
      "Iteration 548, loss = 0.33207408\n",
      "Iteration 549, loss = 0.33164265\n",
      "Iteration 550, loss = 0.33121183\n",
      "Iteration 551, loss = 0.33078159\n",
      "Iteration 552, loss = 0.33035196\n",
      "Iteration 553, loss = 0.32992291\n",
      "Iteration 554, loss = 0.32949452\n",
      "Iteration 555, loss = 0.32906664\n",
      "Iteration 556, loss = 0.32863939\n",
      "Iteration 557, loss = 0.32821272\n",
      "Iteration 558, loss = 0.32778664\n",
      "Iteration 559, loss = 0.32736114\n",
      "Iteration 560, loss = 0.32693622\n",
      "Iteration 561, loss = 0.32651189\n",
      "Iteration 562, loss = 0.32608815\n",
      "Iteration 563, loss = 0.32566498\n",
      "Iteration 564, loss = 0.32524240\n",
      "Iteration 565, loss = 0.32482040\n",
      "Iteration 566, loss = 0.32439898\n",
      "Iteration 567, loss = 0.32397814\n",
      "Iteration 568, loss = 0.32355788\n",
      "Iteration 569, loss = 0.32313820\n",
      "Iteration 570, loss = 0.32271924\n",
      "Iteration 571, loss = 0.32230098\n",
      "Iteration 572, loss = 0.32188432\n",
      "Iteration 573, loss = 0.32147027\n",
      "Iteration 574, loss = 0.32105702\n",
      "Iteration 575, loss = 0.32064445\n",
      "Iteration 576, loss = 0.32023263\n",
      "Iteration 577, loss = 0.31982151\n",
      "Iteration 578, loss = 0.31941108\n",
      "Iteration 579, loss = 0.31900132\n",
      "Iteration 580, loss = 0.31859223\n",
      "Iteration 581, loss = 0.31818379\n",
      "Iteration 582, loss = 0.31777599\n",
      "Iteration 583, loss = 0.31736882\n",
      "Iteration 584, loss = 0.31696228\n",
      "Iteration 585, loss = 0.31655636\n",
      "Iteration 586, loss = 0.31615106\n",
      "Iteration 587, loss = 0.31574635\n",
      "Iteration 588, loss = 0.31534225\n",
      "Iteration 589, loss = 0.31493874\n",
      "Iteration 590, loss = 0.31453583\n",
      "Iteration 591, loss = 0.31413350\n",
      "Iteration 592, loss = 0.31373175\n",
      "Iteration 593, loss = 0.31333058\n",
      "Iteration 594, loss = 0.31292998\n",
      "Iteration 595, loss = 0.31252996\n",
      "Iteration 596, loss = 0.31213051\n",
      "Iteration 597, loss = 0.31173163\n",
      "Iteration 598, loss = 0.31133331\n",
      "Iteration 599, loss = 0.31093555\n",
      "Iteration 600, loss = 0.31053836\n",
      "Iteration 601, loss = 0.31014173\n",
      "Iteration 602, loss = 0.30974565\n",
      "Iteration 603, loss = 0.30935013\n",
      "Iteration 604, loss = 0.30895517\n",
      "Iteration 605, loss = 0.30856076\n",
      "Iteration 606, loss = 0.30816691\n",
      "Iteration 607, loss = 0.30777360\n",
      "Iteration 608, loss = 0.30738085\n",
      "Iteration 609, loss = 0.30698865\n",
      "Iteration 610, loss = 0.30659709\n",
      "Iteration 611, loss = 0.30620621\n",
      "Iteration 612, loss = 0.30581590\n",
      "Iteration 613, loss = 0.30542614\n",
      "Iteration 614, loss = 0.30503694\n",
      "Iteration 615, loss = 0.30464829\n",
      "Iteration 616, loss = 0.30426020\n",
      "Iteration 617, loss = 0.30387266\n",
      "Iteration 618, loss = 0.30348567\n",
      "Iteration 619, loss = 0.30309923\n",
      "Iteration 620, loss = 0.30271334\n",
      "Iteration 621, loss = 0.30232800\n",
      "Iteration 622, loss = 0.30194320\n",
      "Iteration 623, loss = 0.30155895\n",
      "Iteration 624, loss = 0.30117525\n",
      "Iteration 625, loss = 0.30079352\n",
      "Iteration 626, loss = 0.30041289\n",
      "Iteration 627, loss = 0.30003291\n",
      "Iteration 628, loss = 0.29965357\n",
      "Iteration 629, loss = 0.29927486\n",
      "Iteration 630, loss = 0.29889677\n",
      "Iteration 631, loss = 0.29851930\n",
      "Iteration 632, loss = 0.29814242\n",
      "Iteration 633, loss = 0.29776613\n",
      "Iteration 634, loss = 0.29739043\n",
      "Iteration 635, loss = 0.29701531\n",
      "Iteration 636, loss = 0.29664203\n",
      "Iteration 637, loss = 0.29627016\n",
      "Iteration 638, loss = 0.29589895\n",
      "Iteration 639, loss = 0.29552838\n",
      "Iteration 640, loss = 0.29515846\n",
      "Iteration 641, loss = 0.29478916\n",
      "Iteration 642, loss = 0.29442047\n",
      "Iteration 643, loss = 0.29405238\n",
      "Iteration 644, loss = 0.29368488\n",
      "Iteration 645, loss = 0.29331797\n",
      "Iteration 646, loss = 0.29295265\n",
      "Iteration 647, loss = 0.29258861\n",
      "Iteration 648, loss = 0.29222520\n",
      "Iteration 649, loss = 0.29186242\n",
      "Iteration 650, loss = 0.29150024\n",
      "Iteration 651, loss = 0.29113866\n",
      "Iteration 652, loss = 0.29077767\n",
      "Iteration 653, loss = 0.29041726\n",
      "Iteration 654, loss = 0.29005742\n",
      "Iteration 655, loss = 0.28969814\n",
      "Iteration 656, loss = 0.28933943\n",
      "Iteration 657, loss = 0.28898126\n",
      "Iteration 658, loss = 0.28862365\n",
      "Iteration 659, loss = 0.28826657\n",
      "Iteration 660, loss = 0.28791004\n",
      "Iteration 661, loss = 0.28755404\n",
      "Iteration 662, loss = 0.28719857\n",
      "Iteration 663, loss = 0.28684363\n",
      "Iteration 664, loss = 0.28648921\n",
      "Iteration 665, loss = 0.28613607\n",
      "Iteration 666, loss = 0.28578382\n",
      "Iteration 667, loss = 0.28543213\n",
      "Iteration 668, loss = 0.28508100\n",
      "Iteration 669, loss = 0.28473042\n",
      "Iteration 670, loss = 0.28438039\n",
      "Iteration 671, loss = 0.28403090\n",
      "Iteration 672, loss = 0.28368194\n",
      "Iteration 673, loss = 0.28333352\n",
      "Iteration 674, loss = 0.28298563\n",
      "Iteration 675, loss = 0.28263826\n",
      "Iteration 676, loss = 0.28229141\n",
      "Iteration 677, loss = 0.28194508\n",
      "Iteration 678, loss = 0.28159927\n",
      "Iteration 679, loss = 0.28125397\n",
      "Iteration 680, loss = 0.28090931\n",
      "Iteration 681, loss = 0.28056506\n",
      "Iteration 682, loss = 0.28022135\n",
      "Iteration 683, loss = 0.27987816\n",
      "Iteration 684, loss = 0.27953549\n",
      "Iteration 685, loss = 0.27919339\n",
      "Iteration 686, loss = 0.27885174\n",
      "Iteration 687, loss = 0.27851058\n",
      "Iteration 688, loss = 0.27816990\n",
      "Iteration 689, loss = 0.27782988\n",
      "Iteration 690, loss = 0.27749012\n",
      "Iteration 691, loss = 0.27715104\n",
      "Iteration 692, loss = 0.27681240\n",
      "Iteration 693, loss = 0.27647425\n",
      "Iteration 694, loss = 0.27613659\n",
      "Iteration 695, loss = 0.27579947\n",
      "Iteration 696, loss = 0.27546283\n",
      "Iteration 697, loss = 0.27512666\n",
      "Iteration 698, loss = 0.27479101\n",
      "Iteration 699, loss = 0.27445591\n",
      "Iteration 700, loss = 0.27412124\n",
      "Iteration 701, loss = 0.27378706\n",
      "Iteration 702, loss = 0.27345342\n",
      "Iteration 703, loss = 0.27312025\n",
      "Iteration 704, loss = 0.27278756\n",
      "Iteration 705, loss = 0.27245538\n",
      "Iteration 706, loss = 0.27212373\n",
      "Iteration 707, loss = 0.27179252\n",
      "Iteration 708, loss = 0.27146180\n",
      "Iteration 709, loss = 0.27113166\n",
      "Iteration 710, loss = 0.27080189\n",
      "Iteration 711, loss = 0.27047265\n",
      "Iteration 712, loss = 0.27014401\n",
      "Iteration 713, loss = 0.26981570\n",
      "Iteration 714, loss = 0.26948794\n",
      "Iteration 715, loss = 0.26916077\n",
      "Iteration 716, loss = 0.26883395\n",
      "Iteration 717, loss = 0.26850765\n",
      "Iteration 718, loss = 0.26818195\n",
      "Iteration 719, loss = 0.26785660\n",
      "Iteration 720, loss = 0.26753178\n",
      "Iteration 721, loss = 0.26720754\n",
      "Iteration 722, loss = 0.26688370\n",
      "Iteration 723, loss = 0.26656032\n",
      "Iteration 724, loss = 0.26623753\n",
      "Iteration 725, loss = 0.26591511\n",
      "Iteration 726, loss = 0.26559329\n",
      "Iteration 727, loss = 0.26527189\n",
      "Iteration 728, loss = 0.26495097\n",
      "Iteration 729, loss = 0.26463064\n",
      "Iteration 730, loss = 0.26431065\n",
      "Iteration 731, loss = 0.26399125\n",
      "Iteration 732, loss = 0.26367231\n",
      "Iteration 733, loss = 0.26335382\n",
      "Iteration 734, loss = 0.26303590\n",
      "Iteration 735, loss = 0.26271837\n",
      "Iteration 736, loss = 0.26240138\n",
      "Iteration 737, loss = 0.26208488\n",
      "Iteration 738, loss = 0.26176882\n",
      "Iteration 739, loss = 0.26145332\n",
      "Iteration 740, loss = 0.26113821\n",
      "Iteration 741, loss = 0.26082368\n",
      "Iteration 742, loss = 0.26050980\n",
      "Iteration 743, loss = 0.26019622\n",
      "Iteration 744, loss = 0.25988310\n",
      "Iteration 745, loss = 0.25957059\n",
      "Iteration 746, loss = 0.25925846\n",
      "Iteration 747, loss = 0.25894697\n",
      "Iteration 748, loss = 0.25863579\n",
      "Iteration 749, loss = 0.25832530\n",
      "Iteration 750, loss = 0.25801512\n",
      "Iteration 751, loss = 0.25770544\n",
      "Iteration 752, loss = 0.25739628\n",
      "Iteration 753, loss = 0.25708763\n",
      "Iteration 754, loss = 0.25677941\n",
      "Iteration 755, loss = 0.25647210\n",
      "Iteration 756, loss = 0.25616513\n",
      "Iteration 757, loss = 0.25585884\n",
      "Iteration 758, loss = 0.25555285\n",
      "Iteration 759, loss = 0.25524756\n",
      "Iteration 760, loss = 0.25494256\n",
      "Iteration 761, loss = 0.25463815\n",
      "Iteration 762, loss = 0.25433415\n",
      "Iteration 763, loss = 0.25403076\n",
      "Iteration 764, loss = 0.25372770\n",
      "Iteration 765, loss = 0.25342526\n",
      "Iteration 766, loss = 0.25312322\n",
      "Iteration 767, loss = 0.25282164\n",
      "Iteration 768, loss = 0.25252069\n",
      "Iteration 769, loss = 0.25222007\n",
      "Iteration 770, loss = 0.25191990\n",
      "Iteration 771, loss = 0.25162035\n",
      "Iteration 772, loss = 0.25132114\n",
      "Iteration 773, loss = 0.25102253\n",
      "Iteration 774, loss = 0.25072424\n",
      "Iteration 775, loss = 0.25042658\n",
      "Iteration 776, loss = 0.25012928\n",
      "Iteration 777, loss = 0.24983246\n",
      "Iteration 778, loss = 0.24953625\n",
      "Iteration 779, loss = 0.24924037\n",
      "Iteration 780, loss = 0.24894493\n",
      "Iteration 781, loss = 0.24865017\n",
      "Iteration 782, loss = 0.24835563\n",
      "Iteration 783, loss = 0.24806171\n",
      "Iteration 784, loss = 0.24776819\n",
      "Iteration 785, loss = 0.24747510\n",
      "Iteration 786, loss = 0.24718267\n",
      "Iteration 787, loss = 0.24689046\n",
      "Iteration 788, loss = 0.24659908\n",
      "Iteration 789, loss = 0.24630851\n",
      "Iteration 790, loss = 0.24601840\n",
      "Iteration 791, loss = 0.24572885\n",
      "Iteration 792, loss = 0.24543974\n",
      "Iteration 793, loss = 0.24515113\n",
      "Iteration 794, loss = 0.24486306\n",
      "Iteration 795, loss = 0.24457539\n",
      "Iteration 796, loss = 0.24428819\n",
      "Iteration 797, loss = 0.24400157\n",
      "Iteration 798, loss = 0.24371532\n",
      "Iteration 799, loss = 0.24342957\n",
      "Iteration 800, loss = 0.24314433\n",
      "Iteration 801, loss = 0.24285949\n",
      "Iteration 802, loss = 0.24257518\n",
      "Iteration 803, loss = 0.24229130\n",
      "Iteration 804, loss = 0.24200786\n",
      "Iteration 805, loss = 0.24172497\n",
      "Iteration 806, loss = 0.24144246\n",
      "Iteration 807, loss = 0.24116041\n",
      "Iteration 808, loss = 0.24087890\n",
      "Iteration 809, loss = 0.24059778\n",
      "Iteration 810, loss = 0.24031711\n",
      "Iteration 811, loss = 0.24003696\n",
      "Iteration 812, loss = 0.23975723\n",
      "Iteration 813, loss = 0.23947794\n",
      "Iteration 814, loss = 0.23919910\n",
      "Iteration 815, loss = 0.23892081\n",
      "Iteration 816, loss = 0.23864288\n",
      "Iteration 817, loss = 0.23836539\n",
      "Iteration 818, loss = 0.23808845\n",
      "Iteration 819, loss = 0.23781186\n",
      "Iteration 820, loss = 0.23753574\n",
      "Iteration 821, loss = 0.23726017\n",
      "Iteration 822, loss = 0.23698493\n",
      "Iteration 823, loss = 0.23671018\n",
      "Iteration 824, loss = 0.23643588\n",
      "Iteration 825, loss = 0.23616209\n",
      "Iteration 826, loss = 0.23588868\n",
      "Iteration 827, loss = 0.23561571\n",
      "Iteration 828, loss = 0.23534319\n",
      "Iteration 829, loss = 0.23507119\n",
      "Iteration 830, loss = 0.23479957\n",
      "Iteration 831, loss = 0.23452839\n",
      "Iteration 832, loss = 0.23425771\n",
      "Iteration 833, loss = 0.23398743\n",
      "Iteration 834, loss = 0.23371759\n",
      "Iteration 835, loss = 0.23344819\n",
      "Iteration 836, loss = 0.23317939\n",
      "Iteration 837, loss = 0.23291120\n",
      "Iteration 838, loss = 0.23264352\n",
      "Iteration 839, loss = 0.23237628\n",
      "Iteration 840, loss = 0.23210959\n",
      "Iteration 841, loss = 0.23184328\n",
      "Iteration 842, loss = 0.23157741\n",
      "Iteration 843, loss = 0.23131198\n",
      "Iteration 844, loss = 0.23104698\n",
      "Iteration 845, loss = 0.23078259\n",
      "Iteration 846, loss = 0.23051866\n",
      "Iteration 847, loss = 0.23025542\n",
      "Iteration 848, loss = 0.22999269\n",
      "Iteration 849, loss = 0.22973031\n",
      "Iteration 850, loss = 0.22946839\n",
      "Iteration 851, loss = 0.22920702\n",
      "Iteration 852, loss = 0.22894605\n",
      "Iteration 853, loss = 0.22868547\n",
      "Iteration 854, loss = 0.22842537\n",
      "Iteration 855, loss = 0.22816563\n",
      "Iteration 856, loss = 0.22790628\n",
      "Iteration 857, loss = 0.22764735\n",
      "Iteration 858, loss = 0.22738884\n",
      "Iteration 859, loss = 0.22713107\n",
      "Iteration 860, loss = 0.22687356\n",
      "Iteration 861, loss = 0.22661662\n",
      "Iteration 862, loss = 0.22636016\n",
      "Iteration 863, loss = 0.22610415\n",
      "Iteration 864, loss = 0.22584866\n",
      "Iteration 865, loss = 0.22559349\n",
      "Iteration 866, loss = 0.22533881\n",
      "Iteration 867, loss = 0.22508456\n",
      "Iteration 868, loss = 0.22483075\n",
      "Iteration 869, loss = 0.22457738\n",
      "Iteration 870, loss = 0.22432444\n",
      "Iteration 871, loss = 0.22407193\n",
      "Iteration 872, loss = 0.22381989\n",
      "Iteration 873, loss = 0.22356825\n",
      "Iteration 874, loss = 0.22331706\n",
      "Iteration 875, loss = 0.22306629\n",
      "Iteration 876, loss = 0.22281595\n",
      "Iteration 877, loss = 0.22256603\n",
      "Iteration 878, loss = 0.22231654\n",
      "Iteration 879, loss = 0.22206748\n",
      "Iteration 880, loss = 0.22181885\n",
      "Iteration 881, loss = 0.22157063\n",
      "Iteration 882, loss = 0.22132285\n",
      "Iteration 883, loss = 0.22107549\n",
      "Iteration 884, loss = 0.22082858\n",
      "Iteration 885, loss = 0.22058206\n",
      "Iteration 886, loss = 0.22033597\n",
      "Iteration 887, loss = 0.22009031\n",
      "Iteration 888, loss = 0.21984506\n",
      "Iteration 889, loss = 0.21960023\n",
      "Iteration 890, loss = 0.21935582\n",
      "Iteration 891, loss = 0.21911183\n",
      "Iteration 892, loss = 0.21886825\n",
      "Iteration 893, loss = 0.21862509\n",
      "Iteration 894, loss = 0.21838235\n",
      "Iteration 895, loss = 0.21814002\n",
      "Iteration 896, loss = 0.21789811\n",
      "Iteration 897, loss = 0.21765662\n",
      "Iteration 898, loss = 0.21741553\n",
      "Iteration 899, loss = 0.21717486\n",
      "Iteration 900, loss = 0.21693461\n",
      "Iteration 901, loss = 0.21669476\n",
      "Iteration 902, loss = 0.21645533\n",
      "Iteration 903, loss = 0.21621634\n",
      "Iteration 904, loss = 0.21597772\n",
      "Iteration 905, loss = 0.21573953\n",
      "Iteration 906, loss = 0.21550176\n",
      "Iteration 907, loss = 0.21526438\n",
      "Iteration 908, loss = 0.21502742\n",
      "Iteration 909, loss = 0.21479087\n",
      "Iteration 910, loss = 0.21455472\n",
      "Iteration 911, loss = 0.21431897\n",
      "Iteration 912, loss = 0.21408363\n",
      "Iteration 913, loss = 0.21384870\n",
      "Iteration 914, loss = 0.21361417\n",
      "Iteration 915, loss = 0.21338005\n",
      "Iteration 916, loss = 0.21314633\n",
      "Iteration 917, loss = 0.21291301\n",
      "Iteration 918, loss = 0.21268010\n",
      "Iteration 919, loss = 0.21244758\n",
      "Iteration 920, loss = 0.21221547\n",
      "Iteration 921, loss = 0.21198377\n",
      "Iteration 922, loss = 0.21175253\n",
      "Iteration 923, loss = 0.21152173\n",
      "Iteration 924, loss = 0.21129134\n",
      "Iteration 925, loss = 0.21106133\n",
      "Iteration 926, loss = 0.21083174\n",
      "Iteration 927, loss = 0.21060254\n",
      "Iteration 928, loss = 0.21037374\n",
      "Iteration 929, loss = 0.21014535\n",
      "Iteration 930, loss = 0.20991735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 931, loss = 0.20968975\n",
      "Iteration 932, loss = 0.20946253\n",
      "Iteration 933, loss = 0.20923572\n",
      "Iteration 934, loss = 0.20900930\n",
      "Iteration 935, loss = 0.20878327\n",
      "Iteration 936, loss = 0.20855764\n",
      "Iteration 937, loss = 0.20833239\n",
      "Iteration 938, loss = 0.20810755\n",
      "Iteration 939, loss = 0.20788322\n",
      "Iteration 940, loss = 0.20765988\n",
      "Iteration 941, loss = 0.20743695\n",
      "Iteration 942, loss = 0.20721445\n",
      "Iteration 943, loss = 0.20699238\n",
      "Iteration 944, loss = 0.20677071\n",
      "Iteration 945, loss = 0.20654946\n",
      "Iteration 946, loss = 0.20632861\n",
      "Iteration 947, loss = 0.20610817\n",
      "Iteration 948, loss = 0.20588812\n",
      "Iteration 949, loss = 0.20566848\n",
      "Iteration 950, loss = 0.20544923\n",
      "Iteration 951, loss = 0.20523038\n",
      "Iteration 952, loss = 0.20501191\n",
      "Iteration 953, loss = 0.20479384\n",
      "Iteration 954, loss = 0.20457615\n",
      "Iteration 955, loss = 0.20435885\n",
      "Iteration 956, loss = 0.20414193\n",
      "Iteration 957, loss = 0.20392539\n",
      "Iteration 958, loss = 0.20370923\n",
      "Iteration 959, loss = 0.20349345\n",
      "Iteration 960, loss = 0.20327809\n",
      "Iteration 961, loss = 0.20306411\n",
      "Iteration 962, loss = 0.20285053\n",
      "Iteration 963, loss = 0.20263737\n",
      "Iteration 964, loss = 0.20242461\n",
      "Iteration 965, loss = 0.20221224\n",
      "Iteration 966, loss = 0.20200033\n",
      "Iteration 967, loss = 0.20178887\n",
      "Iteration 968, loss = 0.20157780\n",
      "Iteration 969, loss = 0.20136713\n",
      "Iteration 970, loss = 0.20115684\n",
      "Iteration 971, loss = 0.20094694\n",
      "Iteration 972, loss = 0.20073742\n",
      "Iteration 973, loss = 0.20052828\n",
      "Iteration 974, loss = 0.20031952\n",
      "Iteration 975, loss = 0.20011113\n",
      "Iteration 976, loss = 0.19990311\n",
      "Iteration 977, loss = 0.19969546\n",
      "Iteration 978, loss = 0.19948818\n",
      "Iteration 979, loss = 0.19928127\n",
      "Iteration 980, loss = 0.19907472\n",
      "Iteration 981, loss = 0.19886853\n",
      "Iteration 982, loss = 0.19866271\n",
      "Iteration 983, loss = 0.19845725\n",
      "Iteration 984, loss = 0.19825215\n",
      "Iteration 985, loss = 0.19804742\n",
      "Iteration 986, loss = 0.19784303\n",
      "Iteration 987, loss = 0.19763901\n",
      "Iteration 988, loss = 0.19743535\n",
      "Iteration 989, loss = 0.19723203\n",
      "Iteration 990, loss = 0.19702907\n",
      "Iteration 991, loss = 0.19682647\n",
      "Iteration 992, loss = 0.19662422\n",
      "Iteration 993, loss = 0.19642232\n",
      "Iteration 994, loss = 0.19622077\n",
      "Iteration 995, loss = 0.19601957\n",
      "Iteration 996, loss = 0.19581871\n",
      "Iteration 997, loss = 0.19561822\n",
      "Iteration 998, loss = 0.19541806\n",
      "Iteration 999, loss = 0.19521825\n",
      "Iteration 1000, loss = 0.19501879\n",
      "Iteration 1, loss = 2.11924132\n",
      "Iteration 2, loss = 2.05423690\n",
      "Iteration 3, loss = 1.96460339\n",
      "Iteration 4, loss = 1.85617934\n",
      "Iteration 5, loss = 1.73522759\n",
      "Iteration 6, loss = 1.60847227\n",
      "Iteration 7, loss = 1.48308007\n",
      "Iteration 8, loss = 1.36616055\n",
      "Iteration 9, loss = 1.26373583\n",
      "Iteration 10, loss = 1.18028404\n",
      "Iteration 11, loss = 1.11874504\n",
      "Iteration 12, loss = 1.08022103\n",
      "Iteration 13, loss = 1.06166653\n",
      "Iteration 14, loss = 1.05804359\n",
      "Iteration 15, loss = 1.06365294\n",
      "Iteration 16, loss = 1.07332653\n",
      "Iteration 17, loss = 1.08301383\n",
      "Iteration 18, loss = 1.09005047\n",
      "Iteration 19, loss = 1.09309073\n",
      "Iteration 20, loss = 1.09169868\n",
      "Iteration 21, loss = 1.08612415\n",
      "Iteration 22, loss = 1.07706397\n",
      "Iteration 23, loss = 1.06545643\n",
      "Iteration 24, loss = 1.05231450\n",
      "Iteration 25, loss = 1.03859710\n",
      "Iteration 26, loss = 1.02511770\n",
      "Iteration 27, loss = 1.01245966\n",
      "Iteration 28, loss = 1.00104971\n",
      "Iteration 29, loss = 0.99107416\n",
      "Iteration 30, loss = 0.98253757\n",
      "Iteration 31, loss = 0.97530644\n",
      "Iteration 32, loss = 0.96915756\n",
      "Iteration 33, loss = 0.96382418\n",
      "Iteration 34, loss = 0.95903510\n",
      "Iteration 35, loss = 0.95454411\n",
      "Iteration 36, loss = 0.95015696\n",
      "Iteration 37, loss = 0.94572092\n",
      "Iteration 38, loss = 0.94114025\n",
      "Iteration 39, loss = 0.93637433\n",
      "Iteration 40, loss = 0.93142555\n",
      "Iteration 41, loss = 0.92632854\n",
      "Iteration 42, loss = 0.92113840\n",
      "Iteration 43, loss = 0.91591956\n",
      "Iteration 44, loss = 0.91073602\n",
      "Iteration 45, loss = 0.90564369\n",
      "Iteration 46, loss = 0.90068533\n",
      "Iteration 47, loss = 0.89588801\n",
      "Iteration 48, loss = 0.89126283\n",
      "Iteration 49, loss = 0.88680662\n",
      "Iteration 50, loss = 0.88249501\n",
      "Iteration 51, loss = 0.87831300\n",
      "Iteration 52, loss = 0.87423847\n",
      "Iteration 53, loss = 0.87024524\n",
      "Iteration 54, loss = 0.86630928\n",
      "Iteration 55, loss = 0.86241076\n",
      "Iteration 56, loss = 0.85853508\n",
      "Iteration 57, loss = 0.85467318\n",
      "Iteration 58, loss = 0.85082110\n",
      "Iteration 59, loss = 0.84697916\n",
      "Iteration 60, loss = 0.84315080\n",
      "Iteration 61, loss = 0.83934140\n",
      "Iteration 62, loss = 0.83555714\n",
      "Iteration 63, loss = 0.83180364\n",
      "Iteration 64, loss = 0.82808645\n",
      "Iteration 65, loss = 0.82440962\n",
      "Iteration 66, loss = 0.82077567\n",
      "Iteration 67, loss = 0.81718568\n",
      "Iteration 68, loss = 0.81363948\n",
      "Iteration 69, loss = 0.81013592\n",
      "Iteration 70, loss = 0.80667314\n",
      "Iteration 71, loss = 0.80324893\n",
      "Iteration 72, loss = 0.79986094\n",
      "Iteration 73, loss = 0.79650699\n",
      "Iteration 74, loss = 0.79318512\n",
      "Iteration 75, loss = 0.78989374\n",
      "Iteration 76, loss = 0.78663164\n",
      "Iteration 77, loss = 0.78342459\n",
      "Iteration 78, loss = 0.78025458\n",
      "Iteration 79, loss = 0.77711324\n",
      "Iteration 80, loss = 0.77400005\n",
      "Iteration 81, loss = 0.77091473\n",
      "Iteration 82, loss = 0.76785741\n",
      "Iteration 83, loss = 0.76483747\n",
      "Iteration 84, loss = 0.76188071\n",
      "Iteration 85, loss = 0.75898562\n",
      "Iteration 86, loss = 0.75612682\n",
      "Iteration 87, loss = 0.75329667\n",
      "Iteration 88, loss = 0.75049476\n",
      "Iteration 89, loss = 0.74772063\n",
      "Iteration 90, loss = 0.74497380\n",
      "Iteration 91, loss = 0.74225374\n",
      "Iteration 92, loss = 0.73956648\n",
      "Iteration 93, loss = 0.73692532\n",
      "Iteration 94, loss = 0.73431045\n",
      "Iteration 95, loss = 0.73172137\n",
      "Iteration 96, loss = 0.72915755\n",
      "Iteration 97, loss = 0.72661845\n",
      "Iteration 98, loss = 0.72411320\n",
      "Iteration 99, loss = 0.72164327\n",
      "Iteration 100, loss = 0.71919756\n",
      "Iteration 101, loss = 0.71677555\n",
      "Iteration 102, loss = 0.71437667\n",
      "Iteration 103, loss = 0.71200035\n",
      "Iteration 104, loss = 0.70964602\n",
      "Iteration 105, loss = 0.70731313\n",
      "Iteration 106, loss = 0.70500112\n",
      "Iteration 107, loss = 0.70270947\n",
      "Iteration 108, loss = 0.70043767\n",
      "Iteration 109, loss = 0.69819592\n",
      "Iteration 110, loss = 0.69597942\n",
      "Iteration 111, loss = 0.69378244\n",
      "Iteration 112, loss = 0.69160453\n",
      "Iteration 113, loss = 0.68944830\n",
      "Iteration 114, loss = 0.68732382\n",
      "Iteration 115, loss = 0.68521798\n",
      "Iteration 116, loss = 0.68313031\n",
      "Iteration 117, loss = 0.68105995\n",
      "Iteration 118, loss = 0.67900701\n",
      "Iteration 119, loss = 0.67697224\n",
      "Iteration 120, loss = 0.67496834\n",
      "Iteration 121, loss = 0.67298157\n",
      "Iteration 122, loss = 0.67101158\n",
      "Iteration 123, loss = 0.66905804\n",
      "Iteration 124, loss = 0.66712065\n",
      "Iteration 125, loss = 0.66521014\n",
      "Iteration 126, loss = 0.66332504\n",
      "Iteration 127, loss = 0.66145567\n",
      "Iteration 128, loss = 0.65960180\n",
      "Iteration 129, loss = 0.65776323\n",
      "Iteration 130, loss = 0.65593971\n",
      "Iteration 131, loss = 0.65413630\n",
      "Iteration 132, loss = 0.65235101\n",
      "Iteration 133, loss = 0.65058743\n",
      "Iteration 134, loss = 0.64883828\n",
      "Iteration 135, loss = 0.64710327\n",
      "Iteration 136, loss = 0.64538228\n",
      "Iteration 137, loss = 0.64367518\n",
      "Iteration 138, loss = 0.64198139\n",
      "Iteration 139, loss = 0.64030096\n",
      "Iteration 140, loss = 0.63863336\n",
      "Iteration 141, loss = 0.63697857\n",
      "Iteration 142, loss = 0.63533656\n",
      "Iteration 143, loss = 0.63370710\n",
      "Iteration 144, loss = 0.63209328\n",
      "Iteration 145, loss = 0.63049200\n",
      "Iteration 146, loss = 0.62890499\n",
      "Iteration 147, loss = 0.62733237\n",
      "Iteration 148, loss = 0.62577287\n",
      "Iteration 149, loss = 0.62422463\n",
      "Iteration 150, loss = 0.62268772\n",
      "Iteration 151, loss = 0.62116214\n",
      "Iteration 152, loss = 0.61964785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 153, loss = 0.61814389\n",
      "Iteration 154, loss = 0.61665085\n",
      "Iteration 155, loss = 0.61516861\n",
      "Iteration 156, loss = 0.61369629\n",
      "Iteration 157, loss = 0.61223455\n",
      "Iteration 158, loss = 0.61078326\n",
      "Iteration 159, loss = 0.60934227\n",
      "Iteration 160, loss = 0.60791117\n",
      "Iteration 161, loss = 0.60648838\n",
      "Iteration 162, loss = 0.60507531\n",
      "Iteration 163, loss = 0.60367184\n",
      "Iteration 164, loss = 0.60227787\n",
      "Iteration 165, loss = 0.60089331\n",
      "Iteration 166, loss = 0.59951738\n",
      "Iteration 167, loss = 0.59815037\n",
      "Iteration 168, loss = 0.59679243\n",
      "Iteration 169, loss = 0.59544349\n",
      "Iteration 170, loss = 0.59410349\n",
      "Iteration 171, loss = 0.59277236\n",
      "Iteration 172, loss = 0.59145003\n",
      "Iteration 173, loss = 0.59013526\n",
      "Iteration 174, loss = 0.58882911\n",
      "Iteration 175, loss = 0.58753114\n",
      "Iteration 176, loss = 0.58624044\n",
      "Iteration 177, loss = 0.58495713\n",
      "Iteration 178, loss = 0.58368209\n",
      "Iteration 179, loss = 0.58241531\n",
      "Iteration 180, loss = 0.58115675\n",
      "Iteration 181, loss = 0.57990639\n",
      "Iteration 182, loss = 0.57866420\n",
      "Iteration 183, loss = 0.57743013\n",
      "Iteration 184, loss = 0.57620499\n",
      "Iteration 185, loss = 0.57498931\n",
      "Iteration 186, loss = 0.57378058\n",
      "Iteration 187, loss = 0.57257849\n",
      "Iteration 188, loss = 0.57138276\n",
      "Iteration 189, loss = 0.57019311\n",
      "Iteration 190, loss = 0.56901052\n",
      "Iteration 191, loss = 0.56783518\n",
      "Iteration 192, loss = 0.56666620\n",
      "Iteration 193, loss = 0.56550252\n",
      "Iteration 194, loss = 0.56434593\n",
      "Iteration 195, loss = 0.56319647\n",
      "Iteration 196, loss = 0.56205444\n",
      "Iteration 197, loss = 0.56091982\n",
      "Iteration 198, loss = 0.55979232\n",
      "Iteration 199, loss = 0.55867116\n",
      "Iteration 200, loss = 0.55755269\n",
      "Iteration 201, loss = 0.55643331\n",
      "Iteration 202, loss = 0.55531984\n",
      "Iteration 203, loss = 0.55421241\n",
      "Iteration 204, loss = 0.55311109\n",
      "Iteration 205, loss = 0.55201604\n",
      "Iteration 206, loss = 0.55092740\n",
      "Iteration 207, loss = 0.54984507\n",
      "Iteration 208, loss = 0.54876909\n",
      "Iteration 209, loss = 0.54769855\n",
      "Iteration 210, loss = 0.54663361\n",
      "Iteration 211, loss = 0.54557287\n",
      "Iteration 212, loss = 0.54451680\n",
      "Iteration 213, loss = 0.54346511\n",
      "Iteration 214, loss = 0.54241738\n",
      "Iteration 215, loss = 0.54137555\n",
      "Iteration 216, loss = 0.54033961\n",
      "Iteration 217, loss = 0.53930813\n",
      "Iteration 218, loss = 0.53828039\n",
      "Iteration 219, loss = 0.53725717\n",
      "Iteration 220, loss = 0.53623982\n",
      "Iteration 221, loss = 0.53522846\n",
      "Iteration 222, loss = 0.53422323\n",
      "Iteration 223, loss = 0.53322185\n",
      "Iteration 224, loss = 0.53222524\n",
      "Iteration 225, loss = 0.53123417\n",
      "Iteration 226, loss = 0.53024702\n",
      "Iteration 227, loss = 0.52926308\n",
      "Iteration 228, loss = 0.52828446\n",
      "Iteration 229, loss = 0.52731120\n",
      "Iteration 230, loss = 0.52634332\n",
      "Iteration 231, loss = 0.52538113\n",
      "Iteration 232, loss = 0.52442462\n",
      "Iteration 233, loss = 0.52347355\n",
      "Iteration 234, loss = 0.52252790\n",
      "Iteration 235, loss = 0.52158584\n",
      "Iteration 236, loss = 0.52064856\n",
      "Iteration 237, loss = 0.51971698\n",
      "Iteration 238, loss = 0.51878867\n",
      "Iteration 239, loss = 0.51786414\n",
      "Iteration 240, loss = 0.51694453\n",
      "Iteration 241, loss = 0.51602990\n",
      "Iteration 242, loss = 0.51512027\n",
      "Iteration 243, loss = 0.51421557\n",
      "Iteration 244, loss = 0.51331581\n",
      "Iteration 245, loss = 0.51242099\n",
      "Iteration 246, loss = 0.51153106\n",
      "Iteration 247, loss = 0.51064599\n",
      "Iteration 248, loss = 0.50976581\n",
      "Iteration 249, loss = 0.50889039\n",
      "Iteration 250, loss = 0.50802028\n",
      "Iteration 251, loss = 0.50715633\n",
      "Iteration 252, loss = 0.50629716\n",
      "Iteration 253, loss = 0.50544295\n",
      "Iteration 254, loss = 0.50459374\n",
      "Iteration 255, loss = 0.50374934\n",
      "Iteration 256, loss = 0.50291034\n",
      "Iteration 257, loss = 0.50207640\n",
      "Iteration 258, loss = 0.50124706\n",
      "Iteration 259, loss = 0.50042218\n",
      "Iteration 260, loss = 0.49960171\n",
      "Iteration 261, loss = 0.49878562\n",
      "Iteration 262, loss = 0.49797383\n",
      "Iteration 263, loss = 0.49716629\n",
      "Iteration 264, loss = 0.49636321\n",
      "Iteration 265, loss = 0.49556439\n",
      "Iteration 266, loss = 0.49476975\n",
      "Iteration 267, loss = 0.49397919\n",
      "Iteration 268, loss = 0.49319276\n",
      "Iteration 269, loss = 0.49241049\n",
      "Iteration 270, loss = 0.49163222\n",
      "Iteration 271, loss = 0.49085784\n",
      "Iteration 272, loss = 0.49008732\n",
      "Iteration 273, loss = 0.48932061\n",
      "Iteration 274, loss = 0.48855766\n",
      "Iteration 275, loss = 0.48779844\n",
      "Iteration 276, loss = 0.48704290\n",
      "Iteration 277, loss = 0.48629100\n",
      "Iteration 278, loss = 0.48554270\n",
      "Iteration 279, loss = 0.48479795\n",
      "Iteration 280, loss = 0.48405672\n",
      "Iteration 281, loss = 0.48331896\n",
      "Iteration 282, loss = 0.48258464\n",
      "Iteration 283, loss = 0.48185371\n",
      "Iteration 284, loss = 0.48112614\n",
      "Iteration 285, loss = 0.48040190\n",
      "Iteration 286, loss = 0.47968093\n",
      "Iteration 287, loss = 0.47896321\n",
      "Iteration 288, loss = 0.47824871\n",
      "Iteration 289, loss = 0.47753751\n",
      "Iteration 290, loss = 0.47682945\n",
      "Iteration 291, loss = 0.47612450\n",
      "Iteration 292, loss = 0.47542263\n",
      "Iteration 293, loss = 0.47472386\n",
      "Iteration 294, loss = 0.47402813\n",
      "Iteration 295, loss = 0.47333537\n",
      "Iteration 296, loss = 0.47264555\n",
      "Iteration 297, loss = 0.47195865\n",
      "Iteration 298, loss = 0.47127463\n",
      "Iteration 299, loss = 0.47059346\n",
      "Iteration 300, loss = 0.46991511\n",
      "Iteration 301, loss = 0.46923955\n",
      "Iteration 302, loss = 0.46856675\n",
      "Iteration 303, loss = 0.46789586\n",
      "Iteration 304, loss = 0.46722732\n",
      "Iteration 305, loss = 0.46656138\n",
      "Iteration 306, loss = 0.46589802\n",
      "Iteration 307, loss = 0.46523721\n",
      "Iteration 308, loss = 0.46457895\n",
      "Iteration 309, loss = 0.46392320\n",
      "Iteration 310, loss = 0.46326995\n",
      "Iteration 311, loss = 0.46261831\n",
      "Iteration 312, loss = 0.46196843\n",
      "Iteration 313, loss = 0.46132088\n",
      "Iteration 314, loss = 0.46067564\n",
      "Iteration 315, loss = 0.46003272\n",
      "Iteration 316, loss = 0.45939209\n",
      "Iteration 317, loss = 0.45875375\n",
      "Iteration 318, loss = 0.45811770\n",
      "Iteration 319, loss = 0.45748397\n",
      "Iteration 320, loss = 0.45685249\n",
      "Iteration 321, loss = 0.45622323\n",
      "Iteration 322, loss = 0.45559619\n",
      "Iteration 323, loss = 0.45497139\n",
      "Iteration 324, loss = 0.45434878\n",
      "Iteration 325, loss = 0.45372834\n",
      "Iteration 326, loss = 0.45310806\n",
      "Iteration 327, loss = 0.45248831\n",
      "Iteration 328, loss = 0.45187043\n",
      "Iteration 329, loss = 0.45125447\n",
      "Iteration 330, loss = 0.45064049\n",
      "Iteration 331, loss = 0.45002901\n",
      "Iteration 332, loss = 0.44941948\n",
      "Iteration 333, loss = 0.44881187\n",
      "Iteration 334, loss = 0.44820538\n",
      "Iteration 335, loss = 0.44760052\n",
      "Iteration 336, loss = 0.44699751\n",
      "Iteration 337, loss = 0.44639634\n",
      "Iteration 338, loss = 0.44579703\n",
      "Iteration 339, loss = 0.44519958\n",
      "Iteration 340, loss = 0.44460398\n",
      "Iteration 341, loss = 0.44401022\n",
      "Iteration 342, loss = 0.44341831\n",
      "Iteration 343, loss = 0.44282825\n",
      "Iteration 344, loss = 0.44224002\n",
      "Iteration 345, loss = 0.44165360\n",
      "Iteration 346, loss = 0.44106899\n",
      "Iteration 347, loss = 0.44048619\n",
      "Iteration 348, loss = 0.43990519\n",
      "Iteration 349, loss = 0.43932597\n",
      "Iteration 350, loss = 0.43874850\n",
      "Iteration 351, loss = 0.43817278\n",
      "Iteration 352, loss = 0.43759880\n",
      "Iteration 353, loss = 0.43702655\n",
      "Iteration 354, loss = 0.43645603\n",
      "Iteration 355, loss = 0.43588720\n",
      "Iteration 356, loss = 0.43532025\n",
      "Iteration 357, loss = 0.43475364\n",
      "Iteration 358, loss = 0.43418862\n",
      "Iteration 359, loss = 0.43362404\n",
      "Iteration 360, loss = 0.43305982\n",
      "Iteration 361, loss = 0.43249700\n",
      "Iteration 362, loss = 0.43193560\n",
      "Iteration 363, loss = 0.43137565\n",
      "Iteration 364, loss = 0.43081718\n",
      "Iteration 365, loss = 0.43025941\n",
      "Iteration 366, loss = 0.42970258\n",
      "Iteration 367, loss = 0.42914713\n",
      "Iteration 368, loss = 0.42859307\n",
      "Iteration 369, loss = 0.42804041\n",
      "Iteration 370, loss = 0.42748913\n",
      "Iteration 371, loss = 0.42693782\n",
      "Iteration 372, loss = 0.42638742\n",
      "Iteration 373, loss = 0.42583759\n",
      "Iteration 374, loss = 0.42528784\n",
      "Iteration 375, loss = 0.42473874\n",
      "Iteration 376, loss = 0.42418997\n",
      "Iteration 377, loss = 0.42364188\n",
      "Iteration 378, loss = 0.42309464\n",
      "Iteration 379, loss = 0.42254617\n",
      "Iteration 380, loss = 0.42199869\n",
      "Iteration 381, loss = 0.42145139\n",
      "Iteration 382, loss = 0.42090331\n",
      "Iteration 383, loss = 0.42035075\n",
      "Iteration 384, loss = 0.41979514\n",
      "Iteration 385, loss = 0.41923742\n",
      "Iteration 386, loss = 0.41867890\n",
      "Iteration 387, loss = 0.41812064\n",
      "Iteration 388, loss = 0.41756278\n",
      "Iteration 389, loss = 0.41700494\n",
      "Iteration 390, loss = 0.41644558\n",
      "Iteration 391, loss = 0.41588561\n",
      "Iteration 392, loss = 0.41532556\n",
      "Iteration 393, loss = 0.41476512\n",
      "Iteration 394, loss = 0.41419505\n",
      "Iteration 395, loss = 0.41362480\n",
      "Iteration 396, loss = 0.41305327\n",
      "Iteration 397, loss = 0.41248186\n",
      "Iteration 398, loss = 0.41191071\n",
      "Iteration 399, loss = 0.41133701\n",
      "Iteration 400, loss = 0.41076320\n",
      "Iteration 401, loss = 0.41018980\n",
      "Iteration 402, loss = 0.40961693\n",
      "Iteration 403, loss = 0.40904470\n",
      "Iteration 404, loss = 0.40847321\n",
      "Iteration 405, loss = 0.40790256\n",
      "Iteration 406, loss = 0.40733053\n",
      "Iteration 407, loss = 0.40675497\n",
      "Iteration 408, loss = 0.40617885\n",
      "Iteration 409, loss = 0.40560333\n",
      "Iteration 410, loss = 0.40502851\n",
      "Iteration 411, loss = 0.40445345\n",
      "Iteration 412, loss = 0.40387760\n",
      "Iteration 413, loss = 0.40330253\n",
      "Iteration 414, loss = 0.40272833\n",
      "Iteration 415, loss = 0.40215507\n",
      "Iteration 416, loss = 0.40158284\n",
      "Iteration 417, loss = 0.40101096\n",
      "Iteration 418, loss = 0.40043983\n",
      "Iteration 419, loss = 0.39986834\n",
      "Iteration 420, loss = 0.39929598\n",
      "Iteration 421, loss = 0.39872338\n",
      "Iteration 422, loss = 0.39815173\n",
      "Iteration 423, loss = 0.39758108\n",
      "Iteration 424, loss = 0.39701150\n",
      "Iteration 425, loss = 0.39644305\n",
      "Iteration 426, loss = 0.39587577\n",
      "Iteration 427, loss = 0.39530970\n",
      "Iteration 428, loss = 0.39474486\n",
      "Iteration 429, loss = 0.39418130\n",
      "Iteration 430, loss = 0.39361903\n",
      "Iteration 431, loss = 0.39305809\n",
      "Iteration 432, loss = 0.39249847\n",
      "Iteration 433, loss = 0.39194021\n",
      "Iteration 434, loss = 0.39138330\n",
      "Iteration 435, loss = 0.39082777\n",
      "Iteration 436, loss = 0.39027362\n",
      "Iteration 437, loss = 0.38972085\n",
      "Iteration 438, loss = 0.38916947\n",
      "Iteration 439, loss = 0.38861948\n",
      "Iteration 440, loss = 0.38807089\n",
      "Iteration 441, loss = 0.38752368\n",
      "Iteration 442, loss = 0.38697787\n",
      "Iteration 443, loss = 0.38643344\n",
      "Iteration 444, loss = 0.38589040\n",
      "Iteration 445, loss = 0.38534873\n",
      "Iteration 446, loss = 0.38480844\n",
      "Iteration 447, loss = 0.38426953\n",
      "Iteration 448, loss = 0.38373197\n",
      "Iteration 449, loss = 0.38319578\n",
      "Iteration 450, loss = 0.38266094\n",
      "Iteration 451, loss = 0.38212661\n",
      "Iteration 452, loss = 0.38159190\n",
      "Iteration 453, loss = 0.38105835\n",
      "Iteration 454, loss = 0.38052598\n",
      "Iteration 455, loss = 0.37999480\n",
      "Iteration 456, loss = 0.37946481\n",
      "Iteration 457, loss = 0.37893603\n",
      "Iteration 458, loss = 0.37840846\n",
      "Iteration 459, loss = 0.37788210\n",
      "Iteration 460, loss = 0.37735696\n",
      "Iteration 461, loss = 0.37683302\n",
      "Iteration 462, loss = 0.37631030\n",
      "Iteration 463, loss = 0.37578880\n",
      "Iteration 464, loss = 0.37526853\n",
      "Iteration 465, loss = 0.37474947\n",
      "Iteration 466, loss = 0.37423161\n",
      "Iteration 467, loss = 0.37372307\n",
      "Iteration 468, loss = 0.37322344\n",
      "Iteration 469, loss = 0.37272582\n",
      "Iteration 470, loss = 0.37223014\n",
      "Iteration 471, loss = 0.37173634\n",
      "Iteration 472, loss = 0.37124434\n",
      "Iteration 473, loss = 0.37075409\n",
      "Iteration 474, loss = 0.37026550\n",
      "Iteration 475, loss = 0.36977851\n",
      "Iteration 476, loss = 0.36930425\n",
      "Iteration 477, loss = 0.36883517\n",
      "Iteration 478, loss = 0.36837009\n",
      "Iteration 479, loss = 0.36791683\n",
      "Iteration 480, loss = 0.36746547\n",
      "Iteration 481, loss = 0.36701599\n",
      "Iteration 482, loss = 0.36657269\n",
      "Iteration 483, loss = 0.36613549\n",
      "Iteration 484, loss = 0.36570198\n",
      "Iteration 485, loss = 0.36527267\n",
      "Iteration 486, loss = 0.36484410\n",
      "Iteration 487, loss = 0.36441855\n",
      "Iteration 488, loss = 0.36399268\n",
      "Iteration 489, loss = 0.36356660\n",
      "Iteration 490, loss = 0.36314044\n",
      "Iteration 491, loss = 0.36271422\n",
      "Iteration 492, loss = 0.36228793\n",
      "Iteration 493, loss = 0.36186151\n",
      "Iteration 494, loss = 0.36143671\n",
      "Iteration 495, loss = 0.36101247\n",
      "Iteration 496, loss = 0.36058834\n",
      "Iteration 497, loss = 0.36016441\n",
      "Iteration 498, loss = 0.35974081\n",
      "Iteration 499, loss = 0.35931770\n",
      "Iteration 500, loss = 0.35889547\n",
      "Iteration 501, loss = 0.35847452\n",
      "Iteration 502, loss = 0.35805442\n",
      "Iteration 503, loss = 0.35763518\n",
      "Iteration 504, loss = 0.35721690\n",
      "Iteration 505, loss = 0.35679987\n",
      "Iteration 506, loss = 0.35638362\n",
      "Iteration 507, loss = 0.35596808\n",
      "Iteration 508, loss = 0.35555322\n",
      "Iteration 509, loss = 0.35513900\n",
      "Iteration 510, loss = 0.35472540\n",
      "Iteration 511, loss = 0.35431332\n",
      "Iteration 512, loss = 0.35390210\n",
      "Iteration 513, loss = 0.35349141\n",
      "Iteration 514, loss = 0.35308169\n",
      "Iteration 515, loss = 0.35267144\n",
      "Iteration 516, loss = 0.35226234\n",
      "Iteration 517, loss = 0.35185383\n",
      "Iteration 518, loss = 0.35144594\n",
      "Iteration 519, loss = 0.35104119\n",
      "Iteration 520, loss = 0.35063775\n",
      "Iteration 521, loss = 0.35023532\n",
      "Iteration 522, loss = 0.34983381\n",
      "Iteration 523, loss = 0.34943300\n",
      "Iteration 524, loss = 0.34903292\n",
      "Iteration 525, loss = 0.34863381\n",
      "Iteration 526, loss = 0.34823520\n",
      "Iteration 527, loss = 0.34783741\n",
      "Iteration 528, loss = 0.34744290\n",
      "Iteration 529, loss = 0.34704964\n",
      "Iteration 530, loss = 0.34665728\n",
      "Iteration 531, loss = 0.34626553\n",
      "Iteration 532, loss = 0.34587491\n",
      "Iteration 533, loss = 0.34548469\n",
      "Iteration 534, loss = 0.34509544\n",
      "Iteration 535, loss = 0.34470722\n",
      "Iteration 536, loss = 0.34432206\n",
      "Iteration 537, loss = 0.34393739\n",
      "Iteration 538, loss = 0.34355373\n",
      "Iteration 539, loss = 0.34317055\n",
      "Iteration 540, loss = 0.34278837\n",
      "Iteration 541, loss = 0.34240660\n",
      "Iteration 542, loss = 0.34202574\n",
      "Iteration 543, loss = 0.34164531\n",
      "Iteration 544, loss = 0.34126584\n",
      "Iteration 545, loss = 0.34088672\n",
      "Iteration 546, loss = 0.34050838\n",
      "Iteration 547, loss = 0.34013051\n",
      "Iteration 548, loss = 0.33975353\n",
      "Iteration 549, loss = 0.33937680\n",
      "Iteration 550, loss = 0.33900107\n",
      "Iteration 551, loss = 0.33862561\n",
      "Iteration 552, loss = 0.33825066\n",
      "Iteration 553, loss = 0.33787657\n",
      "Iteration 554, loss = 0.33750277\n",
      "Iteration 555, loss = 0.33712983\n",
      "Iteration 556, loss = 0.33675738\n",
      "Iteration 557, loss = 0.33638586\n",
      "Iteration 558, loss = 0.33601455\n",
      "Iteration 559, loss = 0.33564394\n",
      "Iteration 560, loss = 0.33527375\n",
      "Iteration 561, loss = 0.33490446\n",
      "Iteration 562, loss = 0.33453536\n",
      "Iteration 563, loss = 0.33416705\n",
      "Iteration 564, loss = 0.33379911\n",
      "Iteration 565, loss = 0.33343197\n",
      "Iteration 566, loss = 0.33306510\n",
      "Iteration 567, loss = 0.33269900\n",
      "Iteration 568, loss = 0.33233319\n",
      "Iteration 569, loss = 0.33196817\n",
      "Iteration 570, loss = 0.33160343\n",
      "Iteration 571, loss = 0.33123938\n",
      "Iteration 572, loss = 0.33087568\n",
      "Iteration 573, loss = 0.33051277\n",
      "Iteration 574, loss = 0.33015005\n",
      "Iteration 575, loss = 0.32978825\n",
      "Iteration 576, loss = 0.32942666\n",
      "Iteration 577, loss = 0.32906549\n",
      "Iteration 578, loss = 0.32870525\n",
      "Iteration 579, loss = 0.32834516\n",
      "Iteration 580, loss = 0.32798556\n",
      "Iteration 581, loss = 0.32762662\n",
      "Iteration 582, loss = 0.32726819\n",
      "Iteration 583, loss = 0.32691012\n",
      "Iteration 584, loss = 0.32655285\n",
      "Iteration 585, loss = 0.32619575\n",
      "Iteration 586, loss = 0.32583938\n",
      "Iteration 587, loss = 0.32548334\n",
      "Iteration 588, loss = 0.32512792\n",
      "Iteration 589, loss = 0.32477289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 590, loss = 0.32441851\n",
      "Iteration 591, loss = 0.32406447\n",
      "Iteration 592, loss = 0.32371104\n",
      "Iteration 593, loss = 0.32335806\n",
      "Iteration 594, loss = 0.32300553\n",
      "Iteration 595, loss = 0.32265365\n",
      "Iteration 596, loss = 0.32230207\n",
      "Iteration 597, loss = 0.32195144\n",
      "Iteration 598, loss = 0.32160103\n",
      "Iteration 599, loss = 0.32125142\n",
      "Iteration 600, loss = 0.32090209\n",
      "Iteration 601, loss = 0.32055323\n",
      "Iteration 602, loss = 0.32020507\n",
      "Iteration 603, loss = 0.31985719\n",
      "Iteration 604, loss = 0.31951012\n",
      "Iteration 605, loss = 0.31916318\n",
      "Iteration 606, loss = 0.31881700\n",
      "Iteration 607, loss = 0.31847116\n",
      "Iteration 608, loss = 0.31812575\n",
      "Iteration 609, loss = 0.31778118\n",
      "Iteration 610, loss = 0.31743670\n",
      "Iteration 611, loss = 0.31709292\n",
      "Iteration 612, loss = 0.31674954\n",
      "Iteration 613, loss = 0.31640658\n",
      "Iteration 614, loss = 0.31606436\n",
      "Iteration 615, loss = 0.31572352\n",
      "Iteration 616, loss = 0.31538364\n",
      "Iteration 617, loss = 0.31504536\n",
      "Iteration 618, loss = 0.31470789\n",
      "Iteration 619, loss = 0.31437165\n",
      "Iteration 620, loss = 0.31403638\n",
      "Iteration 621, loss = 0.31370163\n",
      "Iteration 622, loss = 0.31336753\n",
      "Iteration 623, loss = 0.31303380\n",
      "Iteration 624, loss = 0.31270077\n",
      "Iteration 625, loss = 0.31236799\n",
      "Iteration 626, loss = 0.31203593\n",
      "Iteration 627, loss = 0.31170425\n",
      "Iteration 628, loss = 0.31137304\n",
      "Iteration 629, loss = 0.31104248\n",
      "Iteration 630, loss = 0.31071221\n",
      "Iteration 631, loss = 0.31038251\n",
      "Iteration 632, loss = 0.31005339\n",
      "Iteration 633, loss = 0.30972462\n",
      "Iteration 634, loss = 0.30939628\n",
      "Iteration 635, loss = 0.30906863\n",
      "Iteration 636, loss = 0.30874127\n",
      "Iteration 637, loss = 0.30841442\n",
      "Iteration 638, loss = 0.30808810\n",
      "Iteration 639, loss = 0.30776216\n",
      "Iteration 640, loss = 0.30743686\n",
      "Iteration 641, loss = 0.30711180\n",
      "Iteration 642, loss = 0.30678734\n",
      "Iteration 643, loss = 0.30646339\n",
      "Iteration 644, loss = 0.30613981\n",
      "Iteration 645, loss = 0.30581666\n",
      "Iteration 646, loss = 0.30549420\n",
      "Iteration 647, loss = 0.30517188\n",
      "Iteration 648, loss = 0.30485027\n",
      "Iteration 649, loss = 0.30452902\n",
      "Iteration 650, loss = 0.30420821\n",
      "Iteration 651, loss = 0.30388785\n",
      "Iteration 652, loss = 0.30356810\n",
      "Iteration 653, loss = 0.30324867\n",
      "Iteration 654, loss = 0.30292966\n",
      "Iteration 655, loss = 0.30261128\n",
      "Iteration 656, loss = 0.30229316\n",
      "Iteration 657, loss = 0.30197557\n",
      "Iteration 658, loss = 0.30165855\n",
      "Iteration 659, loss = 0.30134187\n",
      "Iteration 660, loss = 0.30102561\n",
      "Iteration 661, loss = 0.30070983\n",
      "Iteration 662, loss = 0.30039458\n",
      "Iteration 663, loss = 0.30007969\n",
      "Iteration 664, loss = 0.29976525\n",
      "Iteration 665, loss = 0.29945140\n",
      "Iteration 666, loss = 0.29913788\n",
      "Iteration 667, loss = 0.29882479\n",
      "Iteration 668, loss = 0.29851224\n",
      "Iteration 669, loss = 0.29820009\n",
      "Iteration 670, loss = 0.29788837\n",
      "Iteration 671, loss = 0.29757717\n",
      "Iteration 672, loss = 0.29726681\n",
      "Iteration 673, loss = 0.29695681\n",
      "Iteration 674, loss = 0.29664723\n",
      "Iteration 675, loss = 0.29633810\n",
      "Iteration 676, loss = 0.29602948\n",
      "Iteration 677, loss = 0.29572124\n",
      "Iteration 678, loss = 0.29541341\n",
      "Iteration 679, loss = 0.29510608\n",
      "Iteration 680, loss = 0.29479919\n",
      "Iteration 681, loss = 0.29449272\n",
      "Iteration 682, loss = 0.29418667\n",
      "Iteration 683, loss = 0.29388114\n",
      "Iteration 684, loss = 0.29357601\n",
      "Iteration 685, loss = 0.29327132\n",
      "Iteration 686, loss = 0.29296706\n",
      "Iteration 687, loss = 0.29266327\n",
      "Iteration 688, loss = 0.29235998\n",
      "Iteration 689, loss = 0.29205708\n",
      "Iteration 690, loss = 0.29175460\n",
      "Iteration 691, loss = 0.29145257\n",
      "Iteration 692, loss = 0.29115107\n",
      "Iteration 693, loss = 0.29084993\n",
      "Iteration 694, loss = 0.29054926\n",
      "Iteration 695, loss = 0.29024902\n",
      "Iteration 696, loss = 0.28994922\n",
      "Iteration 697, loss = 0.28965005\n",
      "Iteration 698, loss = 0.28935109\n",
      "Iteration 699, loss = 0.28905266\n",
      "Iteration 700, loss = 0.28875469\n",
      "Iteration 701, loss = 0.28845714\n",
      "Iteration 702, loss = 0.28816018\n",
      "Iteration 703, loss = 0.28786346\n",
      "Iteration 704, loss = 0.28756726\n",
      "Iteration 705, loss = 0.28727152\n",
      "Iteration 706, loss = 0.28697618\n",
      "Iteration 707, loss = 0.28668130\n",
      "Iteration 708, loss = 0.28638696\n",
      "Iteration 709, loss = 0.28609291\n",
      "Iteration 710, loss = 0.28579938\n",
      "Iteration 711, loss = 0.28550627\n",
      "Iteration 712, loss = 0.28521360\n",
      "Iteration 713, loss = 0.28492136\n",
      "Iteration 714, loss = 0.28462956\n",
      "Iteration 715, loss = 0.28433826\n",
      "Iteration 716, loss = 0.28404738\n",
      "Iteration 717, loss = 0.28375690\n",
      "Iteration 718, loss = 0.28346686\n",
      "Iteration 719, loss = 0.28317726\n",
      "Iteration 720, loss = 0.28288810\n",
      "Iteration 721, loss = 0.28259937\n",
      "Iteration 722, loss = 0.28231109\n",
      "Iteration 723, loss = 0.28202333\n",
      "Iteration 724, loss = 0.28173587\n",
      "Iteration 725, loss = 0.28144891\n",
      "Iteration 726, loss = 0.28116238\n",
      "Iteration 727, loss = 0.28087628\n",
      "Iteration 728, loss = 0.28059061\n",
      "Iteration 729, loss = 0.28030539\n",
      "Iteration 730, loss = 0.28002059\n",
      "Iteration 731, loss = 0.27973623\n",
      "Iteration 732, loss = 0.27945231\n",
      "Iteration 733, loss = 0.27916883\n",
      "Iteration 734, loss = 0.27888579\n",
      "Iteration 735, loss = 0.27860317\n",
      "Iteration 736, loss = 0.27832099\n",
      "Iteration 737, loss = 0.27803924\n",
      "Iteration 738, loss = 0.27775792\n",
      "Iteration 739, loss = 0.27747703\n",
      "Iteration 740, loss = 0.27719657\n",
      "Iteration 741, loss = 0.27691654\n",
      "Iteration 742, loss = 0.27663694\n",
      "Iteration 743, loss = 0.27635803\n",
      "Iteration 744, loss = 0.27607965\n",
      "Iteration 745, loss = 0.27580172\n",
      "Iteration 746, loss = 0.27552424\n",
      "Iteration 747, loss = 0.27524720\n",
      "Iteration 748, loss = 0.27497062\n",
      "Iteration 749, loss = 0.27469446\n",
      "Iteration 750, loss = 0.27441875\n",
      "Iteration 751, loss = 0.27414347\n",
      "Iteration 752, loss = 0.27386863\n",
      "Iteration 753, loss = 0.27359423\n",
      "Iteration 754, loss = 0.27332025\n",
      "Iteration 755, loss = 0.27304671\n",
      "Iteration 756, loss = 0.27277360\n",
      "Iteration 757, loss = 0.27250092\n",
      "Iteration 758, loss = 0.27222867\n",
      "Iteration 759, loss = 0.27195685\n",
      "Iteration 760, loss = 0.27168545\n",
      "Iteration 761, loss = 0.27141449\n",
      "Iteration 762, loss = 0.27114394\n",
      "Iteration 763, loss = 0.27087383\n",
      "Iteration 764, loss = 0.27060414\n",
      "Iteration 765, loss = 0.27033487\n",
      "Iteration 766, loss = 0.27006602\n",
      "Iteration 767, loss = 0.26979760\n",
      "Iteration 768, loss = 0.26952960\n",
      "Iteration 769, loss = 0.26926202\n",
      "Iteration 770, loss = 0.26899487\n",
      "Iteration 771, loss = 0.26872813\n",
      "Iteration 772, loss = 0.26846181\n",
      "Iteration 773, loss = 0.26819591\n",
      "Iteration 774, loss = 0.26793043\n",
      "Iteration 775, loss = 0.26766537\n",
      "Iteration 776, loss = 0.26740072\n",
      "Iteration 777, loss = 0.26713649\n",
      "Iteration 778, loss = 0.26687268\n",
      "Iteration 779, loss = 0.26660928\n",
      "Iteration 780, loss = 0.26634630\n",
      "Iteration 781, loss = 0.26608373\n",
      "Iteration 782, loss = 0.26582158\n",
      "Iteration 783, loss = 0.26555992\n",
      "Iteration 784, loss = 0.26529878\n",
      "Iteration 785, loss = 0.26503805\n",
      "Iteration 786, loss = 0.26477774\n",
      "Iteration 787, loss = 0.26451786\n",
      "Iteration 788, loss = 0.26425838\n",
      "Iteration 789, loss = 0.26399933\n",
      "Iteration 790, loss = 0.26374069\n",
      "Iteration 791, loss = 0.26348247\n",
      "Iteration 792, loss = 0.26322466\n",
      "Iteration 793, loss = 0.26296726\n",
      "Iteration 794, loss = 0.26271027\n",
      "Iteration 795, loss = 0.26245369\n",
      "Iteration 796, loss = 0.26219753\n",
      "Iteration 797, loss = 0.26194177\n",
      "Iteration 798, loss = 0.26168642\n",
      "Iteration 799, loss = 0.26143149\n",
      "Iteration 800, loss = 0.26117695\n",
      "Iteration 801, loss = 0.26092283\n",
      "Iteration 802, loss = 0.26066911\n",
      "Iteration 803, loss = 0.26041579\n",
      "Iteration 804, loss = 0.26016289\n",
      "Iteration 805, loss = 0.25991038\n",
      "Iteration 806, loss = 0.25965828\n",
      "Iteration 807, loss = 0.25940658\n",
      "Iteration 808, loss = 0.25915529\n",
      "Iteration 809, loss = 0.25890439\n",
      "Iteration 810, loss = 0.25865390\n",
      "Iteration 811, loss = 0.25840381\n",
      "Iteration 812, loss = 0.25815412\n",
      "Iteration 813, loss = 0.25790483\n",
      "Iteration 814, loss = 0.25765619\n",
      "Iteration 815, loss = 0.25740767\n",
      "Iteration 816, loss = 0.25715963\n",
      "Iteration 817, loss = 0.25691204\n",
      "Iteration 818, loss = 0.25666483\n",
      "Iteration 819, loss = 0.25641808\n",
      "Iteration 820, loss = 0.25617173\n",
      "Iteration 821, loss = 0.25592573\n",
      "Iteration 822, loss = 0.25568011\n",
      "Iteration 823, loss = 0.25543496\n",
      "Iteration 824, loss = 0.25519017\n",
      "Iteration 825, loss = 0.25494576\n",
      "Iteration 826, loss = 0.25470172\n",
      "Iteration 827, loss = 0.25445822\n",
      "Iteration 828, loss = 0.25421500\n",
      "Iteration 829, loss = 0.25397216\n",
      "Iteration 830, loss = 0.25372972\n",
      "Iteration 831, loss = 0.25348777\n",
      "Iteration 832, loss = 0.25324613\n",
      "Iteration 833, loss = 0.25300487\n",
      "Iteration 834, loss = 0.25276412\n",
      "Iteration 835, loss = 0.25252364\n",
      "Iteration 836, loss = 0.25228358\n",
      "Iteration 837, loss = 0.25204393\n",
      "Iteration 838, loss = 0.25180475\n",
      "Iteration 839, loss = 0.25156588\n",
      "Iteration 840, loss = 0.25132738\n",
      "Iteration 841, loss = 0.25108925\n",
      "Iteration 842, loss = 0.25085171\n",
      "Iteration 843, loss = 0.25061427\n",
      "Iteration 844, loss = 0.25037743\n",
      "Iteration 845, loss = 0.25014103\n",
      "Iteration 846, loss = 0.24990494\n",
      "Iteration 847, loss = 0.24966921\n",
      "Iteration 848, loss = 0.24943383\n",
      "Iteration 849, loss = 0.24919905\n",
      "Iteration 850, loss = 0.24896435\n",
      "Iteration 851, loss = 0.24873025\n",
      "Iteration 852, loss = 0.24849648\n",
      "Iteration 853, loss = 0.24826307\n",
      "Iteration 854, loss = 0.24803001\n",
      "Iteration 855, loss = 0.24779752\n",
      "Iteration 856, loss = 0.24756560\n",
      "Iteration 857, loss = 0.24733419\n",
      "Iteration 858, loss = 0.24710322\n",
      "Iteration 859, loss = 0.24687252\n",
      "Iteration 860, loss = 0.24664216\n",
      "Iteration 861, loss = 0.24641238\n",
      "Iteration 862, loss = 0.24618274\n",
      "Iteration 863, loss = 0.24595370\n",
      "Iteration 864, loss = 0.24572491\n",
      "Iteration 865, loss = 0.24549644\n",
      "Iteration 866, loss = 0.24526856\n",
      "Iteration 867, loss = 0.24504085\n",
      "Iteration 868, loss = 0.24481378\n",
      "Iteration 869, loss = 0.24458687\n",
      "Iteration 870, loss = 0.24436037\n",
      "Iteration 871, loss = 0.24413429\n",
      "Iteration 872, loss = 0.24390863\n",
      "Iteration 873, loss = 0.24368325\n",
      "Iteration 874, loss = 0.24345842\n",
      "Iteration 875, loss = 0.24323377\n",
      "Iteration 876, loss = 0.24300974\n",
      "Iteration 877, loss = 0.24278588\n",
      "Iteration 878, loss = 0.24256245\n",
      "Iteration 879, loss = 0.24233940\n",
      "Iteration 880, loss = 0.24211682\n",
      "Iteration 881, loss = 0.24189447\n",
      "Iteration 882, loss = 0.24167271\n",
      "Iteration 883, loss = 0.24145113\n",
      "Iteration 884, loss = 0.24123001\n",
      "Iteration 885, loss = 0.24100918\n",
      "Iteration 886, loss = 0.24078893\n",
      "Iteration 887, loss = 0.24056884\n",
      "Iteration 888, loss = 0.24034926\n",
      "Iteration 889, loss = 0.24012993\n",
      "Iteration 890, loss = 0.23991112\n",
      "Iteration 891, loss = 0.23969252\n",
      "Iteration 892, loss = 0.23947450\n",
      "Iteration 893, loss = 0.23925667\n",
      "Iteration 894, loss = 0.23903927\n",
      "Iteration 895, loss = 0.23882218\n",
      "Iteration 896, loss = 0.23860563\n",
      "Iteration 897, loss = 0.23838926\n",
      "Iteration 898, loss = 0.23817341\n",
      "Iteration 899, loss = 0.23795778\n",
      "Iteration 900, loss = 0.23774266\n",
      "Iteration 901, loss = 0.23752777\n",
      "Iteration 902, loss = 0.23731339\n",
      "Iteration 903, loss = 0.23709921\n",
      "Iteration 904, loss = 0.23688560\n",
      "Iteration 905, loss = 0.23667218\n",
      "Iteration 906, loss = 0.23645915\n",
      "Iteration 907, loss = 0.23624652\n",
      "Iteration 908, loss = 0.23603424\n",
      "Iteration 909, loss = 0.23582234\n",
      "Iteration 910, loss = 0.23561076\n",
      "Iteration 911, loss = 0.23539961\n",
      "Iteration 912, loss = 0.23518871\n",
      "Iteration 913, loss = 0.23497837\n",
      "Iteration 914, loss = 0.23476817\n",
      "Iteration 915, loss = 0.23455844\n",
      "Iteration 916, loss = 0.23434900\n",
      "Iteration 917, loss = 0.23413998\n",
      "Iteration 918, loss = 0.23393126\n",
      "Iteration 919, loss = 0.23372295\n",
      "Iteration 920, loss = 0.23351498\n",
      "Iteration 921, loss = 0.23330732\n",
      "Iteration 922, loss = 0.23310014\n",
      "Iteration 923, loss = 0.23289317\n",
      "Iteration 924, loss = 0.23268664\n",
      "Iteration 925, loss = 0.23248039\n",
      "Iteration 926, loss = 0.23227457\n",
      "Iteration 927, loss = 0.23206905\n",
      "Iteration 928, loss = 0.23186387\n",
      "Iteration 929, loss = 0.23165915\n",
      "Iteration 930, loss = 0.23145465\n",
      "Iteration 931, loss = 0.23125056\n",
      "Iteration 932, loss = 0.23104679\n",
      "Iteration 933, loss = 0.23084337\n",
      "Iteration 934, loss = 0.23064036\n",
      "Iteration 935, loss = 0.23043761\n",
      "Iteration 936, loss = 0.23023529\n",
      "Iteration 937, loss = 0.23003323\n",
      "Iteration 938, loss = 0.22983158\n",
      "Iteration 939, loss = 0.22963027\n",
      "Iteration 940, loss = 0.22942925\n",
      "Iteration 941, loss = 0.22922867\n",
      "Iteration 942, loss = 0.22902833\n",
      "Iteration 943, loss = 0.22882838\n",
      "Iteration 944, loss = 0.22862883\n",
      "Iteration 945, loss = 0.22842953\n",
      "Iteration 946, loss = 0.22823058\n",
      "Iteration 947, loss = 0.22803205\n",
      "Iteration 948, loss = 0.22783376\n",
      "Iteration 949, loss = 0.22763591\n",
      "Iteration 950, loss = 0.22743831\n",
      "Iteration 951, loss = 0.22724108\n",
      "Iteration 952, loss = 0.22704426\n",
      "Iteration 953, loss = 0.22684768\n",
      "Iteration 954, loss = 0.22665142\n",
      "Iteration 955, loss = 0.22645570\n",
      "Iteration 956, loss = 0.22626004\n",
      "Iteration 957, loss = 0.22606492\n",
      "Iteration 958, loss = 0.22587005\n",
      "Iteration 959, loss = 0.22567549\n",
      "Iteration 960, loss = 0.22548125\n",
      "Iteration 961, loss = 0.22528746\n",
      "Iteration 962, loss = 0.22509390\n",
      "Iteration 963, loss = 0.22490071\n",
      "Iteration 964, loss = 0.22470786\n",
      "Iteration 965, loss = 0.22451530\n",
      "Iteration 966, loss = 0.22432315\n",
      "Iteration 967, loss = 0.22413127\n",
      "Iteration 968, loss = 0.22393971\n",
      "Iteration 969, loss = 0.22374854\n",
      "Iteration 970, loss = 0.22355766\n",
      "Iteration 971, loss = 0.22336709\n",
      "Iteration 972, loss = 0.22317688\n",
      "Iteration 973, loss = 0.22298702\n",
      "Iteration 974, loss = 0.22279744\n",
      "Iteration 975, loss = 0.22260816\n",
      "Iteration 976, loss = 0.22241936\n",
      "Iteration 977, loss = 0.22223068\n",
      "Iteration 978, loss = 0.22204243\n",
      "Iteration 979, loss = 0.22185455\n",
      "Iteration 980, loss = 0.22166694\n",
      "Iteration 981, loss = 0.22147963\n",
      "Iteration 982, loss = 0.22129262\n",
      "Iteration 983, loss = 0.22110610\n",
      "Iteration 984, loss = 0.22091967\n",
      "Iteration 985, loss = 0.22073371\n",
      "Iteration 986, loss = 0.22054806\n",
      "Iteration 987, loss = 0.22036271\n",
      "Iteration 988, loss = 0.22017765\n",
      "Iteration 989, loss = 0.21999290\n",
      "Iteration 990, loss = 0.21980861\n",
      "Iteration 991, loss = 0.21962445\n",
      "Iteration 992, loss = 0.21944067\n",
      "Iteration 993, loss = 0.21925732\n",
      "Iteration 994, loss = 0.21907415\n",
      "Iteration 995, loss = 0.21889134\n",
      "Iteration 996, loss = 0.21870883\n",
      "Iteration 997, loss = 0.21852671\n",
      "Iteration 998, loss = 0.21834483\n",
      "Iteration 999, loss = 0.21816328\n",
      "Iteration 1000, loss = 0.21798203\n",
      "Iteration 1, loss = 2.13781129\n",
      "Iteration 2, loss = 1.14757506\n",
      "Iteration 3, loss = 1.27893867\n",
      "Iteration 4, loss = 1.18115237\n",
      "Iteration 5, loss = 1.04508546\n",
      "Iteration 6, loss = 0.99081576\n",
      "Iteration 7, loss = 0.96707488\n",
      "Iteration 8, loss = 0.89592426\n",
      "Iteration 9, loss = 0.84436369\n",
      "Iteration 10, loss = 0.83234768\n",
      "Iteration 11, loss = 0.79588320\n",
      "Iteration 12, loss = 0.71871495\n",
      "Iteration 13, loss = 0.64932540\n",
      "Iteration 14, loss = 0.60070477\n",
      "Iteration 15, loss = 0.53726808\n",
      "Iteration 16, loss = 0.46170830\n",
      "Iteration 17, loss = 0.41757738\n",
      "Iteration 18, loss = 0.39654289\n",
      "Iteration 19, loss = 0.36742789\n",
      "Iteration 20, loss = 0.33638818\n",
      "Iteration 21, loss = 0.31041164\n",
      "Iteration 22, loss = 0.27262308\n",
      "Iteration 23, loss = 0.24596371\n",
      "Iteration 24, loss = 0.21122753\n",
      "Iteration 25, loss = 0.18950172\n",
      "Iteration 26, loss = 0.15951834\n",
      "Iteration 27, loss = 0.14534874\n",
      "Iteration 28, loss = 0.12381927\n",
      "Iteration 29, loss = 0.11259145\n",
      "Iteration 30, loss = 0.10213223\n",
      "Iteration 31, loss = 0.09074910\n",
      "Iteration 32, loss = 0.08739960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.07782142\n",
      "Iteration 34, loss = 0.07623452\n",
      "Iteration 35, loss = 0.07036117\n",
      "Iteration 36, loss = 0.06836652\n",
      "Iteration 37, loss = 0.06592829\n",
      "Iteration 38, loss = 0.06300508\n",
      "Iteration 39, loss = 0.06261752\n",
      "Iteration 40, loss = 0.05932972\n",
      "Iteration 41, loss = 0.05969452\n",
      "Iteration 42, loss = 0.05714861\n",
      "Iteration 43, loss = 0.05734413\n",
      "Iteration 44, loss = 0.05594075\n",
      "Iteration 45, loss = 0.05525005\n",
      "Iteration 46, loss = 0.05500470\n",
      "Iteration 47, loss = 0.05365918\n",
      "Iteration 48, loss = 0.05397915\n",
      "Iteration 49, loss = 0.05276949\n",
      "Iteration 50, loss = 0.05290866\n",
      "Iteration 51, loss = 0.05235549\n",
      "Iteration 52, loss = 0.05184130\n",
      "Iteration 53, loss = 0.05195758\n",
      "Iteration 54, loss = 0.05114568\n",
      "Iteration 55, loss = 0.05125121\n",
      "Iteration 56, loss = 0.05084659\n",
      "Iteration 57, loss = 0.05051570\n",
      "Iteration 58, loss = 0.05056614\n",
      "Iteration 59, loss = 0.05007039\n",
      "Iteration 60, loss = 0.05008747\n",
      "Iteration 61, loss = 0.04989994\n",
      "Iteration 62, loss = 0.04958249\n",
      "Iteration 63, loss = 0.04964754\n",
      "Iteration 64, loss = 0.04935717\n",
      "Iteration 65, loss = 0.04920876\n",
      "Iteration 66, loss = 0.04920963\n",
      "Iteration 67, loss = 0.04893865\n",
      "Iteration 68, loss = 0.04887436\n",
      "Iteration 69, loss = 0.04882022\n",
      "Iteration 70, loss = 0.04860544\n",
      "Iteration 71, loss = 0.04856668\n",
      "Iteration 72, loss = 0.04849041\n",
      "Iteration 73, loss = 0.04832388\n",
      "Iteration 74, loss = 0.04828574\n",
      "Iteration 75, loss = 0.04820921\n",
      "Iteration 76, loss = 0.04807446\n",
      "Iteration 77, loss = 0.04802909\n",
      "Iteration 78, loss = 0.04796155\n",
      "Iteration 79, loss = 0.04784698\n",
      "Iteration 80, loss = 0.04779185\n",
      "Iteration 81, loss = 0.04773470\n",
      "Iteration 82, loss = 0.04763552\n",
      "Iteration 83, loss = 0.04756968\n",
      "Iteration 84, loss = 0.04751947\n",
      "Iteration 85, loss = 0.04743538\n",
      "Iteration 86, loss = 0.04736015\n",
      "Iteration 87, loss = 0.04730995\n",
      "Iteration 88, loss = 0.04724134\n",
      "Iteration 89, loss = 0.04716230\n",
      "Iteration 90, loss = 0.04710462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(hidden_layer_sizes=(50,),\n",
       "                                     learning_rate_init=0.1, max_iter=1000,\n",
       "                                     random_state=1, solver='sgd', verbose=10),\n",
       "             param_grid={'hidden_layer_sizes': (3, 5, 10, 15),\n",
       "                         'learning_rate_init': (0.1, 0.01, 0.001),\n",
       "                         'solver': ('adam', 'lfbs', 'sgd')})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': None,\n",
       " 'error_score': nan,\n",
       " 'estimator__activation': 'relu',\n",
       " 'estimator__alpha': 0.0001,\n",
       " 'estimator__batch_size': 'auto',\n",
       " 'estimator__beta_1': 0.9,\n",
       " 'estimator__beta_2': 0.999,\n",
       " 'estimator__early_stopping': False,\n",
       " 'estimator__epsilon': 1e-08,\n",
       " 'estimator__hidden_layer_sizes': (50,),\n",
       " 'estimator__learning_rate': 'constant',\n",
       " 'estimator__learning_rate_init': 0.1,\n",
       " 'estimator__max_fun': 15000,\n",
       " 'estimator__max_iter': 1000,\n",
       " 'estimator__momentum': 0.9,\n",
       " 'estimator__n_iter_no_change': 10,\n",
       " 'estimator__nesterovs_momentum': True,\n",
       " 'estimator__power_t': 0.5,\n",
       " 'estimator__random_state': 1,\n",
       " 'estimator__shuffle': True,\n",
       " 'estimator__solver': 'sgd',\n",
       " 'estimator__tol': 0.0001,\n",
       " 'estimator__validation_fraction': 0.1,\n",
       " 'estimator__verbose': 10,\n",
       " 'estimator__warm_start': False,\n",
       " 'estimator': MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=0.1, max_iter=1000,\n",
       "               random_state=1, solver='sgd', verbose=10),\n",
       " 'iid': 'deprecated',\n",
       " 'n_jobs': None,\n",
       " 'param_grid': {'learning_rate_init': (0.1, 0.01, 0.001),\n",
       "  'hidden_layer_sizes': (3, 5, 10, 15),\n",
       "  'solver': ('adam', 'lfbs', 'sgd')},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_enc_train = X\n",
    "X_enc_test = X\n",
    "clf.get_params(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "y_pre = clf.predict(X)\n",
    "print(y_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.980000\n",
      "Test set score: 0.980000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set score: %f\" % clf.score(X_train, y))\n",
    "print(\"Test set score: %f\" % clf.score(X_test, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 48  2]\n",
      " [ 0  1 49]]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "y_predict = clf.predict(X)\n",
    "cm = confusion_matrix(y, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "c8140ce4bc0b7d2950b144ea7c31b351628765ac689ae932c908b21084263577"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}