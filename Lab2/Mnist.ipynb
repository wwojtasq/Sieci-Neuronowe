{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "seed = np.random.seed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = 255\n",
    "x_train = x_train/normalize\n",
    "x_test = x_test/normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[5]\n [0]\n [4]\n ...\n [5]\n [6]\n [8]]\n"
     ]
    }
   ],
   "source": [
    "yy_train_y = y_train.reshape(-1,1)\n",
    "print(yy_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit(yy_train_y)\n",
    "yy_hot_full_train = enc.transform(yy_train_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_test_y = y_test.reshape(-1,1)\n",
    "enc.fit(yy_test_y)\n",
    "yy_hot_full_test = enc.transform(yy_test_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes = (50,), max_iter = 1000, alpha = 1e-4,\n",
    "                   solver = 'adam', verbose = 10, random_state = 1, learning_rate_init = 0.1, tol = 0.1)\n",
    "\n",
    "mlp.out_activation_ = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 1.19203757\n",
      "Iteration 2, loss = 0.78728511\n",
      "Iteration 3, loss = 0.75479509\n",
      "Iteration 4, loss = 0.75738864\n",
      "Iteration 5, loss = 0.75038194\n",
      "Iteration 6, loss = 0.75118480\n",
      "Iteration 7, loss = 0.74215261\n",
      "Iteration 8, loss = 0.76120685\n",
      "Iteration 9, loss = 0.76680486\n",
      "Iteration 10, loss = 0.75487867\n",
      "Iteration 11, loss = 0.76569301\n",
      "Iteration 12, loss = 0.75410941\n",
      "Iteration 13, loss = 0.75126379\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=0.1, max_iter=1000,\n",
       "              random_state=1, tol=0.1, verbose=10)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "mlp.fit(x_train, yy_hot_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set score: 0.831783\n",
      "Test set score: 0.822800\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % mlp.score(x_train, yy_hot_full_train))\n",
    "print(\"Test set score: %f\" % mlp.score(x_test, yy_hot_full_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate_init': (0.1, 0.01),\n",
    "             'hidden_layer_sizes': (20, 100),\n",
    "             'solver': ('lbfgs', 'sgd', 'adam')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ss = 0.39998596\n",
      "Iteration 9, loss = 0.39265065\n",
      "Iteration 10, loss = 0.38489124\n",
      "Iteration 11, loss = 0.37988924\n",
      "Iteration 12, loss = 0.37736222\n",
      "Iteration 13, loss = 0.37119549\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88195112\n",
      "Iteration 2, loss = 0.54886724\n",
      "Iteration 3, loss = 0.50339365\n",
      "Iteration 4, loss = 0.47057993\n",
      "Iteration 5, loss = 0.45149661\n",
      "Iteration 6, loss = 0.43634430\n",
      "Iteration 7, loss = 0.42572195\n",
      "Iteration 8, loss = 0.41544984\n",
      "Iteration 9, loss = 0.40885308\n",
      "Iteration 10, loss = 0.39915186\n",
      "Iteration 11, loss = 0.39572276\n",
      "Iteration 12, loss = 0.39139258\n",
      "Iteration 13, loss = 0.38512772\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88083391\n",
      "Iteration 2, loss = 0.54447423\n",
      "Iteration 3, loss = 0.49975530\n",
      "Iteration 4, loss = 0.47378904\n",
      "Iteration 5, loss = 0.45250235\n",
      "Iteration 6, loss = 0.43734229\n",
      "Iteration 7, loss = 0.42101456\n",
      "Iteration 8, loss = 0.41300623\n",
      "Iteration 9, loss = 0.40728698\n",
      "Iteration 10, loss = 0.39685376\n",
      "Iteration 11, loss = 0.39239278\n",
      "Iteration 12, loss = 0.38699212\n",
      "Iteration 13, loss = 0.38229197\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.91144038\n",
      "Iteration 2, loss = 0.56516572\n",
      "Iteration 3, loss = 0.51519960\n",
      "Iteration 4, loss = 0.48891451\n",
      "Iteration 5, loss = 0.46796496\n",
      "Iteration 6, loss = 0.45120738\n",
      "Iteration 7, loss = 0.43134692\n",
      "Iteration 8, loss = 0.41853957\n",
      "Iteration 9, loss = 0.40595861\n",
      "Iteration 10, loss = 0.39141003\n",
      "Iteration 11, loss = 0.38742112\n",
      "Iteration 12, loss = 0.37737452\n",
      "Iteration 13, loss = 0.36501872\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.57029856\n",
      "Iteration 2, loss = 1.05345161\n",
      "Iteration 3, loss = 1.03443738\n",
      "Iteration 4, loss = 0.99616776\n",
      "Iteration 5, loss = 0.94068822\n",
      "Iteration 6, loss = 0.95477802\n",
      "Iteration 7, loss = 0.95839734\n",
      "Iteration 8, loss = 0.93052192\n",
      "Iteration 9, loss = 0.93546171\n",
      "Iteration 10, loss = 0.93118567\n",
      "Iteration 11, loss = 0.93679736\n",
      "Iteration 12, loss = 0.95204734\n",
      "Iteration 13, loss = 0.94672284\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54626050\n",
      "Iteration 2, loss = 1.07246610\n",
      "Iteration 3, loss = 1.01262133\n",
      "Iteration 4, loss = 0.98528995\n",
      "Iteration 5, loss = 0.98459346\n",
      "Iteration 6, loss = 0.99156914\n",
      "Iteration 7, loss = 0.98901434\n",
      "Iteration 8, loss = 0.98783846\n",
      "Iteration 9, loss = 1.00746964\n",
      "Iteration 10, loss = 1.00407645\n",
      "Iteration 11, loss = 0.97987417\n",
      "Iteration 12, loss = 1.01065801\n",
      "Iteration 13, loss = 0.99861319\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83688818\n",
      "Iteration 2, loss = 1.18771398\n",
      "Iteration 3, loss = 1.13751008\n",
      "Iteration 4, loss = 1.11783484\n",
      "Iteration 5, loss = 1.11967041\n",
      "Iteration 6, loss = 1.12316346\n",
      "Iteration 7, loss = 1.11832985\n",
      "Iteration 8, loss = 1.11743352\n",
      "Iteration 9, loss = 1.11055527\n",
      "Iteration 10, loss = 1.11161794\n",
      "Iteration 11, loss = 1.10776248\n",
      "Iteration 12, loss = 1.12799261\n",
      "Iteration 13, loss = 1.13057580\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.63157624\n",
      "Iteration 2, loss = 1.06910976\n",
      "Iteration 3, loss = 0.99707760\n",
      "Iteration 4, loss = 0.98684334\n",
      "Iteration 5, loss = 0.99576755\n",
      "Iteration 6, loss = 0.99536049\n",
      "Iteration 7, loss = 0.97676303\n",
      "Iteration 8, loss = 0.96623621\n",
      "Iteration 9, loss = 0.97425654\n",
      "Iteration 10, loss = 0.96269075\n",
      "Iteration 11, loss = 0.94316368\n",
      "Iteration 12, loss = 0.94700411\n",
      "Iteration 13, loss = 0.97357044\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82459032\n",
      "Iteration 2, loss = 1.07623342\n",
      "Iteration 3, loss = 0.97424516\n",
      "Iteration 4, loss = 0.97630057\n",
      "Iteration 5, loss = 0.98403082\n",
      "Iteration 6, loss = 0.98896192\n",
      "Iteration 7, loss = 0.98176700\n",
      "Iteration 8, loss = 0.98218882\n",
      "Iteration 9, loss = 0.98843799\n",
      "Iteration 10, loss = 0.97400357\n",
      "Iteration 11, loss = 0.96862315\n",
      "Iteration 12, loss = 0.98767522\n",
      "Iteration 13, loss = 1.00768744\n",
      "Iteration 14, loss = 0.97616187\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58261072\n",
      "Iteration 2, loss = 0.78731647\n",
      "Iteration 3, loss = 0.67727392\n",
      "Iteration 4, loss = 0.61725357\n",
      "Iteration 5, loss = 0.57660929\n",
      "Iteration 6, loss = 0.54516813\n",
      "Iteration 7, loss = 0.52141014\n",
      "Iteration 8, loss = 0.50108244\n",
      "Iteration 9, loss = 0.48456898\n",
      "Iteration 10, loss = 0.47073373\n",
      "Iteration 11, loss = 0.45944188\n",
      "Iteration 12, loss = 0.44873112\n",
      "Iteration 13, loss = 0.44021268\n",
      "Iteration 14, loss = 0.43280599\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.56815840\n",
      "Iteration 2, loss = 0.77756653\n",
      "Iteration 3, loss = 0.66969880\n",
      "Iteration 4, loss = 0.61104177\n",
      "Iteration 5, loss = 0.57153731\n",
      "Iteration 6, loss = 0.53977869\n",
      "Iteration 7, loss = 0.51460244\n",
      "Iteration 8, loss = 0.49378171\n",
      "Iteration 9, loss = 0.47671741\n",
      "Iteration 10, loss = 0.46191047\n",
      "Iteration 11, loss = 0.44976742\n",
      "Iteration 12, loss = 0.43965713\n",
      "Iteration 13, loss = 0.43011720\n",
      "Iteration 14, loss = 0.42215919\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.56703047\n",
      "Iteration 2, loss = 0.77542896\n",
      "Iteration 3, loss = 0.66668899\n",
      "Iteration 4, loss = 0.60830788\n",
      "Iteration 5, loss = 0.57005977\n",
      "Iteration 6, loss = 0.53918109\n",
      "Iteration 7, loss = 0.51545677\n",
      "Iteration 8, loss = 0.49527614\n",
      "Iteration 9, loss = 0.47773725\n",
      "Iteration 10, loss = 0.46371336\n",
      "Iteration 11, loss = 0.45102735\n",
      "Iteration 12, loss = 0.44044140\n",
      "Iteration 13, loss = 0.43028132\n",
      "Iteration 14, loss = 0.42170114\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.56581341\n",
      "Iteration 2, loss = 0.77184350\n",
      "Iteration 3, loss = 0.66006029\n",
      "Iteration 4, loss = 0.59991101\n",
      "Iteration 5, loss = 0.56093903\n",
      "Iteration 6, loss = 0.53011305\n",
      "Iteration 7, loss = 0.50783690\n",
      "Iteration 8, loss = 0.48847985\n",
      "Iteration 9, loss = 0.47250911\n",
      "Iteration 10, loss = 0.45888294\n",
      "Iteration 11, loss = 0.44728960\n",
      "Iteration 12, loss = 0.43749785\n",
      "Iteration 13, loss = 0.42783449\n",
      "Iteration 14, loss = 0.41956337\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58260646\n",
      "Iteration 2, loss = 0.79263873\n",
      "Iteration 3, loss = 0.68324676\n",
      "Iteration 4, loss = 0.62427943\n",
      "Iteration 5, loss = 0.58257414\n",
      "Iteration 6, loss = 0.54949463\n",
      "Iteration 7, loss = 0.52465582\n",
      "Iteration 8, loss = 0.50376063\n",
      "Iteration 9, loss = 0.48662574\n",
      "Iteration 10, loss = 0.47195184\n",
      "Iteration 11, loss = 0.45986942\n",
      "Iteration 12, loss = 0.44921872\n",
      "Iteration 13, loss = 0.43904232\n",
      "Iteration 14, loss = 0.43035817\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13832879\n",
      "Iteration 2, loss = 0.64541710\n",
      "Iteration 3, loss = 0.59696040\n",
      "Iteration 4, loss = 0.56737165\n",
      "Iteration 5, loss = 0.54147082\n",
      "Iteration 6, loss = 0.52022627\n",
      "Iteration 7, loss = 0.50666101\n",
      "Iteration 8, loss = 0.49430266\n",
      "Iteration 9, loss = 0.48989215\n",
      "Iteration 10, loss = 0.47708022\n",
      "Iteration 11, loss = 0.47131741\n",
      "Iteration 12, loss = 0.46334078\n",
      "Iteration 13, loss = 0.45981121\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14438115\n",
      "Iteration 2, loss = 0.63446993\n",
      "Iteration 3, loss = 0.58273029\n",
      "Iteration 4, loss = 0.55217143\n",
      "Iteration 5, loss = 0.52464025\n",
      "Iteration 6, loss = 0.50633773\n",
      "Iteration 7, loss = 0.49393795\n",
      "Iteration 8, loss = 0.48669784\n",
      "Iteration 9, loss = 0.47596072\n",
      "Iteration 10, loss = 0.47192642\n",
      "Iteration 11, loss = 0.46380821\n",
      "Iteration 12, loss = 0.45858216\n",
      "Iteration 13, loss = 0.45557842\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.15029883\n",
      "Iteration 2, loss = 0.64139374\n",
      "Iteration 3, loss = 0.58921370\n",
      "Iteration 4, loss = 0.56006142\n",
      "Iteration 5, loss = 0.53376174\n",
      "Iteration 6, loss = 0.51341154\n",
      "Iteration 7, loss = 0.49948814\n",
      "Iteration 8, loss = 0.49055347\n",
      "Iteration 9, loss = 0.47880745\n",
      "Iteration 10, loss = 0.47003575\n",
      "Iteration 11, loss = 0.46623672\n",
      "Iteration 12, loss = 0.45780319\n",
      "Iteration 13, loss = 0.45345687\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14579206\n",
      "Iteration 2, loss = 0.63750425\n",
      "Iteration 3, loss = 0.58327025\n",
      "Iteration 4, loss = 0.55474439\n",
      "Iteration 5, loss = 0.53306517\n",
      "Iteration 6, loss = 0.50610787\n",
      "Iteration 7, loss = 0.49352718\n",
      "Iteration 8, loss = 0.48068823\n",
      "Iteration 9, loss = 0.46692234\n",
      "Iteration 10, loss = 0.45321606\n",
      "Iteration 11, loss = 0.44967673\n",
      "Iteration 12, loss = 0.44533915\n",
      "Iteration 13, loss = 0.43850976\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17663336\n",
      "Iteration 2, loss = 0.65247818\n",
      "Iteration 3, loss = 0.59766945\n",
      "Iteration 4, loss = 0.56951165\n",
      "Iteration 5, loss = 0.55059524\n",
      "Iteration 6, loss = 0.52602858\n",
      "Iteration 7, loss = 0.51391189\n",
      "Iteration 8, loss = 0.50044106\n",
      "Iteration 9, loss = 0.48361198\n",
      "Iteration 10, loss = 0.47296160\n",
      "Iteration 11, loss = 0.46799549\n",
      "Iteration 12, loss = 0.46312386\n",
      "Iteration 13, loss = 0.45829569\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63530836\n",
      "Iteration 2, loss = 0.28926596\n",
      "Iteration 3, loss = 0.22271219\n",
      "Iteration 4, loss = 0.18322657\n",
      "Iteration 5, loss = 0.15788981\n",
      "Iteration 6, loss = 0.13904590\n",
      "Iteration 7, loss = 0.12535632\n",
      "Iteration 8, loss = 0.11001399\n",
      "Iteration 9, loss = 0.09927894\n",
      "Iteration 10, loss = 0.08977847\n",
      "Iteration 11, loss = 0.08098079\n",
      "Iteration 12, loss = 0.07415812\n",
      "Iteration 13, loss = 0.06806876\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63227356\n",
      "Iteration 2, loss = 0.28966529\n",
      "Iteration 3, loss = 0.22182686\n",
      "Iteration 4, loss = 0.18436314\n",
      "Iteration 5, loss = 0.15857320\n",
      "Iteration 6, loss = 0.14129089\n",
      "Iteration 7, loss = 0.12588741\n",
      "Iteration 8, loss = 0.11312336\n",
      "Iteration 9, loss = 0.10327456\n",
      "Iteration 10, loss = 0.09362154\n",
      "Iteration 11, loss = 0.08464568\n",
      "Iteration 12, loss = 0.07721882\n",
      "Iteration 13, loss = 0.07139790\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63021691\n",
      "Iteration 2, loss = 0.28770762\n",
      "Iteration 3, loss = 0.22075978\n",
      "Iteration 4, loss = 0.18403984\n",
      "Iteration 5, loss = 0.15831347\n",
      "Iteration 6, loss = 0.14016692\n",
      "Iteration 7, loss = 0.12493790\n",
      "Iteration 8, loss = 0.11064177\n",
      "Iteration 9, loss = 0.09937744\n",
      "Iteration 10, loss = 0.09255447\n",
      "Iteration 11, loss = 0.08204230\n",
      "Iteration 12, loss = 0.07671209\n",
      "Iteration 13, loss = 0.06950671\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62828497\n",
      "Iteration 2, loss = 0.28698998\n",
      "Iteration 3, loss = 0.21866742\n",
      "Iteration 4, loss = 0.18229753\n",
      "Iteration 5, loss = 0.15433454\n",
      "Iteration 6, loss = 0.13584656\n",
      "Iteration 7, loss = 0.12035432\n",
      "Iteration 8, loss = 0.10711260\n",
      "Iteration 9, loss = 0.09571660\n",
      "Iteration 10, loss = 0.08749880\n",
      "Iteration 11, loss = 0.07819609\n",
      "Iteration 12, loss = 0.07070935\n",
      "Iteration 13, loss = 0.06412679\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63970943\n",
      "Iteration 2, loss = 0.29268083\n",
      "Iteration 3, loss = 0.22228016\n",
      "Iteration 4, loss = 0.18537619\n",
      "Iteration 5, loss = 0.15838878\n",
      "Iteration 6, loss = 0.13916568\n",
      "Iteration 7, loss = 0.12232297\n",
      "Iteration 8, loss = 0.10917017\n",
      "Iteration 9, loss = 0.09804753\n",
      "Iteration 10, loss = 0.08971879\n",
      "Iteration 11, loss = 0.08276039\n",
      "Iteration 12, loss = 0.07282373\n",
      "Iteration 13, loss = 0.06689992\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33949380\n",
      "Iteration 2, loss = 0.79743440\n",
      "Iteration 3, loss = 0.74243821\n",
      "Iteration 4, loss = 0.73060444\n",
      "Iteration 5, loss = 0.74623144\n",
      "Iteration 6, loss = 0.71754748\n",
      "Iteration 7, loss = 0.71864908\n",
      "Iteration 8, loss = 0.70853090\n",
      "Iteration 9, loss = 0.68686839\n",
      "Iteration 10, loss = 0.69041486\n",
      "Iteration 11, loss = 0.70719703\n",
      "Iteration 12, loss = 0.67784090\n",
      "Iteration 13, loss = 0.70236960\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12652642\n",
      "Iteration 2, loss = 0.64794585\n",
      "Iteration 3, loss = 0.63369922\n",
      "Iteration 4, loss = 0.63510700\n",
      "Iteration 5, loss = 0.59858383\n",
      "Iteration 6, loss = 0.57213601\n",
      "Iteration 7, loss = 0.56170554\n",
      "Iteration 8, loss = 0.55528668\n",
      "Iteration 9, loss = 0.56082097\n",
      "Iteration 10, loss = 0.55386439\n",
      "Iteration 11, loss = 0.53891812\n",
      "Iteration 12, loss = 0.55149157\n",
      "Iteration 13, loss = 0.57570202\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22989255\n",
      "Iteration 2, loss = 0.68754418\n",
      "Iteration 3, loss = 0.63031013\n",
      "Iteration 4, loss = 0.63217876\n",
      "Iteration 5, loss = 0.62059015\n",
      "Iteration 6, loss = 0.63778547\n",
      "Iteration 7, loss = 0.62548984\n",
      "Iteration 8, loss = 0.63654016\n",
      "Iteration 9, loss = 0.62327133\n",
      "Iteration 10, loss = 0.65127125\n",
      "Iteration 11, loss = 0.64895821\n",
      "Iteration 12, loss = 0.65112559\n",
      "Iteration 13, loss = 0.64795706\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07901309\n",
      "Iteration 2, loss = 0.62582959\n",
      "Iteration 3, loss = 0.56919219\n",
      "Iteration 4, loss = 0.52988295\n",
      "Iteration 5, loss = 0.50017053\n",
      "Iteration 6, loss = 0.51101421\n",
      "Iteration 7, loss = 0.52835666\n",
      "Iteration 8, loss = 0.51121999\n",
      "Iteration 9, loss = 0.51671061\n",
      "Iteration 10, loss = 0.52016171\n",
      "Iteration 11, loss = 0.52768166\n",
      "Iteration 12, loss = 0.53685006\n",
      "Iteration 13, loss = 0.55060624\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12141916\n",
      "Iteration 2, loss = 0.60412672\n",
      "Iteration 3, loss = 0.56735959\n",
      "Iteration 4, loss = 0.53691113\n",
      "Iteration 5, loss = 0.53324621\n",
      "Iteration 6, loss = 0.53072703\n",
      "Iteration 7, loss = 0.51503539\n",
      "Iteration 8, loss = 0.51447328\n",
      "Iteration 9, loss = 0.54749208\n",
      "Iteration 10, loss = 0.54968252\n",
      "Iteration 11, loss = 0.54024950\n",
      "Iteration 12, loss = 0.54995883\n",
      "Iteration 13, loss = 0.55293576\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32953906\n",
      "Iteration 2, loss = 0.67995541\n",
      "Iteration 3, loss = 0.55940825\n",
      "Iteration 4, loss = 0.48542337\n",
      "Iteration 5, loss = 0.43261740\n",
      "Iteration 6, loss = 0.39309850\n",
      "Iteration 7, loss = 0.36178632\n",
      "Iteration 8, loss = 0.33606426\n",
      "Iteration 9, loss = 0.31478796\n",
      "Iteration 10, loss = 0.29590409\n",
      "Iteration 11, loss = 0.27922019\n",
      "Iteration 12, loss = 0.26522296\n",
      "Iteration 13, loss = 0.25240293\n",
      "Iteration 14, loss = 0.24134800\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31680061\n",
      "Iteration 2, loss = 0.67300805\n",
      "Iteration 3, loss = 0.55541110\n",
      "Iteration 4, loss = 0.48288961\n",
      "Iteration 5, loss = 0.43127521\n",
      "Iteration 6, loss = 0.39228248\n",
      "Iteration 7, loss = 0.36164873\n",
      "Iteration 8, loss = 0.33672575\n",
      "Iteration 9, loss = 0.31531999\n",
      "Iteration 10, loss = 0.29715457\n",
      "Iteration 11, loss = 0.28039818\n",
      "Iteration 12, loss = 0.26701921\n",
      "Iteration 13, loss = 0.25439428\n",
      "Iteration 14, loss = 0.24287908\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31600546\n",
      "Iteration 2, loss = 0.67212609\n",
      "Iteration 3, loss = 0.55397659\n",
      "Iteration 4, loss = 0.48111360\n",
      "Iteration 5, loss = 0.42979250\n",
      "Iteration 6, loss = 0.39091878\n",
      "Iteration 7, loss = 0.36070687\n",
      "Iteration 8, loss = 0.33514792\n",
      "Iteration 9, loss = 0.31387440\n",
      "Iteration 10, loss = 0.29566590\n",
      "Iteration 11, loss = 0.27879186\n",
      "Iteration 12, loss = 0.26483119\n",
      "Iteration 13, loss = 0.25203539\n",
      "Iteration 14, loss = 0.24035289\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31857905\n",
      "Iteration 2, loss = 0.66924733\n",
      "Iteration 3, loss = 0.54987597\n",
      "Iteration 4, loss = 0.47678361\n",
      "Iteration 5, loss = 0.42536938\n",
      "Iteration 6, loss = 0.38673991\n",
      "Iteration 7, loss = 0.35699698\n",
      "Iteration 8, loss = 0.33137000\n",
      "Iteration 9, loss = 0.30988314\n",
      "Iteration 10, loss = 0.29235456\n",
      "Iteration 11, loss = 0.27625021\n",
      "Iteration 12, loss = 0.26242539\n",
      "Iteration 13, loss = 0.25017611\n",
      "Iteration 14, loss = 0.23863223\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33376903\n",
      "Iteration 2, loss = 0.68266744\n",
      "Iteration 3, loss = 0.56079140\n",
      "Iteration 4, loss = 0.48607474\n",
      "Iteration 5, loss = 0.43378984\n",
      "Iteration 6, loss = 0.39408872\n",
      "Iteration 7, loss = 0.36366023\n",
      "Iteration 8, loss = 0.33815509\n",
      "Iteration 9, loss = 0.31645929\n",
      "Iteration 10, loss = 0.29831630\n",
      "Iteration 11, loss = 0.28192007\n",
      "Iteration 12, loss = 0.26777055\n",
      "Iteration 13, loss = 0.25497969\n",
      "Iteration 14, loss = 0.24314807\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63014710\n",
      "Iteration 2, loss = 0.29041776\n",
      "Iteration 3, loss = 0.22295251\n",
      "Iteration 4, loss = 0.18420120\n",
      "Iteration 5, loss = 0.15781593\n",
      "Iteration 6, loss = 0.14035384\n",
      "Iteration 7, loss = 0.13072701\n",
      "Iteration 8, loss = 0.11739204\n",
      "Iteration 9, loss = 0.10825284\n",
      "Iteration 10, loss = 0.09683925\n",
      "Iteration 11, loss = 0.08821022\n",
      "Iteration 12, loss = 0.08877250\n",
      "Iteration 13, loss = 0.08154152\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62231136\n",
      "Iteration 2, loss = 0.28287409\n",
      "Iteration 3, loss = 0.21486214\n",
      "Iteration 4, loss = 0.18210268\n",
      "Iteration 5, loss = 0.15757339\n",
      "Iteration 6, loss = 0.14607947\n",
      "Iteration 7, loss = 0.12614553\n",
      "Iteration 8, loss = 0.11334248\n",
      "Iteration 9, loss = 0.10564124\n",
      "Iteration 10, loss = 0.09720062\n",
      "Iteration 11, loss = 0.08735591\n",
      "Iteration 12, loss = 0.08019169\n",
      "Iteration 13, loss = 0.07946667\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63035359\n",
      "Iteration 2, loss = 0.29375649\n",
      "Iteration 3, loss = 0.22430494\n",
      "Iteration 4, loss = 0.18449365\n",
      "Iteration 5, loss = 0.15562978\n",
      "Iteration 6, loss = 0.14145535\n",
      "Iteration 7, loss = 0.12731781\n",
      "Iteration 8, loss = 0.11434573\n",
      "Iteration 9, loss = 0.10610803\n",
      "Iteration 10, loss = 0.10021894\n",
      "Iteration 11, loss = 0.09472857\n",
      "Iteration 12, loss = 0.08798280\n",
      "Iteration 13, loss = 0.08378259\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61891280\n",
      "Iteration 2, loss = 0.27594535\n",
      "Iteration 3, loss = 0.20800705\n",
      "Iteration 4, loss = 0.17075396\n",
      "Iteration 5, loss = 0.14650690\n",
      "Iteration 6, loss = 0.13314658\n",
      "Iteration 7, loss = 0.11966113\n",
      "Iteration 8, loss = 0.10745614\n",
      "Iteration 9, loss = 0.09504361\n",
      "Iteration 10, loss = 0.09233603\n",
      "Iteration 11, loss = 0.07888506\n",
      "Iteration 12, loss = 0.07527969\n",
      "Iteration 13, loss = 0.07862563\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63457234\n",
      "Iteration 2, loss = 0.29126954\n",
      "Iteration 3, loss = 0.21917781\n",
      "Iteration 4, loss = 0.18307302\n",
      "Iteration 5, loss = 0.15855621\n",
      "Iteration 6, loss = 0.14200924\n",
      "Iteration 7, loss = 0.12542861\n",
      "Iteration 8, loss = 0.11320219\n",
      "Iteration 9, loss = 0.10578471\n",
      "Iteration 10, loss = 0.09840696\n",
      "Iteration 11, loss = 0.09321775\n",
      "Iteration 12, loss = 0.08194489\n",
      "Iteration 13, loss = 0.07480217\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57392367\n",
      "Iteration 2, loss = 0.26492203\n",
      "Iteration 3, loss = 0.20341718\n",
      "Iteration 4, loss = 0.17182673\n",
      "Iteration 5, loss = 0.14922526\n",
      "Iteration 6, loss = 0.13269168\n",
      "Iteration 7, loss = 0.11979711\n",
      "Iteration 8, loss = 0.10900425\n",
      "Iteration 9, loss = 0.09936599\n",
      "Iteration 10, loss = 0.09118143\n",
      "Iteration 11, loss = 0.08300347\n",
      "Iteration 12, loss = 0.07589823\n",
      "Iteration 13, loss = 0.07087704\n",
      "Training loss did not improve more than tol=0.100000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(hidden_layer_sizes=(50,),\n",
       "                                     learning_rate_init=0.1, max_iter=1000,\n",
       "                                     random_state=1, tol=0.1, verbose=10),\n",
       "             param_grid={'hidden_layer_sizes': (20, 100),\n",
       "                         'learning_rate_init': (0.1, 0.01),\n",
       "                         'solver': ('lbfgs', 'sgd', 'adam')})"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameters)\n",
    "clf.fit(x_train, yy_hot_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set score: 0.985267\nTest set score: 0.957400\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % clf.score(x_train, yy_hot_full_train))\n",
    "print(\"Test set score: %f\" % clf.score(x_test, yy_hot_full_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'hidden_layer_sizes': 100, 'learning_rate_init': 0.1, 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((x_train, x_test))\n",
    "y = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[6889,    0,    2,    1,    1,    0,    3,    2,    2,    3],\n",
       "       [  24, 7840,    3,    0,    0,    0,    3,    5,    2,    0],\n",
       "       [  52,   15, 6910,    1,    0,    1,    1,    8,    2,    0],\n",
       "       [  90,    6,   28, 7005,    0,    4,    0,    3,    3,    2],\n",
       "       [  41,   10,    7,    1, 6746,    0,    2,    2,    1,   14],\n",
       "       [  97,    3,    2,   21,    2, 6176,    7,    0,    3,    2],\n",
       "       [  60,   14,    2,    0,    7,    5, 6788,    0,    0,    0],\n",
       "       [  46,   19,   22,   11,    5,    0,    0, 7187,    1,    2],\n",
       "       [ 169,   10,    9,   14,    3,    5,    6,    5, 6603,    1],\n",
       "       [ 117,    6,    2,   17,   17,    3,    0,   24,    9, 6763]],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "confusion_matrix(y, y_predict.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-fb3122474901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "c8140ce4bc0b7d2950b144ea7c31b351628765ac689ae932c908b21084263577"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}